{
  "AAVE": {
    "chunks": [
      "Protocol Whitepaper V1.0 wowaave.com January 2020 Abstract This document describes the de\ufb01nitions and theory behind the Aave Protocol explaining the di\ufb00erent aspects of the implementation. Contents Introduction Basic Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Formal De\ufb01nitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Protocol Architecture Lending Pool Core . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Lending Pool Data Provider . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Lending Pool . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Lending Pool Con\ufb01gurator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Interest Rate Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      ". . Governance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . The LendingPool Contract Deposit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Redeem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Borrow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Repay . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Swap Rate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Liquidation Call . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Flash Loans . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Tokenization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      ". . . . . . . . . . . . . . . . 3.8.1 Limitations of the tokenization model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Stable Rate Theory Lending Rate Oracle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Current Stable Borrow Rate Rs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Limitations on Stable Rate Positions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Stable Rate Rebalancing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . The Rebalancing Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Conclusion Introduction The birth of the Aave Protocol marks Aaves shift from a decentralized P2P lending strategy (direct loan relationship between lenders and borrowers, like in ETHLend) to a pool-based strategy. Lenders provide liquidity by depositing cryptocurrencies in a pool contract.",
      "Simultaneously, in the same contract, the pooled funds can be borrowed by placing a collateral. Loans do not need to be individually matched, instead they rely on the pooled funds, as well as the amounts borrowed and their collateral. This enables instant loans with characteristics based on the state of the pool. A simpli\ufb01ed scheme of the protocol is presented in \ufb01gure 1 below. Figure 1: The Aave Protocol The interest rate for both borrowers and lenders is decided algorithmically:  For borrowers, it depends on the cost of money - the amount of funds available in the pool at a speci\ufb01c time. As funds are borrowed from the pool, the amount of funds available decreases which raises the interest rate. For lenders, this interest rate corresponds to the earn rate, with the algorithm safeguarding a liquidity reserve to guarantee withdrawals at any time.",
      "Basic Concepts Figure 2: Lending Pool Basics At the heart of a lending pool is the concept of reserve: every pool holds reserves in multiple currencies, with the total amount in Ethereum de\ufb01ned as total liquidity. A reserve accepts deposits from lenders. Users can borrow these funds, granted that they lock a greater value as collateral, which backs the borrow position. Speci\ufb01c currencies in the pooled reserves can be con\ufb01gured as collateral or not for borrow positions, only low risk tokens should be considered. The amount one can borrow depends on the currencies deposited still available in the reserves. Every reserve has a speci\ufb01c Loan-To-Value (LTV), calculated as the weighted average of the di\ufb00erent LTVs of the currencies composing the collateral, where the weight for each LTV is the equivalent amount of the collateral in ETH; \ufb01gure 3 shows an example of parameters. Every borrow position can be opened with a stable or variable rate.",
      "Borrows have in\ufb01nite duration, and there is no repayment schedule: partial or full repayments can be made anytime. Figure 3: Lending Pool Parameters In case of price \ufb02uctuations, a borrow position might be liquidated. A liquidation event happens when the price of the collateral drops below the threshold, LQ, called liquidation threshold. Reaching this ratio channels a liquida- tion bonus, which incentivizes liquidators to buy the collateral at a discounted price. Every reserve has a speci\ufb01c liquidation threshold, following the same approach as for the LTV. Calculation of the average liquidation threshold La is performed dynamically, using the weighted average of the liquidation thresholds of the collaterals underlying assets.",
      "At any point in time, a borrow position is characterized by its health factor Hf, a function for the total col- lateral and the total borrows which determines if a loan is undercollateralized: Hf  T otalCollateralET HLa T otalBorrowsET HT otalF eesET H when Hf  1, a loan is considered undercollateralized and can be liquidated Further details on liquidation can be found in section 3.6. Formal De\ufb01nitions Variable Description current times- tamp Current number of seconds de\ufb01ned by block.timestamp. last updated timestamp Timestamp of the last update of the reserve data. Tl is updated every time a borrow, deposit, redeem, repay, swap or liquidation event occurs. T, delta time T  T Tl Tyear, seconds Number of seconds in a year. Tyear  31536000 Tyear, yearly riod Tyear  Tyear Lt, total liquidity Total amount of liquidity available in the reserve. The decimals of this value depend on the decimals of the currency. Bs, total stable bor- rows Total amount of liquidity borrowed at a stable rate.",
      "The decimals of this value depend on the decimals of the currency. total variable borrows Total amount of liquidity borrowed at a variable rate. The decimals of this value depend on the decimals of the currency. Bt, total borrows Total amount of liquidity borrowed. The deci- mals of this value depend on the decimals of the currency. Bt  Bs  Bv U, utilization rate Representing the utilization of the deposited funds. 0, if Lt  0 Lt , if Lt  0 Uoptimal, target uti- lization rate The utilization rate targeted by the model, beyond the variable interest rate rises sharply. Rv0, base variable borrow rate Constant for Bt  0. Expressed in ray. Rslope1, interest rate slope below Uoptimal Constant representing the scaling of the interest rate versus the utilization, when U  Uoptimal. Expressed in ray. Rslope2, interest rate slope above Uoptimal Constant representing the scaling of the interest rate versus the utilization, when U Uoptimal. Expressed in ray.",
      "Rv, variable borrow rate Rv  Rv0  Uoptimal Rslope1, if U Uoptimal Rv0  Rslope1  UUoptimal 1Uoptimal Rslope2, if U  Uoptimal Rs, stable rate Implemented in section 4.2. Expressed in ray. Mr, average market lending rate Base stable borrow rate, de\ufb01ned for i platforms with P r i the lending rate and P v i the borrowing volume. Expressed in ray. Mr  i1 P i rP i i1 P iv Variable Description sa, average stable rate borrow rate When a stable borrow of amount Bnew is issued at rate Rs: sa  BsRt1 sa BnewRs BsBnew When a user repays an amount Bx at stable rate Rsx: sa  0, if Bs Bx  0 BsRt1 sa BxRsx BsBx , if Bs Bx  0 Check the methods decreaseTotalBorrowsStableAndUpdateAverageRate() and increaseTotalBorrowsStableAndUpdateAverageRate(). Expressed in ray. RO, overall borrow rate Overall borrow rate of the reserve, calculated as the weighted average between the total variable borrows Bv and the total stable borrows Bs.",
      "RO  0, if Bt  0 BvRvBsRsa , if Bt  0 Rl, current liquidity rate Function of the overall borrow rate RO and the utilization rate U. Rl  ROU cumulated liq- uidity index Interest cumulated by the reserve during the time interval T , updated whenever a borrow, deposit, repay, redeem, swap, liquidation event occurs. i  (RlTyear  1)Ct1 i  1  1027  1 ray n, reserve normal- ized income Ongoing interest cumulated by the reserve. n  (RlTyear  1)Ct1 vc, cumulated vari- able borrow index Interest cumulated by the variable borrows Bv, at rate Rv, updated whenever a borrow, deposit, repay, redeem, swap, liquidation event occurs. vc  (1  Tyear )TxBt1 vc  1  1027  1 ray vcx, user cumu- lated variable bor- row index Variable borrow index of the speci\ufb01c user, stored when a user opens a variable borrow position. vcx  Bt user principal borrow balance Balance stored when a user opens a borrow position.",
      "In case of multiple borrows, the compounded interest is cumulated each time and it becomes the new principal borrow balance. Bxc, user com- pounded borrow balance Principal Bx plus the cumulated interests. For a variable position: Bxc  Bvcx (1  Tyear )TxBx For a stable position: Bxc  (1  Tyear )TxBx Hf, health factor when Hf  1, a loan is considered undercollater- alized and can be liquidated Hf  T otalCollateralET HLa BtT otalF eesET H Protocol Architecture The current implementation of the protocol is as follows: Figure 4: Protocol Architecture Lending Pool Core The LendingPoolCore contract is the center of the protocol, it:  holds the state of every reserve and all the assets deposited,  handles the basic logic (cumulation of the indexes, calculation of the interest rates...).",
      "Lending Pool Data Provider The LendingPoolDataProvider contract performs calculations on a higher layer of abstraction than the LendingPoolCore and provides data for the LendingPool; speci\ufb01cally:  Calculates the ETH equivalent a users balances (Borrow Balance, Collateral Balance, Liquidity Balance) to assess how much a user is allowed to borrow and the health factor. Aggregates data from the LendingPoolCore to provide high level information to the LendingPool. Calculate of the Average Loan to Value and Average Liquidation Ratio. Lending Pool The LendingPool contract uses the LendingPoolCore and LendingPoolDataProvider to interact with the reserves through the actions:  Deposit  Redeem  Borrow  Repay  Rate swap  Liquidation  Flash loan One of the advanced features implemented in the LendingPool contract is the tokenization of the lending position.",
      "When a user deposits in a speci\ufb01c reserve, he receives a corresponding amount of aTokens, tokens that map the liquidity deposited and accrue the interests of the deposited underlying assets. Atokens are minted upon deposit, their value increases until they are burned on redeem or liquidated. Whenever a user opens a borrow position, the tokens used as collateral are locked and cannot be transferred. Further details on the tokenization are in section 3.8. Lending Pool Con\ufb01gurator The LendingPoolConfigurator provides main con\ufb01guration functions for LendingPool and LendingPoolCore:  Reserve initialization  Reserve con\ufb01guration  Enabledisable borrowing on a reserve  Enabledisable the usage of a speci\ufb01c reserve as collateral. The LendingPoolConfigurator contract will be integrated in Aave Protocol governance. Interest Rate Strategy The InterestRateStrategy contract holds the information needed to update the interest rates of a speci\ufb01c reserve and implements the update of the interest rates.",
      "Every reserve has a speci\ufb01c InterestRateStrategy contract. Speci\ufb01cally, within the base strategy contract DefaultReserveInterestRateStrategy the following are de\ufb01ned:  Base variable borrow rate Rv0  Interest rate slope below optimal utilisation Rslope1  Interest rate slope beyond optimal utilisation Rslope2 The current variable borrow rate is: Rv  Rv0  Uoptimal Rslope1, if U Uoptimal Rv0  Rslope1  UUoptimal 1Uoptimal Rslope2, if U  Uoptimal This interest rate model allows for calibration of key interest rates:  At U  0, Rv  Rv0  At U  Uoptimal, Rv  Rv0  Rslope1  Above Uoptimal, the interest rate rises sharply to take into account the cost of capital. The stable borrow rate follows the same model described in section 4.2. Governance The rights of the protocol are controlled by the LEND token. Initially, the Aave Protocol will be launched with a decentralized on-chain governance based on the DAOStack framework which will evolve to a fully autonomous protocol.",
      "On-chain implies all votes are binding: actions that follow a vote are hard-coded and must be executed. To understand the scope of the governance its important to make the distinction:  The Aave Protocol is bound to evolve and will allow the creation of multiple lending pools with segregated liquidity, parameters, permissions, and type of assets. The Aave Lending Pool is the \ufb01rst pool of the Aave protocol until the Pool Factory Update is released and anyone can create their own pool. Within the Aave Protocol, the governance will take place at two level : 1. The Protocols Governance voting is weighted by LEND for decisions related to protocol parameters and upgrades of the smart contract. It can be compared to MakerDAOs governance where stakeholders vote on current and future parameters of the protocol. 2. The Pools Governance where your vote is weighted based on your share of pool liquidity expressed in aTokens.",
      "The votes cover pool speci\ufb01c parameters such as assets used as collateral or to be borrowed. Each Pool will have its own governance, under the umbrella of the Protocols Governance. More details on the Governance will be published in a Governance Proposal to the community. The LendingPool Contract The actions implemented within LendingPool allow users to interact with the reserve. All the actions follow this speci\ufb01c sequence: Figure 5: The LendingPool Contract Deposit The deposit action is the simplest one and does not have any particular state check. The sequence of action is: Figure 6: Deposit funds Redeem The redeem action allows users to exchange an amount of aTokens for the underlying asset. The actual amount to redeem is calculated using the aTokenunderlying exchange rate Ei in section 3.8. The action is de\ufb01ned as follows: Figure 7: Redeem funds Borrow The borrow action transfers to the user a speci\ufb01c amount of underlying asset, in exchange of a collateral that remains locked.",
      "The \ufb02ow of action can be described as follows: Figure 8: Borrow funds Repay The repay action allows the user to repay completely or partially the borrowed amount plus the origination fee and the accrued interest. Figure 9: Repay a loan Swap Rate The swap rate action allows a user with a borrow in progress to swap between variable and stable borrow rate. Figure 10: Swap Rate Liquidation Call The liquidationcall contract allows any external actor to purchase part of a collateral at a discounted price. In case of a liquidation event, a maximum of 50 of the loan can be liquidated, which will bring the health factor back above 1. Figure 11: Liquidation Flash Loans The \ufb02ash loan action will allow users to borrow from the reserves within a single transaction, as long as the user returns more liquidity that has been taken. Figure 12: Flash Loan Flash loans temporarily transfer the funds to a smart contract that respects the IFlashLoanEnabledContract.sol interface.",
      "The address of the contract is a parameter of the action. After the funds are transferred, the method executeOperation() is executed on the external contract. The contract can do whatever action is needed with the borrowed funds. After the method executeOperation() is completed, a check is performed to verify that the funds plus fee have been returned to the LendingPool contract. The fee is then accrued to the reserve, and the state of the reserve is updated. If less funds than what was borrowed have been returned to the reserve, the transaction is reverted. Tokenization The Aave protocol implements a tokenization strategy for liquidity providers. Upon deposit, the depositor receives a corresponding amount of derivative tokens, called Aave Tokens (aTokens for short) that map 1:1 the underlying assets. The balance of aTokens of every depositor grows over time, driven by the perpetual accrual of interest of deposits. aTokens are fully ERC20 compliant.",
      "aTokens also natively implement the concept of interest rate redirection. Indeed, the value accrued over time by the borrowers interest rate payments is distinct from the principal value. Once there is a balance of aTokens, the accrued value can be redirected to any address, e\ufb00ectively splitting the balance and the generated interest. We call the continuous \ufb02ow of accumulated interest over time the interest stream. To implement this tokenization strategy, Aave introduced the following concepts in the aToken contract: 1. User x balance index It x: Is the value of the reserve normalized income It x at the moment of execution of the last action by the user. 2. Principal balance Bp: Is the balance stored in the balances mapping of the ERC20 aToken contract. The principal balance gets updated on every action that the user executes on the aToken contract (deposit, redeem, transfer, liquidation, interest rate redirection) 3.",
      "Redirection address Ar: When a user decides to redirect his interest stream to another address, a new redirection address Ar is provided. If no redirection of the interest stream is performed, Ar is 0 4. Redirected Balance Bx r : Whenever a user redirects his interest stream, the balance of user redirecting is added to the redirected balance Br of the address speci\ufb01ed by Br. De\ufb01ned as follows: r  P X Bp Where X is the set of users redirecting the interest stream to the user x The redirected balance decreases whenever a user x0 X redeems or transfers his aTokens to another user that is not redirecting to x. 5. Current balance Bc: Is the balance returned by the balanceOf() function of the aToken contract.",
      "De\ufb01ned as follows: 0, if Bx p  0 and Bx r  0 p  Bx r ( In Ix 1), if Ar  0 Ix  Bx r ( In Ix 1), if Ar  0 3.8.1 Limitations of the tokenization model The described tokenization model has many advantages compared to the widely used, exchange rate based approach, but also some drawbacks, speci\ufb01cally: 1. Its impossible to transfer the whole balance at once: Given the perpetual accrual of the interest rate, there is no way to specify the exact amount to transfer, since the interest will keep accruing even while the transfer transaction is being con\ufb01rmed. This means that having exactly 0 balance after a transfer is impossible, rather, a very small balance (dust balance) will be left to the from account executing the transfer. Note that this could have been avoided by adding speci\ufb01c logic to handle this particular edge case, but this would have meant adding a non standard behavior to the ERC20 transfer function, and for this reason we avoided it.",
      "Even though this is not a relevant issue, its important to note that is possible to completely clear the remaining balance by either 1. execute another transfer, which will most likely transfer the remaining dust balance as it would be too small to accrue interest in a reasonably short amount of time, or 2. redeem the dust balance and transfer the underlying asset. 2. Interest stream can only be redirected if there is a principal balance: This means that only accounts that have a principal balance Bp can redirect their interest. If users redeem or transfer everything, their interest redirection is reset. As a side e\ufb00ect of this, interest generated only by the redirected balance Br cannot be redirected. Stable Rate Theory The following chapter explains how the stable rates are applied to the system and the limitations. Implementation of a \ufb01xed rate model on top of a pool is complicated.",
      "Indeed, \ufb01xed rates are hard to handle algorithmically, as the cost of borrowing money varies with market conditions and the liquidity available. There might therefore be situations (sudden market changes, bank runs ...) in which handling stable rate borrow positions would need using speci\ufb01c heuristics based on time or economical constraints. Following this reasoning, we identi\ufb01ed two possible ways of handling \ufb01xed rates: 1. Imposing time constraints: \ufb01xed rates might work perfectly \ufb01ne in a time constrained fashion. If a loan has a stable duration, it should survive extreme market conditions, as the borrower must repay at the end of the loan period. Unfortunately, time constrained \ufb01xed rate loans arent suitable for our speci\ufb01c use case of open ended loan. It would require a certain degree of UX friction where users would need to create and handle multiple loans with di\ufb00erent times constraints. 2.",
      "Imposing rates constraints: An interest rate calculated at the beginning of a loan might be impacted by market conditions, keeping it from staying \ufb01xed. If the rate diverges too much from the market, it can be readjusted. This would not be a pure \ufb01xed rate, open term loan - as the rate might vary throughout the loan duration  yet users will experience actual \ufb01xed rates during speci\ufb01c time periods, or when there is enough liquidity available. This particular implementation has been chosen to be integrated into Aaves Protocol under the name stable rate. Lending Rate Oracle Figure 13: Lending Rate Oracle The \ufb01rst component to be integrated into the Protocol protocol is a Lending Rate Oracle, which will provide information to the contracts on the actual market rates that other lending platforms, both centralized and decentralized, are providing.",
      "The average market lending rate Mr is de\ufb01ned for i platforms with P i r the lending rate and P i v the borrowing volume: Mr  i1 P i rP i i1 P iv The market rate will be updated daily, initially by Aave. Current Stable Borrow Rate Rs The current stable borrow rate is calculated as follows: Mr  Uoptimal Rslope1, if U Uoptimal Mr  Rslope1  UUoptimal 1Uoptimal Rslope2, if U  Uoptimal With: - Mr the average market lending rate. - Rslope1 the interest rate slope below Uoptimal, increases the rate as U increases. - Rslope2 the interest rate slope beyond Uoptimal, increases as the di\ufb00erence between U and Uoptimal increases. - U is the utilization rate. Note: Rs does NOT impact existing stable rates positions  this is applied only to new opened positions. Limitations on Stable Rate Positions To avoid abuses on stable rate loans, the following limitations have been applied to the stable rate borrowing model: 1. Users cannot deposit as collateral more liquidity than what they are trying to borrow.",
      "Eg. a user deposits 10 million DAI collateral, tries to borrow 1 million DAI. This is to prevent the following attack vector: Given: Bs  18APR, Mr  9APR, Rl  12APR Users might try to arti\ufb01cially lower Bs to the value of Mr by depositing a huge amount of liquidity which would cause Bs to drop, then borrow from the same liquidity at a lower rate, withdraw the liquidity previously deposited to cause Bs and the liquidity rate Rl to raise again; then \ufb01nally deposit the amount borrowed to earn interest on the previously borrowed funds. Although this attack can still be carried out using multiple accounts, this particular constraint makes the attack more complicated as it requires more money (and a di\ufb00erent collateral currency). This works well in combination with the interest rate rebalancing in the next section. 2. Borrowers will only be able to borrow up to Tr of the available liquidity at the current borrow rate.",
      "So, for every speci\ufb01c value of Bs, there is only up to Tr of liquidity available for a single borrower. This is to avoid that a speci\ufb01c borrower would borrow too much available liquidity at a too competitive rate. Stable Rate Rebalancing The last and perhaps most important constraint of the stable rate model is the rate rebalancing. This is to work around changes in market conditions or increased cost of money within the pool. The stable rate rebalancing will happen in two speci\ufb01c situations: 1. Rebalancing up. The stable rate of a user x is rebalanced to the most recent value of Bs when a user could earn interest by borrowing: s  Rl with Bx s the stable borrow rate of user x 2. Rebalancing down. The stable rate of a user x is rebalanced to the most recent value of Bs, if: s  Bs(1  Bs) with Bs a rate delta established by governance which de\ufb01nes the window above Bs to rebalance interest rates. If a user pays too much interest beyond that range, the rate is balanced down.",
      "The Rebalancing Process The LendingPool contract exposes a function rebalanceStableBorrowRate(address reserve, address user) which allows to rebalance the stable rate interest of a speci\ufb01c user. Anybody can call this function: however, there isnt any direct incentive for the caller to rebalance the rate of a speci\ufb01c user. For this reason, Aave will provide an agent that will periodically monitor all the stable rates positions and rebalance the ones that will be deemed necessary. The rebalance strategy will be decided o\ufb00chain by the agent, this means that users that satisfy the rebalance conditions may not be rebalanced immediately. Since those conditions depend on the liquidity avail- able and the state of market, there might be some transitory situations in which an immediate rebalance is not needed. This does not add any element of centralization to the protocol. Even if the agent stops working, anybody can call the rebalance function of the LendingPool contract.",
      "Although there isnt any direct incentive in doing it (why should I do it?) there is an indirect incentive for the ecosystem. In fact, even if the agent should cease to exist, depositors might still want to trigger a rebalance up of the lowest borrow rate positions, to increase the liquidity rate andor force borrowers to close up their positions, increasing the available liquidity. In case of a rescale down, instead, borrowers have a direct incentive in performing a rebalance of their positions to lower the interest rate. The following \ufb02owchart explains the sequence of actions of the function rebalanceStableBorrowRate(). The compounded balance that is accumulated until the instant at which the rebalance happens, is not a\ufb00ected by the rebalance. Figure 14: Rebalancing Conclusion The Aave Protocol relies on a lending pool model to o\ufb00er high liquidity. Loans are backed by collateral and represented by aTokens, derivative tokens which accrue the interests.",
      "The parameters such as interest rate and Loan-To-Value are token speci\ufb01c. Aave improves Decentralized Finances current o\ufb00ering, bringing two key innovations to the lending ecosystem:  Stable Rates to help borrowers \ufb01nancial planning;  Flash Loans to borrow without collateral during a single transaction. Following the launch of the mainnet, Aave will uphold its commitment to decentralization through additional features. The Pool Factory will allow anyone to launch their own lending pool based on our smart-contracts. Governance will be on-chain with rights represented by:  The LEND token at Protocol level for updates of the smart contract;  aTokens at Pool level for pool speci\ufb01c parameters."
    ],
    "word_count": 5126,
    "page_count": 23
  },
  "ADA": {
    "chunks": [
      "Reward Sharing Schemes for Stake Pools Lars Br\u00fcnjes Aggelos Kiayias Elias Koutsoupias Aikaterini-Panagiota Stouka Saturday 6th June, 2020 Abstract We introduce and study reward sharing schemes (RSS) that promote the fair formation of stake pools in collaborative projects that involve a large number of stakeholders such as the maintenance of a proof-of-stake (PoS) blockchain. Our mechanisms are parameterized by a target value for the desired number of pools. We show that by properly incentivizing participants, the desired number of stake pools is a Nash equilibrium arising from rational play. Our equilibria also exhibit an ef\ufb01ciency  security tradeoff via a parameter that calibrates between including pools with the smallest cost and providing protection against Sybil attacks, the setting where a single stakeholder creates a large number of pools in the hopes to dominate the collaborative project.",
      "We then describe how RSS can be deployed in the PoS setting, mitigating a number of potential deployment attacks and protocol deviations that include censoring transactions, performing Sybil attacks with the objective to control the majority of stake, lying about the actual cost and others. Finally, we experimentally demonstrate fast convergence to equilibria in dynamic environments where players react to each others strategic moves over an inde\ufb01nite period of interactive play. We also show how simple reward sharing schemes that are seemingly more fair, perhaps counterintuitively, converge to centralized equilibria. Introduction One of the main open questions in blockchain systems research is developing reward mechanisms that incentivize honest protocol execution and decentralization.",
      "Bitcoin, the dominant example of proof-of-work blockchains, has been criticized for its susceptibility to protocol deviation attacks (e.g., sel\ufb01sh-mining 16 and mining games 24), its tendency to centralise via the creation of mining pools 1, 43, 28, 21, and its high-energy expenditure. To address mainly the latter problem, many proof-of- stake (PoS) 30, 25, 13, 6 blockchains have been proposed. Despite progress in the understanding of the security properties of PoS blockchains, designing a robust incentive mechanism that promotes decentralization remains open. We can abstract the problem that is to be solved as follows. Consider a society of agents that have stake in a joint effort that is recorded in a ledger and want to run a collaborative project (which might be maintaining the ledger itself).",
      "Stakeholders actively engaged in the project will incur operational costs (potentially different across the stakeholder population) and hence the project should provide some rewards to offset these costs. The stakeholders have the option to actively participate in maintenance or abstain from it. We will assume that the project can draw funds from a reward pool enabling, potentially at regular intervals, to distribute in some way a reward R to the stakeholders. In the PoS setting, the reward pool can be facilitated either via the creation of new cryptocurrency, the collection of transaction fees, or a combination thereof. A viable solution would thus be in the form of a reward sharing scheme which will take as input the current snapshot of the collaborative project and distribute the rewards R to all stakeholders.",
      "The aim is that, after potentially multiple iterations of reward sharing, IOHK, lars.bruenjesiohk.io University of Edinburgh  IOHK, Aggelos.Kiayias,A.stoukaed.ac.uk University of Oxford, elias.koutsoupiascs.ox.ac.uk there are still agents, who incentivized by the rewards, are engaged in maintenance (for if not, the project should be considered dead). Beyond being viable, a solution also needs to possess additional desirable characteristics, e.g., it is decentralised in the sense that a suf\ufb01cient number of distinct stakeholders are active in the project. There are three dominant approaches that have been considered in the PoS context. In the direct democracy approach, every stakeholder participates proportionally to their stake, which has downside that the operational costs can be so high that they discourage participation from small stakeholders resulting in so-called whales completely dominating the system or, in the worst-case, having operations stopping altogether.",
      "In the jury approach, followed by PoS systems like 30, 12, a random subset of k stakeholders is elected at various intervals to carry out the task, which has the downside that either the jury tenure is short and most of the nodes need to be either constantly operationally ready without necessarily doing anything, or the jury tenure is long (or predictable way ahead of time) and then the risk of someone subverting the project by paying the elected nodes with small stake is high. Finally, in the representative democracy approach, broadly followed by 25, 11, 27, the stakeholders can empower other stakeholders to represent them in project maintenance and subsequently share the rewards. Given that empowering is performed via stake as recorded in the ledger, representatives can be thought to form stake pools in analogy to the mining pools of Bitcoin. The focus of this work is to develop reward mechanisms and analyze them game theoretically for this third approach. Our Results.",
      "In our setting there are n agents or players with stakes s  (s1,...,sn) and a private vector of costs for running a stake pool c  (c1,...,cn) for each one of players, should any of them choose to do so. Note that without loss of generality we assume si,ci (0,1) for all i and ! i si 1. The stake is publicly recorded in some way but without necessarily identifying how much stake belongs to each player, the player identities, or even their number n. The cost stems from the inherent task of maintenance the players are supposed to perform if they setup a pool; in the PoS setting which is our primary focus that would be the cost of setting up a server that receives, organizes and veri\ufb01es transactions to be recorded in the ledger. Each player mainly decides whether to participate directly or delegate its stake to another stakeholder to act on their behalf  or even split its stake into multiple such activities (see below about Sybil behavior).",
      "Delegation creates pools of stakeholders, where each pool consists of its leader who participates directly and its members that delegate their stake to the pool. The game is determined by the reward scheme that determines the way by which the total reward R is distributed to the pools and how individual pool rewards are distributed to the pool members. Looking ahead, we will focus on the class of reward schemes that allocate reward r(\u03c3,\u03bb) to a pool of total stake \u03c3 and allocated pool leader stake \u03bb; we call r the reward function. The other component of a reward scheme determines how the pool reward r(\u03c3,\u03bb) is distributed to the pool leader and pool members. It makes sense that the reward for the pool leader is different from the reward for pool members to compensate the pool leader for the cost it incurs by contributing to the collaborative project as well as to incentivize them to take the initiative to form a pool.",
      "We focus on reward schemes that distribute the pool reward as follows: the pool leader gets an amount to cover its cost of running the project as well as a fraction m j of the remaining amount which we call its (pro\ufb01t) margin. The remaining amount is distributed among the pool members, including the pool leader, proportionally to the stake that they contributed to the pool. In our analysis we will take advantage of automatic enforcement of our reward scheme, as e.g., this can be guaranteed by a smart contract built-in the underlying ledger. Given a reward sharing scheme that belongs to the above class, the players will pick their strategy that determines whether they will run a pool or not and whether they will allocate some or all of their stake to pools created by other players. Natural questions about these games are: Do they have pure equilibria? Do they possess desirable properties such as decentralisation? Do the best-response dynamics converge fast to them?",
      "An important and interesting observation here is that the standard notion of utility and Nash equilibrium for this game fails to capture what we intuitively expect to happen. The reason is that at a Nash equilibrium the players do not have to take into account the impact their selection will make on the moves of the other players. In particular, all Nash equilibria (if they exist) will have margins m j  1 for a simple reason: once the other players select their strategies and in particular the allocation of their stake, the best response of a pool leader is to increase its margin as much as possible. Similar situations occur in other games, such as the Cournot competition 19. The appropriate framework for such games is to consider non-myopic utilities, i.e., consider equilibria in a setting where utility is de\ufb01ned in a non-myopic fashion, accounting for the effects that a certain move of a player will incur anticipating a strategic response by the other players.",
      "Our main result is the introduction and analysis of a novel reward sharing scheme that is pa- rameterized by (1) the desired number of pools k, and (2) a Sybil resilience parameter \u03b1. The two parameters can be selected to \ufb01ne-tune two desirable properties of the resulting con\ufb01guration. The primary property is decentralisation and fairness, which is captured by the creation of k pools of roughly the same size 1k. The secondary property we are interested in is Sybil resilience, which is captured by being able to in\ufb02uence the equilibrium con\ufb01guration so that it takes the parties stake into account. Our mechanism is described in the following de\ufb01nition. De\ufb01nition 1 (A Sybil-resilient cap-and-margin reward scheme). Given a target number of pools k N, and a Sybil resilience parameter \u03b1 0,), the reward function r(\u03c3,\u03bb) of a pool with total stake \u03c3, out of which \u03bb stake belongs to the pool leader, is proportional to \u03c3 \u03b1\u03bb, i.e., r(\u03c3,\u03bb) \u03c3 \u03b1\u03bb, where \u03c3  min\u03c3,\u03b2, \u03b2  1k, and \u03b1  \u03b1 \u03c3\u03bb(1\u03c3\u03b2) .",
      "The proportionality factor is selected so that the sum of rewards does not exceed the available funds. If the primary aim of the reward scheme, i.e., to have pools of size \u03c3  \u03b2  1k, is achieved, then \u03b1  \u03b1 and the expression in the reward function simpli\ufb01es to r(\u03c3,\u03bb)  \u03c3\u03b1\u03bb, that is, a linear combination of the pool stake and the stake of the pool leader. The expression in r(\u03c3,\u03bb) for pool size \u03c3  \u03b2 has been selected to get a Nash equilibrium with the desired properties. Note also that when a pool has stake \u03c3 \u03b2, the additional stake above \u03b2 is essentially ignored. We will call such a pool saturated. Our main theorem about this reward sharing scheme is the following. Theorem 1 (Informal statement).",
      "There exists a Nash equilibrium for the reward scheme of De\ufb01nition 1 that satis\ufb01es  exactly k pools are created, each of size equal to 1k,  the pool leaders are the players with the highest value of P(si,ci)  r(\u03b2,si) 1\u03b1 ci, where si and ci are the stake and cost of player i, and R is the total reward distributed to the players, and  the players have no incentive to lie about their cost ci. The quantity P(si,ci) in (2) is the potential pro\ufb01t of stakeholder i when this player creates a pool using their whole stake si and the pool attracts total stake \u03b2. It follows immediately from the above theorem that we obtain an equilibrium that achieves the primary decentralization and fairness objective. Regarding Sybil resilience, observe that the potential of the players is controlled by the parameter \u03b1.",
      "When \u03b1  0, the pool leaders are the players with the smallest cost (resulting in the most cost-effective equilibrium) while as \u03b1 grows, the stake backing up the pools starts to become more and more relevant in the equilibrium con\ufb01guration, with the extreme case when \u03b1 and the costs of all players are roughly equal when the stakepools will be managed by backing up each pool with the largest amount of stake possible. We illustrate how we can facilitate Sybil resilience by calibrating the \u03b1 parameter in the sense that any Sybil behaving player at the equilibrium has to invest resources linear in the number of identities (i.e., stake-pools in our setting) that they register, arguably the best one can hope for in the anonymous setting we operate. We note that although the above reward function may at \ufb01rst appear rather complicated, there is a strong justi\ufb01cation behind it (cf. Section 4). Non-myopic utility and dynamics.",
      "We also tackle the question of whether the equilibrium guaranteed by our theoretical analysis is effectively reachable when players are engaged in the game. We consider non-myopic dynamics with players applying a natural best-responce strategy to each others moves in succession. Speci\ufb01cally, the players compute the desirability of each announced pool, which is the answer to the following question of the players: if I allocate a small stake x to pool j, how much do I expect to gain?. In other words, the desirability is the marginal reward of pool j provided that it will become a successful pool and obtain stake \u03b2. A non-myopic player then assumes that each of the k most desirable pools will increase in size to become saturated and the remaining pools will end up with the stake of their pool leader, and allocates its stake accordingly.",
      "The player is non-myopic as they judge pools by their potential to issue pro\ufb01ts, not their current membership size which potentially might be quite small especially at the beginning of the game. For pool leaders the situation is similar, but they have also to compute their margin. To do so, they calculate the maximum possible margin that still allows them to be one of the k most desirable pools. The question then is whether these dynamics converge? how fast? and to which equilibrium? We provide experimental evidence that under reasonable assumptions of the stake distribution (for example, Pareto distribution) and of the cost distribution (for example, uniform distribution in an interval), the dynamics converge quickly to our Nash equilibrium that has k saturated pools and the characteristic that all pools are formed by the players that are ranked best according to their potential pro\ufb01t as predicted by the theoretical analysis. Equilibria and incentive compatibility.",
      "Our reward sharing scheme has a Nash equilibrium in which the reward is distributed fairly among all stakeholders, except for pool leaders that get an additional gain (Proposition 2). A nice property of this additional gain is that, all else being equal, it increases by at most \u03b4x whenever the pool leaders cost decreases by \u03b4x. This means that our reward sharing scheme is incentive compatible: no player will bene\ufb01t by lying about its cost. Deployment considerations in the PoS setting. We provide a comprehensive list of potential attacks and deviations as well as how they are mitigated in a deployment of our RSS in the setting of a PoS protocol such as 25. These include rich get richer considerations censorship1 and Sybil attacks, as well as how to deal with underperforming pool leaders that fail to meet their obligations in terms of maintaining the service. Related work.",
      "A number of previous works considered the incentives of mining pools in the setting of PoW-based cryptocurrencies (as opposed to PoS-based ones) such as Bitcoin 37, 38, 15, 41. The main differences between mining pools in Bitcoin and stake pools in our setting are that (i) in Bitcoin all pool members perform mining and hence incur costs, while in PoS setting, only the pool leader runs the underlying protocol and incurs a cost while delegators have no cost, (ii) in Bitcoin each pool leader can choose a different way to reward pool membersminers while in our setting we prescribe a speci\ufb01c way for rewards to be shared between pool members. Regarding centralization, Arnosti and Weinberg, 1, have established that some level of centralisation takes place in Bitcoin in settings where differences in electricity costs are present between the miners.",
      "Also according to 28 in a setting where each unit of resource has a different value depending on the distribution of the resources among the players, miners have incentives to create coalitions. These results are inline with our (even more centralised) negative result on fair RSSs for the PoS setting, cf. Section 2.2. Another aspect we do not explore here, is the instability of such protocols when the rewards come mostly from transaction fees; this was explored in 7, 43. 1A censorship attack happens when the current pool leaders block new pool registrations. With respect to PoS blockchain systems, a different and notable approach to stake pools is to use the stake as voting power to elect a number of representatives, all of equal power, as in delegated PoS (DPoS) 27; for example, the cryptocurrency EOS 22 has 21 representatives (called block producers).",
      "This type of scheme differs from ours in that (i) the incentives of voters are not taken into account thus issues of low voter participation are not addressed, (ii) elected representatives, despite getting equal power, are rewarded according to votes received; this inconsistency between representation and power may result in a relatively small fraction of stake controlling the system (e.g., at some point, controlling EOS delegates representing just 2.2 of stakeholders was suf\ufb01cient to halt the system,5 which ideally could withstand a ratio less than 13), (iii) it may leave a large fraction of stakeholders without representation (e.g., in EOS, at some point, only 8 of total stake is represented by the 21 leading delegates2). Yet another alternative to stake pools is that of Casper 6, where players can propose themselves as validators committing some of their stake as collateral. The committed stake can be slashed in case of a proven protocol deviation.",
      "This type of scheme differs from ours in that (i) stakeholders wishing to abstain from protocol maintenance operations have no prescribed way of contributing to the mechanism (as in the case of voting in DPoS or joining a stake pool in our setting), (ii) a small fraction of stake may end up controlling the system while at the same time leaving a lot of stake decoupled from the protocol operation; this is because substantial barriers may be imposed in becoming a validator (e.g., in the EIP proposal for Casper3 it is suggested that 1500 ETH will be the minimum deposit, which, at the time of writing is more than 370K ); this can make it infeasible for many parties to engage directly; on the other hand reducing this threshold drastically may make the entry barrier too low and hence still allow a small amount of stake to control the system.",
      "As a separate point, it is worth noting that for both the above approaches there is no known game theoretic analysis that establishes a similar result to the one presented herein, i.e., that the mechanism can provably lead to a Nash equilibrium with desirable decentralisation characteristics that include a high number of protocol actors and Sybil attack resilience. The compounding of wealth in PoS cryptocurrencies was studied in 17 where a new notion denoted by equitability\" is introduced to measure how much players can increase their initial fraction of stake. Also they prove that a geometric\" reward function is the best choice for optimizing equitability under certain assumptions; we remark that it is a folklore belief that PoS systems are inherently less equitable than ones based on PoW, however this belief seems to be unfounded, cf. 23.",
      "With respect to equitability we show that by calibrating our Sybil resilience parameter to be small our system becomes equitable in the sense of providing similar rewards to stake pool leaders independently of their wealth. From a game-theoretic perspective, our setting has certain similarities to cooperative game theory in which coalitions of players have a value. In our setting the players have weights (stake) and they are allowed to split it into various coalitions (pools). Our objective is to have a given number of equal-weight coalitions, which contrasts with the typical question in cooperative game theory on how the values of the coalitions are distributed (e.g., core or Shapley value) in such a way that the grand coalition is stable 33. Actually, the games that we study are variants of congestion games with rewards on a network of parallel links, one for every potential pool.",
      "The reward on each link is determined by the reward function, which essentially determines an atomic splittable congestion game. But unlike simple atomic splittable congestion games 31, our games have different reward for pool leaders and for pool members. There are two main research directions for such games: whether they have unique equilibria and how to ef\ufb01ciently compute them 4. Regarding the question of unique inner equilibria the most relevant paper to our inner game is 32 (but see also 36, 3) which shows that under general continuity and convexity assumptions, games on parallel links have unique equilibria. However, the conditions on convexity do not meet our design objectives and they do not seem to be useful in our setting.",
      "Our work is related to two aspects of delegation games, which are games that address the bene\ufb01ts and other strategic considerations for players delegating to someone else to play a game on their behalf, 2Statistics extracted from http:eos.dapptools.infoblock-producers on July 27th, 2018. 3See https:eips.ethereum.orgEIPSeip-1011. such as owners of \ufb01rms hiring CEOs to run a company. The \ufb01rst aspect is somewhat super\ufb01cially related to this work in pool formation the pool members delegate their power to pool leaders. The second aspect which is much more relevant to our approach is that delegation changes the utility of the players (for example, by considering credible threats 39, 40) or creates a two-stage game 44, 18, 42. A typical two-stage delegation game is non-myopic Cournot competition 19 in which in the outer game the \ufb01rms (players) decide whether to be pro\ufb01t-maximizers or revenue-maximizers, while in the inner game they play a simple Cournot competition 29.",
      "Unlike our case, the inner Cournot competition has a simple unique equilibrium which de\ufb01nes a simple two-stage game. Another research area that is relevant to this work is mechanism design, because participants may have an incentive not to reveal their true parameters, e.g., the cost for running a pool 31, 45. In the proof of work setting, 20 considers reward sharing rules for proof-of-work systems under the assumption of discounted expected utility and identi\ufb01es schemes that achieve fairness. Further- more, an axiomatic approach to reward schemes of proof-of-work systems is taken in 9 in order to study fairness, symmetry, budget balancing and other properties. Unlike our work that considers incentives for pool formation with desirable properties, these two papers study intrinsic properties of the system given an existing pool formation. Finally, after the \ufb01rst version of the present paper was made public (on the arXiv repository, cf.",
      "5), another work, 26, studied a parameterized notion of decentralization, where, in an ideal system, all participants should exert the same power in running the system, independently of their stake. This is a signi\ufb01cantly more demanding notion of decentralization than the one considered here, where in an ideal system, participants exert power proportional to their stake. It is argued in 26 that in order for a system to achieve full decentralization, there must exist a strictly positive Sybil cost, that is, the cost of running two or more nodes should be higher when the nodes belong to the same entity than to multiple entities. Clearly in systems with anonymous users, Sybil costs cannot be positive and such concept of decentralization is impossible. The construction we present has also been implemented and deployed on the Cardano incen- tivised testnet4 in tandem with the Ouroboros protocol 25 with an outcome in line to our theoretical and experimental analysis.",
      "The parameters selected for the testnet were k  100,\u03b1  0. At the time of this writing, more than 900 pools have been created with the \ufb01rst 100 pools controlling approximately 70 of the stake. Reward Sharing Schemes Model and De\ufb01nitions There are n stakeholders (aka players) with stakes s  (s1,...,sn) such that !n i1 si  1 and costs c  (c1,...,cn) (all assumed non-zero real values). The value si represents the i-th players stake in the collaborative project (which is e.g., maintaining the blockchain), while the value ci represents the i-th players cost, should he decide to be active in the projects maintenance. The players want to engage in the collaborative project and each player decides whether to participate directly by activating its pool or delegate his stake to other stakeholders.",
      "The total stake that is delegated to an active stakeholder j (note that the sum of all players stakes is 1 so with the term stake we mean relative stake) forms a stakepool; we will call such a pool \u03c0j, indexed by its pool leader j, and we will denote by \u03c3j the total stake delegated to this pool by all players, including the pool-leader j. We will use ai,j to denote the stake that player i allocates to pool \u03c0j. The pools participate in the collaborative project through their leaders and this participation incurs cost c j for pool leader j. This cost is \ufb01xed for each player and does not depend on the size of the pool. To incentivize the stakeholders and pool leaders to form pools and work for the collaborative project, we introduce a reward scheme. We assume that there is a \ufb01xed reward R to be distributed among all pools. A reward scheme determines the way by which 4See, https:staking.cardano.org Notation  n N, number of players. k N,k  n, the desired number of pools. R R, total reward.",
      "si (0,1), stake of player i. It holds !n i1 si  1. ci (0,R), cost of player i to form a pool \u03c0i. mi 0,1, margin of pool \u03c0i. \u03bbi (0,1), stake that player i will commit if he activates his own pool \u03c0i. ai  (ai,1,...ai,n), allocation of players i stake. ! j ai,j si . \u03c3j, stake of pool \u03c0j: \u03c3j  !n i1 ai,j. We denote the vector of pool stakes by\u03c3  (\u03c31,...,\u03c3n). Pools can have zero stake. r(\u03c3,\u03bb), reward of a pool with total stake \u03c3 and allocated pool leader stake \u03bb. It holds j r(\u03c3j,a j,j) R. Potential pro\ufb01t of a saturated pool with allocated pool leader stake \u03bb and cost c, P(\u03bb,c)  r(\u03b2,\u03bb)c. We order the players according to P(si,ci). Player i is the player with the ith highest P(si,ci). S(m,\u03bb)  (ai)n i1, joint strategy regarding allocation given (m,\u03bb). ai,i 0,\u03bbi and S(m,\u03bb)  ai. \u03b1 0,): parameter that can be adapted to trade between ef\ufb01ciency and Sybil resilience. Note that the total rewards R and \u03b1 should be selected such as it holds also P(sk1,ck1)  0.",
      "u j,i(S(m,\u03bb)), (myopic) utility that player j gets from pool \u03c0i. u j,i  r(\u03c3i,ai,i) ci a j,i \u03c3i (r(\u03c3i,ai,i)ci)(1mi) otherwise  ui,i(S(m,\u03bb)), (myopic) utility that player i gets from their own pool \u03c0i. ui,i  r(\u03c3i,ai,i)ci r(\u03c3i,ai,i) ci (mi (1mi) ai,i \u03c3i )(r(\u03c3i,ai,i)ci) otherwise  u j(S(m,\u03bb)), total (myopic) utility of player j: u j(S(m,\u03bb))  !n i1 u j,i(S(m,\u03bb)). Non-myopic utility is de\ufb01ned in the same way as myopic utility but by using non-myopic stake \u03c3NM instead. Refer to discussion above and De\ufb01nitions 7,8. (x)  max0,x Figure 1: Notations and concepts introduced. the reward R is distributed to the pools and pool members, and the central issue of this work is to determine reward schemes with desired properties. We assume that the stakeholders are rational in the sense that they want to maximize their utility and that there are no externalities, i.e., outside factors that affect the reward of the pool and the players.",
      "Our primary objective is to incentivize the stakeholders to form a certain number of pools (smaller than the number of players). We further want no pool to have a disproportionally large size, so that no group can exert disproportionally large in\ufb02uence. Ideally, we want to \ufb01nd a reward scheme that, at equilibrium, leads to the creation of many almost equal-stake pools independently of (i) number of players (ii) the distribution of stake and costs (iii) the degree of concurrency in selecting a strategy. This seems like an impossible task5, so we have to settle for solutions that achieve the above goals approximately under some natural assumptions about the distribution of stake and costs and about the equilibria selection dynamics. We summarize the model here. Formal de\ufb01nitions of the concepts follow next. Reward sharing schemes (RSS) for stake pools. The class of reward sharing schemes we investigate is parameterised by a function r : 0,12 R0 and operates as follows.",
      "The reward scheme distributes a total \ufb01xed amount R to the pools according to their stake \u03c3i and the stake of their pool leader ai,i. In particular pool \u03c0i gets reward r(\u03c3i,ai,i) with i r(\u03c3i,ai,i) R. Note that we dont have to distribute the whole amount R. Formally, the function r(,) takes the stake of a pool and the stake of the pool leader allocated to this pool and returns the payment for this pool so that: ! i r(\u03c3i,ai,i) R. r(0,0)  0, which means that a pool with no stake will get zero rewards. The reward r(\u03c3i,ai,i) of each pool \u03c0i is shared among its pool leader and its stakeholders. This may be done in a number of ways but in any case, the pool leader should get an amount i  min(ci,r(\u03c3i,ai,i)) to cover the declared cost for running the pool.",
      "We will focus our investigation on reward schemes that are proportional, i.e., those schemes that have the property that the ratio of the rewards obtained by stakeholder j1 over the rewards of stakeholder j2 in pool \u03c0i equals a j1,ia j2,i, with the only exception being for pool leaders who may be considered for additional rewards. The stake pools game and utility function. Based on a reward scheme as described above, we can de\ufb01ne the stake pools game where the strategies of the players are their allocations of their stake to their own as well as the other available pools. In this game each player i tries to maximize his utility. The rewards of a pool \u03c0i are r(\u03c3i,ai,i) and the cost the pool leaderoperator incurs for running this pool is ci. The pool operator gets his cost reimbursed, apart from that, all rewards are split proportional to stake.",
      "So if a player i with cost ci runs a pool with total stake \u03c3i, his utility ui,i from this pool \u03c0i is ui,i  r(\u03c3i,ai,i)ci for r(\u03c3i,ai,i) ci, ai,i \u03c3i (r(\u03c3i,ai,i)ci) otherwise, and a player j  i delegating stake a j,i to that pool \u03c0i will get rewards u j,i  for r(\u03c3i,ai,i) ci, a j,i \u03c3i (r(\u03c3i,ai,i)ci) otherwise from that pool. We de\ufb01ne the utility of each player j to be u j  ! i u j,i. Given the above, the hard question is to de\ufb01ne the reward sharing scheme, and importantly r(,), so that the underlying stake pools game has Nash equilibria that meet (at least) our primary objective: having a large number of active pools. 5Actually, here is a simple reward scheme that achieves all goals: give no reward to the pools, unless there are many equal-stake pools, in which case each pool gets reward Rk.",
      "However, we are interested in reward schemes that can lead to a good Nash equilibrium starting at the state in which all players play no-participation and following natural symmetric, almost myopic dynamics, such as repeatedly having a random player playing best-response. Figure 2: Example dynamics for the fair reward sharing scheme (c 0.001,0.002) showing centralisa- tion after about 100 iterations with n  100 players. Initially, the players are maximally decentralzed. Here and in all following similar diagrams, the vertical line indicates the time when equilibrium is reached. Fair RSSs and their Failure to Decentralise In this subsection we will show that if we use a fair reward sharing scheme, then we will end up in an equilibrium with at most one pool, which means that this scheme fails our decentralization objective. Speci\ufb01cally consider the fair allocation that sets r(\u03c3i,ai,i)  \u03c3i R, i.e., pools are rewarded propor- tionally to their size. For simplicity we will take R  1.",
      "(Note that if we consider R  1 then all the costs are between zero and one.) Moreover, we will assume that all pool participants are also treated fairly receiving rewards proportionally to the stake they have delegated in the pool of their choice. We prove the following (see Appendix A.1 for the proof) the following theorem: Theorem 2. Given the above reward sharing scheme: (I) There is no equilibrium where more than one pool is created. (II) If there exists i such that si  ci then the only equilibria are the following: there exists just one pool, say \u03c0i and it holds (i) ci 1 and (ii) s j ci c j for each member j of this pool (iii) all players have delegated their stake to \u03c0i. Experimental results  dynamics Given the above theorem, we then experimentally investigate how fast such systems centralize. We use three different initial states for these experiments: 1.",
      "Maximally decentralized, where every player whose cost ci is lower than his stake si runs a pool and all other players are passive. 2. Inactive, where no player runs a pool. 3. Nicely decentralized, where ten players run a pool, and the others delegate to these pools in a way that makes them all equally big. Our experiments show that the convergence to the results predicted by the theory is fast: If at least one player has stake greater than cost and hence runs a pool, all players will end up delegating all their stake to this single pool ending up in a dictatorial single pool con\ufb01guration. The simulation in the experiment has players selected at random taking turns and playing best-response attempting to maximise their utility. More details regarding how the experiments are executed refer to Section 7 where we overview our experiments. In Figures 2, 3 and 4 we present a graphical representation of the experiments. Different colors correspond to different pools.",
      "The x-axis represents time while the y-axis the stakeholders. Costs are uniformly selected in the speci\ufb01ed range. Stake is following a Pareto distribution. In the following theorem (i) we generalise the impossibility result to the case of any function r for which (r(\u03c3,\u03bb)c)\u03c3 is strictly increasing in \u03c3 and (ii) we prove that there are con\ufb01gurations for which there is no equilibrium with a number of pools smaller than the number of players in the case of a strictly decreasing (r(\u03c3,\u03bb)c)\u03c3 in \u03c3. Figure 3: Example dynamics for the fair reward sharing scheme (c 0.001,0.002) showing centralisa- tion after about 100 iterations with n  100 players. Initially, no stake-pools exist. Figure 4: Example dynamics for the fair reward sharing scheme (c 0.001,0.002) showing centralisa- tion after about 100 iterations with n  100 players. Initially, the players are nicely decentralized. Theorem 3.",
      "I) If r(\u03c3,\u03bb)c as a function of \u03c3 is strictly increasing in \u03c3 (0,1 then there is no equilib- rium with more than one pool. Note that a fair reward function r(\u03c3,\u03bb)  \u03c3 is such an example. II) If r(\u03c3,\u03bb)  r(\u03c3) a continuous and strictly increasing function on \u03c3 and r(\u03c3,\u03bb)c as a function of \u03c3 is strictly decreasing in \u03c3 \u03c30,1, where \u03c30 such that r(\u03c30)c  0, then there is an assignment of costs and stakes to the players such that there is no equilibrium with fewer than n pools where n the number of players. We will assume for the proof that each player can delegate to a pool stake at least smin where f (1,) and smin the minimum stake among all the players. For the proofs see in Appendix A.2. RSS with Cap and Margin Motivated by the failure of the fair reward sharing scheme, in this section we will put forth a wider class of reward sharing schemes that fare better (as we will demonstrate) in terms of incentivizing players to create many pools of similar size.",
      "Our \ufb01rst key observation for a reward function to have better potential for decentralization is that while it should be increasing for small values of the pools stake, something that will incentivize players to join together in pools to share their costs, the rewards should plateaux after a certain point in order to discourage the creation of large pools, or equivalently to incentivize the breakup of large pools into smaller ones. This suggests that rewards will be capped. Our second observation is that it is sensible to treat pool leaders in a preferential way with respect to rewards. Recall that in the case when the rewards of the pool are more than the cost, the cost is subtracted from the rewards of the pool and, if we treat everyone proportionally, the pool leader should get the same rewards as a pool member having delegated the same stake to the pool.",
      "On the other hand, in the case when the pool does not get enough rewards to compensate its operational cost then the difference is paid by the pool leader. So the pool leader bears an extra risk compared to regular pool members and it makes sense to be compensated for that. Thus, in our reward scheme we will consider that the pool leader can ask for an extra reward compared to the other members. This reward will be a fraction of the pools pro\ufb01t and this fraction will be denoted by the margin value m. The margin will be part of the strategy of potential pool leaders. Reward sharing scheme with cap and margin. A reward scheme for stake pools that incorporates the above features will be called reward sharing scheme with cap and margin. Formally : De\ufb01nition 2 (Reward sharing schemes with cap and margin).",
      "A reward sharing scheme with cap and margin is a reward sharing scheme that (1) is parameterised by a function r : 0,12 R0 (that takes as input the stake \u03c3i of a pool \u03c0i and the stake ai,i of the pool leader allocated to this pool and returns the total reward for this pool) and a value k N and satis\ufb01es the following properties:  (as before) !n i1 r(\u03c3i,ai,i) R, where R the total rewards. (as before) r(0,0)  0. d(r(\u03c3,\u03bb)c) 1  0, when \u03c3 \u03b2 k . This means that the reward function is increasing for small values of pools stake to incentivize players to join together in pools to share the cost. \u03bb r(\u03c3,\u03bb)  r(\u03b2,\u03bb) when \u03c3  \u03b2. This means that the reward function is constant for large values of the pools stake to discourage the creation of large pools. (2) the reward r(\u03c3i,ai,i) of each pool \u03c0i is shared among its pool leader and its stakeholders. The pool leader gets an amount c i  min(ci,r(\u03c3i,ai,i)) to cover the declared cost for running the pool.",
      "A fraction mi of the remaining amount (r(\u03c3i,ai,i)c i ) is the pool leader compensation for running the pool. This fraction is referred to as margin. The rest (1mi)(r(\u03c3i,ai,i)c i ) is distributed to the stakeholders of the pool, including the pool leader, proportionally to their contributed stake. To analyze the outcome of a reward scheme, we need to de\ufb01ne the game induced by it, which in turn depends on our assumptions about how far-sighted the players are when calculating their best response. We analyze the natural assumption that each player computes their utility using the estimated \ufb01nal size of the pools (under the assumption that the other players act in the same way). The utility of the players in this setting depends on the desirability D j(S(m,\u03bb))  (1m j)P(\u03bbj,c j) of pool \u03c0j, where P(\u03bbj,c)  r(\u03b2,\u03bbj)c is the potential pro\ufb01t of the pool when it is saturated.",
      "Each player ranks the pools according to their desirability and computes the expected stake \u03c3NM of them (this is related to the non-myopic stake, see de\ufb01nition 7), which is either max(\u03b2,\u03c3j), when the pool is ranked among the k most desirable pools, or simply \u03bbj  ai,j, when the pool is not very desirable and the player expects to be alone with the pool leader. With this, we see that the non-myopic utility that the player gets by committing stake ai,j to a saturated pool \u03c0j is D j(S(m,\u03bb))ai,j\u03c3NM . The utility of the pool leaders is computed accordingly. Formal Treatment of the Stake Pools Game The stake pools game with cap and margin. Without loss of generality we assume that every player can be the leader of only one pool and each player has stake at most \u03b2  1k; players with stake more than \u03b2 or wishing to create more than one pool can be thought of as a strategic coalition of players which we analyse in Section 4 where we consider Sybil attacks of this nature.",
      "Below, we will use the notation: (x)  max(0,x), and n  1,...,n. De\ufb01nition 3 (Strategy of a player). The strategy of a player i has two parts:  (mi,\u03bbi), where mi 0,1 is the margin and \u03bbi the stake that player i will commit if he activates his own pool. S(m,\u03bb)  a(m,\u03bb) that is the allocation of player i stake given (m,\u03bb). When the (m,\u03bb) can be inferred from the context we will use ai for simplicity. ai,j 0,1 denotes the stake that player i allocates to pool \u03c0j so that his total allocated stake is !n j1 ai,j si. This allows for stake si !n j1 ai,j of the player to remain unallocated. In addition a(m,\u03bb) 0,\u03bbi. De\ufb01nition 4 (Pools). Given a joint strategy S(m,\u03bb), the stake allocated to a pool \u03c0j is denoted by \u03c3j(S(m,\u03bb)), or simply \u03c3j for a less cluttered notation. A pool \u03c0j is called active when player j allocates non-zero stake to it, that is, a j,j  \u03bbj  0. Note that only player j can activate pool \u03c0j. If a pool \u03c0j is active its stake is \u03c3j  !n i1 ai,j, otherwise we assume that \u03c3j  0.",
      "A pool is called saturated when its stake is at least \u03b2 . The restriction that only player j can activate pool \u03c0j, by allocating non-zero stake to it, is neces- sary to prevent other players to force player j pay the cost c j of operating the pool without consenting to open the pool. Non-myopic utility for reward sharing schemes with cap and margin. Recall that the strategy of player i is either to become a pool leader with margin mi by committing stake \u03bbi andor to delegate his stake to other pools. A crucial observation is that if we extend directly the utility we have de\ufb01ned in the game for stake pools so that it includes margin, then in the game de\ufb01ned by the above set of strategies, the notion of Nash equilibrium does not match the intuitive notion of stability that an equilibrium is supposed to provide.",
      "Note that, in the context of a Nash equilibrium, when players try to maximize utility, they play in a myopic way, which means that they decide based on the current size of the pools and they do not take into account what effect their moves have on the moves of the other players and thus, ultimately, in the eventual size of the pools. To see the issue, suppose that we have reached a Nash equilibrium in this game, that is, a set of strategies from which no player has an incentive to deviate unilaterally. The obvious problem is that at Nash equilibrium all margins will be 1. This is so, because by the de\ufb01nition of the Nash equilibrium the other players will keep their current strategy, and the best response of a pool leader is to select the maximum possible margin. Thus, if there is room to increase the margin, the strategy cannot be a Nash equilibrium and hence the only equilibrium, if it exists, will exhibit all margins to be to their maximum value 1.",
      "There are two problems here: \ufb01rst we de\ufb01nitely dont want the margins to be 1, and second, such an outcome is not expected to be a stable solution anyway! (In a sense contradicting the intuitive notion of what a Nash equilibrium is supposed to offer). If all margins are 1, a non-myopic player (a forward-looking player who tries to predict the \ufb01nal size of the pools after the other players play) who is not a pool leader can start a new pool with smaller margin which will attract enough stake to make it pro\ufb01table. For these reasons, in order to analyse our reward sharing schemes with cap and margin we will use a natural non-myopic type of utility which enables the players to be more far-sighted. Thus, in the analysis, players will not consider myopic best responses but non-myopic best responses. Speci\ufb01cally, a player computes his utility using the estimated \ufb01nal size of the pools instead of the current size of the pools.",
      "The estimated \ufb01nal size is either the stake that the pool leader has allocated to this pool or the size of a saturated pool. The latter is used when the pool is currently ranked to belong among the most desirable pools and the former when the pool does not belong among them. It follows that a non-myopic player that considers where to allocate his stake, would want to rank the pools with respect to the estimated reward at the Nash equilibrium. But this reward is not well-de\ufb01ned because the Nash equilibrium depends on the decisions of the other players. It makes sense then to use a crude ranking of the pools. Such a ranking can be based on the following thinking: An unsaturated pool where I will place my stake will also be preferred by other like-minded players if it has relatively low margin and cost, and substantial stake committed by the pool leader (the last one is essential only when \u03b1  0) so the pool will become saturated. So, I will assume that the stake of the pool is actually \u03b2.",
      "On the other hand, if a pool has relatively high margin and cost andor not substantial stake committed by the pool leader will not grow and will lose also its members as other unsaturated pools offer better combination of margin and cost. This motivates the following ranking of pools: De\ufb01nition 5 (Desirability and Potential Pro\ufb01t). The potential pro\ufb01t of a saturated pool with allocated pool leader stake \u03bb and cost c is P(\u03bb,c)  r(\u03b2,\u03bb) c. Given a joint strategy S(m,\u03bb), we de\ufb01ne the desirability of a pool \u03c0j D j(S(m,\u03bb))  (1m j)P(\u03bbj,c j) if P(\u03bbj,c j) 0 elsewhere Note that the desirability of a pool depends on its margin, the stake of the pool leader allocated to this pool and its cost. De\ufb01nition 6 (Ranking). Given a joint strategy S(m,\u03bb), the rank of a pool \u03c0j denoted by rankj(S(m,\u03bb)) is its ranking with respect to the desirability D j(S(m,\u03bb)). The maximum desirability gets rank 1, the second maximum desirability gets rank 2, etc.",
      "Again to get a less cluttered notation, we will write rankj instead of rankj(S(m,\u03bb)) whenever the joint strategy S(m,\u03bb) can be inferred from the context. Ties break according to the potential pro\ufb01t, speci\ufb01cally the pool with the higher potential pro\ufb01t will be ranked higher; (with higher we mean smaller rank) for convenience we assume that all potential pro\ufb01t values are distinct. The k most desirable pools will be these ones with rank smaller or equal to k. Given the ranking, we de\ufb01ne the non-myopic stake of a pool to be either the stake allocated by the pool leader or the size of a saturated pool. The \ufb01rst one is used when the pool does not belong to the k most desirable pools and the second one when the pool is among them. De\ufb01nition 7 (Non-myopic stake ). The non-myopic stake of pool \u03c0j is de\ufb01ned as (S(m,\u03bb))  max(\u03b2,\u03c3j) if rankj k a j,j otherwise.",
      "To simplify the notation we use \u03c3NM instead of \u03c3NM (S(m,\u03bb)), \u03c3j instead of \u03c3j(S(m,\u03bb)), rankj instead of rankj(S(m,\u03bb)) and a j,j instead of a j,j(S(m,\u03bb)). De\ufb01nition 8 (Non myopic utility). The utility ui(S(m,\u03bb)) of player i from being a member of pool \u03c0j with non myopic stake \u03c3NM ui,j(S(m,\u03bb))  \u03c0j is inactive (a j,j  0) (1m j) r(\u03b2,\u03bbj)c j ( ai,j rankj k a j,j  0 (1m j) r(\u03bbj  ai,j,\u03bbj)c j ai,j \u03bbj ai,j otherwise. The utility u j(S(m,\u03bb)) that the pool leader j gets from pool \u03c0j is u j,j(S(m,\u03bb))  \u03c0j is inactive (a j,j  0) r(\u03c3NM ,\u03bbj)c j r(\u03c3NM ,\u03bbj)c j  0a j,j  0 (r(\u03c3NM ,\u03bbj)c j) m j (1m j) otherwise. The utility of player i is the sum of the utilities coming from all pools in which he participates as a pool leader or a pool member: ui(S(m,\u03bb))  !n j1 ui,j(S(m,\u03bb)). A Sybil Resilient Reward Sharing Scheme In this section, we \ufb01rst outline the motivation behind our choice of the parameterized reward function. Motivating our solution. We propose a reward sharing scheme with cap and margin cf.",
      "De\ufb01nition 2. To motivate this choice, let us \ufb01rst consider a reward function r(\u03c3,\u03bb)  r(\u03c3,0) that depends only on the total stake \u03c3 of the pool (note we assume without loss of generality that the stake of any agent or pool belongs to (0,1) and represents the fraction of the total stake controlled by the speci\ufb01c entity) and it is independent of the stake \u03bb of the pool leader. The natural choice is to select r(\u03c3,0) proportional to \u03c3, which has the nice property that it rewards all players proportionally to their stake. However as we have seen already in Section 2.2, it leads to dictatorial equilibria in which a single pool is created. (Note that the cost of running a stake pool remains the same regardless its size). Moreover, it is clear that if we want to achieve a target number of pools, say k, it is clear any similar reward scheme cannot achieve this goal since it is independent of the target k.",
      "This motivates a simple modi\ufb01cation of this reward scheme which goes a long way in meeting this target. Consider the modi\ufb01cation r(\u03c3,0) min\u03c3,\u03b2, where \u03b2 is a constant (this is the cap) and indicates proportionality with a multiplier that guarantees that the total reward is suf\ufb01cient to pay all pools6 (see Figure 5). Recall a pool is saturated when its total stake \u03c3 is at least \u03b2, so we can say that such a capped reward function discourages oversaturated pools. By setting \u03b2  1k, this reward scheme seems to provide the right incentives to create pools of size up to \u03b2  1k, which naturally leads to k pools of equal size. However, this picture is to a large extent misleading because the usual myopic best-response dynamics creates a single pool instead of k, because even with this reward function, for a pool member, a saturated pool is preferable to a pool whose reward is mainly used to cover the cost of its leader.",
      "The good news is that, as we will show, dynamics of non-myopic best response achieves the goal by leading to an equilibrium of k pools of equal size, given a reasonable de\ufb01nition of an appropriate non-myopic notion of utility. To evaluate the quality of a reward scheme, we should compare the resulting equilibrium with an optimal solution. An optimal solution when all participants act honestly and sel\ufb01shly is to have k pools of equal size that are run by pool leaders of minimal cost. This would make the system ef\ufb01cient, in both computational and economic sense. But besides ef\ufb01ciency, we want the system to withstand attacks from some players that try to run many pools, even at a loss. Sybil behavior and resilience. In particular we want to disincentivize Sybil strategies 14) that create multiple identities declaring potentially lower costs for each one. We distinguish two types of Sybil behaviors: the \ufb01rst one captures a non-utility maximizer who wants to control 50 of the system.",
      "Such level of control enables a party to perform double spending attacks on the blockchain or arbitrarily censor transactions. The second type of Sybil behavior is that of a utility maximizer that creates multiple identities with their corresponding stake-pools sharing the same server back-end and thus also the operational costs. Such a player limits decentralisation by reducing the number of independent server deployments that provide the service. Observe that this also can include coalitions of players that decide to act as one. Such behavior cannot be excluded in the anonymous setting that we operate. The best possible that we can hope for is to lower bound the stake of the Sybil player to be linear in the number of identities that it creates. We analyse the Sybil resilience of a reward sharing scheme by estimating the minimum stake smin needed for the Sybil behavior to be effective.",
      "To address this issue we design a reward sharing scheme that guarantees that players can attract stake from other players only if they commit substantial stake to their own pool. This is precisely the reason for considering reward functions that depend, besides the total stake of the pool, on the stake of the pool leader. 6A smooth function that approximates this reward function may be preferable to improve the dynamics of convergence to equilibrium. Figure 5: Reward function for \u03b2  110 with \u03b1  0 (top) and \u03b1  14 (bottom). Ideally, we want the pools to be created by the players ranked best according to \u03b1s j c j (a linear combination of their stake s j and their cost c j), where \u03b1 is a nonnegative parameter that can be adapted to trade between ef\ufb01ciency and Sybil resilience.",
      "By selecting \u03b1  0 we get the most ef\ufb01cient solution, and on the other extreme, by selecting a very large \u03b1, we can obtain a potentially inef\ufb01cient solution in which the pool leaders might be the k wealthiest but the Sybil resilience of the system improves. The objective is to design a reward scheme that provides incentives to obtain an equilibrium that compares well with the above optimal solution. On the other hand, we feel that it is important that the mechanism is not unnecessarily restrictive and all players have the right to become pool leaders. The natural way to accomodate this in our scheme, would be to use the above reward function but apply it to \u03c3\u03b1\u03bb, a weighted sum of the total pool stake \u03c3 and the allocated pool leader stake \u03bb. With this in mind, the reward function becomes r(\u03c3,\u03bb) min\u03c3,\u03b2\u03b1\u03bb.",
      "Again this reward function goes some way towards meeting the objective but the best response dynamics, even non-myopic best response dynamics, do not lead to equilibria that resemble the optimal solution and in particular, it may create pools of very large size. The reason is that the in\ufb02uence of the stake \u03bb of the pool leader when a pool is still small is very signi\ufb01cant. Given that the ideal size of the pool is \u03b2, one way to alleviate this effect is to change the in\ufb02uence factor \u03b1 to be proportional to the stake that the pool has already attracted, that is to change the in\ufb02uence factor to \u03b1  \u03b1 \u03c3\u03bb \u03b2 . This creates the (more minor) problem that the in\ufb02uence factor will not be the same for all pools, which is quite desirable when a parameterisation is attempted and the value of \u03b1 will be used to control Sybil attacks.",
      "The \ufb01nal touch in the reward function which resolves this issue is to make the in\ufb02uence of the stake of the pool leader on the factor \u03b1 to disappear when the pool has the desired size of \u03b2. The resulting reward function described brie\ufb02y in the informal theorem of the introduction (De\ufb01nition 1) is de\ufb01ned and analyzed in the rest of the current section. Our RSS construction Given our target number of pools k, we de\ufb01ne the reward function rk : 0,12 R0 of a pool \u03c0 with stake \u03c3 and pool leaders allocated stake \u03bb as follows: rk(\u03c3,\u03bb)  1\u03b1 \u03c3 \u03bb \u03b1 \u03c3 \u03bb (1\u03c3\u03b2) where \u03bb  min\u03bb,\u03b2, \u03c3  min\u03c3,\u03b2 and \u03b2,\u03b1 are \ufb01xed parameters. A natural choice is \u03b2  1k, where k is the target of number of pools. For simplicity we will write r instead of rk. We have: \u03b1 0,), k N,(k  n) and R R. Note that the total rewards R and \u03b1 should be selected such as it holds also P(sk1,ck1)  0. The next proposition shows that the proposed function is suitable for a reward sharing scheme with cap and margin. Proposition 1.",
      "The function r(,) satis\ufb01es the properties of a reward sharing scheme with cap and margin, cf. De\ufb01nition 2. Proof. It holds i r(\u03c3i,ai,i) 0 , as a i,i \u03c3 i and also: 1. !n i1 r(\u03c3i,ai,i) R, as i a i,i  (\u03b2\u03c3) 1 and !n i1\u03c3i  ai,i \u03b1  !n i1 \u03c3i \u03b1!n i1 ai,i 1\u03b1. 2. r(0,0)  0. 3. When \u03c3 \u03b2 it holds: dr(\u03c3,\u03bb)c) 1  0. 4. \u03bb r(\u03c3,\u03bb)  r(\u03b2,\u03bb), when \u03c3  \u03b2 because we have \u03c3  min\u03c3,\u03b2. This completes the proof. Perfect Strategies We de\ufb01ne a class of strategies and we prove that they are Nash equilibria of our game (Theorem 4). This class has the following characteristics: exactly k pools of equal size are created and the pool leaders are the players with the highest value P(s,c) (when \u03b1  0 those are the players with the smallest cost). Recall that the players are ordered in terms of potential pro\ufb01t, e.g., player 1 is the player with the highest P(si,ci). Recall also that players decide to create or not a pool and how much stake they will allocate to other pools.",
      "In addition they decide a margin for their potential pool. De\ufb01nition 9 (Perfect strategies). We de\ufb01ne a class of strategies, which we will call perfect. The margins 1P(sk1,ck1) P(s j ,c j ) when j k otherwise, the stake allocated by each pool leader to their own pool is equal to their whole stake and the alloca- tions are such that each of the \ufb01rst k pools has stake \u03b2. Note that when j k it holds rankj k. The following proposition gives the utilities at perfect strategies and it follows directly from De\ufb01nition 8 of the non-myopic utilities of pool members and pool leaders and our reward function described in this section. Proposition 2. In every perfect strategy, (i) the utilities of the players are: ui  P(sk1,ck1)si \u03b2 (P(si,ci)P(sk1,ck1)), and (ii) the desirability of the \ufb01rst k 1 players is the same and equal to P(sk1,ck1).",
      "To justify the proposition note that all the players get a fair reward, in the sense that it is a constant P(sk1,ck1)\u03b2 times their stake, with the exception of each pool leader i, who gets an additional reward P(si,ci)P(sk1,ck1). This additional reward can be viewed as a bonus for the ef\ufb01ciency and security that the pool leader brings to the system. We will show that every perfect strategy is a Nash equilibrium of the game with the de\ufb01ned utilities. Theorem 4. Every perfect strategy is a Nash equilibrium. Before presenting the proof of the theorem we start with some de\ufb01nitions and preliminary results. De\ufb01nition 10 (Desirability of a player). Desirability of a player will be the desirability of their pool. If they do not have one, their desirability will be the desirability of a hypothetical pool with their cost, the margin they have chosen and their personal stake. Note that for uniformity we assume that all the players decide a margin even if they do not create a pool.",
      "In addition, when we rank the pools in this subsection, we will take into account also the hypothetical pools described above. Ties break in favor of potential pro\ufb01t. In the two-stage game that we examine in Section 5 we remove these assumptions (regarding hypothetical pools and ties as (i) we do not take into account non active pools in the ranking because we consider their desirability as zero (ii )ties in ranking break arbitrarily). The following lemma is very useful and its proof follows directly from the de\ufb01nition of the reward function. Lemma 1. The quantity (r(x,s j)c j)x as a function of x is increasing in (0,\u03b2) and, if it is positive, decreasing in (\u03b2,). Its maximum is achieved at x  \u03b2. The following lemma gives an upper bound on the utility of pool members. We will give an equilibrium that matches this upper bound. Lemma 2.",
      "In every joint strategy in which some player j is not a pool leader, their utility is at most maxl Dl (s j\u03b2), where maxl Dl is the maximum desirability among all players. Proof. It suf\ufb01ces to show that player j gets at most Dl a j,l \u03b2 from every pool l. The lemma follows directly from this by summing for all l: ! l Dl a j,l \u03b2 maxl Dl a j,l \u03b2  maxl Dl \u03b2 . The argument that for every pool l, player j gets at most Dl a j,l \u03b2 follows directly from the de\ufb01nition of the utility of pool members when we consider the two cases depending on whether rankl is at most k and more than k. Speci\ufb01cally, when rankl k, by the de\ufb01nition of the utility of pool members, the utility to player j from pool l is Dl a j,l\u03c3NM Dl a j,l\u03b2. When rankl  k, his utility is given by (1ml) r(\u03bbl  a j,l,\u03bbl)cl a j,l \u03bbl  a j,l (1ml) r(\u03b2,\u03bbl)cl ( a j,l  Dl a j,l where the inequality comes from Lemma 1. We are now ready to present the proof of the Theorem. Proof.",
      "(of Theorem 4) We \ufb01rst consider the simpli\ufb01ed setting where players are mutually exclusively pool leaders or pool members. Consider \ufb01rst a player j with rank at most k. This player is a pool leader of a pool of size \u03b2. We show that none of the possible responses improves their utility:  Suppose that the player decreases their margin. This increases their desirability so that the new rank is still one of the \ufb01rst k ranks. Since the non-myopic stake remains the same, this move will decrease the utility of the player. Suppose that the player increases their margin. Since before the change the \ufb01rst k 1 players have the same desirability, the players desirability drops and the rank becomes larger than k. As a result the player will be alone in a pool and their utility can only decrease (Lemma 1). Suppose that the player becomes a pool member of other pools.",
      "By Lemma 2, their utility can be P(sk1,ck1)s j\u03b2 at most, which is lower that their current utility by P(s j,c j)P(sk1,ck1) (by Equation 5). We now consider a player j with rank higher than k. Again we show that none of the possible responses improves their utility. Notice \ufb01rst that by changing their allocation of stake, it can only hurt their utility since some of their stake ends up in pools with stake different than \u03b2, which can only lower their utility by Lemma 1. The other alternative is that the player becomes a pool leader. Since their rank is higher than k, the (non-myopic) stake of the pool contains only their own stake, which by Lemma 1 is again no better than the current utility. We now sketch the full argument that considers the more complex strategies of possibly simultane- ously delegating and creating a pool for each player (we remark that this case is also subsumed in the two-stage game in Section 5).",
      "Note that the desirability and thus the rank of the pools does not depend on the size of the pools. So if we allow strategies where a player is pool leader and simultaneously delegates some stake to other pools, then the perfect strategies remain Nash equilibria. In addition, it is easily veri\ufb01ed that Lemmas 1,2 hold also in this case. If a player 1,...,k with stake s and cost c increases their margin from mto m and delegates stake s \u03bb to other pools then their pool will have rank higher than k and their utility will become \u03bb \u03bb (r(\u03bb,\u03bb)c)P(sk1,ck1) s\u03bb \u03b2 which is no higher than \u03bb \u03b2 P(\u03bb,c)P(sk1,ck1) s\u03bb because r(\u03c3,\u03bb)c increasing for \u03c3 1k (Lemmas 1). This is at most (m(1m) \u03bb \u03b2)P(s,c) P(sk1,ck1) s\u03bb \u03b2 that is equal to their current utility. If a player 1,...,k with stake s and cost c decreases their margin from mto m and simultane- ously transfers stake s \u03bb to other pools, then the desirability of their pool remains the same, increases or decreases.",
      "We will prove that in all cases their utility will be at most their current utility (m(1m) s \u03b2)P(s,c). 1. If the desirability of their pool remains the same, then (i) the utility for the part of their stake that remains in their pool denoted by \u03bb will decrease because of the lower margin or will remain the same and (ii) the utility for the stake that has been transferred to other pools denoted by s \u03bb will also decrease because these pools have the same desirability and their non-myopic stake will become higher than 1k. 2. If the desirability of their pool decreases, then the rank of their pool will become higher than k regardless the stake this player delegated to other pools. So again the utility for both parts of stake will decrease. 3. If the desirability of their pool increases then their utility will become (m (1m) \u03bb P(\u03bb,c) s\u03bb \u03b2 P(sk1,ck1) (m(1m) \u03bb \u03b2)P(s,c) s\u03bb \u03b2 P(sk1,ck1)  (m(1m) \u03b2)P(s,c).",
      "If a player 1,...,k with stake s and cost c does not change margin and transfers stake s \u03bb to other pools then again their utility will become \u03bb \u03bb (r(\u03bb,\u03bb)c)P(sk1,ck1) s\u03bb because their pool will have rank higher than k. If a player k 1,...,n with stake s and cost c creates a pool with stake \u03bb and delegates the remaining stake to other pools then their pool will have rank lower than k so their utility will be (r(\u03bb,\u03bb)c) s\u03bb \u03b2 P(sk1,ck1) P(\u03bb,c) \u03bb \u03b2  s\u03bb \u03b2 P(sk1,ck1) which is not higher than their current utility s \u03b2 P(sk1,ck1) . It is interesting to note that in the \ufb01rst case of the proof of Theorem 4, the pool leader of a pool with stake \u03b2 decreases their margin. This does not affect our equilibrium argument since by the de\ufb01nition of non-myopic stake, the stake of their pool remains the same and hence the non-myopic utility is unaffected.",
      "But this pool will score a higher desirability and in the real world far-sighted pool members may prefer it and, in such case, its size may increase beyond \u03b2. This raises the question whether perfect strategies are stable when the players play non-myopically beyond the strict de\ufb01nition that is captured by the way we have considered so far in the analysis. To answer this question and understand the implications of these far-sighted strategies, we can conduct a two-stage game analysis which we present in Section 5. Sybil resilience and whale stakeholder analysis We now turn to the analysis of Sybil attacks as well as of the effect that large (whale) stakeholders have in the game. Recall that in the previous section we restricted players to having stake at most \u03b2  1k and each one creating at most one pool, hence explicitly excluding Sybil attacks and whale stakeholders.",
      "To remove these constraints, we consider an extended setting that involves a set of n n agents, each one with (private) stake s1,..., s n and associated (private) cost c1,..., c n. Each agent i can declare themselves as a single player in the stake-pool game as long as si \u03b2, or alternatively declare more than one players (called Sybils) splitting their stake in some way between the declared players. This pre-game stage de\ufb01nes a speci\ufb01c instance of the stake-pool game. The utility of each agent is the sum of the utility of all the players that the agent controls. We analyze two scenarios in this setting. In the \ufb01rst one, there is a utility non-maximizer agent with total stake less than 12, who creates k2 players, potentially lying about their costs, with the objective of dominating the system by creating k2 saturated pools at the Nash equilibrium. In the second scenario, a utility maximizer agent creates t  1 players that share their costs by using the same server.",
      "In both cases, to simplify the analysis, we will assume that the stake-pool game proceeds with players acting rationally and independently. For a given agent, denote by A 1,...,n the set of players the agent introduces in the stakepool game. For each A, we denote by (s A i ,c A i ) the stake and cost of the i-th player in the game, ordering them in decreasing order of potential pro\ufb01t, excluding A. Moreover, the maximum cost and the minimum stake, excluding players in A, will be denoted c A max and s A min respectively. We prove the following. Theorem 5. Consider an agent controlling a set of players A. First, if the agent has stake less than k21 c A R (1 1 then it will control fewer than k2 saturated pools at the Nash equilibrium, even if the agent is a utility non-maximizer. Second, if the agent is a utility maximizer with cost c and stake less than t  kt1 (c A maxct) (1 1 , it will control fewer than t saturated pools at the Nash equilibrium for any k t  1.",
      "The proof is described in Appendix A.3. We observe that in both cases, the minimum stake needed by the Sybil attacker agent is asymptotically linear in the number of stake pools (k2 in the \ufb01rst case and t in the second). Moreover, the coef\ufb01cient, in both cases, can be adjusted by varying the Sybil resilience parameter \u03b1. Speci\ufb01cally, when c A  s A min, these bounds are positive for suitable value of \u03b1; in particular, the higher \u03b1 is, the higher these bounds become. Note that s A k21 and s A kt1 are nondecreasing in \u03b1, because the ordering of the remaining agents depends on P(si,ci) and thus also in \u03b1 (the higher \u03b1 is the higher impact agents stake has on the ordering). For example, in the \ufb01rst case when R  1 and k  10, and the stake and cost are sampled from a Pareto distribution with parameter \u03b1  2 and the uniform distribution from 0.0005,0.0010 respectively, if we choose \u03b1  0.5 then k21  0.00076024,s A k21  0.02002176.",
      "Then if a non-utility attacker declares cost c  0.9c A k21, the stake required for the attack is at least 0.0989. This is not far from optimal, since the largest possible lower bound is 50.02002176  0.1001088, which would apply to the setting of negligible costs and a choice of \u03b1 that goes to . Next we examine the probability under reasonable probability distributions that there exists an agent who has stake more than k 2  s A k21, which allows them to engage in Sybil behavior in the above settings (i.e., with negligible costs and a choice of \u03b1 that goes to ). Let Si and si  ! n i1 Si be the absolute and the relative stake respectively of agent i. Let S1,...,S n be independent samples from random variable X that follows the upper truncated Pareto distribution 10 with parameter a  0. Let \u03b8 and T be the minimum and maximum value of the distribution, respectively. Then the cumulative function of X is FX (x)  1( \u03b8 x )a 1( \u03b8 T )a when \u03b8 x T .",
      "Also if Xr is the stake of the agent with the r-th smallest stake, then the cumulative function of Xr is FXr (x)  ! n jr  ' n F j X (x)(1FX (x)) nj, see 8. We also denote by Si  X ni1 the stake of the agent with the i-th highest stake. Let fS k 2 1(t) be the density function of S k 2 1 and FB(k; n,p)  !k ' n pi (1p) ni the cumulative function of Binomial distribution. The following theorem quanti\ufb01es the probability that a Sybil attack is possible. Theorem 6. Assume that S1,...,S n, where Si is the absolute stake of agent i, are drawn from an upper truncated Pareto distribution with parameters a,\u03b8,T . Then when \u03b4  1( \u03b8 T )a 1( \u03b8k 2T )a (1k 2 n )1  0: Pr(s1  k 2  s k 2 1) e\u03b42\u00b53, where \u00b5  n FX ( 2T k ). For the proof see Appendix A.4. Note that if we take a  1, \u03b8 T  1100,000 and k  100, then in order for \u03b4 to be positive, it suf\ufb01ces n  150,000, a reasonable number of users of a general cryptocurrency. Also if we choose higher n or \u03b8 and lower T , then \u03b4 will increase.",
      "It holds that \u03b4 is  increasing as a function of n and decreasing as a function of T and a  increasing as a function of k if and only if \u03b8aka1 2aT a (k 2a  n a k)  1. In particular when a  1, \u03b4 is increasing as a function of k if and only if T  \u03b8  n. A Two-Stage Game Analysis We will next prove that our reward sharing scheme effectively retains the same perfect equilibria outcome of Theorem 4 also in a more realistic two-stage or inner-outer game.",
      "The advantages of this approach are as follows: (i) it allows us to analyze non-myopic moves in response to pool leaders changing margin or allocation, (ii) it allows us to remove the assumption that a player can be either a pool leader or a pool member, (iii) in this setting when a pool has not been activated, we de\ufb01ne its desirability to be zero, something that gives us a more realistic result, because in practice only pools that have already been created will be ranked; (iv) in this game we break ties in ranking in arbitrary ways, not only according to potential pro\ufb01t. We note that similar non-myopic type of play has already been considered in other settings, notably in Cournot Equilibria, as is discussed in the introduction and related work. Our inner-outer game consists of two games. In the outer game, player i decides on the margin mi and on the stake \u03bbi to be allocated to its own pool, in case the player will decide to activate it in the inner game.",
      "So a strategy of a player i in the outer game is a tuple (mi,\u03bbi) of margin and allocated stake, and let (m,\u03bb) be the joint strategy of the outer game. Each joint strategy of the outer game determines one inner game. In the inner game, the margins m and the stakes \u03bb, which potential pool leaders would allocate to their pools, are given, and the strategies of the players are their allocations. So in the inner game determined by (m,\u03bb), a strategy of player i is S(m,\u03bb)  ai, and a joint strategy is S(m,\u03bb). Note that if a player i decides to activate its own pool, which means ai,i  0, then the player is committed to allocate stake \u03bbi to its own pool, where \u03bbi is part of the strategy of the outer game. So ai,i 0,\u03bbi. We assume, that in the inner game the players decide their allocation with the goal of maximizing their non-myopic utility, as it is de\ufb01ned in 8.",
      "(Recall that we have assumed that each player can create at most one pool and that the utility an inactive pool gives to its members is zero.) For a joint strategy (m,\u03bb) of the outer game, we de\ufb01ne the utility of a player j to be equal to the non-myopic utility of this player in the equilibrium of the associated inner game. Formally uouter (m,\u03bb)  u j(S(m,\u03bb)), where S(m,\u03bb) is the unique equilibrium of the inner game determined by (m,\u03bb). (We study also the case when the inner game has more than one or no equilibrium, by de\ufb01ning proper utilities and proper notion of equilibrium in this case, see Appendix 5). In this framework, we describe a set of joint strategies that (i) are approximate non-myopic Nash equilibria of the outer game and (ii) have the characteristic that in the inner games de\ufb01ned by these joint strategies, all the equilibria form k saturated pools. Recall that a pool is saturated when its stake is at least \u03b2.",
      "The pool leaders of these pools in these equilibria of the inner games are again the players with the highest values P(si,ci). The intuition for how the set of margins of these joint strategies is determined is the following: The k players with the highest values P(si,ci) set the maximum possible margin, as long as their pools belong to the k most desirable pools (the pools with the highest desirability), no matter which margins the other players have currently. Note that if all players activated a pool of size 1k with the same margin and their whole stake, then the k pools with the highest potential pro\ufb01t (P(si,ci)) would give the highest utility to their members. The formal analysis, the theorems and the proofs appear in Appendix 5. De\ufb01nition of the game. In order to also capture non-myopic moves in response to pool leaders changing margin or allocation, we de\ufb01ne a two-stage game, the inner-outer game.",
      "Similar non- myopic play has already been considered in other games, most notably in Cournot Equilibria, as is discussed in the introduction and related work. In this section we reuse non-myopic utility and desirability as de\ufb01ned in previous sections, but when a pool has not been activated in the inner game, we de\ufb01ne its desirability to be zero. This gives us a more realistic result, because in practice only pools that have already been created will be ranked. In addition we remove the assumption that a player can be either a pool leader or a pool member. We order players by P(si,ci), and i will denote the player with the ith highest value according to this ordering. We break ties in ranking in arbitrary ways, our analysis will hold for all of them. In fact, we de\ufb01ne two games here, the inner game, which focuses on the allocation of stake, and the outer game, which focuses on the margins and on the stake that potential pool leaders commit to their pools.",
      "In the outer game, player i decides on their margin mi and on how much stake \u03bbi to allocate to their pool, should they decide to activate it in the inner game. So a strategy of a player i in the outer game is a tuple (mi,\u03bbi) of margin and allocated stake. (m,\u03bb) is a joint strategy of the outer game. In the inner game, the margins m and the stakes \u03bb, that potential pool leaders would allocate to their pools, are given, and the strategies of the players are their allocations. So in the inner game determined by (m,\u03bb), a strategy of player i is S(m,\u03bb)  ai, and a joint strategy is S(m,\u03bb). Note that if a player i decides to activate their own pool, which means ai,i  0, then they are committed to allocate stake \u03bbi to their pool, where \u03bbi is part of their strategy of the outer game. So ai,i 0,\u03bbi. We assume that players decide their allocation trying to maximize their non-myopic utility.",
      "Recall that we have assumed that each player can create at most one pool and that the utility that an inactive pool gives to its members is zero. Note that each joint strategy of the outer game determines one inner game. De\ufb01nition of Equilibria for Inner and Outer Game De\ufb01nition 11. A joint strategy S(m,\u03bb) is a Nash equilibrium of the inner game de\ufb01ned by (m,\u03bb) when for every player j u j(S(m,\u03bb) ,S(m,\u03bb) ) u j(S(m,\u03bb)) for every S(m,\u03bb)  S(m,\u03bb) . This is the standard Nash equilibrium notion when the players try to maximize their non-myopic utility. To de\ufb01ne the non-myopic equilibrium of the outer game, let us temporarily assume that there is a unique Nash equilibrium in every inner game. Then we de\ufb01ne the utility of player j in the outer game, where players have selected joint strategy (m,\u03bb), as: uouter (m,\u03bb)  u j(S(m,\u03bb)), where S(m,\u03bb) is the unique equilibrium of the inner game determined by (m,\u03bb).",
      "So a joint strategy (m,\u03bb) is an approximate \u03f5-non-myopic Nash equilibrium of the outer game when for every player j uouter j, mj,\u03bb j,\u03bbj) uouter (m,\u03bb)\u03f5 for every (m j,\u03bb j)  (m j,\u03bbj). When there are multiple equilibria in the inner game, we de\ufb01ne uouter (m,\u03bb) as the set of values u j(S(m,\u03bb)), where S(m,\u03bb) is a Nash equilibrium of the inner game determined by (m,\u03bb). uouter,up (m,\u03bb)  supuouter (m,\u03bb) if uouter (m,\u03bb)  ., elsewhere. In the same way we de\ufb01ne: uouter,low (m,\u03bb)  infuouter (m,\u03bb) if uouter (m,\u03bb)  ., elsewhere. Note that when uouter (m,\u03bb) is not empty, it is a non-empty bounded subset of the reals and therefore al- ways has both supremum and in\ufb01mum: Upper- and lower bounds are given by R and (maxc1,...,cn) respectively. De\ufb01nition 12. A joint strategy (m,\u03bb) is an \u03f5-non-myopic Nash equilibrium when for every player j uouter,up j, mj,\u03bb j,\u03bbj) uouter,low (m,\u03bb)\u03f5 (10) for every (m j,\u03bb j)  (m j,\u03bbj). For the formal theorems and proofs referring to the two-stage game see in Appendix A.5.",
      "Deployment Considerations In this section we overview various deployment considerations of our RSS solution as well as we address speci\ufb01c attacks and deviations against our reward sharing scheme, speci\ufb01cally, (i) pools that underperform in general, (ii) participants who play myopically, (iii) pools that censor undesirable delegation transactions, (iv) pool leaders not truthfully declaring their costs, and (v) parties who try to gain advantage by exploiting how wealth may compound over time (the rich get richer problem) in a series of iterations of the game. Regarding deployment, in order to facilitate the use of an RSS within a PoS cryptocurrency, e.g., 30, 25, 13, 6, the ledger should be enhanced to enable special transactions which allow players to delegate their stake to a pool and reassign it at will during the course of the execution. Describing in more detail the exact cryptographic mechanism for performing this operation is outside the scope of the present paper.",
      "It is suf\ufb01cient to note that the mechanism is simple and very similar to issuing public-key certi\ufb01cates; see e.g., 25 for a description of such a delegation mechanism. Recall that in a PoS cryptocurrency, the protocol is executed by electing participants in some way based on the stake they possessed in the ledger; informally every protocol message is signed on behalf of particular coin that is veri\ufb01ably elected for that particular point of the protocols execution. In the stake pool setting, the PoS protocol will be executed with the pool leaders representing the pool members whenever the coin of a member is elected for protocol participation. Ill-performing stake pools. In our system, rewards for a pool are calculated based on the declared stake of the pool leader as well as the stake delegated to that pool.",
      "This provides an opportunity for a pool leader to declare a competitive pool and subsequently do not provide the service that it promised (presumably gaining in terms of the actual cost that system maintenance incurs). This can be addressed by calibrating the total rewards R to depend also on the total performance of the system as evidenced in the distributed ledger. For instance, in a PoS blockchain, it is possible to count the number of blocks that were produced in a period of time and compare that value to its expectation. In case the actual number of blocks is below expectation we may reduce R accordingly (effectively punishing all pools) and in this way generating a counter-incentive to deviate from system maintenance according to the protocol. Note that punishing all pools in case of underperformance makes sense due to the possibility of mining games 16, 24 which may be used by pools to attack each other in case we use performance as indicator for punishment.",
      "However, punishing everyone may be hard to parameterise as a large reduction in R will be unfair to genuinely performing participants (who will be losing rewards due to the ill performance of others) while a small reduction may be insuf\ufb01cient as a counterincentive. If the underlying blockchain is also fair (in the sense of 35) then it might be also possible to penalise only speci\ufb01c pools that underperform and hence be able to better \ufb01ne tune performance sensitivity. It is an interesting question to design such robust performance metrics that can be used in the context of a reward sharing scheme. Players who play myopically and Rational Ignorance. Myopic play is not in line with the way we model rational behavior in our analysis. We explain here how it is possible to force rational parties to play non-myopically.",
      "With respect to pool leaders we already mentioned in Section 2.3 that rational play cannot be myopic since the latter leads to unstable con\ufb01gurations with unrealistically high margins that are not competitive. Next we argue that it is also possible to force pool members to play non-myopically. The key idea is that the effect of delegation transactions should be considered only in regular intervals (as opposed to be effective immediately) and in a certain restricted fashion. This can be achieved by e.g., restricting delegation instructions to a speci\ufb01c subset of stakeholders at any given time in the ledger operation and making them effective at some designated future time of the ledgers operation. Due to these restrictions, players will be forced to think ahead about the play of the other players, i.e., stakeholders will have to play based on their understanding of how other stakeholders will as well as the eventual size of the pools that are declared.",
      "A related problem is that of rational ignorance, where there is some signi\ufb01cant inertia in terms of stakeholders engaging with the system resulting to a large amount of stake remaining undelegated. This can be handled by calibrating the total rewards R to lessen according to the total active stake delegated, in this way incentivising parties to engage with the system. Censorship of delegation transactions. In this attack, a pool (or a group of pools) censors delegation transactions that attempt to re-delegate stake or create a new pool that is competitive to the existing ones. In the extreme version of this attack a cartel of pool leaders control the whole PoS ecosystem and prevent new (potentially more competitive) pools from entering or existing members from delegating their stake. Actually, this is a typical threat to all political systems in which power is delegated to representatives.",
      "However, in PoS systems even a single pool that does not censor attacks is suf\ufb01cient to prevent this attack assuming there is suf\ufb01cient bandwidth to record the delegation transactions in the blocks that are contributed by that pool. It is an interesting question to address the case where all stake pools form a coalition that decides to prevent any more pools from being created. A potential way forward to preventing such abuse of power by pool leaders, is by either creating the right system safeguards and incentives for the coalition to break or rely on direct member participation that will override the pool leader cartel. In this latter case, pool members acting as system watchdogs, without getting any reward, could still create alternative blocks, that take precedence over the blocks issued by the block leader in this way creating a ledger fork along which censorship is stopped. Costs and incentive compatibility.",
      "In our analysis, we assumed for simplicity that the costs are publicly known; in reality the actual costs for participating in the collaborative project are known only by the player, who may lie about it in the cost declaration. This will happen when the players may Figure 6: Example dynamics of our reward sharing scheme (c 0.001,0.002, \u03b1  0.02) showing convergence to decentralization. see it as an advantage to lie about their cost. This problem is one of mechanism design which has objective to design an incentive compatible mechanism, i.e., a mechanism that gives incentives to players to declare their costs truthfully. We next argue that, in fact, our RSS is incentive-compatible as presented. Let us consider the perfect Nash equilibrium from De\ufb01nition 9 in which the utilities are given by Equation 5. Suppose that a pool leader j declared a different cost \u02c6c j, but remained pool leader. Since P(s j, \u02c6c j)P(s j,c j)  c j \u02c6c j, the player will not get any bene\ufb01t from lying.",
      "To see this, let u j(\u02c6c jc j) denote the utility when the player declares cost \u02c6c j instead of the true cost c j. Then by taking into account the cost, we have u j(\u02c6c j\u02c6c j)  u j(\u02c6c jc j) c j  \u02c6c j. Also from Equation 5, we see that u j(c jc j)u j(\u02c6c j\u02c6c j)  P(s j,c j)P(s j, \u02c6c j). Putting them together we see that u j(\u02c6c jc j)  u j(c jc j), thus the player has no reason to lie. With similar reasoning, a pool leader has no reason to lie by raising his cost so much that the rank of his pool increases above k. Similar considerations, show that no pool member (i.e., a player whose pool, if created, would have had a rank at least k 1) has an incentive to lie. This includes the special case of the player with rank k 1. As a conclusion, we see that under the assumption that the players end up at a perfect equilibrium, it is a dominant strategy to declare the true cost. As a side note, we could also adapt any similar reward scheme to implement the Vickrey-Clarke-Groves (VCG) mechanism, cf.",
      "31, which applies to all mechanism design problems. In this particular case, the VCG mechanism, would ask the players to declare their costs c, but the reward scheme would use a different vector of costsc for the game. The new costsc will be such that the desirability of the player with rank j k would be slightly superior to the desirability of the player with rank k 1. Rich getting richer considerations. In a PoS deployment, our game will be played in epochs with each iteration succeeding the previous one. Using the mechanisms we described above regarding censorship and Sybil resilience, it is easy to see that players are not bound by their past decisions and thus they will treat each epoch as a new independent game.",
      "A special consideration here is what frequently is referred to as the rich get richer problem, i.e., the setting where the richest stakeholder(s) amass over time even more wealth due to receiving rewards leading to an inherently centralised system (it is sometimes believed that this issue is intrinsic to only PoS systems but in fact it equally applies to PoW systems, cf. 23). In order to address this issue we observe that the maximum rewards obtained by each pool at each epoch are in the range R(1  \u03b1)k,Rk with \u03b1 0,) determining the size of the range which controls how much more rich pools (i.e., pools with rich pool leaders who can pledge more stake to their pool) bene\ufb01t.",
      "It follows that using \u03b1 we can control the disparity created by the reward mechanism by choosing \u03b1 closer to 0, with the choice \u03b1 0 achieving a perfectly egalitarian effect where rich pools and poor pools of the same size are receiving exactly the same rewards, something that does not affect the relative stake from epoch to epoch if we do not take into account margins. Note that while this completely equalises the power of a rich dollar versus a poor dollar (cf. 23) it comes with the downside of a reduction of the systems resilience to Sybil attacks. Given we have no way of guaranteeing the independence of the players as declared in the stake pool game, we offer a tradeoff between egalitarianism and Sybil resilience. Figure 7: The stake-distribution used for all experiments (but see the paragraph on other choices of the parameter at the end of this appendix 7). Experimental Results In this section we describe how the experiments were executed.",
      "Initialization We simulate 100 players, and we use k  10 for the desired number of pools. We assign stake to each player by sampling from a Pareto distribution7 with parameter 2, truncated to ensure that no player has higher stake than 1 k . The distribution is shown in Figure 7. Furthermore, we assign a cost to each player, uniformly sampled from cmin,cmax, where both cmin and cmax are con\ufb01gurable. Player strategies The following strategies are available for players:  A player can lead a pool with margin m 0,1. Alternatively, a player can delegate their stake to existing pools. They can freely choose how much stake to delegate to each pool, and they do not have to delegate all of their stake. Initially, there are no pools and all players play the second strategy and do not delegate any stake. When it is a players turn to move, they can freely switch to another strategy:  A pool leader can keep their pool, but change their margin, or close their pool and delegate to other pools.",
      "A player without pool can change its delegation or start a pool. If a pool leader decides to close their pool, all stake delegated to that pool by other players automatically becomes un-delegated. Simulation step In each step, we look for a player with a move that increases the players utility by a minimal amount8. If a player with such a move is found, we apply that move and repeat. If not, we have reached an equilibrium. We have to deal with the technical problem that for each player, there is an in\ufb01nity of potential moves to consider. We solve this problem in an approximate manner as follows:  For pool moves, instead of considering all margins in 0,1, we restrict ourselves to one or two margins, namely 1 (to consider the case where the player plans running a one-man pool) and the highest margin m  1 that has a chance (we make this precise below) to attract members (calculated to a precision of 1012 if such a margin exists).",
      "7Distributions of wealth tend to follow such a distribution, see 10, 34 and https:en.wikipedia.orgwiki Pareto_distribution. 8Speci\ufb01cally, we consider utility unew non-trivially better than utility uold iff unew  uold 108. For delegation moves, we approximate the optimal delegation strategy using a local search heuristic (beam search9). Furthermore, we restrict ourselves to a resolution of multiples of 108 of player stake. How players choose their strategy in a non-myopic way We have the problem of how to avoid myopic margin increases: It is always tempting for a pool leader to increase their margin (or for a delegating player to start a pool with a high margin), but such a move only makes sense if suf\ufb01ciently few other players have incentive to create more desirable pools during the next steps.",
      "To be more precise: If a player A contemplates running a pool with margin m  1, A wants players to delegate to their pool and saturate it (if they wanted to run a one-man pool instead, the margin would be irrelevant and could be set to 1). Only pools with rank k attract delegations and have a chance of becoming saturated, so running a pool with margin m only makes sense if the pool can reasonably be expected to end up with rank k. This means that when running such a pool, at most k 1 other players should have incentive to run more desirable pools. In order to determine whether m satis\ufb01es this condition, we look at all other players. For players who already run pools, we assume that they will continue running their pools and keep their margins. For each other player B, we check whether there exists a margin m such that by creating a pool with margin m and by assuming that that pool would have rank m k, B would increase its utility. Let B have stake s, costs c and utility u.",
      "If B manages to create a pool with rank m k, then that pools stake will be \u03c3 : max(s,\u03b2), and we can calculate its rewards r. Setting q : s \u03c3 and plugging in pool leader utility, we are looking for the minimal margin m satisfying (r c) m (1m)q  u. We see that r  c is a necessary condition. For q  1 (i.e. s \u03b2), m  0 is the obvious solution. For q  1, we get m  u (r c)q (r c)(1q), and we pick u(rc)q (rc)(1q) as margin for player B. We end up with a list of pools, one for each player, and we only allow A to consider their pool move with margin m if As pool would be amongst the k most desirable pools in this list. Note that this procedure of choosing the strategy represents the fact that players in our theoretical analysis try to maximize their non-myopic utility. Explaining the results The outcome of each simulation is a diagram with various plots, visualizing the dynamics, and a table with data describing the reached equilibrium.",
      "For the simulations reported here, we have always used the same stake distribution (sampled randomly from a Pareto distribution, as explained above) to make results more comparable (see Figure 7). dynamics displays the dynamic assignment of stake to pools. At the end of each simulation, once an equilibrium has been reached, we expect all stake to be assigned to ten pools of equal size. pools shows the number of pools over time  this should end up at ten pools. In the tables describing the equilibrium (all found in Appendix 7), the meaning of the columns is as follows: player Number of the player who leads the pool. Players are ordered by their potential P(s,c) (cost c, stake s). Our expectation is to end up with ten pools, led by players 110. 9See https:en.wikipedia.orgwikiBeam_search. Figure 8: Low costs, low stake in\ufb02uence (c 0.001,0.002, \u03b1  0.02). rk Pool rank. We expect our \ufb01nal pools to have ranks 110.",
      "crk Pool leaders cost-rank: The player with the lowest costs has cost-rank 1, the player with the second lowest costs has cost-rank 2 and so on. For low values of \u03b1, this should be close to the pool rank. srk Pool leaders stake-rank: The player with the highest stake has stake-rank 1, the player with the second highest stake has stake-rank 2 and so on. For high values of \u03b1, this should be close to the pool rank. cost Pool costs. margin Pool margin. player stake Pool leaders stake. pool stake Pool stake (including leader and members). reward Pool rewards (before distributing them amongst leader and members). desirability Pool desirability. We show the results of six exemplary simulations with various costs and values for parameter \u03b1 (which governs the in\ufb02uence of pool leader stake on pool desirability):  low costs and low \u03b1, see \ufb01gure 8 and table 1. lows costs and medium \u03b1, see \ufb01gure 9 and table 2. lows costs and high \u03b1, see \ufb01gure 10 and table 3.",
      "high costs and low \u03b1, see \ufb01gure 11 and table 4. high costs and medium \u03b1, see \ufb01gure 12 and table 5. high costs and high \u03b1, see \ufb01gure 13 and table 6. Figure 9: Low costs, medium stake in\ufb02uence (c 0.001,0.002, \u03b1  0.05). Additional experiments allowing simultaneous moves As explained above, in each simulation step we look for one player with an advantageous move and allow that player to make his move. In a real-world blockchain system however, players will probably be allowed to move concurrently, so we did some additional experiments allowing for this. Instead of picking just one player, we allowed several players with utility-increasing moves to make their move in one step. It is possible that such moves contradict each other (for example when one player closes a pool that a second player wants to delegate to). We handled this by applying the moves in order and dropping those that were invalid.",
      "Furthermore, in order to allow the system to stabilize, we blocked players from making pool moves (creating or closing a pool or changing the margin) too often by only allowing delegation moves for a number of steps after a player has made a pool move. Of course before we declare an equilibrium having been reached, we wait long enough to see whether any player wants to make a pool move after his waiting period is over. An example for \ufb01ve players being allowed to move simultaneously and a waiting period for the next pool move of 100 steps can be seen in \ufb01gure 14. Other choices for the parameter of the Pareto distribution In all experiments discussed until now we used the same stake distribution of players drawn from a Pareto distribution with parameter 2 (shown in Figure 7). We picked this parameter for resulting in an apparantly realistic distribution, but our results are not sensitive to this choice.",
      "To demonstrate this, we reran some of the experiments (for high costs and high \u03b1) for different parameter values. Parameter 1.1, see \ufb01gures 15, 16 and table 7. Parameter 1.3, see \ufb01gures 17, 18 and table 8. Parameter 1.5, see \ufb01gures 19, 20 and table 9. Conclusions and Future Directions We studied the design of reward sharing schemes (RSS) for collaborative projects, such as maintaining a PoS cryptocurrency, that promote decentralisation. Our main result is the design of an RSS that exhibits Figure 10: Low costs, high stake in\ufb02uence (c 0.001,0.002, \u03b1  0.5). Table 1: Low costs, low stake in\ufb02uence (c 0.001,0.002, \u03b1  0.02).",
      "player cost margin player stake pool stake reward desirability 0.00156856 0.00898774 0.07704926 0.10000000 0.10154099 0.099073896587544 0.00121229 0.00125302 0.02052438 0.10000000 0.10041049 0.099073896587539 0.00108188 0.00088317 0.01216771 0.10000000 0.10024335 0.099073896587511 0.00120205 0.00063505 0.01694531 0.10000000 0.10033891 0.099073896587553 0.00108805 0.00053598 0.01075376 0.10000000 0.10021508 0.099073896587558 0.00100213 0.00047005 0.00613080 0.10000000 0.10012262 0.099073896587589 0.00105867 0.00047469 0.00898080 0.10000000 0.10017962 0.099073896587522 0.00121088 0.00042690 0.01635433 0.10000000 0.10032709 0.099073896587534 0.00103849 0.00026601 0.00693720 0.10000000 0.10013874 0.099073896587515 0.00115913 0.00011986 0.01224503 0.10000000 0.10024490 0.099073896587504 an equilibrium that provides a desired level of decentralisation in conjunction to an ef\ufb01ciencySybil- resilience tradeoff that can be calibrated via suitable parameter selection.",
      "We describe how our RSS can be deployed in the context of a PoS system such as Ouroboros 25 addressing a number of attacks and possible protocol deviations. We view our work as a \ufb01rst step in the direction of understanding the design of mechanisms that promote decentralisation in a setting where multiple anonymous rational stakeholders operate and are interested in the maintenance of a collaborative project. There is a number of questions about reward schemes in this setting that are still open and worthy of further research which we hope our work will motivate. A \ufb01rst question is proving the uniqueness of equilibria with good properties and demonstrating rigorously, via a mathematical analysis, that such equilibria are ef\ufb01ciently reachable con\ufb01rming the dynamics we have observed experimentally. Furthermore, it is interesting to investigate RSSs with a more general reward structure.",
      "In our work, we have considered reward schemes in which the reward of each pool is divided among its members proportionally to their stake and that pool operators perform all the work bene\ufb01ting from their declared pro\ufb01t margin. An open research direction is to consider other organizational schemes in which the work and also the rewards of a pool are divided based on other factors, such as (weighted) Shapley value. With respect to Sibyl attacks and the rich get richer problem, in our scheme, increasing Sibyl-resilience affects inverse proportionally egalitarianism. An interesting research direction is to formally characterize the relationship between these two properties across the whole design space. Finally with respect to censorship, it is interesting to consider extended game theoretic models where censoring transactions is part of the strategic play Figure 11: High costs, low stake in\ufb02uence (c 0.05,0.1, \u03b1  0.02). Table 2: Low costs, medium stake in\ufb02uence (c 0.001,0.002, \u03b1  0.05).",
      "player cost margin player stake pool stake reward desirability 0.00156856 0.02828919 0.07704926 0.10000000 0.10385246 0.099390372362847 0.00173639 0.00742961 0.03741446 0.10000000 0.10187072 0.099390372362843 0.00121229 0.00424342 0.02052438 0.10000000 0.10102622 0.099390372362829 0.00156655 0.00297511 0.02507001 0.10000000 0.10125350 0.099390372362814 0.00120205 0.00255748 0.01694531 0.10000000 0.10084727 0.099390372362863 0.00121088 0.00217321 0.01635433 0.10000000 0.10081772 0.099390372362800 0.00108188 0.00136779 0.01216771 0.10000000 0.10060839 0.099390372362825 0.00152048 0.00090705 0.02002176 0.10000000 0.10100109 0.099390372362877 0.00108805 0.00059595 0.01075376 0.10000000 0.10053769 0.099390372362835 0.00115913 0.00063097 0.01224503 0.10000000 0.10061225 0.099390372362834 of pool leaders and pool members are on watch to protect the system (cf. Section 6). The authors would like to thank Duncan Coutts for extensive discussions and many helpful suggestions.",
      "The second author was partially supported by H2020 project PRIVILEDGE  780477. The third author was partially supported by the ERC Advanced Grant 321171 (ALGAME). 1 Nick Arnosti and S. Matthew Weinberg. Bitcoin: A natural oligopoly. CoRR, abs1811.08572, 2018. 2 K. Baclawski. Introduction to Probability with R. Chapman  HallCRC Texts in Statistical Science. CRC Press, 2008. 3 Umang Bhaskar, Lisa Fleischer, Darrel Hoy, and Cien-Chung Huang. Equilibria of Atomic Flow Games are not Unique. SODA, pages 748757, 2009. 4 Umang Bhaskar and Phani Raj Lolakapuri. Equilibrium computation in atomic splittable routing games with convex cost functions. arXiv preprint arXiv:1804.10044, 2018. Figure 12: High costs, medium stake in\ufb02uence (c 0.05,0.1, \u03b1  0.05). Table 3: Low costs, high stake in\ufb02uence (c 0.001,0.002, \u03b1  0.5).",
      "player cost margin player stake pool stake reward desirability 0.00156856 0.23109808 0.07704926 0.10000000 0.13852463 0.105305784121818 0.00173639 0.09972617 0.03741446 0.10000000 0.11870723 0.105305784121865 0.00156655 0.05102956 0.02507001 0.10000000 0.11253500 0.105305784121842 0.00121229 0.03433392 0.02052438 0.10000000 0.11026219 0.105305784121863 0.00181011 0.03273015 0.02135839 0.10000000 0.11067919 0.105305784121826 0.00152048 0.02935389 0.02002176 0.10000000 0.11001088 0.105305784121833 0.00120205 0.01831648 0.01694531 0.10000000 0.10847266 0.105305784121808 0.00121088 0.01552360 0.01635433 0.10000000 0.10817716 0.105305784121835 0.00163171 0.00394146 0.01470840 0.10000000 0.10735420 0.105305784121835 0.00181572 0.00298747 0.01487410 0.10000000 0.10743705 0.105305784121813 5 Lars Br\u00fcnjes, Aggelos Kiayias, Elias Koutsoupias, and Aikaterini-Panagiota Stouka. Reward sharing schemes for stake pools. CoRR, abs1807.11218, 2018. 6 Vitalik Buterin and Virgil Grif\ufb01th.",
      "Casper the friendly \ufb01nality gadget. CoRR, abs1710.09437, 2017. 7 Miles Carlsten, Harry A. Kalodner, S. Matthew Weinberg, and Arvind Narayanan. On the instabil- ity of bitcoin without the block reward. In Edgar R. Weippl, Stefan Katzenbeisser, Christopher Kruegel, Andrew C. Myers, and Shai Halevi, editors, Proceedings of the 2016 ACM SIGSAC Confer- ence on Computer and Communications Security, Vienna, Austria, October 24-28, 2016, pages 154167. ACM, 2016. 8 G. Casella and R.L. Berger. Statistical Inference. Duxbury advanced series in statistics and decision sciences. Thomson Learning, 2002. 9 Xi Chen, Christos H. Papadimitriou, and Tim Roughgarden. An axiomatic approach to block rewards. In Proceedings of the 1st ACM Conference on Advances in Financial Technologies, AFT 2019, Zurich, Switzerland, October 21-23, 2019, pages 124131. ACM, 2019. 10 David R. Clark. A note on the upper-truncated pareto distribution. 2013 Enterprise Risk Manage- ment Symposium.",
      "Figure 13: High costs, high stake in\ufb02uence (c 0.05,0.1, \u03b1  0.5). Table 4: High costs, low stake in\ufb02uence (c 0.05,0.1, \u03b1  0.02). player cost margin player stake pool stake reward desirability 0.05010638 0.14091746 0.00613080 0.10000000 0.10012262 0.042968072279390 0.05192430 0.10881324 0.00693720 0.10000000 0.10013874 0.042968072279376 0.05293337 0.09055053 0.00898080 0.10000000 0.10017962 0.042968072279392 0.05373632 0.07397050 0.00683225 0.10000000 0.10013665 0.042968072279365 0.05409406 0.06893326 0.01216771 0.10000000 0.10024335 0.042968072279396 0.05440243 0.06209143 0.01075376 0.10000000 0.10021508 0.042968072279369 0.05441660 0.06010553 0.00662237 0.10000000 0.10013245 0.042968072279379 0.05465632 0.05586892 0.00835115 0.10000000 0.10016702 0.042968072279383 0.05621197 0.02127128 0.00569447 0.10000000 0.10011389 0.042968072279383 0.05651394 0.01461751 0.00597063 0.10000000 0.10011941 0.042968072279394 11 Decred Contributors. Decred research overview.",
      "https:docs.decred.orgresearchoverview, 2018. 12 Phil Daian, Rafael Pass, and Elaine Shi. Snow white: Provably secure proofs of stake. Cryptology ePrint Archive, Report 2016919, 2016. http:eprint.iacr.org2016919. 13 Bernardo David, Peter Ga\u017ei, Aggelos Kiayias, and Alexander Russell. Ouroboros praos: An adaptively-secure, semi-synchronous proof-of-stake protocol. Cryptology ePrint Archive, Report 2017573, 2017. http:eprint.iacr.org2017573. To appear at EUROCRYPT 2018. 14 John R. Douceur. The sybil attack. In Revised Papers from the First International Workshop on Peer-to-Peer Systems, IPTPS 01, pages 251260, London, UK, UK, 2002. Springer-Verlag. 15 Ittay Eyal. The miners dilemma. CoRR, abs1411.7099, 2014. 16 Ittay Eyal and Emin G\u00fcn Sirer. Majority is not enough: Bitcoin mining is vulnerable.",
      "In Nicolas Christin and Reihaneh Safavi-Naini, editors, Financial Cryptography and Data Security - 18th International Conference, FC 2014, Christ Church, Barbados, March 3-7, 2014, Revised Selected Papers, volume 8437 of Lecture Notes in Computer Science, pages 436454. Springer, 2014. 17 Giulia Fanti, Leonid Kogan, Sewoong Oh, Kathleen Ruan, Pramod Viswanath, and Gerui Wang. Compounding of wealth in proof-of-stake cryptocurrencies, 2018. Figure 14: Low costs, low stake in\ufb02uence (c 0.001,0.002, \u03b1  0.02), allowing \ufb01ve players to make moves simultaneously, allowing pool moves every 100 rounds. Figure 15: The stake-distribution for parameter 1.1. 18 C. Fershtman and K. L. Judd. Equilibrium incentives in oligopoly. American Economic Review, 77(5):pp. 927  940, 1987. 19 Amos Fiat, Elias Koutsoupias, Katrina Ligett, Yishay Mansour, and Svetlana Olonetsky. Beyond myopic best response (in cournot competition). Games and Economic Behavior, 2013. 20 Ben Fisch, Rafael Pass, and Abhi Shelat.",
      "Socially optimal mining pools. In Nikhil R. Devanur and Pinyan Lu, editors, Web and Internet Economics - 13th International Conference, WINE 2017, Bangalore, India, December 17-20, 2017, Proceedings, volume 10660 of Lecture Notes in Computer Science, pages 205218. Springer, 2017. 21 A. Gervais, G. O. Karame, V. Capkun, and S. Capkun. Is bitcoin a decentralized currency? IEEE Security Privacy, 12(3):5460, May 2014. 22 Ian Grigg. Eos, an introduction. https:eos.iodocumentsEOS_An_Introduction.pdf, 2017. 23 Dimitris Karakostas, Aggelos Kiayias, Christos Nasikas, and Dionysis Zindros. Cryptocurrency egalitarianism: A quantitative approach. Tokenomics, International Conference on Blockchain Economics, Security and Protocols, 2019. Figure 16: High costs, high stake in\ufb02uence (c 0.001,0.002, \u03b1  0.5), Pareto parameter 1.1. Figure 17: The stake-distribution for parameter 1.3. 24 Aggelos Kiayias, Elias Koutsoupias, Maria Kyropoulou, and Yiannis Tselekounis. Blockchain mining games.",
      "In Vincent Conitzer, Dirk Bergemann, and Yiling Chen, editors, Proceedings of the 2016 ACM Conference on Economics and Computation, EC 16, Maastricht, The Netherlands, July 24-28, 2016, pages 365382. ACM, 2016. 25 Aggelos Kiayias, Alexander Russell, Bernardo David, and Roman Oliynykov. Ouroboros: A provably secure proof-of-stake blockchain protocol. Cryptology ePrint Archive, Report 2016889, 2016. http:eprint.iacr.org2016889. 26 Yujin Kwon, Jian Liu, Minjeong Kim, Dawn Song, and Yongdae Kim. Impossibility of full decen- tralization in permissionless blockchains. CoRR, abs1905.05158, 2019. 27 Dan Larimer. Delegated proof-of-stake consensus. https:bitshares.orgtechnology delegated-proof-of-stake-consensus, accessed 21.3.2018, 2018. 28 Nikos Leonardos, Stefanos Leonardos, and Georgios Piliouras. Oceanic games: Centralization risks and incentives in blockchain mining. CoRR, abs1904.02368, 2019. 29 A. Mas-Colell, J. Green, and M. D. Whinston. Microeconomic Theory.",
      "Oxford University Press, 1995. Figure 18: High costs, high stake in\ufb02uence (c 0.001,0.002, \u03b1  0.5), Pareto parameter 1.3. Figure 19: The stake-distribution for parameter 1.5. 30 Silvio Micali. ALGORAND: The ef\ufb01cient and democratic ledger, 2016. 31 Noam Nisan, Tim Roughgarden, Eva Tardos, and Vijay V Vazirani. Algorithmic game theory. Cambridge University Press, 2007. 32 A Orda, R Rom, and N Shimkin. Competitive Routing in Multiuse Communication Networks. IEEEACM Transactions on Networking, 1(5):510521, 1993. 33 Guillermo Owen. Game theory academic press. San Diego, 1995. 34 Vilfredo Pareto. Cours d\u00c8conomie Politique. Librairie Droz, Geneva, 1964. 35 Rafael Pass and Elaine Shi. Fruitchains: A fair blockchain. IACR Cryptology ePrint Archive, 2016:916, 2016. 36 Oran Richman and Nahum Shimkin. Topological Uniqueness of the Nash Equilibrium for Sel\ufb01sh Routing with Atomic Users. Mathematics of Operations Research, 32(1):215232, 2007.",
      "37 Matteo Romiti, Aljosha Judmayer, Alexei Zamyatin, and Bernhard Haslhofer. A deep dive into bitcoin mining pools: An empirical analysis of mining shares, 2019. 38 Meni Rosenfeld. Analysis of bitcoin pooled mining reward systems. CoRR, abs1112.4980, 2011. Figure 20: High costs, high stake in\ufb02uence (c 0.001,0.002, \u03b1  0.5), Pareto parameter \u03b1  1.5. Table 5: High costs, medium stake in\ufb02uence (c 0.05,0.1, \u03b1  0.05).",
      "player cost margin player stake pool stake reward desirability 0.05010638 0.14071474 0.00613080 0.10000000 0.10030654 0.043136255076695 0.05192430 0.10917025 0.00693720 0.10000000 0.10034686 0.043136255076684 0.05293337 0.09216778 0.00898080 0.10000000 0.10044904 0.043136255076688 0.05373632 0.07443446 0.00683225 0.10000000 0.10034161 0.043136255076709 0.05409406 0.07262429 0.01216771 0.10000000 0.10060839 0.043136255076679 0.05440243 0.06500457 0.01075376 0.10000000 0.10053769 0.043136255076675 0.05441660 0.06050948 0.00662237 0.10000000 0.10033112 0.043136255076690 0.05465632 0.05736264 0.00835115 0.10000000 0.10041756 0.043136255076704 0.05621197 0.02124899 0.00569447 0.10000000 0.10028472 0.043136255076672 0.05651394 0.01480748 0.00597063 0.10000000 0.10029853 0.043136255076696 39 Thomas C Schelling. An essay on bargaining. The American Economic Review, 46(3):281306, 1956. 40 Thomas C Schelling. The strategy of con\ufb02ict. Harvard university press, 1980.",
      "41 Okke Schrijvers, Joseph Bonneau, Dan Boneh, and Tim Roughgarden. Incentive compatibility of bitcoin mining pool reward functions. In Jens Grossklags and Bart Preneel, editors, Finan- cial Cryptography and Data Security, pages 477498, Berlin, Heidelberg, 2017. Springer Berlin Heidelberg. 42 S. D. Sklivas. The strategic choice of managerial incentives. RAND Journal of Economics, 18(3):pp. 452  458, 1987. 43 Itay Tsabary and Ittay Eyal. The gap game. CoRR, abs1805.05288, 2018. 44 J. Vickers. Delegation and the theory of the \ufb01rm. Economic Journal, 95 (Suppl.):pp. 138  147, 1985. 45 William Vickrey. Counterspeculation, auctions, and competitive sealed tenders. The Journal of \ufb01nance, 16(1):837, 1961. Table 6: High costs, high stake in\ufb02uence (c 0.05,0.1, \u03b1  0.5).",
      "player cost margin player stake pool stake reward desirability 0.05010638 0.08665199 0.00613080 0.10000000 0.10306540 0.048370013523060 0.05192430 0.06158357 0.00693720 0.10000000 0.10346860 0.048370013523048 0.05293337 0.06181530 0.00898080 0.10000000 0.10449040 0.048370013523073 0.05409406 0.06962481 0.01216771 0.10000000 0.10608385 0.048370013523074 0.05440243 0.05109295 0.01075376 0.10000000 0.10537688 0.048370013523051 0.05373632 0.02636475 0.00683225 0.10000000 0.10341613 0.048370013523057 0.05465632 0.02320800 0.00835115 0.10000000 0.10417557 0.048370013523048 0.05441660 0.01072862 0.00662237 0.10000000 0.10331119 0.048370013523058 0.07842812 0.19512770 0.07704926 0.10000000 0.13852463 0.048370013523062 0.06061469 0.02573105 0.02052438 0.10000000 0.11026219 0.048370013523056 Table 7: High costs, high stake in\ufb02uence (c 0.05,0.1, \u03b1  0.5), Pareto parameter 1.1.",
      "player cost margin player stake pool stake reward desirability 0.05140459 0.25044417 0.03864464 0.10000000 0.11932232 0.050908130626656 0.05721165 0.22717664 0.04616913 0.10000000 0.12308457 0.050908130626627 0.05285215 0.18214878 0.03019669 0.10000000 0.11509835 0.050908130626618 0.05366829 0.15131031 0.02730535 0.10000000 0.11365268 0.050908130626651 0.05345880 0.11798641 0.02235377 0.10000000 0.11117688 0.050908130626630 0.05015670 0.05887327 0.00849889 0.10000000 0.10424945 0.050908130626642 0.05365108 0.08659790 0.01877143 0.10000000 0.10938572 0.050908130626614 0.05075232 0.03739409 0.00727612 0.10000000 0.10363806 0.050908130626611 0.06397117 0.14410750 0.04690152 0.10000000 0.12345076 0.050908130626607 0.05017958 0.02093887 0.00435293 0.10000000 0.10217647 0.050908130626640 Appendices Proofs Proof for Fair Reward Sharing Scheme In the proofs we will assume that there exists at most one player with zero cost. Proof of Theorem 2 (I) We will prove it by contradiction.",
      "Let suppose that there is an equilibrium where there exist l pools with l  1. We order the pools according to the quantity ci \u03c3i . Let \u03c0j0 be the pool with the lowest value of this quantity denoted by c j0 \u03c3j0 . Then the members of all the other pools have incentives to delegate their stake to \u03c0j0. So this cannot be an equilibrium. Speci\ufb01cally let examine a pool member j of a pool \u03c0i. It holds ci \u03c3i  c j0 \u03c3j0 . The utility of player j if they leave their stake a j,i in \u03c0i is a j,i a j,i  ci \u03c3i . If they remove their stake from \u03c0i and delegates it to \u03c0j0 their utility will be a j,i a j,i  c j0 \u03c3j0a j,i which is strictly higher. Note that we have assumed that there exists at most one player with zero cost. So if there exists a player with zero cost then this player will be j0. (II) Firstly we will prove that the joint strategies that satisfy properties (i),(ii) and (iii) as de\ufb01ned by the theorem are indeed equilibria.",
      "The pool leader has no incentives to dissolve their pool because their utility is si si  ci \u03c3i that is greater or equal to zero, because 1 ci and \u03c3i  1. The pool members have no incentives to create their own pool because their current utility is s j s j  ci \u03c3i and is not lower than the utility they will get if they create their own pool which will be equal to s j c j, given that s j ci c j and \u03c3i  1. Secondly we will prove that there is no other equilibrium. By Theorem 2 we have that there is no equilibrium with more than one pools. We will prove by contradiction that (a) there is no equilibrium Table 8: High costs, high stake in\ufb02uence (c 0.05,0.1, \u03b1  0.5), Pareto parameter 1.3.",
      "player cost margin player stake pool stake reward desirability 0.05232917 0.11113014 0.01640414 0.10000000 0.10820207 0.049663741797333 0.05835834 0.15566652 0.03435679 0.10000000 0.11717839 0.049663741797347 0.06297005 0.16545389 0.04495987 0.10000000 0.12247994 0.049663741797342 0.05114911 0.03941328 0.00570116 0.10000000 0.10285058 0.049663741797336 0.05101953 0.02301505 0.00370642 0.10000000 0.10185321 0.049663741797352 0.05490394 0.05830496 0.01528522 0.10000000 0.10764261 0.049663741797344 0.05218122 0.01586887 0.00529155 0.10000000 0.10264578 0.049663741797329 0.05260393 0.01749394 0.00630391 0.10000000 0.10315195 0.049663741797356 0.08299881 0.22213113 0.09368943 0.10000000 0.14684472 0.049663741797341 0.06385628 0.01242114 0.02828932 0.10000000 0.11414466 0.049663741797354 Table 9: High costs, high stake in\ufb02uence (c 0.05,0.1, \u03b1  0.5), Pareto parameter 1.5.",
      "player cost margin player stake pool stake reward desirability 0.05232917 0.11860271 0.01605699 0.10000000 0.10802849 0.049093236751146 0.05835834 0.13684114 0.03046917 0.10000000 0.11523459 0.049093236751154 0.05114911 0.05702149 0.00642199 0.10000000 0.10321099 0.049093236751120 0.05101953 0.04099197 0.00442242 0.10000000 0.10221121 0.049093236751112 0.05490394 0.06749872 0.01510155 0.10000000 0.10755078 0.049093236751112 0.06297005 0.12744328 0.03846743 0.10000000 0.11923372 0.049093236751156 0.05218122 0.03413400 0.00601885 0.10000000 0.10300943 0.049093236751149 0.05260393 0.03548387 0.00700654 0.10000000 0.10350327 0.049093236751141 0.05357812 0.01836732 0.00717989 0.10000000 0.10358994 0.049093236751129 0.08299881 0.07968728 0.07268579 0.10000000 0.13634289 0.049093236751112 with zero pools and (b) there is no equilibrium with one pool but without (i),(ii) and (iii) properties: (a) Let assume that there is an equilibrium with no pools.",
      "Then player i0 for whom it holds si0  ci0 can increase their utility by creating their own pool. (b) Let assume that there is an equilibrium with one pool where (i) or (ii) does not hold. If (i) does not hold, then the pool leader of the pool has negative utility and thus they have incentives to dissolve their pool. If (ii) does not hold then a pool member can increase their utility by creating their own pool. Now we will prove that an arbitrary equilibrium with one pool satis\ufb01es (iii). We consider that in an equilibrium player i0 for whom it holds si0  ci0 is a pool member or a pool leader of the unique pool \u03c0i. This holds because if they did not participate at all then they could increase their utility by creating their own pool, but this cannot happen as we are in an equilibrium. As this player is pool leader or pool member the pro\ufb01t of the pool \u03c0i \u03c3i ci is positive.",
      "(This holds because otherwise player i0 has incentives to create their own pool.) So other players cannot have chosen to not participate at all in the equilibrium (which means that (iii) holds) because otherwise they could increase their utility by delegating to \u03c0i. Theorem 7. If it holds ci  1 for all players then there is exactly one equilibrium where no pool has been created. Proof. It can be trivially proved that indeed there is an equilibrium where no pool exists, because if a player i decides to create a pool then their utility will become negative (ai,i ci) that is strictly lower compared to their utility when they do not participate at all that is zero. In addition we can easily prove by contradiction that there is no equilibrium with one or more pools. Speci\ufb01cally let suppose that there exists. Then the pool leader of the pool has negative utility, so they have incentives to dissolve their pool.",
      "Proofs when (r(\u03c3,\u03bb)c)\u03c3 is strictly increasing or decreasing Proof of Theorem 3 I) We will prove it by contradiction. Let assume that there is an equilibrium where there are l  1 pools. Then we order the pools according to the quantity r(\u03c3i,ai,i)ci . Let \u03c0j0 be the pool with the highest value. Then this cannot be an equilibrium given that the members of the other pools have incentives to delegate their stake to \u03c0j0. In more detail, the utility of a player j that leaves their stake to pool \u03c0i is r(\u03c3i,ai,i)ci  a j,i and if they remove it and reallocate it to pool \u03c0j0 then it becomes r(\u03c3j0  a j,i,a j0,j0)c j0 \u03c3j0  a j,i  a j,i that is strictly higher. II) Let f (1,). We will \ufb01nd an assignment of costs and stake to the players so that there is no equilibrium with a number of pools smaller than the number of players n.",
      "In more detail: (i) the assignment of stake will be such that each player has stake 1 n (ii) the assignment of costs will be arbitrary except the minimum cost cmin and the maximum cost cmax that we will determine. Speci\ufb01cally we take cmax such that maxr( smin f r(smin),0  cmax  r( smin ) and we will determine cmin as follows:  It holds smin )cmax smin r( 1 n )cmax because r( smin )  cmax, smin n and r(\u03c3,\u03bb)c as a function of \u03c3 is strictly decreasing. Let r0 such that y  smin )cmax smin  r0  r( 1 n )cmax  We de\ufb01ne the function g(x)  r( 1 n )x which is continuous in c,cmax, where c such that smin )cmax smin r( 1 n )c  We know that there exists c (0,cmax) such that y  g(c) by the intermediate value theorem because (i) g(cmax)  y  g(0) given that maxr( smin f r(smin),0  cmax  r( smin ) and (ii) g(x) continuous in 0,cmax. Note that smin  1 n given that all players have stake 1  By the intermediate value theorem again there is x (c,cmax) such that g(c)  g(x)  r0  g(cmax).",
      "We set cmin  x the minimum cost among all players. Note that it holds g(c)  smin )cmax smin r( 1 n )cmin  g(cmin). Now we will prove that for these values of cmin,cmax and assignment of stake (1n for each player) there is no equilibrium with fewer than n pools. Firstly we will prove C A B where C the event where there exist fewer than n pools, A the event where there exists at least one player that has left some of their stake unallocated and B the event where there exists at least one pool with stake more than 1n. In order to prove C A B we will prove (A B) C which is equivalent to A B C. Let assume that there is no stake unallocated (so the total stake that is delegated to pools is 1) and all pools have stake at most 1n. If x is the number of the pools then the total stake that is delegated is at most x  1 n which means that x is at least n.",
      "Now in order to prove that there is no equilibrium with fewer than n pools it is suf\ufb01cient to prove that there is no equilibrium where the event A or the event B happens. Firstly we will prove by contradiction that there is no equilibrium where the event B happens. Let assume that there exists such an equilibrium where a pool \u03c0i has stake more than 1n . Then this pool has pool members given that si  1 n . Then player j who has delegated to this pool stake a j,i smin has incentives to remove stake smin from pool \u03c0i and create their own pool. This happens because the utility of player j from pool \u03c0i for this part of their stake is smaller than r( 1 n )cmin  smin which is smaller than smin )cmax smin  smin and thus smaller than their utility if they create their own pool with stake smin  We can easily prove by contradiction that there is no equilibrium where the event A happens. Let assume that there is a player j that has left some of their stake unallocated.",
      "If this part of stake is smin or more then they can increase their utility by creating their own pool given that r( smin )cmax  0. If this player has some stake x smaller than smin unallocated then this means that either (i) they have allocated stake to another pool and thus there exists a pool with stake more than 1n (this leads to contradiction as we have proved above) or (ii) they have created their own pool. In the latter case this player has incentives to include the unallocated part of stake to this pool given that the function r is strictly increasing (r(s j)  r(s j x)). Note that this pool will not have other members as we have proved above. Proof of Theorem 5 Proof. Consider the the equilibrium of Theorem 4, where the k players with the highest potential pro\ufb01t P(si,ci) have a saturated pool, cf. Proposition 2. Let n the number of players in the stake-pool game, some of which in a subset A are Sybil players controlled by an agent with stake s and cost c.",
      "For simplicity we drop the superscript A from s A i ,c A Let us suppose \ufb01rst that the agent creates t  1 Sybil players with stake s  st each, and claims cost c for each one equal to (i) cfake k2 , in the \ufb01rst case (where t  k2) where cfake is some arbitrary cost potentially below c, and (ii) ct, in the second case. The objective of the agent in the \ufb01rst case is to create k2 saturated pools so that it musters the highest possible in\ufb02uence in the system, and in the second case to maximize its utility by sharing the same server for all its Sybil players. We provide a lower bound in both cases for the stake the agent needs in order to create t saturated pools. After that, we generalize the above result by allowing the attacker to split its stake and cost arbitrarily among the Sybil players. Whenever needed, without loss of generality, we will break any ties in favor of the adversary.",
      "Firstly we will prove a lemma regarding when the attacker succeeds in creating t pools when the Sybil players are identical and play rationally without colluding. Lemma 3. An attacker with t identical Sybil players, with stake s and cost c each one, controls t saturated pools at the Nash equilibrium with rank at most k if and only if P(s,c) P(skt1,ckt1) Proof. () If P(s,c) P(skt1,ckt1) then at most k t players may have higher P(si,ci) value than the Sybil players and thus all the Sybil players will have a saturated pool. () if P(s,c)  P(skt1,ckt1) then there would exist at least k t  1 players with P(si,ci) higher than the agents players hence superseding all the players in the pool rankings. Thus at most t 1 Sybil players can have a saturated pool. This completes the proof of the lemma. Now we observe that (11) P(s,c) P(skt1,ckt1) s skt1 1 R (ckt1 c)(1 1 Finally, with respect to the two scenarios, we have the following.",
      "In the \ufb01rst scenario, the attacker does not care about cost and hence can set c  0. s  k 2 (sk21 cmax R (1 1 \u03b1)), we obtain s  st  sk21 cmax (1 1 \u03b1) skt1 1 R (ckt1 c)(1 1 by setting t  k2 and observing that ckt1c cmax, from which we obtain that P(s,c)  P(sk21,ck21) and hence the adversary fails to control k2 pools in the equilibrium. In the second scenario where it holds that c  ct , and s  t  skt1 (cmaxct) (1 1 we obtain s  st  skt1 (cmax ct) (1 1 \u03b1) skt1 1 R (ckt1 c)(1 1 recalling that c  ct and cmax ckt1. It follows that P(s,c)  P(skt1,ckt1) and hence the adversary fails to control t pools in the equilibrium. In order to generalize the above results so that they hold even if the stake and the cost of the attacker is split arbitrarily among the Sybil players we will prove by contradiction the following lemma. Lemma 4.",
      "If an agent splits its stake and cost among the Sybil players so that it succeeds in creating t saturated pools at the Nash equilibrium then it would succeed even if the split of stake and cost is done equally among the Sybil players. Proof. We will prove the theorem by contradiction. We will assume that the attacker splits its stake and cost among the Sybil players so that it has t saturated pools at the Nash equilibrium but if it splits them equally then it wont. Let s1,..., st and c1,..., ct the stake and the cost of the Sybil players. It holds s1 ... st  s and c1 ... ct c ( in the case of a utility maximizing agent). If the attacker has t saturated pools at the Nash equilibrium, then player k t 1 will not have a pool. If the (k t 1)-th player had a pool, then also players 1,2...,k t would have pools and the attacker would have at most t 1 pools. Recall that we break ties in favor of the attacking agent.",
      "As a result P(skt1,ckt1) is smaller or equal than the potential pro\ufb01t of all the Sybil players (recall that all the Sybil players have a pool and that at the Nash equilibrium the k players with the highest potential pro\ufb01t have a pool), otherwise k t 1 would have a pool. Thus it holds by equation 11 that i : si skt1 1 R (ckt1 ci)(1 1 \u03b1). Summing for all i, we obtain that s tX  1 R (1 1 \u03b1)!t i1 ci, where X  skt1 1 R ckt1 (1 1 If, on the other hand, the agent splits its stake equally and declares the same cost c, for some c 0, for all Sybil players but it does not succeed in having t saturated pools then by Lemma 3 and equation 11 we have st  skt1 1 R (ckt1 c)(1 1 \u03b1), which implies s  t  X  t  1 R (1 1 \u03b1)c. Combining the above constraints on s, we obtain that !t i1 ci  t c. In the case of a utility maximizer, we have that !t i1 ci  c and since we can set c  ct, we obtain a contradiction.",
      "In the case of a non-utility maximizer, on the other hand, we can set c  !t i1 cit, hence also obtaining a contradiction. Combining the above results, we arrive at the proof of the theorem. Proof of Theorem 6 Pr(S1  k 2 S k 2 1)  Pr(AS k 2 1  t) fS k 2 1(t)dt, by the Continuous Law of Alternatives (see Section 6.4 in 2)  2T Pr(AS k 2 1  t)F  2 1(t)dt  2T Pr(S1  k 2  t)F  2 1(t)dt  2T (1FS1(k 2  t))F  2 1(t)dt  2T (1F n X (k 2  t))F  2 1(t)dt  2T 2 1(t)dt  FS k 2 1(2T k )FS k 2 1(\u03b8)  FS k 2 1(2T  FXnk 2 (2T jnk F j X (2T k ) 1FX (2T nj  Pr(XB n k  Pr(XB (1\u03b4)\u00b5) e\u03b42\u00b53 where \u03b4  1( \u03b8 T )\u03b1 1( \u03b8k 2T )\u03b1 (1k 2n )1  0 and \u00b5  n FX ( 2T k ). Analysis of the Two-stage Game Now we will describe a set of joint strategies that (i) are approximate non-myopic Nash equilibria of the outer game and (ii) have the characteristic that in the inner games de\ufb01ned by these joint strategies, all the equilibria form k saturated pools. Recall that a pool is saturated when its stake is at least \u03b2.",
      "The pool leaders of these pools in these equilibria of the inner game are again the players with the highest values P(si,ci). Note that if all players activated a pool of size 1k with the same margin and their whole stake, then the k pools with the highest potential pro\ufb01t (P(si,ci)) would give the highest utility to their members. The intuition for how the set of margins of these joint strategies is determined is the following: The k players with the highest values P(si,ci) set the maximum margin they can, so that their pools belong to the k most desirable pools (the pools with the highest desirability), no matter which margins the other players set. Note that in this model, these players want their pools to have strictly higher desirability than the potential pool of player k  1, because in a tie in ranking, they might otherwise lose. In more detail: Let G  1,...,k be the set of those k players with the highest P(si,ci), \u03f5  P(sk,ck)P(sk1,ck1) and \u03f51  P(sk1,ck1)P(sk2,ck2).",
      "By assumption \u03f5,\u03f51  0. For \u03f5 such that 0  \u03f5  min\u03f5,P(sk1,ck1) and \u03b1 such that sk1 \u03b2  \u03b1  1, we de\ufb01ne m(\u03f5,\u03b1) as follows: i (\u03f5,\u03b1)  P(si ,ci )P(sk1,ck1)\u03f5(1\u03b1) P(si ,ci ) when i G, \u03f5\u03b1 P(sk1,ck1) when i  k 1, elsewhere, and let\u03bbbe the vectors with \u03bb i  si for i n. Note that margins are well de\ufb01ned because 0 m i  1. We prove that: (i) For each \u03f5 such that 0  \u03f5  min\u03f5,P(sk1,ck1),\u03f51, the joint strategies (m(\u03f5,\u03b1),\u03bb) sk1 \u03b2 \u03b11 are \u03f5-non-myopic Nash equilibria of the outer game. So we prove that for every \u03f5 as de\ufb01ned above, there is a class of joint strategies that are \u03f5-non-myopic Nash equilibria of the outer game (Theorem 9). (ii) For each \u03f5 such that 0  \u03f5  min\u03f5,P(sk1,ck1),\u03f51 and \u03b1 such that sk1 \u03b2  \u03b1  1, all the equilibria of the inner game determined by joint strategy (m(\u03f5,\u03b1),\u03bb) form k saturated pools (Theorem 8). (iii) In the general case (for any \u03b1), the pool leaders of the k saturated pools described above are the players of G, which are the players with the highest P(s,c).",
      "If \u03b1  0, the players with highest P(s,c) are the players with lowest cost, so this achieves an optimum in terms of social welfare in this case, since it minimizes the costs of running the system (Theorem 8). First, we will state a basic lemma that we will use in the proofs of the following lemmas and theorems. This lemma is a generalization of Lemma 2 and intuitively says that according to any joint strategy of any inner game, the utility that a player takes from allocating stake s to other active pools is upper bounded by Dmax  s\u03b2, where Dmax is the maximum desirability of all the other active pools. Formally: Lemma 5. For every joint strategy of the outer game (m,\u03bb) and for every joint strategy S(m,\u03bb) of the inner game determined by (m,\u03bb), it holds: For every player j that has allocated stake s to other active pools in4j:i active u j,i(S(m,\u03bb)) Dmax  s \u03b2, where Dmax is the maximum desirability according to S(m,\u03bb) of all the other active pools.",
      "Its proof is similar to the proof of Lemma 2. Recall that inactive pools in this model have desirability zero and their members (if they exist) take utility zero from these pools. A.5.1 Equilibria of the Inner Game Theorem 8. For every \u03f5: 0  \u03f5  min\u03f5,P(sk1,ck1),\u03f51 and for every \u03b1: sk1 \u03b2  \u03b1  1, it holds: A joint strategy S(m(\u03f5,\u03b1),\u03bb) of the inner game determined by (m(\u03f5,\u03b1),\u03bb) is a Nash equilibrium if and only if it forms k active, saturated pools, whose pool leaders belong to G. Proof. This can be proved by the following two Lemmas 6,7. Lemma 6. For every \u03f5: 0  \u03f5  min\u03f5,P(sk1,ck1,\u03f51 and for every \u03b1: sk1  \u03b1  1, it holds: In an inner game determined by (m(\u03f5,\u03b1),\u03bb), joint strategies S(m(\u03f5,\u03b1),\u03bb) that form k active saturated pools, whose pool leaders belong to G, are Nash equilibria. Proof. We omit this proof because it is similar to the proof of Theorem 4. Lemma 7.",
      "For every \u03f5: 0  \u03f5  min\u03f5,P(sk1,ck1),\u03f51 and for every \u03b1: sk1 \u03b2  \u03b1  1 it holds: In an inner game determined by (m(\u03f5,\u03b1),\u03bb), joint strategies S(m(\u03f5,\u03b1),\u03bb) that do not form k active saturated pools, whose pool leaders belong to G, are not a Nash equilibrium. Proof Sketch. Let \u03f5: 0  \u03f5  min\u03f5,P(sk1,ck1,\u03f51) and \u03b1: sk1 \u03b2  \u03b1  1. For simplicity we write m instead of m(\u03f5,\u03b1). First, we prove that there is no Nash equilibrium joint strategy S(m,\u03bb) for which there exist one or more players in G that have not chosen to activate their own pools. Second, we prove that there no Nash equilibrium joint strategy S(m,\u03bb) for which there exist one or more players G that have activated their pools. Last, we prove that there no Nash equilibrium joint strategy S(m,\u03bb) for which there exist one or more players who have allocated some of their stake to a pool with total stake more than \u03b2 or for which there exists some stake that is unallocated or has been allocated to an inactive pool.",
      "A.5.2 Equilibria of the Outer Game Theorem 9. For every \u03f5: 0  \u03f5  min\u03f5,P(sk1,ck1),\u03f51 and for every \u03b1: sk1 \u03b2  \u03b1  1 it holds: Joint strategy (m(\u03f5,\u03b1),\u03bb) is an \u03f5-non-myopic Nash equilibrium of the outer game. Proof. Let \u03f5: 0  \u03f5  min\u03f5,P(sk1,ck1),\u03f51 and \u03b1: sk1 \u03b2  \u03b1  1. For simplicity we write minstead of m(\u03f5,\u03b1). We will prove that i n and (mi,\u03bbi)  (m i ,\u03bb i ), it holds that uouter,up (mi, m i,\u03bbi,\u03bb i) uouter,low (m,\u03bb)\u03f5. Speci\ufb01cally, we will examine the following cases for i G:  (mi,\u03bbi), where 1 mi  m i and 0  \u03bbi si  \u03bb  (mi,\u03bbi), where 0 mi  m i and 0  \u03bbi si  \u03bb  (mi,\u03bbi), where mi  m i and 0  \u03bbi  si  \u03bb  (mi,\u03bbi)  (mi,0). For i G we will examine the case (mi,\u03bbi)  (m i ,\u03bb i ). These are all the ways in which players can change the strategy described in the theorem. For each case we will prove that in the inner game that is determined by (mi, m i,\u03bbi,\u03bb i), there is no equilibrium in which the utility of i is higher than uouter,low (m,\u03bb)\u03f5.",
      "The cases are described below:  First we will prove that no player i G has incentives to increase their margin more than m in the outer game, regardless of \u03bbi  0. By Lemmas 6,7 the only equilibria of the inner game determined by (m,\u03bb) are that ones where k saturated, active pools have been activated by players in G. So for every i G we have: uouter,low (m,\u03bb)  uouter,up (m,\u03bb)  (m i (1m i ) si \u03b2 )P(si,ci)  si \u03b2 (1m i )P(si,ci)  si \u03bbi (P(sk1,ck1)\u03f5 (1\u03b1))  \u03bbi \u03b2 (P(sk1,ck1)\u03f5 (1\u03b1)), where 0  \u03bbi si. 1. If an i G increases their margin by choosing mi with P(si ,ci )P(sk1,ck1)\u03f5\u03b1 P(si ,ci ) mi  m and chooses arbitrary \u03bbi with 0  \u03bbi si, then: uouter,up (mi, m i,\u03bbi,\u03bb max(mi (1mi) \u03bbi \u03b2 )P(si,ci)  si \u03bbi (P(sk1,ck1) \u03f5 (1\u03b1)), si \u03b2 (P(sk1,ck1)\u03f5 (1\u03b1)) i P(si,ci)\u03f5 (1m i ) si \u03b2 P(si,ci) uouter,low (m,\u03bb)\u03f5.",
      "Note that in the best case there is an equilibrium where (i) i has not activated their own pool, and their utility is at most si \u03b2 (P(sk1,ck1)\u03f5 (1\u03b1)) by Lemma 5 or (ii) i has activated their own pool, that belongs to the k most desirable pools, and has allocated the remaining stake to the active pools with the highest desirability, which are the pools of players in G. The utility that these pools will give them will be at most si \u03bbi (P(sk1,ck1)\u03f5 (1\u03b1)) by Lemma 5. If there is no equilibrium, recall that uouter,up (mi, m i,\u03bbi,\u03bb i)   2. If an i G increases their margin by choosing mi with 1 mi  P(si ,ci )P(sk1,ck1)\u03f5\u03b1 P(si ,ci ) chooses arbitrary \u03bbi  0, then we can prove that there is no equilibrium in the inner game determined by (mi, m i,\u03bbi,\u03bb i), where the non-myopic utility of i will be higher than their current lower utility of the outer game denoted by uouter,low (m,\u03bb).",
      "This happens because in the inner game determined by (mi, m i,\u03bbi,\u03bb i) we can prove that there is no an equilibrium where player k  1 and the other players of G have not activated their own pools (note that the desirability of \u03c0k1 and of the pools whose pool leaders belong to G, when they are active, are strictly higher than the desirability of \u03c0i, because mi  P(si ,ci )P(sk1,ck1)\u03f5\u03b1 P(si ,ci ) So in the best case, in the inner game determined by (mi, m i,\u03bbi,\u03bb i) (i) there is an equilibrium where player i has activated their own pool with rank worse than k and has delegated their remaining stake (si \u03bbi) to pools whose pool leaders belong to G, or (ii) there is an equilibrium where i has not activated their own pool and has delegated their whole stake to pools whose pool leaders belong to G, which have the highest desirability. We will prove that in both cases is non-myopic utility will not be higher than their current lower utility of the outer game denoted by uouter,low (m,\u03bb).",
      "As a result, uouter,up (mi, m i,\u03bbi,\u03bb i) uouter,low (m,\u03bb). In more detail:  In case (ii), by Lemma 5, their utility is at most (P(sk1,ck1)\u03f5 (1\u03b1)) si  uouter,low (m,\u03bb). In case (i), their utility is mi (1mi) \u03bbi (r(\u03bbi,\u03bbi)ci) (P(sk1,ck1)\u03f5 (1\u03b1)) si \u03bbi P(\u03bbi,ci)(0(10) \u03bbi \u03b2 ) (P(sk1,ck1)\u03f5 (1\u03b1)) si \u03bbi P(\u03bbi ,ci )P(si ,ci ),m i 0 uouter,low (m,\u03bb). We can prove that there is no equilibrium in the inner game determined by (mi, m i,\u03bbi,\u03bb i), where the players in G other than i have not activated their own pools, in the same way as the \ufb01rst case of the proof of Lemma 7. Note that also in this inner game determined by (mi, m i,\u03bbi,\u03bb i): (i) When these players activate their own pools, then these pools always have rank less or equal to k, regardless which other pools have been activated, and (ii) no other pool has strictly higher desirability and offers these players higher utility as members than they get by running their own pools.",
      "Now we will prove by contradiction that there is no equilibrium in the inner game de\ufb01ned by (mi, m i,\u03bbi,\u03bb i), where the (k 1)-st player does not activate their own pool. Let us suppose that there is a joint strategy S(mi ,m i ,\u03bbi ,\u03bb i ) that is an equilibrium of the inner game and for which player k 1 has not activated their own pool. Then by Lemma 5 it holds: uk1(S(mi ,m i ,\u03bbi ,\u03bb i )) (P(sk1,ck1)\u03f5 (1\u03b1)) sk1 Then if they choose a different strategy S(mi ,m i ,\u03bbi ,\u03bb i ) where they activate their own pool, their utility can be increased: uk1(S(mi ,m i ,\u03bbi ,\u03bb i ) (mi ,m i ,\u03bbi ,\u03bb i ) (k1)  P(sk1,ck1)(1 \u03f5 \u03b1 P(sk1,ck1)) sk1 \u03f5 \u03b1 P(sk1,ck1) P(sk1,ck1)  (P(sk1,ck1)\u03f5 (1\u03b1)) sk1 \u03f5  sk1 \u03f5 \u03b1  uk1(S(mi ,m i ,\u03bbi ,\u03bb i ))\u03f5  sk1 \u03f5 \u03b1 sk1 uk1(S(mi ,m i ,\u03bbi ,\u03bb i )). The intuition behind this is that k 1 can activate their own pool, that belongs to the k most desirable pools, and in this way can increase their utility, because of the margin that they will take.",
      "Note that the desirability of their pool is worse than the desirability of pools whose pool leaders are players in G, but the difference is small, and thus being pool leader is more pro\ufb01table for k 1 than being member of one of their pools. Recall that in the inner game determined by (mi, m i,\u03bbi,\u03bb i), if player k 1 activates their own pool, this pool belongs to the k most desirable pools, because P(sk1,ck1)\u03f5 \u03b1  P(sk2,ck2), where P(sk1,ck1)\u03f5 \u03b1 is its desirability. Note that the desirability of the other pools G is at most P(sk2,ck2) and that the desirability of \u03c0i, if it is activated, is also lower than P(sk1,ck1)\u03f5 \u03b1. So there exist at most k 1 pools activated with higher desirability than that of the (k 1)-st player, which causes player (k 1)s pool to belong to the k most desirable pools when it is activated.",
      "No player i G has an incentive to make their margin smaller than m i , given that their pool already belongs to the best k pools in all the equilibria of the inner game determined by (m,\u03bb). In more detail: If an i G decreases their margin by choosing mi  m i and chooses an arbitrary \u03bbi si, then in the best case there is an equilibrium of the inner game where \u03c0i will again belong to the k most desirable pools, and as a result: uouter,up (mi, m i,\u03bbi,\u03bb (mi (1mi) \u03bbi \u03b2 )P(si,ci)  si \u03bbi (P(sk1,ck1)\u03f5 (1\u03b1)) mi m i (1m i ) \u03bbi \u03b2 )P(si,ci)  si \u03bbi (P(sk1,ck1)\u03f5 (1\u03b1)) uouter,low (m,\u03bb). No player i G has incentives to commit less stake to their pool in the outer game, because the other existing pools have the same desirability and will not give it higher utility.",
      "In more detail: If a player i G chooses margin equal to m i , but \u03bbi  si  \u03bb i , then in the best case their pool will belong to the k most desirable pools, and using Lemma 5, we will have: uouter,up (m,\u03bbi,\u03bb (m i (1m i ) \u03bbi \u03b2 )P(si,ci)  si \u03bbi (P(sk1,ck1)\u03f5 (1\u03b1)) uouter,low (m,\u03bb). Player i G has no incentives to set \u03bbi  0, because using Lemma 5, we can prove that in any equilibrium of the inner game determined by (mi, m i,0,\u03bb i), where they have not activated their own pool, as \u03bbi  0, their utility for being a pool member of other pools will be lower than uouter,low (m,\u03bb).",
      "Player i G has no incentives to choose margin mi  m i or to commit stake \u03bbi less than si, because we can prove in the same way as in the second case of the proof of Lemma 7 that in the inner game determined by (mi, m i,\u03bbi,\u03bb i), there is no equilibrium where they have activated their own pool, so margin and stake committed to their own pool do not have an impact on their utility, which by Lemma 5 will be at most si \u03b2 (P(sk1,ck1)\u03f5 (1\u03b1)). For this proof, it is important that players G cannot lower the desirability of the pools activated by players in G by allocating stake to them strategically, because desirability does not depend on pool size. In addition to that, even if player k 1 sets margin zero, the desirability of their pool remains strictly lower than the desirability of the pools of all the players of G."
    ],
    "word_count": 24282,
    "page_count": 48
  },
  "ALGO": {
    "chunks": [
      "p align\"center\"a href\"https:twitter.comalgoworld_nftstatus1450608110268211203\"img width100 src\"https:i.imgur.comsHSLwaM.png\" border\"0\" ap p align\"center\" a href\"https:algoworld.io\"img src\"https:img.shields.iobadgeAlgoWorld-Website-black.svg\" a a href\"https:algoworldexplorer.io\"img src\"https:img.shields.iobadgeAlgoWorldExplorer-Platform-black.svg\" a p   About Source codes for AlgoWorld whitepaper content. Prerequisites - pre-commit(https:pre-commit.com) - rust (cargo)(https:www.rust-lang.orgtoolsinstall) - this step might be optional, but might be needed for MacOS users. Contribution guideline This section assumes that prerequisites are installed and steps below are run from the root folder of this repository. 1. Clone the repo bash git clone https:github.comAlgoWorldNFTalgoworld-whitepaper 2. Configure pre-commit hooks bash pre-commit install 3. Contribute and commit your changes 4. Submit a pull request and await for review"
    ],
    "word_count": 95,
    "page_count": 1
  },
  "API3": {
    "chunks": [
      "Decentralized APIs for Web 3.0 Burak Benligiray, Sa\u02c7sa Milic, Heikki Vanttinen v1.0.5 api3.org Abstract With decentralized applications beginning to provide meaningful services in areas such as decentralized finance, there is an increasing need for these appli- cations to receive data or trigger events using traditional Web APIs. However, the generic oracle solutions fail to appropriately address the API connectiv- ity problem due to an over-generalized and misguided approach. To remedy this issue, Api3 drives a collaborative effort to create a new generation of blockchain-native, decentralized APIs, or dAPIs for short. dAPIs are composed of first-party oracles operated by API providers, and thus are more secure and cost-efficient than alternative solutions that employ middlemen. At the core of the governance, security, and value capture mechanics of this initiative will be the API3 token.",
      "Staking the token grants governing rights over the Api3 DAO along with all the associated rewards. Staked API3 tokens will be used as collateral for the on-chain service coverage that will provide quantifiable and trustless security to dAPI users. These mechanics will remove the need for a central authority at the ecosystem level. As a result, the Api3 Project will allow smart contract platforms to leverage APIs for the building of meaningful applications in a truly decentralized and trust-minimized way. Api3: Decentralized APIs for Web 3.0 Introduction API Connectivity Problem Oracle problem: A source-agnostic misinterpretation . . . . . . . . . Decentralized APIs . . . . . . . . . . . . . . . . . . . . . . . . . . . . Issues with Third-Party Oracles as Middlemen Vulnerability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Middleman tax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Ineffective redundancy . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "Lack of transparency . . . . . . . . . . . . . . . . . . . . . . . . . . . Airnode: A Node Designed for First-Party Oracles Benefits of disintermediation . . . . . . . . . . . . . . . . . . . . . . 4.1.1 Off-chain signing of data . . . . . . . . . . . . . . . . . . . . . Barriers to API providers operating oracles . . . . . . . . . . . . . . Airnode features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Airnode protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . API integrations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Decentralizing Governance through Tokenomics Centralized oracle network governance . . . . . . . . . . . . . . . . . Management of funds . . . . . . . . . . . . . . . . . . . . . . . . . . Api3 DAO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Api3: Decentralized APIs for Web 3.0 API3 tokenomics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.1 Staking . . . . . . . . . . . . . . . . . . . . . .",
      ". . . . . . . . 5.4.2 Collateral . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.3 Governance . . . . . . . . . . . . . . . . . . . . . . . . . . . . Quantifiable Security through Service Coverage The need for quantifiable security . . . . . . . . . . . . . . . . . . . . dAPI Service Coverage . . . . . . . . . . . . . . . . . . . . . . . . . . Service coverage process . . . . . . . . . . . . . . . . . . . . . . . . . Risk Assessment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Scaling solutions and service coverage . . . . . . . . . . . . . . . . . Conclusion A Whitepaper Versions B Glossary Api3: Decentralized APIs for Web 3.0 Introduction We are witnessing the birth of decentralized applications that are able to interact with the real world, which is immediately reflected in the value they capture.",
      "The most prominent example of this phenomenon is the recent surge of value flowing into DeFi (decentralized finance) with more than 100B total value locked in various applications as of November 2021 1. A DeFi application typically requires asset prices to be delivered to its smart contract platform through a data feed 2. This data feed facilitates the applications interaction with the real world, ultimately allowing it to provide meaningful services such as derivative exchanges and lending. What is unfolding right now is not only the rise of DeFi but the rise of decentralized applications that can meaningfully interact with the real world, and DeFi is only the tip of the iceberg. Businesses offer a wide variety of services over Web APIs, ranging from providing asset price data to executing traditional financial transactions.",
      "It is critical for de- centralized applications to be able to access the kind of services that Web APIs offer in order to interact with the real world, yet these APIs are not natively compatible with decentralized applications. Existing middleman-based interfacing solutions are centralized, insecure, and expensive; and are only being used for lack of a better alternative. With Api3, we aim for the concept of an API to take the next evolu- tionary step to meet the inevitably strict decentralization requirements of Web 3.0 without employing third-party intermediaries. We will be using the term dAPI to refer to this new generation of decentralized APIs. A dAPI is a secure and cost-efficient solution to provide a traditional API service to smart contracts in a decentralized way.",
      "It is composed of the following elements:  multiple APIs, where the term API not only refers to a technical interface, but a service that a real-world business provides;  a decentralized network of first-party oracles, i.e., oracles operated by the API providers themselves;  a decentralized governing entity to oversee the oracle network. Api3 is a collaborative effort to build, manage and monetize dAPIs at scale. To achieve this in a fully decentralized way, the incentives of the participants are recon- ciled through the governance, security, and value capture utilities of the API3 token. The project has a completely open and direct governance model, where any API3 token holder is able to stake to obtain direct voting privileges in the Api3 DAO.",
      "In addition, stakers will benefit from dAPI usage through inflationary staking rewards Api3: Decentralized APIs for Web 3.0 Stakers API Providers dApps Staking Pool dAPIs vote stake  earn rewards use dAPI subscription pays pays out valid claims supply data Api3 DAO Figure 1: Overview of Api3 mechanics. and any additional benefits that the DAO may decide on in the future. The staked API3 tokens will collateralize on-chain service coverage to provide dAPI users with quantifiable and trustless security against qualifying malfunctions (see Figure 1). One of the fundamental flaws of existing oracle solutions is attempting to establish and maintain a parasitic connection with the data sources, which cannot produce a sustainable ecosystem. In contrast, we start off with the recognition that the API providers are the engine of this project.",
      "Therefore, they will not be abstracted away, but rather be attributed and compensated so that their interests are fully aligned with the interests of the greater Api3 ecosystem. We have already witnessed API providers eagerness in incentivizing adoption of their services by decentralized applications through providing free testnet calls for their paid APIs 3 and offering cash prizes for hackathons 4. Cultivating this cooperation further is one of the main sources of strength of Api3. Decentralized oracle network solutions employ third-party oracles because it is often not feasible for the API providers to operate their own oracle nodes. This positions third-party oracles as expensive middlemen and forms an additional attack surface. To eliminate these problems and have the API providers engage in the ecosystem Api3: Decentralized APIs for Web 3.0 further, Api3 data feeds will be composed of first-party oracles operated by the API providers.",
      "This will be made possible by Airnode, a fully-serverless oracle node that is designed to require no blockchain know-how, maintenance, or upkeep from the API provider. The resulting dAPIs will be cost-efficient and secure against attacks from an intermediate layer of third parties. In the case of a malfunction, the dAPI user will be able to claim compensation up to a pre-negotiated amount from the staking pool. Kleros 5, an on-chain dispute resolution protocol, will be used to decide if the claim is to be paid out pursuant to the service coverage terms and conditions based on the presented evidence. This will incentivize stakers to actively participate in governance to ensure that dAPIs are being managed transparently and in a way that minimizes security risks.",
      "Successful governancesuccessfully monetizing dAPIs while avoiding mistakes that will result in paying out service coverage claimswill be rewarded in API3 tokens, which will create a positive feedback loop that will continually improve governance. Refer to Figure 1 again for an overview of our solution. dAPIs are networks of first- party oracles that provide traditional API services in a decentralized and blockchain- native way. The Api3 DAO builds, manages and monetizes dAPIs at scale. Decen- tralized applications use the API3 token to gain access to a dAPI. API3 token holders stake into a pool to receive rewards and voting rights at the DAO. This staking pool is used as collateral for on-chain service coverage that provides dAPI users with a quantifiable level of security. Api3 improves upon the existing ora- cle solutions in terms of decentralization, cost-efficiency, security, transparency and ecosystem growth potential.",
      "API Connectivity Problem An application programming interface (API) is a well-standardized and documented protocol that is used to communicate with a specific application to receive services from it. These services may be in the form of receiving data or triggering an event. Applications can communicate with each other through their APIs, allowing them to be integrated to build more complex applications. Because of this, APIs are called the glue that holds the digital world together. As a result of businesses using APIs to monetize their data and services, the con- cept of an API has transcended its initial meaning. The term no longer refers to the technical implementation of an interface, but to a full-fledged product that in- cludes the service it wraps 6. Global enterprises that provide APIs generate 25 of their organizational revenue from APIs on average 7.",
      "Companies such as Sales- force, Expedia and eBay have reported to generate the majority of their revenue Api3: Decentralized APIs for Web 3.0 through APIs 8, and we are at the cusp of fully API-centric business models 9. Even entrenched industries such as banking are expected to be disrupted by this movement 10. Integrating existing services to their applications through APIs has allowed develop- ers to build increasingly complex and capable applications, which has led to the rise of giant Web services and mobile applications. However, these APIs that businesses deliver their services through are not directly compatible with smart contracts due to technical reasons that will be described in Section 2.1, which has curbed the de- velopment of meaningful decentralized applications. Therefore, the difficulty we are facing in building decentralized applications that can interact with the real world can presently be best described as the API connectivity problem.",
      "Misinterpreting this problem will lead to a sub-optimal solution. 2.1. Oracle problem: A source-agnostic misinterpretation Decentralization defines Web 3.0, which is characterized by distributing computation and settling outcomes through predetermined consensus rules 11. The business logic of a decentralized application is implemented as a smart contract 12, which runs on a blockchain-based smart contract platform 13. Decentralization allows participants to cooperate without requiring mutual trust or a trusted third-party, and thus provides robustness against attacks and censorship. To enforce consensus rules, smart contract platform nodes have to verify that each contract invocation has resulted in the correct outcome by repeating the computa- tion locally. For this to be possible, smart contracts can only operate on information that is accessible to and agreed upon by all smart contract platform nodes.",
      "In sim- pler terms, smart contracts can only operate on the information that is readily available in the blockchain, and cannot interact with the outside world directly. This is widely known as the oracle problem, referring to an idealized agent that can deliver an arbitrarily defined piece of truth to the blockchain. The oracle problem is ill-posed, as even its name suggests an impossible solution. An analogy would be to approach the problem of getting from Point A to Point B as the teleportation problem. Nevertheless, the first generation of solutions attempted to implement this literal oracle by posing a question and crowdsourcing its answer, which produces resolution times measurable in days and extreme gas costs due to the number of transactions that need to be made 14, which is not ideal for use cases such as DeFi or prediction markets. We must note that this approach is indeed suitable if the information to be delivered is subjective.",
      "A good example would be the resolution of a judicial dispute 5. Api3: Decentralized APIs for Web 3.0 The second generation solutions narrowed their scope to only cover factual informa- tion that can be accessed programmatically, and ended up with the interoperability problem. In these solutions, an oracle node that is integrated to two arbitrary sys- tems (a blockchain and an API, two blockchains, etc.) acts as an automated inter- mediary between the two 1518. Using multiple of these oracle nodes and settling on the outcome through predetermined consensus rules provides security guarantees that complement the underlying blockchain technology. These solutions are faster and less expensive compared to crowdsourced oracles, and consequently are viable for more use cases, yet they suffer from being designed around an over-generalization of the problem at hand. Interoperability solutions involve three parties: API providers, oracles, and data consumers 19.",
      "However, they fall into the pitfall of modeling their ecosystem as being solely composed of oracles and data consumers, while ignoring where the data originates from. In other words, their models treat the oracle node as the mythical oracle that is the source of the truth. Being blind to one-third of the problem in this way results in impractical solutions to be perceived as feasible. The interoperability solution being source-agnostic results in the following conse- quences:  An intermediate layer of insecure and expensive third-party oracles, which could have been superseded by API provider-operated oracles;  An ecosystem that nurtures rent-seeking middlemen, while excluding the ac- tual sources of the data;  Indiscriminate treatment of data received from different sources in a data feed. Another pervasive issue with interoperability solutions is that since they are low- level protocols, they regard the interface as a technical component, or middleware, rather than a complete product.",
      "As a side effect, the governance of the interface gets left out-of-scope. However, governance is hardly a trivial problem, because a decentralized interface requires independent and competing parties to cooperate. The currently utilized solution is a trusted centralized entity to govern the interface, which is at odds with the main objective of decentralization. The governing entity has full control over the output of an oracle network, which means a decentralized oracle network with centralized governance is a centralized oracle with extra steps. Api3: Decentralized APIs for Web 3.0 dApp Aggregator Centralized Governance Oracle 1 Oracle 2 Oracle 3 (a) Decentralized interoperability solution dApp Decentralized Governance API 1 API 2 API 3 Aggregator (b) Decentralized API (dAPI) Figure 2: Decentralized interoperability solutions employ third-party oracles that do not natively reveal their sources.",
      "dAPIs are composed of first-party oracles, meaning that API providers operate their own Airnodes. In addition, dAPIs are decentralized in how they are governed, resulting in end-to-end decentralization. 2.2. Decentralized APIs The issues of the previous generation of interoperability solutions can only be solved by taking a new perspective: The problem at hand is in essence the problem of decentralized applications not being able to receive services from traditional API providers in a decentralized way. Indeed, the primary use of interoperability solu- tions today is to deliver asset prices curated from centralized exchange APIs to DeFi applications, and emerging use cases such as prediction markets 20 and parametric insurance 21 all have similar requirements. Therefore, further specifying the prob- lem definition as such will allow us to arrive at the next generation of real-world interconnectivity solutions.",
      "This new definition of the problem implies that decentralized applications require specific Web API services to be delivered to the blockchain and this to be done in a fully decentralized, cost-efficient and secure way. Determining the requirements allows us to design a full product that satisfies them optimally: Decentralized APIs, or dAPIs for short, are networks of API provider-operated first-party oracles that are governed in a decentralized way. In contrast, decentralized interoperability solutions consist of an oracle network of third-party middlemen governed by a centralized entity, which is necessitated by their under-specified problem definition. See Figure 2 for a visual comparison. Api3: Decentralized APIs for Web 3.0 Issues with Third-Party Oracles as Middlemen Existing solutions envision an abstract problem where an arbitrary system needs to be able to interoperate with another arbitrary system through their technical interfaces in a very general sense.",
      "This over-generality necessitates an ever-flexible interface that can only be supported by third-party oracles. However, this solution is not optimal because the practical scope of the problem is far more constrained. In most cases, the decentralized interoperability problem is actually the problem of receiving services from traditional API providers in a decentralized way. This more limited definition of the problem allows for optimal solutions that do not require a third-party intermediate layer on the interface path. Through the rest of this section, we will discuss the consequences of relying on middlemen as a part of an interoperability solution. 3.1. Vulnerability A decentralized oracle network uses an aggregation function to reduce the oracle reports into a single answer. This function is essentially a consensus algorithm, and as all consensus algorithms, it is susceptible against a certain ratio of dishonest ac- tors.",
      "This means that a group of malicious oracles can collude to skew the outcome, and even control it completely. In addition, a single actor can fabricate multiple oracle node operator identitiesas well as build a sufficient track record of honest operationto perform the same types of attacks entirely by themselves, which is known as a Sybil attack 22. The most critical downside of having an additional layer of parties on the interface path is the formation of entirely new attack surfaces. This means that each added layer of middlemen would be able to execute the collusion and Sybil attacks de- scribed above independently. Then, in terms of security, the ultimate solution is the complete removal of the middlemen. 3.2. Middleman tax An oracle plays a game where they can report honestly or misreport (which includes denying service). Reporting honestly has only an incremental payoff, but allows the oracle to continue playing the game.",
      "On the other hand, misreporting has a one-time payoff proportional to the value secured by the contracts depending on the report, yet results in the game ending (see Figure 3). Then, the maximum cumulative payoff the oracle can receive starting from transaction ti is Api3: Decentralized APIs for Web 3.0 report correctly (v1) report correctly (v2) report correctly (v3) misreport (x1) misreport (x2) misreport (x3) Figure 3: A decision tree that describes the actions that an oracle can take and their outcomes. At a given transaction ti, an oracle can report honestly and gain vi or misreport and gain xi. A dishonest action results in the oracle no longer being used, i.e., an end to the game. P i  max (xi, vi  P i  1) . A rational oracle will eventually misreport if the amount it can gain from an attack outweighs the potential gains it can make if it did not perform the attack. That is, if the following holds for a given rational oracle, it will eventually misreport: i N, xi  vi  P i  1 .",
      "This indicates that the potential benefit an oracle will gain from acting honestly must exceed the amount that can be gained from misreporting at all times to avoid any misreporting. Although one can approximate v with the amount paid to the oracle per-request and x with the amount that is secured by the oracles response, this would underestimate the risk because there are additional factors that incline oracles towards misreporting, some of which are given below:  According to the time preference theory 23, the oracle node operator will value future rewards less (i.e., vi decays with increasing i). In practice, the oracle acting honestly does not guarantee the game to continue and this risk further lessens the value of future rewards. There may be additional benefits to performing an attack that are unaccounted for, e.g., opening a short position on an asset that will depreciate with the oracle solutions failure.",
      "Due to this uncertainty, one needs to overestimate the required vi, i.e., overpay the oracle for it to not attack. Api3: Decentralized APIs for Web 3.0 dApp API 1 API 2 Aggregator (a) Data feed composed of third-party oracles dApp API 1 API 1 API 1 API 1 API 2 API 3 Aggregator (b) Data feed composed of first-party oracles Figure 4: Using third-party oracles requires over-redundant decentralization at the oracle level, while first-party oracles provide a better degree of decentralization in a more secure and cost-efficient way. This model can be extended to decentralized oracle networks. Since the oracle reports or their artifacts are recorded on-chain, it is trivial to implement a smart contract that will reward colluding oracles trustlessly. This means that a third-party that is able to profit from an attack can employ oracles with the guarantee that they will be paid a deterministic amount if they collude.",
      "At a high-level, an oracles job is essentially: (1) to listen for on-chain requests, (2) to make the respective off-chain API calls, and (3) to write the response back on-chain. Therefore, a third-party oracle is fundamentally a middleman. Although the service provided is as minimal as possible, these middlemen have to be paid proportionally to the amount being secured by the data feed due to the reasons described above, which is especially problematic for high-value use cases such as DeFi. We call this phenomenon the middleman tax, which can be eliminated completely by avoiding third-party oracles, resulting in very significant cost savings for users. Api3: Decentralized APIs for Web 3.0 3.3. Ineffective redundancy Data feeds depending on third-party oracles require over-redundancy at the oracle level (see Figure 4). This is because third-party oracles are far less trustworthy than API providers, the latter having a traditional off-chain business and respective reputation to maintain.",
      "Typically, each API provider is served by 23 oracles in such a data feed. Note that this decentralization does not provide additional security at the data source level, but only decreases the additional vulnerability caused by using third-party oracles. Unfortunately, this results in the operation costs being multiplied on many levels. For example, the data feed essentially employs all the technical personnel that operate the oracle nodes, and having more of these nodes means supporting more people. Furthermore, using more oracles results in a direct increase in gas costs. Specifically, oracle requestresponse gas costs increase linearly with the number of oracles, while the gas costs of aggregation functions that do combinational operations (e.g., median) increase superlinearly. 3.4.",
      "Lack of transparency Decentralization at the API level and decentralization at the oracle level are in- dependent of one anotherthe overall system is only as decentralized as the more centralized of the two, i.e., the weakest link. However, the general public and even the users of decentralized oracle networks overlook this fact and confuse decentral- ization at the oracle level with the overall decentralization of the system. This is primarily caused by a lack of transparency regarding the data sources used by the oracles 24, which disguises the fact that decentralization is severely bottlenecked at the data source (API) level. Data feeds composed of third-party oracles appear more decentralized than they actually are. In addition, when the data feeds are not transparent in the source of their data, developers cannot appraise data feed integrity and have to trust the governing entity.",
      "However, there is no immediate incentive for the governing entity to choose quality over lower prices and convenience if the data sources are not transparent, which may result in the outcome commonly referred to as garbage in, garbage out. Interestingly, what is a favorable tactic for the governing entitynamely, obscur- ing the data sourceis very much necessary for the third-party oracle. Most API terms of service prohibit the resale or unauthorized distribution of the API data, which positions an oracle node operator serving such APIs to be in breach of those terms and susceptible to broad sources of legal liability including claims by the API provider 25. This issue is exacerbated by the API call times, responses, and pay- Api3: Decentralized APIs for Web 3.0 ments all being recorded on a public blockchain.",
      "This not only puts individual node operators at litigation risk, but also creates a systemic risk for the whole oracle network, as coordinated legal action at scale would put existing third-party oracles out of operation immediately and discourage new ones from joining. Note that although lack of transparency and abstraction of data sources is the norm, it is not at all a necessity. Especially when the API provider and ecosystem incentives are aligned, it is perfectly possible for oracles to serve API data to users with the express consent of the API provider, allowing the oracles to disclose their data sources to their users 3. It is in the interest of the API providers to do this, as it increases on-chain demand for their data. Airnode: A Node Designed for First-Party Oracles First-party oracles are integral to the Api3 solution. This means each API is served by an oracle that is operated by the entity that owns the API, rather than a third- party.",
      "In this section, we will discuss the benefits of using first-party oracles and how Api3 makes it feasible for API providers to operate their own oracles with Airnode. 4.1. Benefits of disintermediation There is a simple solution to all problems discussed in Section 3: First-party oracles; that is, oracles operated by the API providers themselves. API providers operating their own oracles means they would be signing their responses with their private keys at the smart contract platform protocol-level, which is the best proof that the data is not tampered with. Moreover, first-party oracles are private by default, as a third party cannot observe the raw data from the API being processed, which allows them to be used in a wider variety of use cases natively.",
      "A data feed composed of first-party oracles would be more cost-efficient compared to one employing middlemen, as one needs to pay middlemen both for their services and to incentivize them against attacking the data feed (referred to as the middleman tax in Section 3.2). In addition, a data feed composed of first-party oracles will need fewer oracles, as it would not need over-redundant decentralization at the oracle level to protect against attacks from third-parties. Assuming that each API is typically served by at least two third-party oracles, data feeds powered by first-party oracles would be at least 50 more efficient in terms of gas costs, by a conservative estimate. First-party oracles also provide much needed transparency in terms of the data source and the degree of decentralization.",
      "Since each API provider will operate an Api3: Decentralized APIs for Web 3.0 oraclewhich will be visible on-chainthe number of oracles serving a data feed will accurately represent how decentralized it is, as there is a one-to-one mapping between oracle and data source. Furthermore, the API providers publish their on- chain identities through off-chain channels, which allows the users to verify whose data they are consuming at a given time. Finally, having the API providers operate the oracles solves the legal issues men- tioned in Section 3.4, as the API services no longer need to be licensed to a third party and the API providers receive the entire revenue. Furthermore, this solves the rent-seeking third-party oracles problem, and allows the funds to be redirected to the group that is doing the heavy lifting, the API providers. Incentivizing API providers aligns their financial interests with the ones of the Api3 ecosystem, resulting in a strong mutual bond between the two. 4.1.1.",
      "Off-chain signing of data There is a hybrid solution that still depends on third-party oracles, yet does not let them tamper with the data. In this scheme, the API provider signs their data with their private key off-chain and serves it over a regular API endpoint. Third- party oracles call this endpoint to get the signed data and post it to the chain. The authenticity of the datathat it is not tampered with by the third party oracles can then be verified on-chain using the public key of the API provider 26. Although it eliminates the risk of data tampering at the oracle level, this solution is essentially a half-measure. By depending on third-party oracles, it continues suffering from the ecosystem issues caused by depending on third-party oracles, and, in addition, requires modifications at the API-side to implement off-chain signing.",
      "This results in a severely limited API selection even compared to the regular third- party oracle based solutions, and restricts the ecosystem growth potential of the solution to the application-scale. 4.2. Barriers to API providers operating oracles During our work on Honeycomb API Marketplace 19, we communicated with API providers extensively and observed the following barriers to oracle onboarding and operation: 1. Traditional API providers are typically not more familiar with blockchain tech- nologies than the general public. This applies even for the ones that curate cryptocurrency market dataas their main operation is collecting data from Api3: Decentralized APIs for Web 3.0 deploys once self-operates serves API Provider Airnode Smart Contracts Figure 5: Airnode is designed to be deployed once by the API provider, then not require any further maintenance.",
      "exchange APIs, processing them, and serving the result through their own APIswhich does not require any blockchain-specific know-how. Therefore, they typically cannot readily operate an oracle node with in-house resources. 2. There is no job market for oracle node operators. Even if some API providers were to obtain the specific know-how needed by hiring the few node operators that are available, this would not be a scalable solution. 3. Operating an oracle node consumes a lot of resources in the form of man- hours and infrastructure costs. Unless one is guaranteed significant subsidies or future profits, operating an oracle node is financially infeasible. 4. Operating an oracle node requires the API provider to transact with cryptocur- rencies. Specifically, they must pay for gas costs in the native currency (e.g., ETH) and receive payments in one or more cryptocurrencies. This disqualifies the vast majority of API providers due to compliance, legal and accounting reasons.",
      "In addition, any scheme that requires API providers to stake funds is categorically rejected for similar financial risk-related reasons. 4.3. Airnode features Airnode is a fully-serverless oracle node that is designed specifically for API providers to operate their own oracles (see Figure 5). It addresses all of the oracle node-related problems in Section 4.2: Api3: Decentralized APIs for Web 3.0 1. It does not require any specific know-how to operate. In fact, it is difficult to even speak of an operation, as Airnode is designed to be completely set and forget. 2. It does not require any day-to-day maintenance such as updating the operating system or monitoring the node for uptime owing to existing fully managed serverless technology. It is designed to be stateless, which makes it extremely resilient against any problem that can cause permanent downtime and require node operator intervention. 3.",
      "It is built on services priced on-demand, meaning that the node operator is charged only as much as their node is used. This allows any API provider to run an oracle for free and start paying only after they start generating revenue. 4. It does not require the node operator to handle cryptocurrency at all. Its protocol is designed in a way that the requester covers all gas costs. One way to see Airnode is as a lightweight wrapper around a Web API that allows it to communicate with smart contract platforms with no overhead or payment token friction. Regarding the level of involvement required from the API provider, using Airnode can be likened to utilizing an API gateway that makes an API accessible over the Web, rather than operating a blockchain node as a side-business. In fact, our aim is for Airnode to become as ubiquitous and mundane for APIs as using an API gateway, which will make a vast variety of first-party oracles available to Api3.",
      "API providers invest significant resources to build a highly available infrastructure. Then, it is important for the oracle node implementation to not contain single points of failure that may cause downtime. Existing solutions using third-party oracles depend on over-redundancy at the oracle level to cover for this, which results in excessive costs as mentioned in Section 3.3. Api3 envisions each API to only be served by its first-party oracle, which means the redundancy has to be implemented at the level of the individual Airnode. The node being fully-serverless enables this to be done easily across different availability zones of a single cloud provider, or even across multiple cloud providers. It should also be mentioned that it is possible to operate the containerized version of Airnode on-premises, yet using the serverless version will be recommended for almost all use cases. Airnode is open-sourced with MIT License and ready to enable first-party oracles1.",
      "Api3 will fund Airnode development through grants, and the core technical team is its current maintainer. 1https:github.comapi3daoairnode Api3: Decentralized APIs for Web 3.0 4.4. Airnode protocol Similar to how we prefer the better specified API connectivity problem over the oracle problem, we believe that an oracle node should be designed to interface APIs to smart contract platforms very well, rather than as a sandbox that can purport- edly be used for any purpose imaginable. Based on this philosophy, the Airnode protocol is designed to follow the self-emergent patterns used by APIs to achieve as transparent and frictionless of an APIsmart contract platform interface as possible. The first and the most commonly used API style follows the requestresponse pat- tern, where the user makes a request with parameters and the API responds as soon as possible.",
      "This will be the first pattern that Airnode will support, as it is easy to standardize and integrate with existing APIs that follow the same pattern. An example use case of this scheme would be requesting the result of a specific match to be delivered, which can be used to resolve the respective prediction market. In addition, Airnode is planned to support the publishsubscribe pattern, where the user requests the oracle to call back a specific method when parametrized conditions are met. For example, a decentralized exchange may request the oracle to trigger a liquidation event for a user in a leveraged position when ETH price drops below 4000. Either of these patterns can be used to implement the live data feeds that DeFi applications use today 2, but they can also support a much larger variety of use cases in the form of dAPIs.",
      "As mentioned in Section 4.3, the Airnode protocol is designed in a way that the requester assumes all gas costs, even including the request fulfillment transactions. This is achieved by each Airnode having a separate wallet for each requester, similar to how cryptocurrency exchanges automatically designate wallets for users to deposit funds to. The requester funds this wallet with the native currency (e.g., ETH), either in a lump sum or through per-request microtransactions. The funds in this wallet are used to fulfill all of the following requests made by the requester. This scheme has significant advantages:  The volatility in gas costs and payment token prices (e.g., LINK) makes it virtually impossible for oracles to set profitable yet competitive prices. Cal- culating prices dynamically on-chain requires multiple data feeds and adds a significant gas overhead per-request.",
      "With the Airnode protocol, the API providers do not have to concern themselves with gas costs, and can use typical pricing models such as monthly subscription fees. As mentioned in Section 4.2, it is not reasonable to expect API providers to be able to convert fiat into cryptocurrency and fund their node wallets as a part of their day-to-day operations. In this scheme, the node operator never has to think about their node wallet balance. Api3: Decentralized APIs for Web 3.0  As seen in a recent attack performed on Chainlink data feeds 27, oracle nodes that use a common wallet to fulfill requests are susceptible to attackers spamming requests to drain their wallets. The solution to this is for the node operators to maintain a whitelist of trusted addresses that they will accept requests from. In addition to the difficulty of determining which contracts are supposed to be trusted in this context, this renders any kind of public listing service practically infeasible.",
      "This is a critical issue, as it stops the little independent ecosystem growth there is dead in its tracks. Airnode is not susceptible to this type of an attack, as a requesters wallet is only used to fulfill requests from the said requester, and cannot be drained by others. Traditional oracle nodes have to fulfill all requests with very high gas prices, as they cannot tolerate their transaction queue being blocked by a single trans- action made with a low gas price. With the Airnode protocol, this is no longer a concern, as each requester will have a separate transaction queue. Then, requesters whose requests are not time-critical would be able to provide the fulfillment gas price as a request parameter and enjoy service at a much lower gas cost. This scheme synergizes especially well with EIP-1559 28. Finally, let us briefly mention how the Airnode protocol approaches monetization.",
      "It is common for a project-specific token to be worked into the core of the proto- col in an attempt to ensure that the said token is needed. However, this induces a significant gas price overhead, severely restricts alternative monetization options and creates overall friction. For these reasons, the Airnode protocol purposefully avoids using such a token. Instead, the node operator is allowed to associate custom authorizer contracts to their oracle endpoints, which essentially decide if a requester should be responded to based on any criteria that can be implemented on-chain. The authorizer contracts can be used to enforce whitelists, blacklists, monthly subscrip- tion payments or per-call fees. This scheme is both very flexible, and is implemented in a way that does not add any gas cost overheads.",
      "Although dAPI monetization is a completely independent matter, the flexibility that Airnode provides will carry over, e.g., it will be possible to implement a dAPI where the users assume all gas costs, which is not possible with the existing oracle solutions. 4.5. API integrations There is a chicken-and-egg problem when it comes to integrating APIs to oracles. If there is no existing demand for an API in an oracle ecosystem, nobody is incentivized to do the integration. If the API is not available due to a lack of integration, nobody develops applications that will create the demand. This was identified as a key friction point for the Chainlink ecosystem, and Honeycomb API Marketplace was Api3: Decentralized APIs for Web 3.0 proposed as a solution 19. Honeycomb had integrated a large number of premium APIs to Chainlink oracles, and as a result, this marketplace served an API variety that was unmatched in any oracle ecosystem at the time.",
      "Honeycomb used a universal external adapter and a novel method to integrate APIs to Chainlink oracles in a declarative way, without requiring any code to be written. This method is superior to developing an external adapter for each API opera- tion 29 in that its integrations are faster, less error-prone, and can be done by non- experts. Using these proprietary tools, Honeycomb was able to integrate hundreds of unique API operations in a few months, which dwarfed the closest competition by an order of magnitude. For Api3 to reach its full potential, it will need hundreds, if not thousands of first- party oracles so that it can easily set up new dAPIs or recompose existing ones. This can only be achieved if APIs can be integrated to Airnode in an even more scalable way. To this end, an improved version of the proprietary integration tools described above are open sourced for Airnode.",
      "Borrowing from the OpenAPI Specification format 30, Oracle Integration Specifications (OIS) define the operations of an API, the endpoints of an oracle, and how the two map to each other. An Airnode user is able to serve an API over their oracle simply by providing its OIS to their node. Integrations made in this standardized format are very easy to collect, version and distribute. OIS is a JSON file, primarily designed to describe the integration specifications for Airnode to use. This means that it does not aim to be human-readable first and creating it manually to specify an integration would be difficult. This problem will be solved by ChainAPI, an integration platform that will allow users to generate OIS for their APIs through an easy-to-use graphical interface.2 This will be accompanied by other quality of life improvements for Airnode users, such as a node dashboard and a marketplace to list their endpoints.",
      "As a result, Api3 will have a wide selection of first-party oracles to compose dAPIs from and ecosystem growth will no longer be bottlenecked by integration capacity. Decentralizing Governance through Tokenomics A single point of failure is a critical component of a system where, if failure occurs, there is no redundancy to compensate, causing the entire system to fail. Centraliza- tion produces single points of failure and decentralization aims to eliminate them. Blockchain-based applications implicitly claim decentralization, yet the majority are still centralized in some aspects, notably governance 31. In this section, we will 2https:chainapi.com Api3: Decentralized APIs for Web 3.0 discuss the problems arising from centralized governance and how Api3 solves these by way of a decentralized autonomous organization (DAO) 32 with well-designed tokenomics. 5.1.",
      "Centralized oracle network governance If a decentralized oracle network is configurable by a centralized entity, its gover- nance is centralized. This may cause governance mistakes to go unnoticed, which may result in the data feeds misreporting even when the underlying APIs and ora- cles are functioning correctly. For example, the Chainlink 15 silver price data feed reported the gold price for a period of time due to a governance mistake caused by human error 33. Synthetix 34, a decentralized exchange for derivatives, was using this data feed at the time, resulting in some of their users exploiting the error for profit 35. Due to its inherent opaqueness, centralized governance allows the usage of substandard practices, which inevitably result in such consequences. How- ever, the more glaring issue that this event has demonstrated is that a centralized governing entity can trivially use their authority to maliciously misreport.",
      "The governing entity has the authority to recompose a data feed, which means switching oracles and their respective data sources in and out. This is required for long term maintenance of the data feed, yet it exposes the data feed user to a variety of abuses and attacks by the governing entity. Then, the users either have to trust a centralized governing entity, or the governance of the data feed has to be decentralized with incentives that favor security. In the case where the data feed user feels they can trust a central governing entity completely, using a decentralized oracle network is irrational and the user would be better served by using a centralized oracle operated by the governing entity. Firstly, as discussed in Section 3.1, this centralized oracle would not have third- party oracles as an attack surface and would thus be more secure.",
      "Furthermore, a centralized oracle would provide much better performance in terms of availability due to the difficulty in coordinating a large number of oracle node operators, which sometimes causes data feed-level outages 36. Finally, the operating cost of such a centralized oracle would be far lower than an oracle network. Therefore, we contend there is no circumstance where centralized governance of oracle networks can be justified. Api3: Decentralized APIs for Web 3.0 5.2. Management of funds Initial coin offerings (ICOs) have been a popular fundraising method for blockchain projects, which typically involve the development team to be fully trusted with the development funds. Although this is sensible on the surface, it gets challenged when the token price increases speculatively, which results in the development team gaining control of a much larger amount than what the investors trusted them with in the first place.",
      "Since it is well established that centralized governance is strongly associated with corruption 37, we can say that this has the potential to lead to deceitful outcomes ranging from exit scams to development funds being misused in order to manipulate the token price further, resulting in unsustainable growth. This risk is heightened with a lack of budget transparency, which unfortunately is the norm. In addition to the technical development fund, some projects have an additional ecosystem development fund. It is even more difficult to justify giving the control of these funds to the development team, as they are only a part of the ecosystem and do not necessarily represent it and share its interests as a whole. DAICOs (an amalgamation of the terms DAO and ICO) have been proposed as a solution to these problems, which involves a DAO of investors to allocate a stipend to the development team, which can be regulated and even completely cut off by the DAO 38.",
      "A more flexible approach that is being employed by DAOs successfully today is to conduct the entire development through grants 39. In this scheme, the DAO does not have a development team, but rather jobs to do, and it contracts third parties to work on them on a case-by-case basis. This typically results in honest and efficient allocation of development and ecosystem funds at actual market rates. 5.3. Api3 DAO To decentralize the governance of both dAPIs and the project as a whole, Api3 is governed by a DAO3. The governance is entirely decentralized and open, meaning that all stakeholders are able to participate in the governance of the project directly. This is achieved through the API3 token, which grants voting power in the Api3 DAO through the mechanics described in Section 5.4. The DAO votes on high-level parameters regarding mechanics such as staking incen- tives and collateralization.",
      "Additionally, the DAO awards grants and by consequence decides on the general direction of the project. More granular tasks are conducted through hierarchical team structures for scalable governance. The expected workflow is for people to form off-chain teams and apply for grants to 3https:api3.eth.limo Api3: Decentralized APIs for Web 3.0 execute one-time projects or continuous operations that will benefit Api3. The team makes the grant application with a multisig that has the team members assigned as users (e.g., Gnosis Safe 40), and the DAO grants the funds to the multisig if the grant proposal is accepted. Furthermore, the DAO may authorize the team multisig to make specific transactions depending on the assigned task, e.g., setting dAPI subscription fees for individual users. Note that team members may have to disclose their real identities for projects with critical responsibilities and large budgets to verify their credentials and avoid potential Sybil attacks.",
      "Examples of technical grant subjects can be listed as follows:  Technical development of Airnode, dAPI contracts, Api3 DAO contracts  Frontend development for Api3 (staking, service coverage, etc.)  Development of Api3 ecosystem projects  Integration of new APIs, dAPI users, smart contract platforms  Statistical and qualitative risk assessment for specific APIs and dAPIs  Managing dAPIs  Developer outreach through articles, tutorials, videos  Technical and security audits  Setting up bug bounty programs, hackathons, etc.",
      "There is also an abundance of non-technical tasks that will be carried out through grants:  Business development to find new API providers, dAPI users  Subscription and service coverage pricing for specific dAPI users  Operational and financial audits  Payment processing  UIUX design  Marketing  Legal counsel Api3: Decentralized APIs for Web 3.0 Business developers Airnode developers dAPI managers Integration developers Integration developers dAPI managers Cross-chain payment processors Risk assessors Subscription and coverage rate-makers Ethereum External blockchain Api3 DAO Figure 6: An example hierarchical governance structure, composed of the main DAO, sub- DAOs and teams distributed across chains. The main DAO governs by selectively allocating funds and delegating authority. When a task reaches a scale that can no longer be fulfilled by a team, it is assigned to a subDAO.",
      "This team-based governance scheme is scalable in terms of gas costs, as it requires fewer proposals to be voted on at the DAO level. It is also more scalable in practical terms, as it does not require the constant attention of all governing parties to a wide variety of minute details. Furthermore, it allows critical operations such as dAPI management to be executed swiftly and based on expert opinion. As Api3 operations scale up, this governance hierarchy may demand additional layers, which implies subDAOs (see Figure 6). The DAO must follow two principles for this scheme to be effective. Firstly, to limit the amount of damage a malicious or incompetent team may cause, the authority that the team has must be constrained to a bare minimum, which is also known as the principle of least privilege.",
      "For example, a dAPI management team should never be able to completely recompose a dAPI that is under use, but should only Api3: Decentralized APIs for Web 3.0 be able to switch individual oracles in and out with a long enough cool-down period to ensure that their authority cannot be abused to a significant degree. Similarly, milestones and deliverables should be utilized to grant teams only the funds they need to carry out the specific responsibilities they have at the time. The second principle is transparency. For the DAO to be able to assess its performance, the team must report to the DAO in great detail. These reports will have the additional benefit of providing accountability and allow the dAPI users and the general public to be able to audit the operations of Api3 at all times. 5.4. API3 tokenomics Decentralized governance requires well-balanced incentive mechanisms that accu- rately model both positive and negative outcomes.",
      "In other words, the governing entities should be rewarded for good results and penalized for bad ones. The API3 token is designed to facilitate this through three main utilities: 1. Staking: Grants inflationary rewards, which are balanced by deflationary me- chanics such as burning or time-locking tokens in exchange for Api3 services. 2. Collateral: Backs service coverage that protects users from qualifying damages caused by dAPI malfunctions. 3. Governance: Grants direct representation in the Api3 DAO. The staking utility provides a financial incentive for participating in Api3 and con- tributing to increase the usage of its services. The collateral utility has the partic- ipants share Api3s operational risk and incentivizes them to minimize it. Finally, the governance utility gives the participants the ultimate instrument to enact these incentives. Note that it is critical for these three utilities to coincide.",
      "All governing entities must receive staking rewards for them to govern in a way that maximizes usage. All governing entities must have their funds used as collateral for them to govern in a way that minimizes security risks. To this end, Api3 has a single staking pool. Staking API3 tokens in this pool grants representation and staking rewards, but at the same time, the staked tokens will be used as collateral to pay out valid service coverage claims as needed. Api3: Decentralized APIs for Web 3.0 5.4.1. Staking A robust consensus mechanism rewards participants with the value that is captured from the enabled functionality, which creates a positive feedback loop. For example, Ethereum enables trustless applications. The miners that validate the correct oper- ation of these applications receive the entirety of the transaction fees paid by their users.",
      "In our case, API3 stakers are the participants of the consensus mechanism that governs the project, which implies that they should be the primary beneficiary of dAPI monetization. Following from this analogy, the recent work on EIP-1559 28 is relevant to our design. Until the recent London fork 41, Ethereum users directly paid the miners to have their transactions included to the chain. EIP-1559 posed that the primarily transac- tion fee-based miner incentivization scheme caused economic instability 42. With the update, the users now pay a base fee that floats with network usage, and this fee gets burned. Instead of the full transaction fee, the miners receive a stable, inflation- ary block reward.",
      "(EIP-1559 also includes a priority fee mechanism that counters Ethereum-specific issues such as uncle blocks and the block size limit, which do not have a direct correspondence in our analogy.) A DAO that distributes revenue to the stakers is similar to pre-EIP-1559 mining rewards in that it will cause instability. Each revenue distribution event creates a discontinuous jump in terms of incentives, which will be abused for profit. For exam- ple, a group of stakers may propose dAPI subscription fees to be paid yearly instead of monthly, and unstake as soon as the subscription fees are paid and the revenue is distributed. In addition to punishing long term participants, this will cause the total staked amount to oscillate according to periodic subscription fee payments. Just as the problem is an analog, so is the solution.",
      "The Api3 DAO will require the users to burn or time-lock API3 tokens to receive services, effectively creating a deflationary mechanism that benefits the staking reward recipients indirectly. Api3 services being used will cause a shortage in API3 supply, which will benefit all token holders, and not only the stakers. This implies that the staking rewards should be regulated to ensure that a certain portion of the API3 token holders stake, which will secure the governance of the project and provide collateral for service coverage products. To this end, the Api3 DAO sets a stake target, which is a percentage of the total token supply. Every week, the reward amount is updated iteratively, i.e., increases if the staked amount is below the stake target, and vice versa. It is challenging to incentivize good governance through staking rewards, as gov- ernance quality is not easily quantified.",
      "For example, participation is generally considered to be desirable, yet voting on a proposal that will already pass is redun- dant in most cases, and actively abstaining is a legitimate stance. As a solution, Api3: Decentralized APIs for Web 3.0 we only reward long-term outcomes. Specifically, the staking rewards are paid out weekly, yet each payment becomes withdrawable only after a full year. This incen- tivizes all stakers to cooperate to maximize the token price one year from now, and the rolling nature of this release prevents instability. 5.4.2. Collateral If staking API3 only yielded rewards, the sole governance incentive would be to maximize payments. This would be done by increasing the number of dAPI users aggressively, and the amount that is secured by the dAPIs with it. In Section 3.2, we have shown that the total load a dAPI is under increases its likelihood to mal- function due to an attack.",
      "Therefore, this is not a sustainable governance strategy for decentralized data feeds. Exposing the governing parties to the risk that we are trying to avoid would align their incentives with the DAOs. Then, the governing parties need to be penalized when a dAPI malfunction occurs. We went one step further and designed an on-chain service coverage product that provides dAPI users with quantifiable and trustless security against qualifying malfunctions. This service coverage uses the API3 token staking pool as collateral, which means that when a dAPI malfunction is confirmed through the dispute resolution protocol, the user damages will be covered from the staking pool. See Section 6 for the details of how this service coverage will be implemented. Let us see the effect of using the staking pool for both collateral and governance in a systems diagram in Figure 7a. When the DAO has appetite for additional risk, it onboards new dAPI users, which increases the load on the dAPIs.",
      "This increases the probability of a dAPI malfunction, the likelihood of paying out a service coverage claim, and the overall collateral risk as a result. With increased collateral risk, the DAOs risk appetite gets suppressed. In other words, the negative feedback caused by the service coverage prevents self-destructive growth. See Figure 7b for the expected dAPI load behavior that will emerge from this. The DAO estimates a failure threshold for the dAPIs, and onboards users to converge to this value, yet does not exceed it. Note that in the case that the DAO overestimates this threshold, the dAPIs will malfunction and the governing parties will be punished, as their staked funds will be used to pay out the service coverage claim made by the affected dAPI users. In other words, dAPI users are protected in either case.",
      "Api3: Decentralized APIs for Web 3.0 Risk appetite dAPI load Collateral risk (a) Systems diagram of governance dAPI load time Failure threshold (b) dAPI load over time in a balanced system Figure 7: Staking and service coverage collateralization utilities of the API3 token results in balanced governance incentives. (a) Loading the dAPIs with more users increases the likelihood of paying out service coverage claims, which produces negative feedback and balances the system. B indicates that the loop is balancing. (b) Due to the balanced nature of the system, the dAPI load does not increase indefinitely, yet settles at a level that the DAO estimates as being below the maximum load the dAPIs can support. 5.4.3. Governance The only way to gain representation at the Api3 DAO is to stake API3 tokens in the service coverage collateral pool. As such, the governing parties are exposed to all risks and rewards of Api3, and will govern to optimize them.",
      "Inflationary rewards and the staked governance tokens being used as collateral will create a positive feedback loop in terms of governance quality. Token holders will have to stake and expose themselves to risk if they do not want to lose value to inflation. If they misgovern and lose collateral through service coverage claims, these tokens will get returned to the open market, from where they will be acquired by new governing parties. In contrast, if initial token holders govern well and cause token scarcity in the market, the representation distribution will be protected. In other words, governance tokens being used as collateral results in a robust Darwinian structure that improves itself and is able to recover from failures. Api3: Decentralized APIs for Web 3.0 Quantifiable Security through Service Coverage Api3 will provide dAPI users with a quantifiable level of security in the form of on- chain service coverage.",
      "This accomplishes two goals: (1) the service coverage acts as a well-defined and trustless safety net for the user in case of a qualifying dAPI malfunction, (2) it holds the governing parties responsible for dAPI malfunctions, and thus incentivizes them to govern towards more secure dAPIs. 6.1. The need for quantifiable security Engineering is the art of modelling materials we do not wholly under- stand, into shapes we cannot precisely analyse, so as to withstand forces we cannot properly assess, in such a way that the public has no reason to suspect the extent of our ignorance. Dr. A. R. Dykes If we asked an engineer How much load can your bridge support? and got the answer I can assure you it has 21 beams of highest quality steel, we would not want to use that bridge, since not being able to provide the maximum load is an engineering red flag. We have introduced a simplistic model for how much a sin- gle oracle and, by derivation, a data feed would be able to support in Section 3.2.",
      "While not exhaustive, this model demonstrates that decentralized oracle networks cannot secure an arbitrarily large monetary value. The amount safely secured by an oracle must be bounded. In other words, like all blockchain technology, decentral- ized oracle networks should only be trusted to a certain extent, rather than being treated as unconditionally trustless 43. Then, a data feedcentralized 44 or decentralized 15should be responsible for quantifying the amount it can secure. One of the most well recognized solutions to this issue is proposed for the UMA protocol 45. The proposed scheme not only allows the quantification of the amount that can be secured by a data feed using game theoretic principles, it also allows this limit to be set precisely. The authors astutely observe that an overly-secure data feed is not desirable because it will be unnecessarily expensive for its users, and being able to set the degree of security to the minimum requirements would reduce costs.",
      "However, they follow this with the claim that the method they have proposed is optimally cost-efficient, which is grossly inaccurate in practice. This mistake stems from approaching the problem in a data source-blind way, i.e., trying to solve the oracle problem instead of the API connectivity problem. The proposed solution is Api3: Decentralized APIs for Web 3.0 only optimally cost-efficient if we consider all oracles to be untrustworthy, which is a close enough approximation for third-party oracles. In contrast, the trustworthiness of first-party oracles can be leveraged to build extremely secure data feeds at a very low cost, as the API providers have too much to lose by attempting an attack. A high value DeFi product being successfully secured by the data provided by a single reputable centralized exchange demonstrates this fact very well 44. Therefore, one cannot hope to disregard substantiated trustworthiness and end up with an actually cost-efficient solution.",
      "The ideal solution that fits the Api3 vision must provide quantifiable security by drawing from the trustworthiness of API providers, which can only be assessed us- ing off-chain information. To achieve this, Api3 will provide service coverage that assures a dAPI user with properly evidenced damages due to a qualifying malfunc- tion will be covered up to a predetermined amount. This solution is preferable for the user, as an alternative game theoretic solution can unexpectedly fail due to incentives that are poorly modeled or unaccounted for. 6.2. dAPI Service Coverage In Section 5.1, we mentioned a security incident where a Chainlink data feed had misreported to Synthetix. This was reported to cause damages less than 40,000 by Chainlink on the day of the incident 33 and approximately 36,000 by Synthetix the day after 35. Furthermore, Synthetix then announced that Chainlink had offered to compensate the damages, which they have subsequently accepted.",
      "This incident has demonstrated the following: 1. Service coverage that reimburses qualifying damages is a natural and obvious solution to data feed malfunction. 2. It is generally understood that the governing entity is responsible for data feed malfunctions. 3. It is possible to determine data feed malfunctions, their causes, and the re- sulting damages in a matter of days. On the surface, this incident was resolved rather uneventfully. This is to be expected, as the amount in question was relatively insignificant to the respective parties. How- ever, neither the general public nor the stakeholders can be sure of the exact terms of the settlement, as both projects are governed in a centralized way. This leads us to ask: What would have happened if the damages were orders of magnitude larger? How are fully decentralized projects supposed to deal with such events?",
      "Api3: Decentralized APIs for Web 3.0 It has been shown that insurance usage not only correlates with macroeconomic growth, but is also a cause of it 46. Nevertheless, insurance is sorely underutilized in the blockchain space. One of the main reasons of this is that insurance naturally requires a third party to resolve insurance disputes, and using a mutually-trusted third party for this purpose is against the ethos of decentralization. However, the emergence of Kleros 5, a general purpose on-chain dispute resolution protocol, allows trustless insurance-like enumerated risk protection or coverage products to be built. Api3 will co-develop an on-chain insurance-like service coverage product with Kleros that provides quantifiable and trustless security to dAPI users. This service coverage will be designed to reimburse the dAPI user for qualifying damages caused by certain dAPI malfunctions up to a payout limit.",
      "Note that even if we did not provide this service, the dAPI user could have received on-chain insurance or risk protection services using a third party solution 47. Such a solution would tend towards charging very high premiums, as they would not have access to the information and expertise to accurately assess dAPI risks. Furthermore, as described in Section 5.4.2, the proposed service coverage is special in the way that it is collateralized by the funds staked by the governing parties of the Api3 DAO. Therefore, it not only provides security to the dAPI user, but also creates a very strong incentive for dAPIs to be governed in a way that their security is maximized, which will further decrease service coverage costs. 6.3. Service coverage process The user requests to subscribe to a dAPI and receive a specific service coverage for the respective service through off-chain channels.",
      "The total amount that can be covered is limited by the size of the collateral pool, and the DAO will govern the collateralization ratio based on existing insurance and risk protection solvency literature 48. Respective Api3 teams investigate the dAPI malfunction risks and the specific use case of the user, calculate the service coverage premium, and enter the user-specific fee to the contract that manages payments. Upon paying the fee to the contract, the dAPI user gains access to the dAPI and gets service coverage for the respective payment period. If the dAPI user notices a malfunction, they will assess damages and make an on- chain service coverage claim for reimbursement of such applicable qualifying dam- ages. Stake withdrawals will have an adequate lead-time that will prevent stakers from front-running claims, i.e., withdrawing as soon as a dAPI malfunctions to evade claims.",
      "On the other hand, the service coverage claimant will need to stake funds to be able to make a claim to disincentivize abuse. The Api3 DAO can either Api3: Decentralized APIs for Web 3.0 pay out the claimed amount or propose a settlement amount directly to settle the claim, or escalate the claim to the Kleros court, which will determine if the claimed amount will be paid out to the dAPI user. The claim being accepted will result in the tokens being transferred to the dAPI user. This corresponds to stakers covering the damages proportional to the amount they have staked; that is, a user who has staked tokens that make up p of the entire pool will pay p of an accepted claim. The scheme described above assumes all amounts to be denominated in API3 to- kens. Depending on the use case, some users may require to be covered in other cryptocurrency types, e.g., ETH.",
      "In this case, simply having a liquidity provider automatically convert the payout to ETH will not be enough, as the API3ETH exchange rate between the time the claim is made and it is paid out will change, resulting in slippage. As a solution, the Api3 DAO can maintain an additional ETH reservesubject to the same solvency considerations as the collateral poolto ab- sorb the price volatility and ensure that the payout meets the amount that the user has originally claimed. 6.4. Risk Assessment Quantifying the amount of security that a data feed can provide is a very diffi- cult problem. However, by embedding the problem into the established domain of insurance and risk protection, we gain access to a wide variety of literature and skills that are readily available to source from traditional insurance and risk protec- tion services.",
      "Therefore, Api3 will be well-equipped with the services of actuaries, statisticians, data scientists, rate-makers, analysts, and legal counsel in taking on the challenging task of providing quantifiable security and properly parameterizing qualifying claims. Risk assessment is a vital step in optimizing service coverage pricing and making correct solvency estimations. This includes two main factors:  Internal: How likely is it for the dAPI to malfunction? External: What is the expected value of damages caused by a possible dAPI malfunction? One of the most important internal risk factors here is failure at the oracle level, which can be estimated by investigating individual APIs qualitatively, and analyzing their performance statistically. For example, qualitative investigations may conclude that an API provider has been in business for 5 years, and thus does not constitute a significant Sybil attack risk.",
      "Similarly, statistical analysis may indicate that the data Api3: Decentralized APIs for Web 3.0 (a) linear (b) leveraged (c) discontinuous, e.g. liquidation Figure 8: The data from the dAPI can be used in different systems, and the linearity and continuity in the mechanics of these systems decide on how sensitive they are to errors in input data. provided from an API provider often diverges from the consensus, which may cause an issue if the data consumer demands high accuracy. These assessments will also provide guidance in designing dAPIs regarding the number and selection of the API providers, e.g., it can be found out that adding more APIs to an overloaded dAPI will end up reducing costs by decreasing the service coverage risk. Operational risks are another important factor, which can be assessed through audits that investigate operational processes.",
      "Note that since this research will be done by the Api3 teams and publicly reported to the DAO, it will provide unmatched transparency and security assurance to the users. External risk factors determine the expected value of damages when a malfunction happens, and how the data is being used is an important aspect of this. See Figure 8, where we illustrate the cost of data errors for various hypothetical DeFi applications. Api3: Decentralized APIs for Web 3.0 In Figure 8a, the dAPI provides price data to a regular exchange, and errors result in a linearly proportional profit and loss for the transacting parties (note that we only consider the losses). Compare this with Figure 8b, which represents an exchange with a similar volume that supports leveraged positions. Here, any misreport has a much larger potential to cause damages. Finally, see Figure 8c, where a user has opened a short position.",
      "A slight skew upwards in the reported data may trigger a liquidation and cause a disproportionate effect. These examples indicate that it is impossible to estimate the service coverage risk for a specific user without considering how exactly they will use the data they receive from the dAPI. There is also a more qualitative aspect of the service coverage. Specifically, Api3 and the dAPI user will agree on an service coverage policy, which references defining terms such as what a dAPI malfunction is, requirements for a valid claim, and how damages are calculated. Kleros jurors will be using these policies and terms as reference while deciding if a service coverage claim is to be paid out. The specific terms are important regarding how service coverage rates are to be determined. For example, service coverage that covers any malfunction would be more expensive than service coverage against downtime. 6.5.",
      "Scaling solutions and service coverage High value use cases such as DeFi becoming popular causes the Ethereum network to be congested. This increases the transaction fees and affects data feed operation costs as a result. Then, it becomes critical to be able to make use of scaling solutions to deliver dAPI services at a reasonable cost. Existing decentralized oracle solutions propose to use off-chain scaling solutions 15, 16. However, these solutions come with obscure security implications that the user cannot assess accurately. Firstly, scaling solutions tend to have more relaxed se- curity guarantees in general, and it is not reasonable to expect the user to have a solid understanding of the consequences. Furthermore, there are additional opera- tional risks, e.g., security issues with the implementation of a custom cryptographic function, the second layer solution denying service, etc.",
      "As a result, it would be reasonable to expect users to be apprehensive about using data feeds depending on scaling solutions. dAPI service coverage comes as an unexpected solution to this problem due to its flexibility. If the Api3 DAO decides that a scaling solution is reasonably trustworthy for a given use case, the respective dAPI can utilize that scaling solution, and its service coverage could cover potential damages that would be caused by the scaling solution. The entire service coverage claims process would work exactly the same, given that what ultimately matters is whether or not the service is correctly delivered Api3: Decentralized APIs for Web 3.0 to the dAPI user. Conclusion Api3 will connect decentralized applications with the abundant data and services of- fered by traditional Web APIs, thereby expanding the applicability of the blockchain without sacrificing decentralization.",
      "This will be achieved by dAPIsfully decen- tralized and blockchain-native APIswhich will be set up, managed, and monetized at scale by the Api3 DAO. The Api3 solution embodies a variety of qualities by design. The most important one among these is security. dAPIs do not depend on third-party oracles, which are a constant and significant risk factor in the alternative solutions. In addition, the dAPI service coverage provides quantifiable and trustless security to its users, further cementing Api3s place as the most secure solution to receiving API services as a decentralized application. The second quality of the Api3 solution is robustness on multiple levels. Airn- ode uses serverless technology, which is highly resistant against downtime. Paired with a stateless node design that is not easily affected by bugs or adverse network conditions, Api3 oracles are engineered for robustness.",
      "Moreover, the dAPIs will be governed by a DAO that maintains a self-regulating balance of risk and reward through well-engineered incentives, which provides a robust risk mitigation frame- work. dAPIs eliminate the middlemen, which grants them their third quality, cost effi- ciency. They do not have to pay the middleman tax, which is the payment made to third-party oracles to incentivize them against attempting an attack. In addi- tion, data feeds composed of first-party oracles do not require over-redundancy at the oracle level. By achieving the same level of decentralization with fewer oracles, dAPIs provide very significant savings in gas costs. Finally, the Api3 solution achieves flexibility through complete decentralization of governance to parties with real skin in the game. As a result, the project will never be limited by what is put forth in this paper, and will evolve constantly to meet new challenges and needs.",
      "The first generation of decentralized applications were limited to the confines of the blockchain. Today, we have decentralized applications that can interact with the off-chain world in a limited and pseudo-decentralized way. Api3 will power the next evolutionary wavethe third generation of decentralized applications that Api3: Decentralized APIs for Web 3.0 valuably interact with the off-chain world, leveraging APIs in a truly decentralized and trust-minimized way. 1 DeFi Pulse. https:defipulse.com. 2 B. Liu and P. Szalachowski, A first look into DeFi oracles, arXiv preprint arXiv:2005.04377, 2020. 3 CLCG, Honeycomb API Marketplace. 4 CLCG, Honeycomb smart contract hackathon. https:honeycomb.devpost.com, 2019. 5 C. Lesaege, F. Ast, and W. George, Kleros: Short paper, Whitepaper v1.0.7, 2019. 6 G. Collins and D. Sisk, API economy: From systems to business services, Deloitte Insights, 2015. 7 MuleSoft, Connectivity benchmark report, 2019. 8 B. Iyer and M.",
      "Subramaniam, The strategic value of APIs, Harvard Business Review, 2015. 9 R. Narain, A. Merrill, and E. Lesser, Evolution of the API economy: Adopting new business models to drive future innovation, IBM Institute for Business Value, 2016. 10 Capgemini, Efma, World FinTech report 2019, 2019. 11 S. Nakamoto, Bitcoin: A peer-to-peer electronic cash system, Whitepaper, 2009. 12 N. Szabo, Smart contracts. 1994. 13 V. Buterin, A next-generation smart contract and decentralized application platform, Whitepaper, 2014. https:ethereum.orgenwhitepaper. 14 J. Peterson, J. Krug, M. Zoltu, A. K. Williams, and S. Alexander, Augur: A de- centralized oracle and prediction market platform, Whitepaper v2.0, 2019. https: augur.netwhitepaper.pdf. 15 S. Ellis, A. Juels, and S. Nazarov, ChainLink: A decentralized oracle network, Whitepaper v1.0, 2017. https:research.chain.linkwhitepaper-v1.pdf. 16 Band Protocol. https:bandprotocol.com. 17 A. S. de Pedro, D. Levi, and L. I.",
      "Cuende, Witnet: A decentralized oracle network protocol, Whitepaper, 2017. https:arxiv.orgpdf1711.09756.pdf. 18 Tellor: A decentralized oracle, Whitepaper. whitepaperintroduction. Api3: Decentralized APIs for Web 3.0 19 B. Benligiray, Connor, Tate, Vanttinen, Honeycomb: ecosystem hub for decentralized oracle networks, Whitepaper, 2019. https: raw.githubusercontent.comclc-grouphoneycomb-whitepapermaster honeycomb20whitepaper.pdf. 20 Gnosis, Omen next generation prediction markets. https: blog.gnosis.pmomen-and-the-next-generation-of-prediction-markets- 2e7a2dd604e, 2020. 21 Capgemini, Efma, World insurance report 2019, 2019. 22 J. R. Douceur, The Sybil attack, in Proc. Int. Workshop on Peer-to-Peer Systems, pp. 251260, 2002. 23 S. Frederick, G. Loewenstein, and T. Odonoghue, Time discounting and time prefer- ence: A critical review, Journal of Economic Literature, vol. 40, no. 2, pp. 351401, 2002. 24 Chainlink, Price reference data. https:feeds.chain.link.",
      "25 Practical Law, Data licensing: Taking into account data ownership and use. taking-into-account-data-ownership. 26 Compound Finance, Open price feed. https:compound.financeprices. 27 The Block, Chainlink nodes were targeted in an attack last weekend that cost them at least 700 ETH. https:www.theblockcrypto.compost76986chainlink- nodes-attack-eth, 2020. 28 V. Buterin, E. Conner, R. Dudley, M. Slipper, and I. Norden, EIP-1559: Fee market change for ETH 1.0 chain. https:eips.ethereum.orgEIPSeip-1559. 29 Chainlink, Chainlink external adapters. https:github.comsmartcontractkit external-adapters-js. 30 OpenAPI Initiative, The OpenAPI specification. OpenAPI-Specification. 31 A. Walch, Deconstructing decentralization: Exploring the core claim of crypto sys- tems, Crypto Assets: Legal and Monetary Perspectives (OUP, Forthcoming), 2019. 32 V. Buterin, DAOs, DACs, more: incomplete terminol- ogy guide. https:blog.ethereum.org20140506daos-dacs-das-and-more- an-incomplete-terminology-guide, 2014.",
      "33 Chainlink, Improving and decentralizing Chainlinks feature release and network upgrade process. chainlinks-feature-release-and-network-upgrade-process, 2020. 34 Synthetix, Synthetix litepaper, Whitepaper v1.4, 2020. https:docs.synthetix. iolitepaper. 35 Synthetix, Update on XAG pricing incident. https:blog.synthetix.ioupdate- on-xag-pricing-incident, 2020. Api3: Decentralized APIs for Web 3.0 36 DeFi Pulse, DeFi status report post-Black Thursday. https:defipulse.com blogdefi-status-report-black-thursday, 2020. 37 R. Fisman and R. Gatti, Decentralization and corruption: Evidence across countries, Journal of Public Economics, vol. 83, no. 3, pp. 325345, 2002. 38 V. Buterin, Explanation of DAICOs. https:ethresear.chtexplanation-of- daicos465, 2018. 39 dxDAO: Toward super-scalable organizations, Whitepaper, 2019. https:github. comgnosisdx-daostackblobmasterdxdao_whitepaper_v1.pdf. 40 Gnosis, Gnosis safe. https:gnosis-safe.io. 41 Ethereum, The history of Ethereum.",
      "https:ethereum.orgenhistorylondon, 2021. 42 T. Roughgarden, Transaction fee mechanism design for the Ethereum blockchain: An economic analysis of EIP-1559, arXiv preprint arXiv:2012.00854, 2020. 43 P. De Filippi, M. Mannan, and W. Reijers, Blockchain as a confidence machine: The problem of trust  challenges of governance, Technology in Society, vol. 62, 2020. 44 Coinbase, Introducing the Coinbase price oracle. https:blog.coinbase.com introducing-the-coinbase-price-oracle-6d1ee22c7068, 2020. 45 UMA, UMA data verification mechanism: Adding economic guarantees to blockchain oracles, White paper v0.2, 2020. https:github.comUMAprotocolwhitepaper blobmasterUMA-DVM-oracle-whitepaper.pdf. 46 J. F. Outreville, The relationship between insurance and economic development: 85 empirical papers for a review of the literature, Risk Management and Insurance Re- view, vol. 16, no. 1, pp. 71122, 2013. 47 H. Karp and R.",
      "Melbardis, Nexus Mutual: A peer-to-peer discretionary mutual on the Ethereum blockchain, Whitepaper. https:nexusmutual.ioassetsdocs nmx_white_paperv2_3.pdf. 48 Directive 2009138EC of the European Parliament and of the Council of 25 Novem- ber 2009 on the taking-up and pursuit of the business of Insurance and Reinsurance (Solvency II) (recast). http:data.europa.euelidir20091382019-01-13. Api3: Decentralized APIs for Web 3.0 Whitepaper Versions v1.0.5  Burak Benligiray, Mar 2025. v1.0.4  Emanuel Tesa\u02c7r, Derek Croote, Burak Benligiray, Jan 2024. v1.0.3  Erich Dylus, Burak Benligiray, June 2022. v1.0.2  Burak Benligiray, Derek Croote, November 2021. v1.0.1  Burak Benligiray, November 2020. v1.0.0  Burak Benligiray, Sa\u02c7sa Milic, Heikki Vanttinen, September 2020. Glossary Airnode: A fully-serverless oracle node designed to be operated by API providers. API: A technical interface of an application that another application can use to interact with it programmatically.",
      "An API that is open to external access (i.e., a Web API) can be used by businesses to monetize their data and services. API provider: A business that monetizes their data and services through an API. Api3: The project that will build, manage, and monetize dAPIs. Api3 DAO: The governing body of the Api3 Project. API3 token: The token that is used to align the Api3 ecosystem incentives. It grants voting power at the Api3 DAO. ChainAPI: A third-party APIAirnode integration platform, accessible at https: chainapi.com. It will provide integration tools, utilities and a marketplace for public listing of oracle endpoints. It can be thought of as the spiritual successor to the Honeycomb API Marketplace. DAO: Decentralized autonomous organization. A multi-user smart contract that is used to democratize the governance of an on-chain organization. dAPI: A decentralized API, i.e., a data feed composed of first-party oracles. It is governed decentrally by the Api3 DAO.",
      "Api3: Decentralized APIs for Web 3.0 Honeycomb API Marketplace: An API-centric marketplacelisting service that has been built for Chainlink oracles by some of the founding members of Api3. Oracle: An agent that can deliver data to-and-from a smart contract platform, e.g., writes asset price data to the chain. It includes a node and a smart contract that implements the protocol it uses to communicate with the requesters. Oracle node: An application that performs the off-chain functions of an oracle, e.g., calls an API to get asset price data and writes it to the chain. Web 3.0: The decentralized Web built over blockchain technologies. Note that we are not using this term to refer to the Semantic Web. Web API: An API that is accessible over the Web, i.e., an API that is not only accessible from a private network."
    ],
    "word_count": 13951,
    "page_count": 40
  },
  "APT": {
    "chunks": [
      "The Aptos Blockchain: Safe, Scalable, and Upgradeable Web3 Infrastructure August 11, 2022 v1.0 Abstract The rise of blockchains as a new Internet infrastructure has led to developers deploying tens of thousands of decentralized applications at rapidly growing rates. Unfortunately, blockchain usage is not yet ubiquitous due to frequent outages, high costs, low throughput limits, and numerous security concerns. To enable mass adoption in the web3 era, blockchain infrastructure needs to follow the path of cloud infrastructure as a trusted, scalable, cost-efficient, and continually improving platform for building widely-used applications. We present the Aptos blockchain, designed with scalability, safety, reliability, and upgradeability as key principles, to address these challenges. The Aptos blockchain has been developed over the past three years by over 350 developers across the globe 1.",
      "It offers new and novel innovations in consensus, smart contract design, system security, performance, and decentralization. combination of these technologies will provide a fundamental building block to bring web3 to the masses:1  First, the Aptos blockchain natively integrates and internally uses the Move language for fast and secure transaction execution 2. The Move prover, a formal verifier for smart contracts written in the Move language, provides additional safeguards for contract invariants and behavior. This focus on security allows developers to better protect their software from malicious entities. Second, the Aptos data model enables flexible key management and hybrid custodial options. This, alongside transaction transparency prior to signing and practical light client protocols, provides a safer and more trustworthy user experience.",
      "Third, to achieve high throughput and low latency, the Aptos blockchain leverages a pipelined and modular approach for the key stages of transaction processing. Specifically, transaction dissemination, block metadata ordering, parallel transaction execution, batch storage, and ledger certification all operate concurrently. This approach fully leverages all available phys- ical resources, improves hardware efficiency, and enables highly parallel execution. Fourth, unlike other parallel execution engines that break transaction atomicity by requiring upfront knowledge of the data to be read and written, the Aptos blockchain does not put such limitations on developers. It can efficiently support atomicity with arbitrarily complex transactions, enabling higher throughput and lower latency for real-world applications and simplifying development. Fifth, the Aptos modular architecture design supports client flexibility and optimizes for frequent and instant upgrades.",
      "Moreover, to rapidly deploy new technology innovations and support new web3 use cases, the Aptos blockchain provides embedded on-chain change management protocols. 1Legal Disclaimer: This white paper and its contents are not an offer to sell, or the solicitation of an offer to buy, any tokens. We are publishing this white paper solely to receive feedback and comments from the public. Nothing in this document should be read or interpreted as a guarantee or promise of how the Aptos blockchain or its tokens (if any) will develop, be utilized, or accrue value. Aptos only outlines its current plans, which could change at its discretion, and the success of which will depend on many factors outside of its control. Such future statements necessarily involve known and unknown risks, which may cause actual performance and results in future periods to differ materially from what we have described or implied in this white paper. Aptos undertakes no obligation to update its plans.",
      "There can be no assurance that any statements in the white paper will prove to be accurate, as actual results and future events could differ materially. Please do not place undue reliance on future statements. Finally, the Aptos blockchain is experimenting with future initiatives to scale beyond individ- ual validator performance: its modular design and parallel execution engine support internal sharding of a validator and homogeneous state sharding provides the potential for horizontal throughput scalability without adding additional complexity for node operators. Introduction In the web2 version of the Internet, services such as messaging, social media, finance, gaming, shop- ping, and audiovideo streaming, are provided by centralized companies that control direct access to user data (e.g., Google, Amazon, Apple, and Meta).",
      "These companies develop infrastructure using application-specific software optimized for targeted use cases and leverage cloud infrastructures to de- ploy these applications to users. Cloud infrastructure provides access to virtualized andor physical infrastructure services, such as rented virtual machines (VMs) and bare metal hardware operating inside data centers worldwide (e.g., AWS, Azure, and Google Cloud). As a result, building web2 Internet services that can scale to billions of users has never been easier than it is today. However, web2 requires that users place explicit trust in centralized entities, a requirement that has become increasingly concerning to society. To combat this concern, a new Internet age has begun: web3. In the web3 version of the Internet, blockchains have emerged to provide decentralized, immutable ledgers that enable users to interact with one another securely and reliably, all without requiring trust in controlling intermediaries or centralized entities.",
      "Similar to how web2 Internet services and applications rely on cloud infrastructure as building blocks, decentralized applications can use blockchains as a decentralized infrastructure layer to reach billions of users across the world. However, despite the existence of many blockchains today, widespread adoption of web3 has not yet taken place 3. While technology continues to advance the industry, existing blockchains are unreliable, impose high transaction fees for users, have low throughput limitations, suffer regular asset losses due to security issues, and cannot support real-time responsiveness. In comparison to how cloud infrastructure has enabled web2 services to reach billions, blockchains have not yet enabled web3 applications to do the same. The Aptos vision The Aptos vision is to deliver a blockchain that can bring mainstream adoption to web3 and empower an ecosystem of decentralized applications to solve real-world user problems.",
      "Our mission is to advance the state-of-the-art in blockchain reliability, safety, and performance by providing a flexible and modular blockchain architecture. This architecture should support frequent upgrades, fast adoption of the latest technology advancements, and first-class support for new and emerging use cases. We envision a decentralized, secure, and scalable network governed and operated by the community that uses it. When infrastructure demands grow across the world, the computational resources of the blockchain scale up horizontally and vertically to meet those needs. As new use cases and technological advances arise, the network should frequently and seamlessly upgrade without interrupting users. Infrastructure concerns should fade into the background. Developers and users will have access to many different options for key recovery, data modeling, smart contract standards, resource usage trade- offs, privacy, and composability.",
      "Users know that their assets are secure, always available, and can be accessed with near at-cost fees. Anyone can safely, easily, and immutably transact with untrusted parties worldwide. Blockchains are as ubiquitous as cloud infrastructure. To achieve this vision, significant technological advances must be made. Our experiences building, developing, advancing, and deploying the Diem blockchain (the predecessor of the Aptos blockchain) over the past three years have proven that a network can continually upgrade its protocols without disrupting its clients 4. The Diem mainnet was deployed to more than a dozen node operators with multiple wallet providers in early 2020. Over the following year, our team issued two major upgrades that changed the consensus protocol and the core framework. Both upgrades completed without downtime for users.",
      "With the Aptos blockchain, we have made a series of radical improvements to the technology stack while also incorporating safe, transparent, and frequent upgrades as a core feature, as inspired by the Diem blockchain. In particular, we highlight novel methods of transaction processing (as described in Section 7) and new approaches to decentralization and network governance. New validator (inactive) Active validator Validators Synchronizing txns  state Full node Light client Submitting txns  synchronizing partial state Synchronizing state Submitting txns  synchronizing state Figure 1: Components of the Aptos ecosystem. As the Aptos blockchain continues to improve and grow, we will issue refreshed versions of this white paper with the latest iteration of our protocols and design choices. In the rest of this document, we describe the current state of the Aptos blockchain as well as future plans.",
      "Overview The Aptos blockchain, as shown in Figure 1, is comprised of a set of validators that jointly receive and process transactions from users using a byzantine fault-tolerant (BFT), proof-of-stake consensus mechanism. Token holders lock up, or stake, tokens in their selected validators. Each validators consensus voting weight is proportionate to the amount staked into it. A validator can be active and participate in consensus. Likewise, a validator may also be inactive if it does not have enough stake to participate, rotates out of the validator set, elects to be offline as it synchronizes blockchain state, or is deemed not participating by the consensus protocol due to poor historical performance. Clients are any part of the system that need to submit transactions or query the state and history of the blockchain. Clients can choose to download and verify validator signed proofs of queried data.",
      "Full nodes are clients that replicate the transaction and blockchain state from the validators or from other full nodes in the network. They may elect to prune transaction history and blockchain state as desired to reclaim storage. Light clients only maintain the current set of validators and can query partial blockchain state securely, typically from full nodes. Wallets are a common example of a light client. To meet the needs of safe, fast, reliable, and upgradeable web3 infrastructure for widespread adop- tion, the Aptos blockchain is built on the following core design principles:  Fast and secure execution along with simple auditability and mechanical analyzability via a new smart contract programming language, Move 5. Move originated with the predecessor to the Aptos blockchain and continues to progress with the evolution of this project. Extremely high throughput and low latency through a batched, pipelined, and parallelized ap- proach to transaction processing.",
      "Novel parallel transaction processing that efficiently supports atomicity with arbitrarily complex transactions through Block-STM, unlike existing parallel execution engines that require upfront knowledge of data locations to be read and written. Optimizations for performance and decentralization via rapid, stake-weight validator set rotation and reputation tracking. Upgradeability and configurability as first-class design principles to embrace new use cases and the latest technology. Modular designs that enable rigorous component level testing along with appropriate threat modeling and seamless deployment, all ensuring highly secure and reliable operations. Horizontal throughput scalability while preserving decentralization, where sharding is a first-class concept exposed to users and native to the programming and data model. Section 4 explains how developers interact with Move in the Aptos blockchain. Section 5 describes the logical data model.",
      "Section 6 details how the Aptos blockchain enables a safe user experience through strong verification methods. Section 7 describes key performance innovations around pipelining, batch- ing, and parallelization. Section 8 details various options for different clients to synchronize state with other nodes. Section 9 describes our plans for community ownership and governance. Finally, Section 10 discusses future performance directions while maintaining decentralization. The Move language Move is a new smart contract programming language with an emphasis on safety and flexibility. The Aptos blockchain uses Moves object model to represent its ledger state (see Section 5.5) and uses Move code (modules) to encode rules of state transitions. Users submit transactions that can publish new modules, upgrade existing modules, execute entry functions defined within a module, or contain scripts that can directly interact with the public interfaces of modules.",
      "The Move ecosystem contains a compiler, a virtual machine, and many other developer tools. Move is inspired by the Rust programming language, which makes the ownership of data explicit in the language via concepts like linear types. Move emphasizes resource scarcity, preservation, and access control. Move modules define the lifetime, storage, and access pattern of every resource. This ensures that resources like Coin are not produced without appropriate credentials, cannot be double spent, and do not disappear. Move leverages a bytecode verifier to guarantee type and memory safety even in the presence of untrusted code. To help write more trusted code, Move includes a formal verifier, the Move Prover 6, capable of verifying the functional correctness of a Move program against a given specification, formulated in the specification language integrated into Move.",
      "Beyond the user accounts and corresponding account content, the ledger state also contains the on-chain configuration of the Aptos blockchain. This network configuration includes the set of active validators, staking properties, and the configuration of various services within the Aptos blockchain. Moves support for module upgradeability and comprehensive programmability enables seamless con- figuration changes and supports upgrades to the Aptos blockchain itself (both sets of upgrades have been executed multiple times with zero downtime on a private mainnet). The Aptos team has further enhanced Move with support for broader web3 use cases. As mentioned later in Section 5.5, the Aptos blockchain enables fine-grained resource control. Not only does this support parallelization of execution, but it also achieves a near-fixed cost associated with accessing and mutating data.",
      "Moreover, the Aptos blockchain provides table support built on top of fine-grained storage, which allows for large-scale datasets (e.g., massive collections of NFTs) in a single account. Furthermore, Aptos supports shared or autonomous accounts that are represented entirely on-chain. This allows complex decentralized autonomous organizations (DAOs) to collaboratively share accounts, as well as use these accounts as containers for a heterogeneous collection of resources. Logical data model The ledger state of the Aptos blockchain represents the state of all the accounts. The ledger state is versioned using an unsigned 64-bit integer corresponding to the number of transactions the system has executed. Anyone can submit a transaction to the Aptos blockchain to modify the ledger state. Upon execution of a transaction, a transaction output is generated.",
      "A transaction output contains zero or more operations to manipulate the ledger state (called write sets), a vector of resulting events (see Section 5.1.1), the amount of gas consumed, and the executed transaction status. Transactions A signed transaction contains the following information:  Transaction authenticator: The sender uses a transaction authenticator that includes one or more digital signatures to verify that a transaction is authenticated. Sender address: The account address of the sender. Payload: The payload either refers to an existing entry function on-chain or contains the function to be executed as inlined bytecode (called a script). In addition, a set of input arguments is encoded in byte arrays. For a peer-to-peer transaction, the inputs contain the recipients information and the amount transferred to them. Gas price (in specified currencygas units): This is the amount the sender is willing to pay per unit of gas to execute the transaction.",
      "Gas is a way to pay for compute, networking, and storage. A gas unit is an abstract measurement of computation with no inherent real-world value. Maximum gas amount: The maximum gas amount is the maximum gas units the transaction is allowed to consume prior to aborting. The account must have at least the gas price multiplied by the maximum gas amount or the transaction will be discarded during validation. Sequence number: The sequence number of the transaction. This must match the sequence number stored in the senders account when the transaction executes. Upon successful transac- tion execution, the account sequence number is incremented to prevent replay attacks. Expiration time: A timestamp after which the transaction ceases to be valid. Chain id: Identifies the blockchain that this transaction is valid for, offering further protection for users to prevent signing errors.",
      "At each version i, the state change is represented by the tuple (Ti, Oi, Si), containing the transaction, transaction output, and the resulting ledger state, respectively. Given a deterministic function Apply, the execution of transaction Ti with ledger state Si1 produces the transaction output Oi and a new ledger state Si. That is, Apply(Si1, Ti) Oi, Si. 5.1.1 Events Events are emitted during the execution of a transaction. Each Move module can define its own events and select when to emit these events upon execution. For example, during a coin transfer, both the sender and receivers accounts will emit SentEvent and ReceivedEvent, respectively. This data is stored within the ledger and can be queried via an Aptos node. Each registered event has a unique key and the key can be used to query event details. Multiple events emitted to the same event key produce event streams, a list of events with each entry containing a sequentially increasing number beginning at 0, a type, and data.",
      "Each event must be defined by some type. There may be multiple events defined by the same or similar types, especially when using generics. Events have associated data. For Move module developers, the general principle is to include all data necessary to understand the changes to the underlying resources before and after the execution of the transaction that changed the data and emitted the event. Transactions can only generate events and cannot read events. This design allows transaction execution to be a function only of the current state and transaction inputs, not historical information (e.g., previously generated events). Accounts Each account is identified by a unique 256-bit value known as an account address. A new account is created in the ledger state (see Section 5.5) when a transaction sent from an existing account invokes the create_account(addr) Move function.",
      "This typically happens when a transaction attempts to send Aptos tokens to an account address that has not yet been created. For convenience, Aptos also supports a transfer(from, to, amount) function that implicitly creates an account if it does not already exist prior to the transfer. module coin  struct Coinphantom T has key, store  .. public fun mint(..)  .. module coin  struct Coin has key, store  .. module wallet  use 0x1::coin::Coin; struct USD  struct JPY  struct MultiCurrencyWallet has key, store  usd: CoinUSD, jpy: CoinJPY, Figure 2: Example on-chain Move modules. To create a new account, a user first generates a signature key-pair: (vk, sk). Next, the new account address for a given signature scheme is derived using the cryptographic hash H of the public verification key vk that is concatenated with the signature scheme identifier (ssid): where addr  H(vk, ssid).",
      "After the new account is created at address addr, the user can sign transactions to be sent from the account at addr, using the private signing key sk. The user can also rotate sk, either to proactively change sk or to respond to a possible compromise. This will not change the account address, as the account address is derived only once, during its creation, from the public verification key. The Aptos blockchain does not link accounts to a real-world identity. A user can create multiple accounts by generating multiple key-pairs. Accounts controlled by the same user have no inherent link to each other. However, a single user can still manage multiple accounts in a single wallet for simple asset management. This flexibility provides pseudonymity for users while we experiment with privacy-preserving primitives for future releases. Multiple accounts owned by a single user or a set of users also provide channels to increase execution concurrency, as described in Section 7.4.",
      "Move modules A Move module contains Move bytecode that declares data types (structs) and procedures. It is identified by the address of the account where the module is declared along with a module name. For example, the identifier for the first currency module in Figure 2 is 0x1::coin. A module can depend on other on-chain modules, as shown by the wallet module in Figure 2, enabling code reuse. A module must be uniquely named within an account, i.e., each account can declare at most one module with any given name. For example, the account at address 0x1 in Figure 2 could not declare another module named coin. On the other hand, the account at address 0x3 could declare a module named coin and the identifier of this module would be 0x3::coin. Note that 0x1::coin::Coin and 0x3::coin::Coin are distinct types and cannot be used interchangeably nor share common module code.",
      "In contrast, 0x1::coin::Coin0x2::wallet::USD and 0x1::coin::Coin0x2::wallet::JPY are different instantiations of the same generic type that cannot be used interchangeably but can share common module code. Modules are grouped into packages located at the same address. An owner of this address publishes the package as a whole on-chain, including the bytecode and package metadata. The package meta- data determines whether a package can be upgraded or is immutable. For an upgradeable package, compatibility checks are performed before the upgrade is permitted: no existing entry point functions must be changed and no resources can be stored in memory. However, new functions and resources can be added. The Aptos framework, consisting of the core libraries and configuration for the Aptos blockchain, is defined as a regular upgradeable package of modules (see Section 9.2).",
      "0x2::wallet::Wallet  usd: 0x1::coin::Coin0x2::wallet::USD  value: 0, jpy: 0x1::coin::Coin0x2::wallet::JPY  value: 10, module my_coin  struct MyCoin  0x1::coin::Coin0x60::my_coin::MyCoin  value: 50, 0x3::coin::Coin  value: 100, 0x50 0x60 Figure 3: Example on-chain data. Resources Similar to modules, account addresses can also have data values associated with them. Within each account address, the values are keyed by their types, with at most one value of each type belonging to the account. Figure 3 provides an example of this. Address 0x50 holds a single value, with 0x3::coin::Coin being the fully-qualified type. 0x3 is the address where the coin module is stored, coin is the name of the module and Coin is the name of the data type. Values of generic types are also allowed, with different instantiations being treated as distinct types. This is essential for extensibility, allowing different instantiations to share the same functional code.",
      "The rules for mutating, deleting, and publishing a value are encoded in the module that defines the data type. Moves safety and verification rules prevent other code or entities from directly creating, modifying, or deleting instances of data types defined in other modules. Having at most one top-level value of each type under an address may at first sound limiting. However, this is not a problem in practice as programmers can define wrapper types with other data as internal fields, thus avoiding any limitations. The Wallet struct in Figure 3 is an example of how to use wrapper types. It should also be noted that not all data types can be stored on-chain. For data instances to qualify as top-level values, the data type must have the key ability. Similarly, the store ability is required for nested values. Data types with both abilities are also called resources.",
      "Ledger state From the perspective of the Move virtual machine (Move VM), each account consists of a set of values and key-value data structures. These data structures are called table entries and are stored in the Binary Canonical Serialization format (BCS). This data layout enables developers to write smart contracts that can operate efficiently on small amounts of data replicated across a large number of accounts, as well as on large amounts of data stored in a small number of accounts. Move modules are stored similarly to account data but under an independent namespace. The genesis ledger state defines the initial set of accounts and their associated state at blockchain initialization. At launch, the Aptos blockchain will be represented by a single ledger state. However, as adoption increases and technology develops, Aptos will scale up the number of shards to increase throughput (i.e., enable multiple ledger states) and support transactions that move or access assets across shards.",
      "Each ledger state will maintain all on-chain assets for the specific shard and provide the same account model with a fine-grained, key-value data store offering near-fixed costs for storage access. A safe user experience To reach billions of Internet users, the web3 user experience must be safe and accessible. In the sections below, we describe several innovations provided by the Aptos blockchain that work towards this goal. Transaction viability protection Signing a transaction means that the signer authorizes the transaction to be committed and executed by the blockchain. Occasionally, users may sign transactions unintentionally or without fully considering all the ways in which their transactions might be manipulated. To reduce this risk, the Aptos blockchain constrains the viability of every transaction and protects the signer from unbounded validity.",
      "There are currently three different protections provided by the Aptos blockchain - the senders sequence number, a transaction expiration time, and a designated chain identifier. A transactions sequence number can only be committed exactly once for each senders account. As a result, senders can observe that if the current account sequence number is the sequence number of a transaction t, then either t has already been committed or t will never be committed (as the sequence number used by t has already been consumed by another transaction). The blockchain time advances with high precision and frequency (typically sub-second), as de- tailed in Section 7.3.1. If the blockchain time exceeds the expiration time of transaction t, then similarly, either t has already been committed or t will never be committed.",
      "Every transaction has a designated chain identifier to prevent malicious entities from replaying transactions between different blockchain environments (e.g., across a testnet and mainnet). Move-based key management As discussed in Section 5.2, Aptos accounts support key rotation, an important feature that can help reduce the risks associated with private key compromise, long-range attacks, and future advances that might break existing cryptographic algorithms. In addition, Aptos accounts are also flexible enough to enable new hybrid models of custody. In one such model, a user can delegate the ability to rotate the accounts private key to one or more custodians and other trusted entities. A Move module can then define a policy that empowers these trusted entities to rotate the key under specific circumstances.",
      "For example, the entities might be represented by a k-out-of-n multi-sig key held by many trusted parties and offer key recovery services to prevent user key loss (e.g., 20 of Bitcoin is currently locked up in unrecoverable accounts 7). Moreover, while many wallets support various key recovery schemes, such as backing up private keys to cloud infrastructure, multi-party computation, and social recovery, they are typically implemented without blockchain support (i.e., off-chain). As a result, each wallet needs to implement its own key management infrastructure and related operations become opaque to users. In contrast, supporting key management functionality at the Aptos blockchain layer provides full transparency of all key-related operations and makes it simpler to implement a wallet with rich key management. Pre-signing transaction transparency Today, wallets provide very little transparency about the transactions they sign.",
      "As a result, users are often easily tricked into signing malicious transactions that may steal funds and have devastating consequences. This is true even for blockchains that require enumerating all on-chain data accessed by each transaction. As a result, there are few user safeguards currently in place, making users vulnerable to a wide variety of attacks. To address this, the Aptos ecosystem provides services for transaction pre-execution: a precaution- ary measure that describes to users (in human-readable form) the outcomes of their transactions prior to signing. Pairing this with a known history of prior attacks and malicious smart contracts will help to reduce fraud. In addition, Aptos also enables wallets to dictate constraints on transactions during execution. Violating these constraints will result in the transactions being aborted, to further protect users from malicious applications or social engineering attacks.",
      "Practical light client protocols Relying solely on the TLSSSL certificates of API providers to establish trust between blockchain clients and servers does not protect clients sufficiently. Even in the presence of valid certificates, wallets and clients have no guarantees as to the authenticity and integrity of the data being presented Validator Continuous transaction dissemination Client Submits transaction Block metadata ordering Parallel transaction execution Batch storage Ledger certi\ufb01cation Other validators Batch transactions and share batches to other validators Other validators Group batches of metadata into blocks and order Other validators Periodically, share and aggregate signatures on the ledger state Other validators If required, fetch the batches of transactions Figure 4: The transaction processing life cycle. All stages are completely independent and are indi- vidually parallelizable. to them.",
      "As a result, API providers may return incorrect or malicious blockchain data, deceiving third parties and performing double-spend attacks. To prevent this, Aptos provides state proofs and light client verification protocols that can be used by wallets and clients to verify the validity of data being presented by an untrusted third-party server. Moreover, by leveraging the timestamp-based state proofs in Section 7.6.2, light clients can always ensure tight bounds on the freshness of account state (e.g., within seconds) and only need to track changes in the network configuration (epoch changes) or use current trusted checkpoints (waypoints) to stay up-to-date 8. By combining high-frequency timestamps and inexpensive state proofs, the Aptos blockchain provides increased security guarantees to clients. In addition, Aptos nodes also expose rich, high-performance storage interfaces that can be further fine-tuned to allow subscriptions to proofs targeting specific data and accounts on-chain.",
      "This can be leveraged by light clients to retain minimal verifiable data without the need to run a full node or process a substantial number of transactions. Pipelining, batching, and parallel transaction processing To maximize throughput, increase concurrency, and reduce engineering complexity, transaction pro- cessing on the Aptos blockchain is divided into separate stages. Each stage is completely independent and individually parallelizable, resembling modern, superscalar processor architectures. Not only does this provide significant performance benefits, but also enables the Aptos blockchain to offer new modes of validator-client interaction. For example:  Clients can be notified when specific transactions have been included in a batch of persisted transactions. Persisted and valid transactions are highly likely to be committed imminently. Clients can be informed when a batch of persisted transactions has been ordered.",
      "Thus, to reduce the latency of determining the executed transaction outputs, clients can select to execute transactions locally rather than wait for the validators to complete execution remotely. Clients can elect to wait for certified transaction execution by the validators and perform state synchronization on the attested results (e.g., see section 8). The Aptos modular design aids development speed and supports faster release cycles, as changes can be targeted to individual modules, instead of a single monolithic architecture. Similarly, the modular design also provides a structured path to scaling validators beyond a single machine, providing access to additional compute, network, and storage resources. Figure 4 shows the transaction life cycle across the various processing stages. Batch processing Batch processing is an important efficiency optimization that is part of every phase of operation in the Aptos blockchain.",
      "Transactions are grouped into batches by each validator during transaction dissemination, and batches are combined into blocks during consensus. The execution, storage, and block_timestamp9 (digest_0, timestamp_0, PoAV_0) transaction transaction transaction (digest_2, timestamp_2, PoAV_2) transaction transaction transaction (digest_7, timestamp_7, PoAV_7) transaction transaction transaction block_timestamp11 (digest_3, timestamp_3, PoAV_3) transaction transaction transaction (digest_1, timestamp_1, PoAV_1) transaction transaction transaction (digest_4, timestamp_4, PoAV_4) transaction transaction transaction Figure 5: Block metadata ordering occurs independently from transaction dissemination. ledger certification phases also work in batches to provide opportunities for reordering, reduction of operations (e.g., duplicate computation or signature verification), and parallel execution.",
      "Grouping transactions into batches can induce small amounts of latency, for example, waiting 200 milliseconds to accumulate a batch of transactions before performing dissemination. However, batching is easily configurable with respect to a maximum waiting period and maximum batch size, enabling a decentralized network to automatically optimize across latency and efficiency. Batching also allows for efficient fee markets to prioritize transactions and avoid unintended denial-of-service (DoS) attacks from overzealous clients. Continuous transaction dissemination Following the primary insight of Narwhal  Tusk 9, transaction dissemination in the Aptos blockchain is decoupled from consensus. Validators continuously stream batches of transactions to each other, utilizing all available network resources concurrently. Each batch distributed by a validator v is persisted, and a signature on the batch digest is sent back to v.",
      "Following the consensus requirements defined in Section 7.3, any 2f  1 stake weighted signatures on the batch digest form a proof of availability (PoAv). Such a proof guarantees that at least f  1 stake weighted honest validators have stored the batch, and thus all honest validators will be able to retrieve it prior to execution. Infinitely persisting batches of transactions can open a DoS attack vector by causing validators to run out of storage and crash. To prevent this, each batch of transactions has an associated timestamp. The timestamp on the batch allows efficient garbage collection at each validator. In addition, a separate per-validator quota mechanism is designed to protect validators from running out of space even in the most extreme circumstances, such as under potential byzantine attacks. Batches also have sizing constraints that are validated prior to agreement to persist to stable storage.",
      "Finally, several optimizations to de-duplicate and cache transactions reduce storage costs and ensure performant integration with the parallel execution engine. Block metadata ordering One common misconception is that consensus is slow and therefore the primary bottleneck for blockchain throughput and latency. One of the key innovations of the Aptos blockchain is to decouple non- agreement related tasks out of the consensus phase, such as transaction dissemination, transaction executionstorage, and ledger certification. By decoupling transaction dissemination from the consen- sus phase, ordering can occur with very low bandwidth (block metadata and proofs only), resulting in high transaction throughput and minimized latency. Today, the Aptos blockchain leverages the latest iteration of DiemBFTv4 10, an optimistically responsive BFT consensus protocol.",
      "Consensus in the common case only requires two network round trips (with round trip times typically less than 300 milliseconds worldwide) and dynamically adjusts to faulty validators through a leader reputation mechanism 11. The on-chain leader reputation mechanism promotes validators that have successfully committed blocks in a window and demotes validators that are not participating. This novel mechanism significantly improves performance in decentralized environments, correspondingly provides infrastructure for appropriate incentives, and quickly minimizes the impact of failed validators on throughput and latency. DiemBFTv4 guarantees liveness under partial synchrony and ensures safety under asynchrony where the total validator stake is 3f  1 with up to f stake-weighted faulty validators. DiemBFTv4 has been extensively tested in several iterations since 2019 with dozens of node operators and a multi- wallet ecosystem.",
      "We are also experimenting with our recent research (e.g., Bullshark 12) and other protocols that rely on the block history and associated communication to determine block metadata ordering and finality. A consensus block and a proposal timestamp are proposed by a leader and agreed upon by the other validators as shown in Figure 5. Note that each consensus block contains only the batch metadata and proofs. Actual transactions are not required in the block, as the PoAV ensure that the batches of transactions will be available at the execution phase after ordering (see Section 7.2). Validators may vote on a leaders proposal after verifying the proof and the block metadata criteria are met (e.g., proposal timestamp block expiration time). 7.3.1 Blockchain time The Aptos blockchain adopts an approximate, agreed-upon, physical timestamp for every proposed block, and correspondingly, all transactions within that block. This timestamp enables many important use cases.",
      "For example:  Time-dependent logic in smart contracts. For instance, a developer would like to encode that all bids on an auction must be received prior to noon on Thursday. As oracles publish on-chain data, an accurate and trusted on-chain timestamp is required to correlate events and handle delays from real-world data. Clients can discern how up-to-date they are with respect to the blockchain. For security rea- sons, to avoid stale data and long-range attacks, a client should have access to a high-precision timestamp on when the account state was updated. Auditing the blockchain with a well-trusted timestamp provides a strong correlation to off-chain events, such as ensuring that legally-enforced payouts meet expected requirements. Transaction expiration is based on the most recent committed timestamp. As an additional safe- guard for client transactions, clients can select an expiration time for a transaction, as described in Section 6.1.",
      "The Aptos blockchain provides the following guarantees with respect to timestamps for all transactions within a block:  Time is monotonically increasing in the blockchain. That is, if block B1  block B2, then B1.Time  B2.Time. If a block of transactions is agreed on with timestamp T, then at least f  1 honest validators have decided that T is in the past. An honest validator will only vote on a block when its own clock timestamp T. See Section 7.2. If a block of transactions has a quorum of signatures in consensus with timestamp T, an honest validator will not serve such a block to other validators until its own clock timestamp T. The most recent timestamp is updated on every committed block and used as the timestamp for all transactions in that block. When the network is synchronous, a block of transactions is committed ev- ery single network round trip and provides a fast updating and highly reliable clock.",
      "A finer granularity of ordering within blocks of transactions can be determined if desired. Figure 6: Block-STM (component-only) benchmarks comparing the number of physical cores with different levels of contention. Parallel transaction execution Once consensus block metadata is ordered, transactions can be executed by any validator, full node, or client. At least 2f  1 stake weighted validators have veritably persisted transactions for the proposed batches. Since transaction dissemination is continuous, additional honest validators will receive the transaction batches over time. If an honest validator has not received the transactions for the ordered batches by the time it reaches the execution stage, it can download them from the 2f 1 stake weighted validators, knowing that at least f  1 stake weighted validators ( half of the stake weighted PoAV signers) are honest. An important goal for any blockchain is to enable as much parallel execution as possible.",
      "The Aptos blockchain advances this direction forward from both the data model and the execution engine. 7.4.1 Parallel data model The Move data model natively supports global addressing of data and modules. Transactions that do not have overlapping conflicts in data and accounts can execute in parallel. Given the pipelined design used by the Aptos blockchain, reordering a group of transactions can reduce the number of conflicts, thereby improving concurrency. Even when transactions modify the same set of on-chain values, much of the transaction execution process can still be parallelized. The Aptos blockchain introduces a new concept, delta writes, that describes a modification to account state rather than the modified account state (e.g., increment an integer rather than simply determine the final value). All of the transaction processing can be completed in parallel, and then delta writes are applied in the correct sequence for conflicting values to ensure deterministic results.",
      "Over time, the Aptos blockchain will continue to enhance the data model in ways that improve concurrency (e.g., leverage readwrite hints) and also improve ergonomics, making it more natural for developers to create, modify, and compose on-chain values. Move provides flexibility to make these improvements at both the language level and also through platform-specific functionality. 7.4.2 Parallel execution engine The Block-STM parallel execution engine detects and manages the conflicts for an ordered set of transactions along with optimistic concurrency control to allow for maximum parallelism given a particular ordering 13. Batches of transactions are executed optimistically in parallel and validated post-execution. Un- successful validations lead to re-executions. Block-STM uses a multi-version data structure to avoid write-write conflicts.",
      "All writes to the same location are stored along with their versions, which contain their transaction IDs and the number of times the writing transaction was optimistically re-executed. When transaction tx reads a memory location, it obtains from the multi-version data structure the value written to this location by the highest transaction that appears before tx in the preset order, along with the associated version. Block-STM is already integrated into the Aptos blockchain. To understand the full potential of Block-STM performance, we ran experiments with non-trivial peer-to-peer Move transactions (i.e., 8 reads and 5 writes per transaction) as an isolated, execution-only (not end-to-end) benchmark with an in-memory database. In Figure 6, we presents our Block-STM execution results. Every block contains 10k transactions and the number of accounts determines the level of conflicts and contention.",
      "Under low contention, Block-STM achieves 16x speedup over sequential execution with 32 threads, while under high contention, Block-STM achieves over 8x speedup. Unique to other parallel execution engines in the blockchain space, Block-STM is able to dynamically and transparently (without any hints from the user) extract the inherent parallelism from any workload. In comparison to parallel execution environments that require upfront knowledge of data locations to be read or written, Block- STM can support more complex transactions concurrently. This property leads to fewer yet more efficient transactions, decreases cost, and provides lower latency for users. Perhaps most importantly, splitting an atomic transaction into multiple, smaller transactions breaks the all-or-nothing semantics of a single transaction with complex state outcomes. Pairing expressive transaction semantics with parallel execution in Block-STM enables developers to have the best of both worlds.",
      "Note that the block metadata ordering step does not preclude reordering transactions in the parallel execution phase. Transactions can be reordered across one or more blocks to optimize concurrency for parallel execution. The only requirement is that the reordering must be deterministic across all honest validators. Optimizing for parallel execution as well as adding randomization into the reordering can increase the performance and potentially discourage maximal extractable value (MEV) techniques for profitable validator transaction reordering. Order-then-reveal MEV resistant strategies can also be incorporated into this pipelined design. Block-STM and transaction reordering are complementary techniques to increase execution paral- lelism. They can be combined with transaction readwrite access hints for additional concurrency. Batch storage The parallel execution phase results in write sets for all transactions in a group.",
      "These write sets can be stored in memory for maximum execution speed and then used as a cache for the next block or set of blocks to be executed. Any overlapping writes only need to be written to stable storage once. If a validator fails before storing the in-memory write sets, it can simply resume parallel execution from the block metadata ordering phase. Decoupling the batch storage of write sets from the parallel execution step ensures parallel execution can operate efficiently. In summary, batching write sets reduces the number of storage operations and takes advantage of more efficient, larger IO operations. The amount of memory reserved for write set caching can be manually configured per machine and provides a natural back-pressure mechanism. The granularity of the batches can be different from the granularity of parallel execution blocks if desired to tune for specific IO and memory environments.",
      "Ledger certification At this point in the pipeline, every individual validator has computed the new state for a committed block of transactions. However, to efficiently support verified light clients and state synchronization, the Aptos blockchain implements ledger certification for the ledger history as well as the ledger state. One key difference for the Aptos blockchain is that ledger certification is not on the critical path of transaction processing and can even be run completely out-of-band if desired. 7.6.1 Ledger history certification A validator appends the transactions together with their execution output to a global authenticated ledger data structure. Part of the transaction output is the state write set, consisting of the alterations made to the global state accessible by Move. The short authenticator of this data structure is a binding commitment to a ledger history, which includes the newly executed batch of transactions.",
      "Similar to transaction execution, the generation of this data structure is deterministic. Each validator signs the short authenticator to the new version of the resulting database. Validators share their recent set of signed short authenticators with each other, collectively aggregate quorum- signed short authenticators, and also share the recent quorum-signed short authenticators with one another. Using this collective signature, clients can trust that a database version represents the complete, valid, and irreversible ledger history according to the BFT properties of the protocol. Clients can query any validator (or any third-party replica of the database, such as a full node) to read a database value and verify the result using the authenticator and a proof of the desired data. 7.6.2 Periodic state certification The entirety of the global state accessible by Move can be summarized to a short authenticator at any point in history, similar to a summary of the ledger history.",
      "Due to the random access nature of the global state (unlike the ledger history which is append-only), the cost of maintaining this authentication is significant. Nevertheless, when updating the data structure in a large batch, we can compute the update in parallel and also exploit any overlap among the parts that must be updated when each individual state value changes. The Aptos blockchain deliberately only periodically certifies the global state to reduce duplicate shared updates. During deterministic and configured intervals, the network issues the state checkpoint transactions that include the global state authenticator as part of their output. Such versions are denoted state checkpoints. The larger the gap between two checkpoints, the lower the amortized cost of updating the state authenticated data structure per transaction. With state checkpoints, one can read any state value from them in a trustless way without storing all of the global state.",
      "This ability is useful for applications like incremental state syncing, sharded storage across validators, stateless validator nodes, and storage-constrained light clients. However, because the state checkpoints are periodic, getting a proof to a specific version of the ledger state requires either additional transaction execution for the missing state alternations or an inclusion proof of them from the authenticated ledger history. State checkpoints are tied to the specific transaction versions in the ledger history, hence bound to the timestamp associated with transaction batches mentioned in Section 7. With the timestamp, a light client can understand the recency of a proven state value. Without a timestamp, a light client proof can only ensure the validity of a previous state that could be far in the past, which provides little assurance of relevance.",
      "Also, timestamps for state proofs are necessary for tracking historical access and auditing purposes, such as calculating the average hourly balance of tokens in a token reserve. State checkpoints can be derived based on a previous state checkpoint and state alternations in the transaction outputs after it. Hence, persisting state checkpoints to stable storage does not need to be on the critical path for transaction processing. Also, beneficial batching effects exist when persisting the state checkpoints as well. Caching the recent state checkpoints (or rather the delta between them) in memory and dumping only the periodic state checkpoints to stable storage can greatly reduce consumption of storage bandwidth. The way checkpoints are chosen to be persisted does not affect the calculation of the authenticated data structure. Hence, this is a per-node choice: node operators can configure the appropriate trade-off between memory capacity and storage bandwidth.",
      "State synchronization The Aptos blockchain aims to provide a high throughput, low latency system for all participants in the ecosystem. As a result, the blockchain must offer an efficient state synchronization protocol to disseminate, verify, and persist blockchain data to light clients, full nodes, and validators 14. In addition, the synchronization protocol must also be tolerant of resource constraints and heterogeneity within the network, accounting for different users and use cases. For example, it must allow archival full nodes to verify and persist the entire blockchain history and state, while also enabling light clients to efficiently track only a small subset of the Aptos blockchain state. To achieve this property, the Aptos blockchain leverages the authenticated ledger history and certified state proofs (see Section 7.6.1) offered by the validators, full nodes, and other replicators to provide a flexible and configurable synchronization protocol.",
      "Specifically, participants in the network can select different synchronization strategies to optimize for their use cases and requirements. For example, in the case of full nodes, Aptos allows multiple synchronization strategies, including the ability to process all transactions since the beginning of time or skip the blockchain history entirely and synchronize only the latest blockchain state using waypoints. In the case of light clients, strategies include synchronizing partial blockchain states, e.g., specific accounts or data values, and enabling verified state reads, e.g., verified account balance fetching. In all cases, Aptos allows participants to configure the amount and age of the data to fetch, process, and retain. By adopting a flexible and configurable approach to state synchronization, Aptos can adapt to a variety of client requirements and continue to offer new and more efficient synchronization strategies in the future.",
      "Community ownership The Aptos blockchain will be owned, operated, and governed by a broad and diverse community. A native Aptos token will be used for transaction and network fees, governance voting on protocol upgrades and on-chainoff-chain processes, and securing the blockchain via a proof-of-stake model. A complete description of Aptos token economics will follow in a future publication. Transaction and network fees All Aptos transactions have a gas unit price (specified in Aptos tokens) that allows validators to prioritize the highest value transactions in the network. Moreover, at every stage of the pipelined model, there are multiple opportunities to discard low-value transactions (allowing the blockchain to operate efficiently when at system capacity). Over time, network fees will be deployed to ensure that the costs of using the Aptos blockchain are proportionate to the real-world costs of hardware deployment, maintenance, and node operation.",
      "Furthermore, developers will have the opportunity to design applications with different cost trade-offs between compute, storage, and networking. Network governance Every significant feature change and improvement on the Aptos blockchain will go through several phases, including proposal, implementation, testing, and deployment. This structure creates opportu- nities for relevant parties and stakeholders to provide feedback, share concerns, and offer suggestions. The final phase, deployment, is typically achieved in two steps. First, a software release with the new functionality will be deployed to each node, and second, the functionality will be enabled, e.g., via a feature flag or on-chain configuration variable. Each software deployment by the node operators must be backward compatible, to ensure the new software is interoperable with the supported releases.",
      "The process of deploying a new software version may span multiple days, to account for operators in different time zones and any external issues. Once a sufficient number of nodes have been upgraded, the enablement of the new functionality can be triggered by a synchronization point, such as an agreed-on block height or epoch change. In emergency conditions (e.g., when downtime is unavoidable), the enablement can be through a manual and forced change by the node operators, and in the worst cases, a hard fork in the network. In comparison to other blockchains, the Aptos blockchain encodes its configuration on-chain. Every validator has the ability to synchronize with the current state of the blockchain and automatically select the correct configuration (e.g., consensus protocol and Aptos framework version) based on the current on-chain values. Upgrades in the Aptos blockchain are seamless and instant due to this functionality.",
      "To provide flexibility and configurability to the enablement process, the Aptos blockchain will support on-chain governance where token holders can vote with respect to their staked token weights. On-chain voting protocols are public, verifiable, and can be instantaneous. On-chain governance can also support the enablement of non-binary outcomes without software deployment. For example, the on-chain leader election protocol parameters can be modified with on-chain governance whereas a pre- known synchronization point would be unable to handle dynamic modifications since all changes would have to be known ahead of time. On-chain governance can, over time, be deployed across the entire upgrade management process. As an example: 1. Token holders vote on-chain about transitioning to a new quantum-resistant signature scheme. 2. Developers implement and verify the new signature scheme and create a new software release. 3. Validators upgrade their software to the new release. 4.",
      "Token holders vote on-chain to enable the new signature scheme, the on-chain configuration is updated, and the change takes effect. As an open source project, the Aptos blockchain will depend on strong community feedback and use on-chain governance to manage the appropriate processes. Off-chain upgrade enablement may still be required under certain conditions, but will be minimized over time. Proof-of-stake consensus To participate in transaction validation on the Aptos blockchain, validators must have a minimum required amount of staked Aptos tokens. The staked amounts proportionately affect the 2f  1 stake weighted PoAv during transaction dissemination as well as vote weights and leader selection during block metadata ordering. Validators decide on the split of rewards between themselves and their respective stakers. Stakers can select any number of validators in which to stake their tokens for a pre-agreed reward split.",
      "At the end of every epoch, validators and their respective stakers will receive their rewards via the relevant on-chain Move modules. Any validator operator with sufficient stake can freely join the Aptos blockchain. All parameters, including the minimum stake required, can be set by the on-chain enablement processes described in Section 9.2. Performance As mentioned in Section 7, the Aptos blockchain is able to achieve optimal throughput and hardware efficiency via its parallel, batch optimized, and modular transaction processing pipeline. Additional performance initiatives, such as consensus upgrades, delta writes, transaction hints, and critical path caching, will continue to increase throughput and improve efficiency over time. Today, blockchain throughput is typically measured in transactions per second. However, given the wide range of costs and complexity across transactions and infrastructures, this is an imprecise method to compare systems.",
      "Transaction latency is also equally flawed as the starting and ending points of submission to finality are varied across experiments. In addition, some systems require apriori knowledge of transaction inputs and outputs and force logical transactions to be split into smaller, less complex transactions. Splitting a transaction results in poor user experiences and artificially impacts latency and throughput, without considering what the developer is trying to accomplish. In contrast, the Aptos approach is to enable developers the freedom to build without limits and to measure throughput and latency with respect to real-world use cases rather than synthetic transactions. The Aptos blockchain will continue to optimize individual validator performance, as well as exper- iment with scaling techniques that add more validators to the network. Both directions have distinct trade-offs.",
      "Any blockchain with parallel execution capabilities can support additional concurrency by requiring more powerful hardware or even structuring each validator as a cluster of individual ma- chines. However, there are practical limits to the number of global validators that is commensurate to the cost and complexity for validator operators. The rise and popularity of serverless databases in cloud services exemplify how few entities can efficiently deploy and maintain these types of complex distributed systems. 10.1 Homogeneous state sharding Initially, the Aptos blockchain will be launched with a single ledger state. Over time, the Aptos network will take a unique approach to horizontal scalability while still maintaining decentralization. This will occur through multiple sharded ledger states, each offering a homogeneous API and sharding as a first-class concept. The Aptos token will be used for transaction fees, staking, and governance on all shards.",
      "Data may be transferred between shards through a homogeneous bridge. Users and developers can choose their own sharding schemes depending on their needs. For example, developers can propose a new shard or cluster users within existing shards to achieve high intra-shard connections. More- over, shards may have different system characteristics. One shard might be compute-optimized with SSDs, and another could be optimized for large hard drives with low compute characteristics. By pro- viding hardware flexibility between different shards, developers can leverage the appropriate system characteristics for their applications. In summary, homogeneous state sharding provides the potential for horizontal throughput scala- bility, allows developers to program with a single universal state across shards, and enables wallets to easily incorporate sharded data for their users. This provides significant performance benefits as well as the simplicity of a single unified Move smart contracts platform.",
      "1 Aptos-core, 2022. Online. Available: https:github.comaptos-labsaptos-core 2 Move, 2022. Online. Available: https:github.commove-languagemove 3 D. Matsuoka, C. Dixon, E. Lazzarin, and R. Hackett. (2022) Introducing the 2022 state of crypto report. Online. Available: https:a16z.comtagstate-of-crypto-2022 4 Z. Amsden, R. Arora, S. Bano, M. Baudet, S. Blackshear, A. Bothra, G. Cabrera, C. Catalini, K. Chalkias, E. Cheng, A. Ching, A. Chursin, G. Danezis, G. D. Giacomo, D. L. Dill, H. Ding, N. Doudchenko, V. Gao, Z. Gao, F. Garillot, M. Gorven, P. Hayes, J. M. Hou, Y. Hu, K. Hurley, K. Lewi, C. Li, Z. Li, D. Malkhi, S. Margulis, B. Maurer, P. Mohassel, L. de Naurois, V. Nikolaenko, T. Nowacki, O. Orlov, D. Perelman, A. Pott, B. Proctor, S. Qadeer, Rain, D. Russi, B. Schwab, S. Sezer, A. Sonnino, H. Venter, L. Wei, N. Wernerfelt, B. Williams, Q. Wu, X. Yan, T. Zakian, and R. Zhou, The libra blockchain, 2019. Online.",
      "Available: https:developers.diem.compapersthe-diem-blockchain2020-05-26.pdf 5 S. Blackshear, E. Cheng, D. L. Dill, V. Gao, B. Maurer, T. Nowacki, A. Pott, S. Qadeer, D. R. Rain, S. Sezer, T. Zakian, and R. Zhou, Move: A language with programmable resources, 2019. Online. Available: resources2019-06-18.pdf 6 D. Dill, W. Grieskamp, J. Park, S. Qadeer, M. Xu, and E. Zhong, Fast and reliable formal verification of smart contracts with the move prover, in Tools and Algorithms for the Construction and Analysis of Systems, D. Fisman and G. Rosu, Eds. Cham: Springer International Publishing, 2022, pp. 183200. 7 N. Popper. (2021) Lost passwords lock millionaires out of their bitcoin fortunes. Online. Available: 8 The Diem Team, State synchronization verification committed information system with reconfigurations, 2020. Online. Available: coreblobmaindocumentationtech-paperslbft-verificationlbft-verification.pdf 9 G. Danezis, L. Kokoris-Kogias, A. Sonnino, and A.",
      "Spiegelman, Narwhal and tusk: A dag-based mempool and efficient bft consensus, in Proceedings of the Seventeenth European Conference on Computer Systems, ser. EuroSys 22. New York, NY, USA: Association for Computing Machinery, 2022, p. 3450. Online. Available: https:doi.org10.11453492321.3519594 10 The Diem Team, Diembft v4: State machine replication in the diem blockchain, 2021. Online. Available: blockchain2021-08-17.pdf 11 S. Cohen, R. Gelashvili, L. Kokoris-Kogias, Z. Li, D. Malkhi, A. Sonnino, and A. Spiegelman, Be aware of your leaders, CoRR, vol. abs2110.00960, 2021. Online. Available: https:arxiv.orgabs2110.00960 12 A. Spiegelman, N. Giridharan, A. Sonnino, and L. Kokoris-Kogias, Bullshark: Dag bft protocols made practical, in Proceedings of the 20th Conference on Computer and Communications Security (CCS), ser. CCS 22. Los Angeles, CA, USA: Association for Computing Machinery, 2022. 13 R. Gelashvili, A. Spiegelman, Z. Xiang, G. Danezis, Z. Li, Y. Xia, R. Zhou, and D.",
      "Malkhi, Block-stm: Scaling blockchain execution by turning ordering curse to a performance blessing, 2022. Online. Available: https:arxiv.orgabs2203.06871 14 J. Lind, The evolution of state sync: The path to 100k transactions per second with sub-second latency at aptos, 2022. Online. Available: https:medium.comaptoslabs52e25a2c6f10"
    ],
    "word_count": 9775,
    "page_count": 17
  },
  "AR": {
    "chunks": [
      "Arweave: A Protocol for Economically Sustainable Information Permanence Sam Williams samarweave.org Viktor Diordiiev viktorarweave.org Lev Berman levarweave.org India Raybould indiaarweave.org Ivan Uemlianin ivanarweave.org DRAFT-1 C0bQOaBVoznjkrabyM4TPs5Rf5-ZthrY8MYDr-FU6DRle4yBhaMlNphglTrb06Hp Abstract Blockchains have been used as mechanisms to store memoised rep- resentations of history since the very \ufb01rst Bitcoin block 47. Despite blockchain technologys clear potential in the area of resilient archive construction without single points of failure, advances in on-chain data storage techniques have remained elusive. This paper addresses this problem through the introduction of the Arweave protocol: a new mechanism design-based approach to achieving a sustainable and per- manent ledger of knowledge and history. As well as outlining incentive mechanisms for achieving sustainable data permanence, this paper out- lines key technologies to allow scalable on-chain storage.",
      "Contents Introduction Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . Structure of This Document . . . . . . . . . . . . . . . . . . . Arweave: Key Contributions The Recall Block, Proof of Access, and the Blockweave . . . . Memoisation of State . . . . . . . . . . . . . . . . . . . . . . . Blockshadows . . . . . . . . . . . . . . . . . . . . . . . . . . . Content Policies and Censorship Resistance . . . . . . . . . . Wild\ufb01re . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . The Network: Mechanism Design Di\ufb03culty: Regulating the Block Generation Rate . . . . . . . Token Economy . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.1 Paying the Network, Rewarding the Miners . . . . . . 3.2.2 Cost of Perpetual Data Storage . . . . . . . . . . . . . 3.2.3 Transaction Pricing . . . . . . . . . . . . . . . . . . . 3.2.4 Storage Endowment . . . . . . . . . . . . . . . . . . .",
      "3.2.5 Future Data Density and Reliability Expectations 3.2.6 Data Permanence, Not Network Permanence . . . . . Proof of Access: Dominant Strategy . . . . . . . . . . . . . . Implementation of the AIIA Wild\ufb01re Agent . . . . . . . . . . 3.4.1 Rationalising Outbound Bandwidth . . . . . . . . . . 3.4.2 Promoting Responsiveness . . . . . . . . . . . . . . . . 3.4.3 Network Ecology . . . . . . . . . . . . . . . . . . . . . Fork Resistance and Recovery . . . . . . . . . . . . . . . . . . 3.5.1 Target Behaviour and Related Incentives . . . . . . . The Node: Behaviours for Protocol Compliance Mine and Propagate New Blocks . . . . . . . . . . . . . . . . 4.1.1 Block Construction Procedure . . . . . . . . . . . . . 4.1.2 Incentives . . . . . . . . . . . . . . . . . . . . . . . . . Receive, Validate, and Propagate Transactions and Blocks . . 4.2.1 Transactions . . . . . . . . . . . . . . . . . . . . . . . 4.2.2 Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "Receive and Respond to Requests . . . . . . . . . . . . . . . . Fork Avoidance and Fork Recovery . . . . . . . . . . . . . . . Decentralised Content Policies Voting Phase . . . . . . . . . . . . . . . . . . . . . . . . . . . Storage Phase . . . . . . . . . . . . . . . . . . . . . . . . . . . Incentives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Trade-O\ufb00s . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Gateways . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Adaptive Mechanism Design in Arweave Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . Comparison with Consensus-Based Games . . . . . . . . . . . Implementation Outline and Emergent Properties . . . . . . . Simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . Arweave Protocol Interoperability and the Permaweb Characteristics . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "7.1.1 A Trustless and Provable Web . . . . . . . . . . . . . 7.1.2 Web Responsiveness Through Incentivisation . . . . . 7.1.3 Open HTTP API . . . . . . . . . . . . . . . . . . . . . Application Architectures . . . . . . . . . . . . . . . . . . . . 7.2.1 Client-Server . . . . . . . . . . . . . . . . . . . . . . . 7.2.2 Serverless Web Application Architecture . . . . . . . . Gateway Nodes . . . . . . . . . . . . . . . . . . . . . . . . . . 7.3.1 ArQL . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.3.2 Arweave DNS and TLS . . . . . . . . . . . . . . . . . Use Cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Example Applications . . . . . . . . . . . . . . . . . . . . . . Future Work Succinct Proofs of Access . . . . . . . . . . . . . . . . . . . . Wallet Logs . . . . . . . . . . . . . . . . . . . . . . . . . . . . Fast Find . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Conclusion 10 Appendices 10.1 Hard Drive Capacities and MTBF Over Time . . . . . . . .",
      ". 10.2 Anatomies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.2.1 Block Data Structure . . . . . . . . . . . . . . . . . . 10.2.2 Deep Hash . . . . . . . . . . . . . . . . . . . . . . . . 10.2.3 Blockshadow Data Structure . . . . . . . . . . . . . . 10.2.4 Transaction Data Structure . . . . . . . . . . . . . . . Introduction Motivation In this work we present the Arweave protocol, a new blockchain-like data structure called the blockweave. The protocol is designed to provide scalable and permanent on-chain data storage in a sustainable manner. The block- weave forms the underlying data structure of the permaweb - the array of data, websites, and decentralised applications hosted on the blockweave, accessible on normal web browsers.",
      "In this paper, we will introduce several technological innovations that, to- gether, allow Arweave to o\ufb00er unique utility in the on-chain data storage space, including: blockweave, blockshadows, AIIA, decentralised content policies, and their mechanism design. Our collective ability to store and share information between individuals and across time to new generations has been essential to humanitys successes. Despite our best e\ufb00orts, however, throughout history, our methods of storing knowledge have been vulnerable to destruction and the information subject to loss or alteration  sometimes with intentional malice, and more often accidentally. Just as in the ancient world, modern history is full of examples of the destruction, alteration, and loss of vital information, from \ufb01res at libraries and archives 41, 22, 35, to book burning in authoritarian states 57.",
      "Today, with a wealth of digital information surrounding us, we can easily begin to assume that because information is readily available online today, it cant be altered or lost. Unfortunately, this is foundationally untrue 21, 37. Although the internet is a wildly successful system of distributed information dissemination, it currently lacks a complementary system of decentralised, permanent knowledge storage. Almost all of the pages making up the web today are housed within cent- ralised data stores, each typically controlled by one organisation or even one individual. This means that when accessing information online, we are wholly reliant on these centralised organisations and individuals continu- ing to allow us to do so. Access can be revoked at any time, or the data can simply be unintentionally lost or degraded.",
      "Serving information on the internet incurs server and other upkeep costs, meaning whole websites, ap- plications, and information stores can simply disappear when funds are no longer available for maintenance. Another signi\ufb01cant risk of this centralised data storage model is that the data is vulnerable to manipulation by these individuals or organisations. Such manipulation tactics typically include the modi\ufb01cation of documents during their storage 34, 48. Arweave o\ufb00ers a solution to this problem. Further still, a number of governments are increasing their e\ufb00orts to censor and remove access to politically sensitive information on the internet 68, 6, 1. Similarly, with media and news organisations, while we once held physical and irrevocable copies of their publications, we now simply access the information digitally and then immediately discard it. It has become commonplace for media organisations to update the contents of their articles over time.",
      "This can cause vital context and content to be lost or obscured. In order for an information store to be truly permanent, it must be both re- silient and decentralised. Blockchain technology has much obvious promise in the area of resilient, decentralised information preservation 14, as a key feature of the technology is that all data inside the blockchain is immutable, and cannot be altered once it is stored. However, traditionally, such tech- nology severely lacks scalability which clearly limits its utility for storing signi\ufb01cant quantities of data. High transaction fees also inhibit typical blockchains ability to scale to accommodate large amounts of data. For instance, with the Ethereum net- work, although it is technically possible to store data on-chain, the high fees make this impractical for most real-world use cases. As the demand for data storage grows exponentially 56, the need for a decentralised, low-cost data storage protocol that can scale is a necessity.",
      "Once a piece of data is stored in the data structure, it is cryptographically entangled with every other previous block in the network. This ensures that any attempt to change the contents of the document will be automatically detected and consequently rejected by the network. Therefore, the Arweave o\ufb00ers a robust way of permanently storing data on-chain, beyond the reach of accidental or intentional data loss or manipulation. Owing to the decentralised and cryptographically veri\ufb01ed nature of the per- maweb, it is beyond the reach of any organisations or groups that might conspire to censor its contents 36. As such, the Arweave drastically re- duces the possibility of an Orwellian memory hole 51 from occurring. In this way, we envisage the Arweave protocol as a useful tool in the preserva- tion of the freedom of information, and subsequently in the strengthening of institutions and democratic processes that depend upon it.",
      "However, in order for the Arweave protocol to ful\ufb01l its potential and enable the permaweb to become the basis of the new, decentralised web, widespread developer adoption of the protocol is essential. As such, the protocol is designed to make it simple, and both cost and time e\ufb03cient for developers to store content on the network. Arweaves HTTP API makes it extremely simple to build decentralised web applications on top of the blockweave. In order to make permaweb development as simple as possible, developers can use all of their favourite familiar web technologies (including HTML, Javascript, and CSS) and deploy to the permaweb in minutes. For the end-users themselves, the permaweb built on the Arweave protocol o\ufb00ers several major advantages. Firstly, as we have touched on previously in this paper, users are guaranteed reliable, immutable access to permaweb content.",
      "Of especially great importance is users ability to reliably maintain access to all permaweb applications and websites themselves, not simply the content they display, forever. This means that, once published, a permaweb app cannot be unpublished, helping to maintain strict application integrity 5. This allows users to exercise meaningful choices based on their personal preferences. The speed of access to permaweb content is also incredibly important for the user experience, and additionally for adoption of the Arweave protocol and permaweb itself. The Arweave protocol is the only system to o\ufb00er truly permanent and decentralised data storage. The protocol is designed to robustly incentivise Arweave nodes to share data around the network quickly, a key aspect of the protocols mechanism design explained in section 6 describing the AIIA meta-game, and its implementation in the wild\ufb01re mechanic.",
      "The Arweave protocol and its family of technologies have all been engineered to ensure that each nodes behaviour is likely to contribute positively to the utility of the network itself. In this way, the protocol design is Dominant Strategy Incentive Compatible (DSIC, 58), a vital facet of a healthy, long- term decentralised and incentive-driven network such as Arweave. As such, the Arweave protocol is designed to address many of the short- comings in both traditional archiving systems and also the scalability issues found with many typical blockchain protocols. The protocol is designed to maximise the quality and e\ufb03ciency of both developer and end-user experi- ences, which is vital when aiming for wide-scale adoption. Structure of This Document 1. Introduction 2. Arweave: Key Contributions 3. The Network: Mechanism Design 4. The Node: Behaviours for Protocol Compliance 5. Decentralised Content Policies 6.",
      "Adaptive Mechanism Design in Arweave: Adaptive Interacting Incent- ive Agents (AIIA) 7. Arweave Protocol Interoperability and the Permaweb 8. Future Work 9. Appendices This document is split into eight sections, plus an appendix section. Here in section 1, The Introduction, we establish the main motivations and goals of creating the Arweave protocol: providing permanent, resilient storage. Section 2 highlights the novel key contributions the Arweave protocol o\ufb00ers to the decentralised data storage space. The speci\ufb01c innovations described include: the blockweave, the Proof of Access consensus mechanism, the recall block, memoisation of state, blockshadows, the Adaptive Interacting Incentive Agents (AIIA) meta-game, and wild\ufb01re, an implementation of an AIIA game. This section explains how the blockweave di\ufb00ers from a traditional blockchain data structure, as well as the implications and bene\ufb01ts of these di\ufb00erences.",
      "Section 3, The Network: Mechanism Design, establishes that distributed, decentralised storage is necessary to meet the requirements of permanence and resilience. Taking a game-theoretic and mechanism design approach, this section describes how the Arweave protocol is carefully engineered to balance incentives and constraints to produce a network that is Domin- ant Strategy Incentive Compatible (DSIC, 58), ultimately maximising pro- social utility of the networks overall output. Section 4, The Node: Behaviours for Protocol Compliance, looks at the network and the protocols from the perspective of how nodes behave within these systems. This section covers in detail the main activities of a node: mining blocks, receiving and validating blocks and transactions, and serving blocks and transactions. To illustrate how the Arweave protocol promotes pro-social behaviour, this section includes some examples of what happens when a node tries to cheat.",
      "Section 5, Democratic Content Policies, describes the mechanism by which Arweave miners - the data storage providers in the network - can accept, store, and share only the content they choose to. This section describes the three primary aspects of content policies: the voting phase, the storage phase, and the incentive (or, mechanism) design. Here, you will discover how the blockweave enables miners to democratically decide which storage content they collectively do or do not wish to store inside the network. Section 6, covering the phenomenon of Adaptive Interacting Incentive Agents (AIIA), describes how the mechanism design and the technical implementa- tion of the Arweave protocol combine to create a powerful web of mutually bene\ufb01cial and counter-balancing incentives.",
      "The powerful emergent e\ufb00ects of this interaction are dissected, demonstrating how the protocol produces high levels of positive externalities  including pro-sociality between nodes and users alike  without forcing compliance on agents in the system. Section 7, Arweave Protocol Interoperability and the Permaweb, describes all of the key technical and infrastructure components that together make up the Arweave protocol, and consequently, the permaweb itself. This section describes how fundamentally, the protocol operates in a trustless, serverless, and distributed manner. Additionally, the speci\ufb01c permaweb developer- and miner-oriented technologies of the protocol are enumerated, including: the HTTP API, the client-server, gateway nodes, hybrid architectures, ArQL, Arweave DNS  TLS, and a number of live permaweb applications.",
      "Section 8, Future Work, explores the core Arweave development teams views on what additional technologies may be added to the Arweave protocol to further enhance its unique o\ufb00ering. Firstly, the section describes succinct proofs of access, whereby data storage remains resilient, veri\ufb01able, and se- cure, but the amount of data contained in transactions passed around the network is reduced, increasing e\ufb03ciency. Secondly, a proposed reduction in the storage space dedicated to maintaining full wallet logs is considered. Finally, a proposal for a potential fast \ufb01nd mechanism, is discussed in de- tail, which would ultimately accelerate how quickly data is located in the network when requested. Section 10, The Appendices, enumerates the speci\ufb01c contents of a range of novel Arweave data structures, including block, blockshadow, and transac- tion data structures, as well as providing key datasets.",
      "Arweave: Key Contributions This section looks at unique innovations engineered for inclusion in the Ar- weave protocol, how these impact usability, resilience, and permanence of data storage on the Arweave network. The Recall Block, Proof of Access, and the Blockweave In Arweaves blockweave data structure each block is linked to two prior blocks: the previous block in the chain (as with traditional blockchain protocols), and a block from the previous history of the blockchain (the recall block). Consequently, the Arweave blockchain data structure is not strictly a chain (i.e., a singly linked list) but it is a slightly more complex graph structure that we call the blockweave. The recall block is selected based on a hash of the previous block and the previous blocks height. This results in a deterministic but unpredictable choice of block from the weaves history. In order to mine or verify a new block, a node must have that blocks recall block.",
      "Demonstrating proof that the miner has access to the recall block is part of block construction (and conversely, verifying this proof is part of validating a new block). Proof of Access (PoA) is an enhancement of Proof of Work (PoW) in which the entire recall block data is included in the material to be hashed for input to the proof of work. For further details of the block construction process, see section 4.1.1. Requiring PoA incentivises storage as miners need access to random blocks from the blockweaves history in order to mine new blocks and receive mining rewards. The PoA algorithm also incentives miners to store rare blocks more than it incentivizes them to store well-replicated blocks. This is because when a rare block is chosen, miners with access to it compete amongst a smaller number of miners in the PoW puzzle for the same level of reward. As a consequence of this, miners that prefer to store rarer blocks on average receive a greater reward over time, all else being equal.",
      "PoA takes a probabilistic and incentive-driven approach to maximising the number of redundant copies of any individual piece of data in the network. By contrast, other decentralised storage networks specify an exact number of redundant copies that should be provided for a given piece of data, and me- diate this using a system of contracts12. The Arweaves competition-based approach is tailored such that it is simpler and more versatile, performing \ufb02exibly in both well-functioning and poorly-functioning environments. For example, in a network storing 1 PB of data where only 2 PB of data stor- age capacity is available (an unhealthy network) the PoA incentive struc- ture pushes miners to make sure they have copies of data that few other miners are storing, reacting \ufb02exibly to the situation.",
      "In a well-functioning network where, for example, only 1PB of an available 100 PB storage ca- pacity is in use, the Arweave also pushes miners to make larger quantities of redundant copies of data in the network. As well as increasing security and stability in such a situation, this approach to redundancy maximisation through incentives also leads to dramatically faster lookup and access times. Memoisation of State In the standard blockchain paradigm (e.g. in the Bitcoin blockchain47), the blockchain is a concrete object replicated entirely on every full node in the network. Each full node must store the blockchain in its entirety. As explained in the previous subsection, with Arweave this is not necessary. The Arweaves novel data structure, the blockweave, does not require miners to store every previous block.",
      "In order to achieve this, all data required to process new blocks and new transactions is memoised into the state of each individual block (see section 10.2.1 for details of the block data structure). As a consequence of this memoisation technique, new users are able to join the network by downloading only the current block from its trusted peers, or verifying some backward portion of proofs of work (the larger the quantity veri\ufb01ed the lower the trust required to join the network). This block data structure includes, among other things, the Block Hash List (BHL) and the Wallet List (WL). Possession of the Block Hash List allows old blocks to be requested andor veri\ufb01ed. Possession of the Wallet List allows new transactions to be veri\ufb01ed without possessing the block in which the wallets last transaction was included.",
      "The Block Hash List and the Wallet List are kept up to date by miners and synchronised by the network when mining and validating new blocks (see section 4 for details of node behaviours during Arweave mining). This reduces barriers to entry for miners  barriers of storage space, pro- cessing power, and time  allowing the blockweave to scale to sizes larger than the capacity of any individual miner. As the Arweave network in- tends to scale past the size, scope, and volume of the traditional web, this mechanism is vital to allowing miners to practically join the network. Parties interested in verifying the full blockweave from the \ufb01rst block to the current block, and in reconstructing the entire blockweave locally can do so in a number of ways.",
      "For example, by following the link in each block to that blocks previous block, or, by requesting a Block Hash List from a trusted peer, verifying it against the unbalanced Merkle tree hash list in the current block, and downloading the blocks directly. Once joined and active, there is no need for a node to store the entire blockweave at all, however, they are rewarded by the Proof of Access mech- anism relative to the proportion of the blockweave they store. The Domin- ant Strategy Incentive Compatible (DSIC58) nature of the network gives network-level guarantees of storage and replication. This frees each indi- vidual node to prioritise and optimise their own storage according to their own preference and resources. A blockchain is typically a very large distributed data structure, and down- loading the full blockchain can take a long time and consume a great deal of computing resources13.",
      "Unlike traditional blockchain systems, Arweave does not have a typical notion of full and light clients  merely clients that downloaded more or less of the blockweave. With Arweave, full synchron- isation is not a risk or an obligation, but an optional upgrade path for which miners receive higher rewards. Blockshadows In a traditional blockchain network, when a new block is mined, each entire block is distributed to every node in the network, no matter how many of the blocks transactions that node already possesses. This signi\ufb01cantly limits the amount of data that can be included in a block, as all of the data needs to be gossiped around the network during consensus. If too much data is transferred during block acceptance, the time required to achieve consensus becomes too large and forks emerge in the network.",
      "The probability of a fork emerging in a blockchain network during consensus is as follows: P(fork)  Bdist time Btime Subsequently, as the block distribution time is linear with the blocks size, so too is the size linear with the likelihood of forks emerging. As the Arweave protocol stores data inside the blocks themselves, the normal blockchain trade-o\ufb00between P(fork) and Bmax size is not acceptable. Therefore, the Arweave protocol takes a new approach to data distribution: blockshadows, building on the work of Graphene52 and compact blocks (BIP-152)19. Blockshadowing works by decoupling transactions from blocks, and only sending a minimal blockshadow between nodes which allows peers to re- construct a full block, rather than transmitting the full block itself. These blockshadows contain a hash of the Wallet List and Block Hash List, and list of transaction hashes (instead of the transactions inside a block).",
      "From this information (typically a few kilobytes), a node that already holds all of the transactions inside the block, plus an up-to-date Block Hash List and Wallet List can rapidly reconstruct an entire block of almost arbitrary size. Using this mechanism, the bottleneck for block size becomes the number of transaction IDs that can be included, and the length of time required to reconstruct a block from its constituent transactions. In order to facilitate block distribution and lower communication overhead, nodes immediately share transactions with one another, but only attempt to place transactions inside a block once they have a high degree of certainty that other nodes in the network also have a copy of the transaction. Further improving on the work presented in Graphene52 and BIP-15219, block- shadows introduce a mechanism design-based AIIA game for calculating the likelihood that other nodes in the network already have local access to a block.",
      "Because nodes do not attempt to fetch missing transactions from one another if they do not have them locally, nodes are incentivised to act in the following way: 1. Not to mine transactions into blocks too early, as this leads to their blocks being rejected. 2. Not to mine transactions into blocks too late, as other miners in the network are likely to mine them beforehand. The result of this blockshadowing system is a fast and \ufb02exible block distri- bution process that allows transactions to be mined into a block as fast as they can be distributed around the network, and consensus about blocks to be achieved at near network speed. Content Policies and Censorship Resistance The Arweave protocol avoids making it an obligation to store everything, which in turn allows each node to decide for itself which blocks and trans- actions to store. This increases the censorship resistance of the network3 as nodes are not forced to store material they dont want to.",
      "The default behaviour of the network is a large number of replications of each single accepted transaction. The current replication rate in the network exceeds 97. As a necessary part of block validation, transactions are stored on-chain and replicated throughout the network, so the data is distributed widely in geographic terms. Storage is guaranteed probabilistically and at the network level (rather than on a per-miner basis), consequently strength- ening these guarantees, making them robust, and resilient to interference. These advantages are bene\ufb01cial to both end-users, for whom data access is faster and more reliable than other alternatives, and also for miners, who are able to select which blocks and transactions they wish to store, allowing them to implement content policies that meet their preferences (see section 5 for more information on such policies).",
      "Wild\ufb01re In the wild\ufb01re mechanic, a form of AIIA game (see section 6), each node in the Arweave network ranks its peers based on two primary factors. Firstly, the peers generosity - sending new transactions and blocks, secondly, the peers responsiveness - responding promptly to requests for information, in a similar mechanism to Bittorrents optimistic tit-for-tat algorithm18. The node then gossips preferentially to higher-ranked peers. This allows a node to rationalise its bandwidth allocation. It also has the e\ufb00ect of promoting pro-social behaviour on the part of nodes generally, given the practical implications of how every peer interacts with every other peer. See section 3.4 for further information about the wild\ufb01re mechanic. See also section 6 on the Adaptive Interacting Incentive Agents (AIIA) meta-game, of which wild\ufb01re is an example agent. The Network: Mechanism Design The network is organised into two categories of actors: miners and users.",
      "Users pay tokens (AR) to add data to the network. Miners in the network receive these tokens for mining new blocks, which requires them to store and serve data. It is possible for a single wallet owner to be both a user and a miner. The miners receive these rewards indirectly from users in the system that pay tokens to add data to the network. The assignment of tokens from users to miners is mediated by the network as a whole through mining and validation of new blocks. Data intended for addition to the network is encapsulated in transactions, which are mined into blocks. A block is both a container for a set of trans- actions, and a memoised representation of the state of the network after the inclusion of the accepted transactions. From the users perspective, there are two types of transactions in the network: data transactions and value transactions. A user can initiate a data transaction to store data in a block.",
      "Value transactions contain only the change of AR balances in two wallets: a decrease in the wallet that initiated the transaction, and an increase in the wallet that received the transaction. In terms of technical implementation, all transactions in the Arweave network are the same. Every transaction can have an optional recipient \ufb01eld, and an optional data \ufb01eld. This means that transactions can be highly \ufb02exible, for example, they can be used to send mail between two participants (see section 7.5 for a range of example permaweb applications, including Weavemail). See section 10.2 for full de- tails of block and transaction contents, as well as section 4 for details of how nodes manipulate these data structures. Di\ufb03culty: Regulating the Block Generation Rate In order to regulate the block generation speed in the network, the Proof of Access (PoA) algorithm allows for variable di\ufb03culty settings. PoA chal- lenges with higher di\ufb03culty take longer to compute.",
      "Di\ufb03culty is a network statistic, included in each block47. If the generation rate of blocks in the network exceeds the target frequency, the di\ufb03culty of the PoA puzzles for future block generation is increased. Similarly, as the block generation speed in the network decreases, the di\ufb03- culty setting is adjusted downwards. In this way, the decentralised network of miners is able to regulate the block generation rate (and subsequently, various reward emission rates  detailed below) regardless of the number of nodes in the network and the amount of computational power and storage available to solve the PoA puzzles. Token Economy 3.2.1 Paying the Network, Rewarding the Miners The Arweave network uses a token, the scarcity of which is enforced through the consensus mechanism of the blockweave data structure. The tokens main unit is the AR, with sub-unit Winston, where 1 AR  1,000,000,000,000 Winstons.",
      "As the token in the system is scarce and is used for two valuable functions, that of encoding data into the system and of rewarding miners, the token itself has a non-zero \ufb01nancial value. Although the token primarily derives its utility from being the only instrument of paying for permanent data storage, it can also be used as a means of value exchange. 55 million AR were created in the genesis block at network launch on the 8th June 2018. A further 11 million AR, an additional 20 of the genesis block supply, are being introduced into circulation gradually as block mining rewards. Consequently, the maximum circulation will be 66 million AR. AR tokens in circulation are held either in wallets or in the endowment pool. See section 3.2.3 for details of the in\ufb02ation function and the endowment pool. In order to write a transaction into a block, a user has to pay some AR as a transaction fee.",
      "This transaction fee is not transferred in its entirety directly to a miner of this block, unlike in traditional blockchain systems47, 67. Rather, most of the transaction fee is contributed towards a storage endowment, which is distributed to the wallets of miners over time according to the mechanism described in section 3.2.3. 3.2.2 Cost of Perpetual Data Storage As the Arweaves core function is to provide permanent storage to its users, a mechanism of pricing this storage must be de\ufb01ned.",
      "As a prerequisite to the calculation of the cost of storage of a piece of data in perpetuity, we must \ufb01rst de\ufb01ne the cost of storage of data for a single time period: PGBH  HDDprice HDDsz HDDmtbf where PGBH  Price of storing 1GB of data on 1 hard disk drive for 1 hour HDDprice  Lowest available market price of buying a hard disk drive HDDsz  Capacity of this hard disk drive HDDmtbf  Mean time between failures of hard disk drives Since the inception of digital data storage techniques, the GBh cost of com- mercially available storage media has been decreasing at a signi\ufb01cant rate (see \ufb01gure 1). Over the past 50 years, the average annual rate of decline of GBh cost has been 30.57 (dataset provided in appendix section 10.1). Figure 1: Carefully and conservatively extrapolating the pattern of the decreasing cost of data storage presents the opportunity to provide a \ufb01nite cost for the inde\ufb01nite storage of data.",
      "The cost of perpetual storage can be modelled as the in\ufb01nite sum of the declining storage costs over time: Pstore  (Datasize PGBHi) where Pstore  The perpetual price of storage PGBHi  The cost of storing 1 GB for an hour at time i Datasize  The quantity of data to store 3.2.3 Transaction Pricing Inline with the costing model described above, the Arweave protocol em- ploys a storage endowment mechanic. This mechanic allows the network to distribute appropriate quantities of tokens to miners over time, in order to sustainably incentivise the perpetual storage of arbitrary quantities of data. Transaction pricing in the Arweave network comes in two components: a highly conservative estimate of the perpetual storage cost, and an instantly- released transaction reward to incentivise a miner to accept new transactions into the new block.",
      "Transaction pricing is calculated as follows: TXcost  TXsize  iBH PGBBi TXreward  TXcost Cfee TXtotal  TXcost  TXreward where TXcost  The sum cost to the network to service the TX perpetually TXsize  Size of the TX data segment (in GB) PGBBi  The price of storing 1 GB for 1 block period at height i TXfee  Instant reward to miner for including the TX in a block Cfee  Constant de\ufb01ning instant reward to miner TXtotal  Total paid by the user to the network for the TX 3.2.4 Storage Endowment In order for mining of the Arweave to remain pro\ufb01table and sustainable over time, the following basic principle must hold: the reward emitted by the network at any given block must be greater than the sum value expenditure required to maintain the blockweave for that period.",
      "Speci\ufb01cally: B Wblocks Rtotal  Wsize PGBB While this constraint is naturally satis\ufb01ed by consistent release of tokens from the endowment (assuming a stable token price in \ufb01at terms and ac- curate PGBB predictions), the protocol avoids releasing endowment tokens when miners have already surpassed pro\ufb01tability through other means. This mechanism further strengthens the economic stability of the network against \ufb02uctuations in token price and storage medium pricing. In order to achieve this stabilising e\ufb00ect, the mining reward mechanics only take from the en- dowment in instances where the value expenditure required to maintain the blockweave exceeds the value emitted by the in\ufb02ationary block reward, and instantly-released transaction fees.",
      "The reward for a miner producing a block is composed of three parts: Rtotal  Rfees  Rinflation  Rendowment Rfees is the sum of all TXfee quantities charged for the transactions mined into a block: Rfees  BT Xs BTXsifee The in\ufb02ation reward is pre-de\ufb01ned for every block, gradually decreasing at a rate dependent only on the block height, as follows: Rinflation  GAR 0.2 ln 2 2BH where GAR  Amount of AR in the genesis block: 55,000,000 By  Number of blocks in a year: 262,800 BH  The height of the current block As outlined above, the quantity to be taken from the endowment is only non- zero in the event that Rinflation and Rrewards do not exceed the cost to the network of maintaining its storage burden for the block period.",
      "Given the extremely low cost of storage relative to hashing (see section 3.3 for details of Proof of Access value expenditure equilibria), the storage burden will likely not reach a point at which taking from the endowment is required by miners until the permaweb is many times larger than the current surface web (as estimated by the size of The Internet Archives annual web scrape62). This means that the storage endowment will gain a signi\ufb01cant \ufb02oat of tokens (likely built up over a number of years) before it is ever necessary to regularly use it in practice. The total number of tokens to be taken from the endowment is calculated in the following manner: Rendowment  Prop(max(0, (Wsize PGBB) (Rinflation  Rfees))) The Prop function takes the base reward from the endowment and makes it proportional to the size of the recall block that was required to mine the new block, relative to the average size of a block in the Arweave network.",
      "This ensures that in the event that a single block is signi\ufb01cantly larger or smaller than other blocks, miners are appropriately incentivised to store the data in the network evenly. Prop(AR)  min(Endowmenti, Brecall sz Bavg sz AR) where Brecall sz  The size of the required recall block Bavg sz  Wsize Bheight (10) Finally, the endowment to be transferred to the next block can be calculated: EndowmentBH1  EndowmentBH Rendowment  TXs TXsicost where BH  current block height EndowmentBH  Endowment size (in AR) at block height BH EndowmentBH1  Endowment size (in AR) at the next block height (11) 3.2.5 Future Data Density and Reliability Expectations Data density or storage medium reliability increasing (and consequently, PGBB decreasing) is important to the arguments made in the above sec- tions. As seen in \ufb01gure 1, this pattern has been sustained and consistent for over 50 years.",
      "Even though historical analysis alone does not guarantee the continuity of this trend, we note a number of compelling factors which indicate that it will continue past the end of the technological cycle during which the Arweave network itself will be active (see section 3.2.6): 1. Unlike trends in the CPU space, where computation clock speeds are approaching the point at which theoretical physical limits are being reached and Moores Law is decelerating17, this is far from the case with data density: Current maximum data density in consumer storage media: 1.66 1012bitscm3 Maximum data density achieved in research: 2.5 1025bitscm3 Theoretical maximum data density 11: 1.53 1067bitscm3 From our current position, at an optimistic 30 annual data density growth rate, it will take 434 years to reach the maximum theoretical limit, at 20  697 years, at 10 AGR  1,329 years. 2.",
      "Even if advances in data density slow, storage medium reliability (Mean Time Between Failures) continues to increase and arguably has an even brighter future33, 39, 69. The metric core to Arweave mining pro\ufb01tability  that of GBh costs  responds equally to changes in data density and data reliability. 3. Because of the rate at which humanitys demand for data is growing56, the incentive to develop storage mechanisms with a cheaper GBh cost in the future is enormous. Therefore, the likelihood that increased data densitystorage medium reliability will remain a possibility, but not an actuality, is extremely low. 3.2.6 Data Permanence, Not Network Permanence All technologies come in cycles10. While the Arweaves mechanism design is generally engineered to promote adaptivity to new circumstances, the core Arweave team does not expect that the network as it is currently formulated will continue to produce blocks in true perpetuity.",
      "This does not, however, mean that we expect that the information stored inside the weave will be lost after the \ufb01nal block is mined. It is our expectation that when eventually a permanent information storage system more suited to the challenges of the time emerges, the Arweaves data will be subsumed into this network. After the mining of the \ufb01nal block, the \ufb01nancial incentive mechanisms for data preservation will subside and give way to social incentives for data preservation. This e\ufb00ect will likely be compounded by the exceptionally low cost of storing the data from the network, due to its decreasing relative cost over time. This pattern of nesting of archives when they are retired is common across human history. An archive of Gopherspace (a knowledge web4, prior to the HTTP-based web25) can be found inside the Arweaves permaweb. In- side the Gopherspace archive, one can \ufb01nd archives of earlier Telnet and bulletin board-based discussion systems.",
      "Similarly, much of the Library of Congress is now archived on the web38, and indeed many of the books con- tained within the library are themselves collections of old storiespoetry- technical writing. We anticipate that the post-network future of Arweaves data preservation will continue in a similar pattern, bolstered by the cryp- tographic interweaving of all data in the network (true veri\ufb01cation of a part requires the presence of the whole), and the incentives in the network to create many replications of its information  most of which will never be deleted, even if the miners themselves are disconnected from the network. Proof of Access: Dominant Strategy A nodes Dominant Strategy 58 with respect to mining is the strategy that maximises mining rewards. As we saw above, the mining reward per block is largely outside of the control of the miner.",
      "A strategy to maximise rewards must become a strategy to maximise a nodes ability to mine a block in the \ufb01rst instance, and to do so before another node mines the block for the current height. The probability that a node can take part in the mining of a block is equivalent to the probability that a node is storing the new blocks recall block  essentially the proportion of the blocks that the node is storing. The probability that a node can mine a block \ufb01rst is a function of the nodes hashing power relative to the average hashing power of all of the nodes in the network that also possess the recall block. P(win)  P(has recall block) P(\ufb01nds hash \ufb01rst) where P(has recall block)  Blockslocal Bheight P(\ufb01nds hash \ufb01rst)  HPlocal HPnet (12) As mentioned above in section 3.2, increasing either or both of the proportion of blocks stored locally and the hashing power of the node will result in higher utility.",
      "Section 4, The Node: Behaviours for Protocol Compliance, explores a nodes tactics within this strategy. Figure 2: Dominant strategy value assignment for miners in the Proof of Access game, as data stored rises. Implementation of the AIIA Wild\ufb01re Agent Participants in the Arweave protocol play in a non-consensus-based adap- tion meta-game, in which nodes score one another based on the utility they provide, in an arbitrary fashion. The current reference Arweave implement- ation implements a basic agent in this meta-game called wild\ufb01re. Wild\ufb01re is a derivative of the Bittorrent protocols optimistic tit-for-tat bandwidth sharing incentive mechanism. More details of the adaptive meta-game of the Arweave network can be found in section 6. Mining nodes will always have a \ufb01nite amount of bandwidth available to them. As such, miners must allocate their bandwidth resources appropri- ately in order to maximise their opportunity to receive mining rewards.",
      "The reference Arweave node implementation utilises its wild\ufb01re (WF) agent in the AIIA meta-game (see section 6 for further explanation of AIIA) to re- ward miners for engaging in pro-social actions, for example, being highly responsive to requests for blocks or transactions from other peers. In WF, mining nodes score and subsequently rank all peers by their responsiveness, with the more responsive peers being prioritised for outbound messages in return (for example, propagation of new blocks and transactions). Sending messages by priority to peers that have been more helpful in the recent past is a utility-maximising strategy for the node. It also has the side e\ufb00ect of incentivising responsiveness on the part of nodes receiving requests: a node needs to avoid being de-prioritised and possibly dropped from its peers contact lists.",
      "Finally, this improves the responsiveness of the network as a whole, as less responsive nodes are encouraged to improve or are ultimately excluded from participation in the network. 3.4.1 Rationalising Outbound Bandwidth Each node keeps a list of peer nodes. These peer nodes include the trusted peers that the node was given at a start-up, and remote nodes, from which the node has received Arweave API requests. These are the peers to which it sends transactions, blocks, and requests for information. Each peer is given a score representing how quickly and accurately it re- sponds to the other nodes API requests. The score is essentially a rolling average of bytes per second over a number of recent requests to that peer. New peers are given a grace period during which they are exempt from ranking. Periodically the peer list is pruned, and less well-performing peers are re- moved probabilistically (i.e. the probability of removal is proportional to their rank).",
      "The leniency toward new peers and the probabilistic removal takes into account short-term variability in peer responsiveness. When propagating a transaction or a block, these peers are split into two categories: the best-performing peers are sent the message \ufb01rst in parallel, and then the rest of the peers (both the remaining worse-performing peers and new peers) are sent the message sequentially. Information requests also bene\ufb01t from wild\ufb01res ordering and pruning of the peer list. This heuristic approach ensures that nodes can rationalise whatever band- width they have, and ensure they are communicating with peers that are accurate and prompt. This maximises propagation and response validity, and minimises time and resource wastage (for example, avoiding sending messages to retired or malicious nodes).",
      "3.4.2 Promoting Responsiveness A node receiving a request for information has to assume the request is from a peer (either a known peer or a new peer) which is using wild\ufb01re (or an alternative AIIA agent, see section 6) to monitor responsiveness. As high connectivity is essential for mining and for fork avoidance, it is in the nodes interest to respond promptly and accurately to all requests. There is another reason that a node should not respond preferentially to certain requests. This is because some requests will not be from miners, but will be from edge or external users. As the network should be free at point of use for such users (for example, users requesting transaction data), nodes must not discriminate against them, or require reciprocation for responding to requests. A node does not have access to its WF score on peer nodes, though it does have access to its rank.",
      "Consequently, we can describe a net utility function as follows: Utility  P Peers Rank(P, Self) (13) However, nodes experience the e\ufb00ects of a reduction in their rank, including: reduced frequency of received propagations and information requests from its peers, reduced frequency of requests from new peers, and ultimately in reduced mining chances and increased forking, among others. 3.4.3 Network Ecology A network of nodes running wild\ufb01re agents within the AIIA game will max- imise throughput, in a similar fashion to Bittorrent swarms. This network can be visualised as a directed graph with weighted arcs, the weights being in units such as bytes per second from last N responses, see \ufb01gure 3. Paths through the network would show possible block or transaction propagation rates. The lowest WF score on any path would be the maximum \ufb02ow rate along that path. As Arweave networks are in general highly connected, a single slow edge would not necessarily form a bottleneck.",
      "In this scenario, over time average connectivity strength between peers would increase as lower ranked peers are gradually dropped from peer lists. The negative e\ufb00ect is especially strong for nodes who are ranked poorly amongst a large subset of their own peers. Lower-scoring nodes that want to con- tinue mining e\ufb00ectively have strong incentives to upgrade their bandwidth or otherwise improve responsiveness. The essence of wild\ufb01re is that a nodes peers are ranked in terms of accuracy and speed of response. The implementation and even the particular metrics used by a speci\ufb01c node are invisible to that nodes peers. We cannot  and we do not need to  assume that every node is using the same AIIA agent (see section 6 for further explanation of this dynamic).",
      "Figure 3: Wild\ufb01re as a weighted graph Fork Resistance and Recovery In situations where multiple blocks are produced and distributed to the network at the same moment, leading to multiple candidate blocks being accepted by di\ufb00erent nodes, network consensus begins to break down, and a fork is formed. It is important to recover network consensus quickly  this section discusses the mechanisms by which the Arweave protocol resolves such issues. 3.5.1 Target Behaviour and Related Incentives Forking presents a risk both to the individual node and to the network as a whole. The target behaviours are included to avoid forking in the \ufb01rst place, and to recover to the best fork as soon as possible. One of the potential causes of forks emerging in the network is propagation delay20. So nodes are incentivised to propagate blocks and transactions as soon as possible after minimum validation is successfully completed.",
      "A node will perform su\ufb03cient edge validation on the data to protect the network from attack, and will then propagate said data. Nodes are incentivised to complete some validation at this stage, as propagating invalid data can negatively impact a nodes peer ranking. Similarly, nodes are incentivised to mine blocks speedily to maximise their likelihood of receiving mining rewards by propagating the \ufb01rst valid new candidate block  if they accept a new block, it also preferable to them that all other peers accept that same block. In addition to these incentives, the motivation to avoid the opportunity cost associated with fork recovery encourages miners to avoid forks occurring in the \ufb01rst instance. However, forks happen, and when this occurs a node must be able to e\ufb03- ciently assess whether it is on the majority fork and, if not, recover to the majority fork immediately. Arweave fork selection is performed in a similar fashion to traditional Nakamoto consensus-based blockchain protocols.",
      "Each block includes a cumulative_difficulty \ufb01eld which represents the amount of work that has gone into that speci\ufb01c fork of the blockweave up until that block. A node can immediately compare its own current block with an incoming block and assess whether the incoming block is (a) valid and can be accepted and propagated; (b) invalid or old and should be ignored, or (c) possible evidence of a preferable fork. The latter would occur when the incoming block has a greater cumulative di\ufb03culty the nodes own current block, and the forks previous block is not the nodes current block. Because each individual miner has a sel\ufb01sh incentive to have their blocks accepted (and subsequently receive a reward), miners are incentivised to adopt the chain with highest cumulative di\ufb03culty as new blocks added to this fork have the highest likelihood of eventual adoption by all of the other nodes of the network.",
      "When a potentially preferable forked block is found, the node requests each block from the divergence between its own blockweave, and the potential forked blockweave, verifying each in turn (see \ufb01gure 4). In the \ufb01gure, a node following the lower line is at height 39  its current block has cumulative di\ufb03culty 421. It receives a block with height 41 and cumulative di\ufb03culty 529. The block is otherwise valid so our node must take this block as the tip of a preferable fork. It traces back from the new height 41 block (taking the previous_block in turn from each block) until it reaches a block in its own history (i.e., in the nodes own Block Hash List). In the \ufb01gure the join is found at height 38 (steps 1, 2  3). This new line becomes the nodes preferred fork, and adopts the block with cumulative di\ufb03culty 529 as its current block (step 4).",
      "The height 39, cumulative di\ufb03culty 421 block is now invalidated, and trans- actions from that block  if they are not found in the three most recent blocks of the new fork  are dropped. Figure 4: Fork with cumulative di\ufb03culties The Node: Behaviours for Protocol Compliance This section describes how an Arweave node complies with the Arweave protocol detailed in the previous section. The nodes dominant strategy is to optimise mining rewards, which it does by mining and propagating new blocks. The node also receives incoming messages and reacts according to the protocols incentive structures. Foremost among these messages are new blocks, new transactions to be mined into blocks, and requests for transactions to be served. Other activities and other incentives impact the ability of a node to mine blocks and have those blocks accepted by the network. These act as pre- requisites or constraints on a nodes mining capability.",
      "In order to mine a block, the nodes operator must consider the following requirements (see \ufb01g. 5): 1. Receiving blocks and transactions. A nodes social rank in the Adaptive Interacting Incentive Agents (AIIA) meta-game will directly impact the nodes latency for receiving new blocks and transactions, and whether they are received at all. A node with low social rank will tend to receive information later than other nodes, and may not receive all information. Ultimately, nodes with low social rank risk being completely ostracised from the network. If a node is not receiving block and transaction information, it cannot e\ufb00ectively mine. Consequently, Figure 5: Prerequisites for mining reasonable peer scores are a fundamental prerequisite to mining in the Arweave network. 2. Accessing the recall block. In order to mine a new block, a node must already be in possession of the recall block associated with that new block.",
      "As Bn.recall cannot be predicted before the mining of Bn1 (see formula 14), the probability that a node possesses the recall block is in proportion to the quantity of the blockweave which that node is storing. This is limited by the nodes storage capacity. 3. Generating a candidate block. Once the \ufb01rst and second stages have been completed, the nodes must then race each other using their computational power in order to generate a candidate block. The more computational power a node provides to the network, the more likely they are to be the \ufb01rst to generate said candidate block. 4. Gain acceptance of the candidate block. This \ufb01nal stage returns to the social nature of the network: nodes with low AIIA social rank risk their messages (including new candidate blocks) being propagated too slowly to be accepted by the network. Once generated, a new candidate block must be accepted by the network in order for the node that generated it to receive a reward.",
      "As well as AIIA scores, the selection of transactions included in the candidate block can impact how likely the rest of the network is to accept it. For example, a block containing transactions that are on content policy blacklists widely held by the rest of the nodes in the network is unlikely to be accepted (see section 5 for details of this process). With this context in mind, we look in the next sections \ufb01rst at mining, then at communications, and \ufb01nally at fork avoidance and recovery. Mine and Propagate New Blocks Mining and propagating new blocks are continuous activities. As described in section 3.2.1, the total reward a node receives for mining a new block consists of a margin reward, in\ufb02ation reward, and potentially, a small pro- portion of the storage endowment.",
      "Due to the instant reward from mining new transactions into the candidate block, there is a signi\ufb01cant immedi- ate incentive for a miner to include pending transactions in the candidate block (in addition to the longer-term incentive to increase the overall stor- age endowment). There is also su\ufb03cient incentive to mine a block without transactions when necessary, as successful miners will still take an in\ufb02ation reward and, in some cases, also a small quantity from the storage endow- ment. 4.1.1 Block Construction Procedure A mining node has two pools of transactions: a waiting pool and a mining pool. As new transactions arrive, they are validated by the node and scanned using the nodes content policies (see section 5 for details of this process). Once the node reaches a high level of con\ufb01dence that other nodes have likely received the transaction (see section 4.2) the transaction is moved from the waiting pool to the mining pool.",
      "Once a block is mined by a node and accepted by the network, the transactions inside the block are removed from the mining pool. At the protocol layer, fork recovery (see section 4.4 below) can result in transactions being re-introduced from a deprecated block into the mining pool. The new Block Data Segment (BDS) is a hash of: 1. The independent hash of the previous block. 2. The entire contents of the recall block. 3. The transactions and other metadata for the candidate block. The BDS serves as the puzzle in the Arweave protocols Proof of Work mechanism. The diagram below (\ufb01gure 6) shows how information from the previous block, the recall block, and transactions are incorporated into the BDS and into the new candidate block. The new block contains everything necessary to rebuild the BDS and validate the proof of work  except for the recall block, which the node must already possess.",
      "Figure 6: Block construction from previous block, recall block, and transac- tions There are \ufb01ves stages involved in generating a new candidate block: 1. Assemble relevant metadata. 2. Gather the set of transactions to be mined into the new block, and val- idate the set together. This involves calculating the current di\ufb03culty and validating the transaction fees against that di\ufb03culty. 3. Generate the new Block Data Segment (BDS). 4. Find a nonce that satis\ufb01es the di\ufb03culty given the BDS47. 5. Package and propagate the new block in its memoised form - its shadow. Step One: Assemble Relevant Metadata Each node should have in its state (or, equivalently, in the current block) a current Block Hash List (BHL) and the current block height (see appendix section 10.2.1 for the detailed anatomy of a block). The independent hash of the current block modulo the current blocks height determines the height of the recall block (see formula 14).",
      "BHrecall  BHLBindep hash mod Bheight (14) This height will be within the range 0, Height). Note that this precludes the current block acting as the recall block. Consequently, the recall block is always a previous block from the blockweaves history, and two di\ufb00erent blocks are always required to mine a new block. The identity (i.e., the height and independent block hash) of the next recall block can be known if and only if the previous block has been mined. Sub- sequently, miners cannot predict ahead of time which recall block will be required to mine the next block. This means that the most e\ufb03cient strategy for any miner to take is to store as many of the old blocks of the network as possible (within the bounds of the equilibrium of Proof of Access as ex- pressed in section 3.3). The probability that a miner is able to hash on any given round of Proof of Access is equal to the proportion of the number of previous blocks in the blockweave it has access to.",
      "Proof that the node has access to this recall block (Proof of Access) is in- cluded in the process for generating a new Block Data Segment, as described in further detail below. Step Two: Fetch and Maintain the Transaction Set Transactions are arriving and being veri\ufb01ed constantly, the node must decide which transactions to include when generating a new candidate block. Each transaction is validated individually when it \ufb01rst arrives at a node, and then it is transferred into the waiting pool. After the appropriate wait time (see Section 3 for details), a subset of these transactions (possibly all of them) is selected for mining and transferred to the mining pool, where the transactions are validated as a set. It must be possible to apply all the transactions in the set individually to the current network state and Wallet List.",
      "Step Three: Generate the Block Data Segment The recall block, including its transactions and the data they contain, is included in the material from which the new Block Data Segment (BDS) is generated (see section 10.2.1 for the detailed anatomy of a block). Con- sequently, for a new block to be validated and accepted, the correct recall block must have been used to generate the BDS. This therefore constitutes proof that the miner had access to the appropriate recall block at the time of proof of work challenge generation. Step Four: Find a Valid Nonce Next, the miner must attempt to generate a nonce that leads to the creation of a valid hash, satisfying the di\ufb03culty for the next block. The mining algorithm used in the Arweave protocol is RandomX64 In the reference implementation, the default number of miners is the total of all CPU cores on their host machines minus one. The found nonce is the proof of work included in the new block data structure.",
      "If new transactions are received by the node during step four that should be included in the new block, the process is interrupted. In this case, step two is repeated with the new transaction set, and a new BDS is generated. Step Five: Propagate the Candidate Block In step \ufb01ve, the block is packaged and propagated to peers, prioritised in order of their peer rankings (see section 6 for further explanation of the AIIA game). Mining nodes only receive a mining reward if their candidate block is accepted by the network before any other nodes candidate block is. This means that the nodes block propagation speed is a vital determining factor of their mining e\ufb03cacy. 4.1.2 Incentives Miners are rewarded by the network for mining new blocks.",
      "While the ma- jority of transaction fees \ufb01ll the storage endowment, the miner instantly receives a transaction reward, in\ufb02ation rewards and, in some cases, a pro- portion of the existing storage endowment (see ection 3.2.3 for details of transaction and reward pricing). The reward is not paid to the miner in a transaction (which would have to be validated by the network, unnecessarily bloating the blockweave), but the storage endowment is decremented and the Wallet List updated directly. The updated endowment and Wallet List are integrated with the new BDS and with the new blocks data structure, de\ufb01ning the new token ownership state of the network. A corollary of this is that acceptance of the new block by the network ef- fectively includes acceptance of the reward. The reward payment to the miner is part of the state of a particular fork. If during fork recovery certain blocks are rejected, then the mining rewards payments for those blocks are also invalidated.",
      "Receive, Validate, and Propagate Transactions and Blocks 4.2.1 Transactions When the node receives a new transaction (sent from another Arweave node, or from an edge client or app), the node validates the transaction fee and that the previous transaction reference matches that which is found in the Wallet List. If the transaction is successfully validated, the node then propagates the transaction to its peers as quickly as possible. Incentive mechanisms associated with the AIIA meta-game and fork avoidance promote this be- haviour (see section 6). It is in a nodes interest to propagate the transaction directly upon receipt instead of just mining it into a block. This is because the individual trans- action needs to be accepted as valid by a majority of other nodes in the network before a block containing that transaction can be accepted. The steps for validating a transaction before entry into the transaction pools are as follows: 1.",
      "Transactions that are not well-formed (see appendix section 10.2.4 for the full anatomy of a transaction) are ignored by the recipient node; 2. Transactions that have already been processed are dropped; 3. The wallet associated with the transaction must contain a su\ufb03cient token balance in order to process said transaction and any additional pending transactions from the same wallet; 4. TXowner and TXtarget should not refer to the same wallet; 5. The transaction cost must be above a dynamic minimum (see section 3.2.3 for further details of transaction cost and pricing); 6. The TXanchor must be present in the current wallet list as TXowners last processed transaction ID, the independent hash of one of the last 50 blocks, or be empty for the \ufb01rst transaction; 4.2.2 Blocks During operation, a node will receive blocks from other peers in the data distribution network, either other miners or special-purpose peers (for ex- ample, Weaver browser nodes27).",
      "The receiving node needs to validate and accept the block as quickly as possible in order to keep up with net- work consensus and continue mining e\ufb00ectively. Prompt block propagation is part of fork avoidance  a node must make sure the blocks it accepts are also accepted by its peers. Critically, in order to make preliminary block veri\ufb01cation inexpensive prior to gossiping the block to other peers (risking increasing block consensus time and subsequently forks  see section 2.3 for more details of blockshadows), the proof of work found in blockshadows is veri\ufb01able independently from other block checks. This avoids the necessity for constructing blockshad- ows into full block structures and verifying the BDS which could otherwise become a denial of service attack vector. Before gossiping a received block to its peers, the following steps of prelim- inary block veri\ufb01cation must occur: 1. Ensure that the blockshadow structure is well-formed (see appendix section 10.2.3); 2.",
      "Con\ufb01rm whether the block has already been processed (by checking its BDS); 3. Ensure that full copies of every transaction referenced in the block are held in the local transaction pools; 4. Validate that the incoming blocks BDS and nonce combination satisfy the present di\ufb03culty in the network; 5. Validate that the incoming blocks timestamp is within acceptable bounds of the previous block (timestamps monotonically increase) If the preliminary validation passes, a full block is generated from the re- ceived blockshadow, and the blockshadow is gossiped before the core veri- \ufb01cation steps.",
      "Next, the core block veri\ufb01cation steps must be performed before the block is accepted by the local node:  Generate a BDS from the block and verify it against the BDS previ- ously provided in the blockshadow;  Verify that the blockweave metadata included in the new block (e.g., Wallet List, Block Hash List, weave size, etc.) are valid updates to the metadata provided in the previous block, given the new blocks transactions and timestamp. Receive and Respond to Requests Mining nodes request information from peers at various stages in the mining process, including upon \ufb01rst joining the network and sychronising with their peers, and when access to a recall block is required to verify a new block. In order to e\ufb03ciently allocate its scarce outbound bandwidth, a node ranks its peers by their social behaviour (for example, their responsiveness to requests).",
      "Nodes are incentivised through the AIIA meta-game to respond promptly and accurately to requests for information, in order to gain rank with their peers in the network. See section 6 for full details of the AIIA meta-games incentive design. As well as mining nodes, other users of the network send requests for inform- ation  especially but not exclusively requests for transaction data. These requests, which will not necessarily impact a nodes AIIA scores, are not dis- tinguishable from mining peer requests without special e\ufb00ort. See section 7.1.3 for details of how the Arweave HTTP API facilitates these alternate information requests. Fork Avoidance and Fork Recovery As propagation delay is a main cause of blockchain forking, prompt propaga- tion is an important fork avoidance tactic. In the Arweave protocol, prompt propagation is incentivised by agents in the AIIA meta-game, for example via the wild\ufb01re mechanism (see section 6).",
      "When a node receives a new candidate block that is two or more blocks ahead of its known previous block, this implies that there is a fork in the network. The node must therefore take prompt action to con\ufb01rm which of these apparent forks is the most accepted by its peers, by evaluating the cumulative di\ufb03culty of the forks. The cumulative di\ufb03culty, represented in every block data structure, is a proxy representation of the amount of Proof of Access work that is encapsulated in this fork of the blockweave. In a fork, paths resulting in higher cumulative di\ufb03culty are preferred. On receiving a new block with greater cumulative di\ufb03culty, a node will take the new block as the preferred fork and trace back through the forks Block Hash List to a shared point in its own history. Once a common point is found, the transactions and blocks required to verify the fork are evaluated.",
      "If the fork is found to contain valid blocks from the point of divergence to the tip, and it is still of higher cumulative di\ufb03culty when fork validation ends, the nodes context switches to the greater fork. Decentralised Content Policies Given that the miners collectively maintain the Arweave network, a mech- anism is required to allow them to express their opinions on what content should and should not be hosted in the system. This is achieved through three related but distinct mechanisms:  A democratic process of voting on content entry into the blockweave. The individual ability of each node to choose what content to store on their machines. The ability of gateway nodes to \ufb01lter blockweave data that users are exposed to. Nodes express preferences about content through content policies. Content policies can be arbitrary computation performed upon transactions that classify them as acceptable or not acceptable to the local node.",
      "In the ref- erence Arweave implementation, content policies are supported in the form of substring matches as well as hashes of the data stored in the transaction. Other protocol implementations can utilise their own content policy mech- anisms, for example computer vision technologies and fuzzy hash matching (such as in PhotoDNA46). Voting Phase When transactions are distributed to the network, they are scanned against the content policies of each node. If a transaction contains content prohib- ited by the content policy, it is not accepted into the nodes transaction pool and is not gossiped to other peers. If a new block received from a peer contains references to transactions that have been dropped by the local node, the block is not veri\ufb01ed and is dis- carded by that node. If other nodes in the network have not dropped the o\ufb00ending transaction, a fork emerges.",
      "During the forking process, those nodes whose content policies have rejected the transaction race against those who have accepted the transaction to produce the next block. Once such a block is produced, nodes on the other fork initiate a fork recovery pro- cess to download the block with its transactions, verify them, then drop the o\ufb00ending transactions from temporary memory. In this way, the network is able to maintain consensus while allowing nodes to take part in a stochastic voting process, expressing whether or not they wish speci\ufb01c content to be added to the blockweave. Therefore, nodes that wish to reject said content are not required to store that content on their non-volatile storage media. Storage Phase After nodes have voted to accept the data into the network, an incentive to store that data is embedded in the Proof of Access mechanism of the block- weave.",
      "When new participants join the network, they scan and download transactions that adhere to their content policies, avoiding those transac- tions which they would prefer not to store. As the node operator changes content policies, they can re-scan their local storage and remove content that now breaches their policies. Incentives The decentralised content policy mechanism generates two complementary incentives:  An incentive not to over-zealously reject too many transactions, as this would lead to a decline in mining rewards, or;  An incentive not to accept transactions that the majority of the net- work is likely to reject, as this will result in mining candidate blocks that the rest of the network will ignore. This set of incentives forces miners to strike a balance between over-leniency that could be harmful to the network in the long term, and over-rejection that could lead to reduced utility of the network.",
      "Trade-O\ufb00s One of the consequences of the decentralised content policy mechanism is that settlement times of transactions are increased by one block period. Therefore, when a transaction is mined into a block, this is not neces- sarily an indication that the transaction will be accepted by the network at large. However, given the networks use of Nakamoto-style eventually- consistent consensus47, this extra block con\ufb01rmation period does not ma- terially change the user experience of the system. Nodes can also choose to drop the head block of the network and re-mine the last block in order to determine a new recall block, if content policies have caused the existing recall block to have too few replications. timeout for block re-mining is not enforced at a protocol level. However, as more hashing power is applied to the prior block, the likelihood of the head block being re-mined increases.",
      "Nodes are disincentivised from re-mining the head block too early, however, as this would require them to expend unnecessary hashing power without a high likelihood of their candidate block being accepted by other nodes. Gateways Just as the decentralised content policy mechanism a\ufb00ects how content is added to and stored within the network, gateways also apply this mechanism to indexing data that is already stored within the network. Gateways index only the content that adheres to their own content policies. Subsequently, when a user views, for example, a decentralised social media application on the Arweave, their chosen gateway determines which content they will be shown, according to that gateways content policies. Fundamentally, users are therefore able to choose what types of network con- tent they are exposed to by deciding between using gateways with di\ufb00erent content policies.",
      "Additionally, these mechanisms solve application developers concerns about removing or censoring content from their own platforms. Although users can choose what they see, they cannot forcibly deny others from seeing the content on the blockweave, as everyone has an equal choice of which gateway (and therefore content policies) they wish to view the blockweave through. We expect that communities of like-minded individuals will coalesce around gateways according to their content policies, maintaining and advancing the way that they wish to see the web, together. When the permaweb reaches an appropriate level of adoption, we anticipate these decentralised gateways will represent a diaspora of varied views, leaving each individual a wide range of choice regarding how they see the web. Adaptive Mechanism Design in Arweave Introduction One of the strengths of the centralised web that led to its long-term suc- cess was the simplicity of its protocols and their adaptability65.",
      "For ex- ample, the hypertext transfer protocol (HTTP) as described in the original speci\ufb01cation25 was purely intended for the transfer of a web of know- ledge. In practice however, HTTP has come to drive almost all movement of name-addressed content on the internet. This is a consequence of the \ufb02exible nature of the protocol. Given Arweaves goal to create long-term, reliable data storage mechanisms built on economic incentives, \ufb02exibility and adaptability are both built into its mechanism designs. The \ufb02exible components of the Arweaves economic mechanisms are built around the Adaptive Interacting Incentive Agents (AIIA) game. The AIIA game is an adaptive mechanism design8 for ranking, exhibiting, and re- warding pro-social behaviour in a network environment that changes over time. Each player in the AIIA game de\ufb01nes a strategy for ranking and prior- itising each other player that it interacts with in the network.",
      "Strategies are enacted by agents, which interact and rank the utility they perceive other agents to have provided, and apportion their players scarce resources re- spectively to reward this. This pattern of rewarding pro-social behaviour re- ciprocally can be viewed as a generalisation of BitTorrents optimistic tit-for- tat algorithm  from bandwidth sharing to generally useful behaviours18. As agents interact and rank one another for non-token social rewards (for ex- ample, data distribution, or ArQL query prioritisation), their own responses to these behaviours are also ranked socially by others. That is, the way that players rank each other, as well as their actions themselves, are ranked by other players. In e\ufb00ect, this creates a second-order meta-game on top of the typical tit-for-tat behaviour. As the agents  each employing their own speci\ufb01c strategy  interact, they have an incentive to incentivise each others behaviour towards pro-social goals.",
      "Comparison with Consensus-Based Games Unlike typical blockchain mechanism designs47, 67, the exact behaviour of the AIIA games players is not enforced through network-wide consensus. This allows each player within the network to essentially be playing a dif- ferent game (their ranking mechanism for other players and their agents) within the meta-game. Subsequently, the totality of the rules expressed in the meta-game is de\ufb01ned by the sum of each of the individual games that are currently being played. This stands in contrast to the consensus-layer games of the Arweave network (and other blockchain protocols). In typ- ical blockchain game mechanics (for example, Bitcoins Proof of Work) all participants are required to play by exactly the same rules. Over time the agent behaviours in the AIIA game will shift in response to changes in the technical and social environment the network \ufb01nds itself within, as the be- haviours that each player (i.e.",
      "network participant) chooses to incentivise morph. The AIIA game is similar to the incentive characteristics displayed by the BitTorrent network18, which does not programmatically enforce every net- work participant ranking every other participant in the network in the same way. Instead, it allows each actor to maintain private, local scores for other peers. In 2007 this gave rise to a modi\ufb01ed BitTorrent client, BitTyrant, that was speci\ufb01cally designed to exploit weaknesses in the original rank- ing mechanisms in use by most participants BitTorrent network54, 55. Improvements to the standard optimistic tit-for-tat agent were then made in response to BitTyrant26, optionally used by ecosystem participants to avoid the issues caused by BitTyrant nodes. In this way, BitTorrents data distribution mechanism has been an AIIA-like meta-game mechanism design for over 12 years.",
      "This has allowed the BitTorrent network to adapt to new challenges (such as the emergence of BitTyrant), without the need for cent- rally enforced protocol upgrades. The Arweaves AIIA meta-game is the deliberate adoption of this strategy to incentivise general pro-social beha- viour as the environment that the network \ufb01nds itself in, changes (for ex- ample, network usage patterns, internet architecture, exploits, and emerging incentive mechanism issues). Implementation Outline and Emergent Properties At its core, the AIIA game encourages network participants to build agents that: 1. Apportion their own resources so that they receive the highest net utility from the other agents in the game; and 2. As a trade-o\ufb00to (1), o\ufb00er small incentives for node operators to exhibit a bias towards nodes they believe are acting in a pro-social manner, given the present situation.",
      "As long as node operators exhibit some level of bias towards other nodes operating in a pro-social manner (even if they themselves are not operating pro-socially), over many iterations of the game, generally agreed-upon beha- viours for incentivisation express themselves (see simulation details below). For example, all miners have a stake in the Arweave token price. As price is partially a function of demand, features that lead to higher demand for the token are positive for all miners in the network. One such feature likely to increase demand is the ArweaveIPFS bridge. The ArweaveIPFS bridge requires a small portion of Arweave nodes to run a parallel IPFS node, ex- posing Arweave data to IPFS clients. However, each individual miner in the current ArweaveIPFS integration is not directly incentivised for operat- ing an ArweaveIPFS bridge, despite the fact that this is clearly pro-social behaviour.",
      "Players in the AIIA game, even if they do not themselves run Ar- weaveIPFS bridges, are able to exert a small nudge on each others beha- viour by marginally rewarding other nodes running ArweaveIPFS bridges higher than those not running a bridge. Expressing this incentive costs the AIIA node operator a very small quantity of lost utility by marginally de- prioritising nodes that are not running bridges. However, operators have an incentive to do this (to some small extent) as the pro-social behaviour will bene\ufb01t them in the long run by means of demand for tokens. Formally, nodes are incentivised to incentivise a pro-social behaviour in other nodes if and only if: Utilityincentivised(i)  (Utilitybefore(i) Costincentivise) (15) Each node has only fuzzy indications of how well they are performing in other peoples AIIA rankings, as over time the perceived rewards for greater AIIA ranking will themselves shift along with the games at play in the net- work.",
      "Rational agent designers subsequently do not keep track of the score they perceive other agents are giving them, but instead they measure the subjective utility they have received from the other agents. By experiment- ing with expressing di\ufb00erent behaviours (for example, running IPFS bridge nodes, not running them, prioritising a small group for data distribution, or distributing data to everyone who requests it, et cetera), nodes can optimise their behaviour for the current state of the AIIA meta-game without ever fully perceiving all of the rules at play within it. Further, in practice it is expected that agent designers in the AIIA meta-game will communicate with each other o\ufb00-chain to discuss strategies and behaviours to incentiv- ise, swap agent code, etc. In this way, it is anticipated that an ecosystem of interacting agents will emerge over time with modi\ufb01cations and perceived improvements being added by many parties in a permissionless manner.",
      "Simulation In order to demonstrate the properties of the AIIA meta-game over time, a simulation was constructed. In the simulation, internal agent characteristics for each player in the network are represented by a vector of \ufb02oating point values between zero and one, and biases for preferred behaviour in a similar vector of characteristics of equal length: ACs  0, ..., n (16) ABs  0, ..., n (17) The biases expressed by each agent represent the perceived pro-social beha- viour of the agent designer, while the characteristics represent the behaviour that the agent actually exhibits.",
      "During each game step, each node calcu- lates its net \ufb01tness score relative to other nodes: Fnet(A, As)  As F(A, Asi) (18) Each node also performs a random mutation and calculates its new utility score, then proceeds to the next game step with the preferable strategy: Anext  when Fnet(M, As)  Fnet(A, As) otherwise where M  Mutate(A) (19) Nodes rank each other on the distance between their own behaviour mixed with their biases, against the behaviour of other nodes. F(A, B)  ACs diff(ACsi, mix(BCsi, BBsi)) (20) This behaviour mimics how AIIA game participants typically favour those who are valuing and assigning resources in a similar fashion to themselves, following an abstraction of the pattern expressed in BitTorrents optimistic tit-for-tat data distribution mechanism. When executed, the simulation shows that when initialised with both ran- dom and shared ACs vectors, over a long enough time period the ACs of AIIA game participants tends towards the common biases in the agent set.",
      "Fur- ther, the simulation demonstrates that convergence to pro-social strategies is always achieved assuming that the rate of change in the perceived pro-social behaviour does not exceed the average rate of expression of each bias. Limitations While AIIA game-like incentive mechanisms can be used to build mechanism designs that adapt to changes in the environment, they are subject to three fundamental limitations: 1. Primarily, the adaptive mechanism design that emerges from AIIA- like games can only encourage the expression of behaviour that the majority of the network participants favour. The mechanisms them- selves cannot calculate pro-social behaviour patterns without the sum guidance of the agent designers. This su\ufb00ers from the same class of attacks that are expressed by blockchain networks in which a majority of the nodes (and their associated power in Sybil-resistance games) act unfaithfully59. 2.",
      "The second major limitation of AIIA-like games is that they cannot be used to shift network-wide consensus mechanisms. Only o\ufb00-chain reputation scores can be subject to the individual adaptations of games by network participants. 3. Finally, as mentioned above in the simulation section, the rate of ex- pression of biases in the network, often at the short-term disadvantage of the expressing node, is a limiting factor for the speed at which the network can adapt to a changing environment. That is, if nodes are too sel\ufb01sh in their non-expression of pro-social biases, they may not be able to keep up with changes in the environment su\ufb03ciently. To date, the Arweave network has been exposed to two AIIA agents: wild- \ufb01re (described earlier in this paper section 3.4)  an agent that achieves op- timistic tit-for-tat-like bandwidth sharing; and Weaver27  an agent that randomly forwards messages to just the \ufb01rst ten nodes that it encounters on the network.",
      "In practice, Weaver nodes are currently the minority and are de-prioritised relative to other nodes by the majority wild\ufb01re nodes in the network. We expect that as the environment of the Arweave network evolves, modi\ufb01cations will be made to Weaver and wild\ufb01re (and other new agents invented) which reward behaviours like servicing ArQL requests, minimising latency for data requests, and aiding rapid location of data. Arweave Protocol Interoperability and the Per- maweb In this section we describe the permaweb, a cluster of communication pro- tocols, built on top of Arweaves permanent knowledge ledger, that together expose a permanent, decentralised web. The permaweb is super\ufb01cially, visu- ally similar to the traditional web. However, users experience of the pages and applications that live on the permaweb di\ufb00er in a number of funda- mental ways from the experience of such tools on the traditional web.",
      "This section explores some of the di\ufb00erences between the permaweb and the traditional web, and how the protocols involved work together to provide various advantage. Characteristics This section will describe the key characteristics of Arweaves permaweb, and surrounding family of protocols. 7.1.1 A Trustless and Provable Web All content stored on the Arweave is timestamped and tamper-proof. This is guaranteed by the basic blockchain infrastructure upon which the block- weave has iterated. As described in the classic blockchain papers29, 9, block timestamps do not rely on time servers, but on network consensus. Timestamps are reliable due to the immutable nature of the blockweave data structure, and are accurate to within a few minutes. The inclusion of transaction data inside block hashes and recall blocks ensures that data remains unadulterated during storage or retrieval, while also validating that said transaction data remains available within the network at any time.",
      "Each wallet record in the Wallet List includes that wallets last transaction. This ensures that full history of every wallet is always readily available. Further, as every piece of information on the Arweave is signed by a wallet, data uploaded to the permaweb is always associated with some form of (at least pseudonymous) identity. In practice, unlike with other content- addressed networks12, every web page in the Arweave is associated with a speci\ufb01c identity, at a speci\ufb01c point in time. This means that records of facts can be traced through the network to the identity that \ufb01rst asserted those facts. This is a major shift from the paradigm of the centralised web, in which facts can be asserted, spread, and then revoked in order to disseminate misinformation whose source is hard to determine63, 23, 40, 60. 7.1.2 Web Responsiveness Through Incentivisation Incentivising data replication is an essential component of the Arweave pro- tocols mechanism design.",
      "This incentivisation ensures that the network as a whole is resilient to damage or attack, and that data is reliably stored in the network. These incentives also vastly improve the responsiveness of the network and the speed of data location and retrieval. Primarily, this is because there is a substantially higher probability that any given node possesses the data it receives a request about. Nodes, therefore, typically do not have to forward data requests to other nodes, but can ful\ufb01l said requests themselves, unlike in contract-based storage systems. The Arweave protocols mechanism design is detailed in full in section 3. 7.1.3 Open HTTP API In order to conduct the business of running the network, Arweave nodes communicate with each other using the Arweave inter-node protocol (the reference implementation of which is a HTTP API.",
      "Because the nodes in the Arweave network share data with one another in the same format as browsers themselves use, the accessibility of data inside the network for browsers on desktops, mobiles, and other devices comes naturally to the protocol itself. Further, third-party applications and browsers are \ufb01rst class citizens inside this network, as they can all communicate using the same basic, well-established technologies. Application Architectures 7.2.1 Client-Server Traditional web or native applications have a client-server architecture. This model is still possible with the Arweave, as a web server can act as a front- end for data stored on the networks permanent ledger. Such an Arweave- enabled server will interact with one or more Arweave nodes using the Arweave HTTP API, reading and writing data on behalf of clients. These services can be websites with clients as visitors, or they can be native ap- plications passing client requests to a server operated by the developers.",
      "In this centralised Arweave-app model, these services can maintain a pool of AR tokens in order to pay for data storage requests on behalf of the cli- ent. Reading data from the blockweave, however, is still free at the point of access using this application structure. Monetisation potential for this architecture is similar to the models of the centralised web. A developer will need to accrue more value through advert- ising, monthly subscriptions or direct payments within their application, than the amount of AR tokens they are utilising to power their storage. There are many use cases for permanent immutable storage. 7.2.2 Serverless Web Application Architecture Decentralised applications reside directly on and operate directly from the blockweave itself, and can be accessed by a typical web browser.",
      "This is pos- sible because the entire application itself is stored on the Arweave network as transaction data, which is executed as code when served to a browser, or other client application. Given the blockweaves immutable link between uploaded data and the (potentially pseudonymous) identity of the uploader, the provenance of any Arweave application can be consistently proven. Applications implemented in the most popular web technologies, including HTML, JavaScript, and CSS, are commonly chosen as the basis for decent- ralised Arweave apps. However, if the client application used to access the transaction data includes an integrated interpreterparser for additional lan- guages, applications implemented in these technologies could also be down- loaded from the blockweave and executed within the client, for example as a desktop application. Applications hosted on the Arweave network are also able to write persist- ent and provable state to the blockweave.",
      "Since Arweave does not impose a particular data structure, developers are free to store their data in the format that makes the most sense for them (both the transaction data itself and the transaction tag \ufb01eld can hold an arbitrary key-value list). If the ap- plication is best served by a highly optimised Merkle structure, such as the one found in the Ethereum Virtual Machine, it can be easily implemented on the Arweave network. Serverless applications hosted on the Arweave network allow users to pay directly for their interactions with the network. This frees the developer from having to subsidise the cost of user interactions themselves. Gateway Nodes An Arweave gateway node is similar to any other node, with additional features integrated for accessing or querying the network, including:  ArQL - a query language and index for querying blockweave metadata, focusing on transactions, their tags, and transaction-linked timestamps (see section below for further details).",
      "Arweave DNS and TLS - a way to access applications stored on the blockweave using relevant wallet and transaction IDs as domain names (see section below for further details). This feature is compatible with existing DNS and TLS systems. 7.3.1 ArQL One of the optional protocols in the Arweave family is that of ArQL: a simple querying language built for primitive searches of the data stored in the Arweave network. ArQL is intended as a lightweight mechanism to help application developers to build simple permaweb apps in the absence of more advanced decentralised databases. ArQL does this by indexing the tags associated with transactions as de\ufb01ned by users andor developers. While ArQL indexes are typically kept consistent with the locally-held blocks and transactions, they are notably not necessarily globally consistent with the network, as not all nodes must hold a full replication of all network data. This gives rise to three unusual properties for a decentralised database: 1.",
      "Query results returned by nodes running ArQL are subject to the same content policies that are expressed by that node. For more details on the content policies mechanism, see section 5. 2. As queries are only executed inside a single node, they can be relied upon to provide results in a timely manner. This is possible as there is no query sharding, or delays in collating results from other nodes, etc. 3. Certainty that a query has returned all possible results is not obtain- able  the results of a query merely indicate the transactions in the network that the servicing node is aware of, or is willing to reveal details of. As a consequence of 3, the use of ArQL may be limited in some circum- stances. However, the consequences of 1 hold profound e\ufb00ects for application developers and users. 7.3.2 Arweave DNS and TLS Arweave gateway nodes can utilise the standard DNS infrastructure to im- prove the display and functioning of Arweave transactions as web applica- tions.",
      "For example, the owner of a domain can run a permanently-available, decentralised web application just by storing a transaction on the Arweave network and registering DNS records via the usual external service providers. 1. DNS Two DNS records are required to link a domain to an Arweave transac- tion on a gateway node. For example, www.mycustomsite.com would need the following records to link it to www.arweave-gateway.net: A DNS CNAME record pointing to an Arweave gateway: www CNAME arweave-gateway.net A DNS TXT record linking the domain with a speci\ufb01c transaction ID: arweavetx TXT kTv4OkVtmc0NAsqIcnHfudKjykJeQ83qXXrxf8hrh0S When a browser requests www.mycustomsite.com the users machine will (through the usual DNS processes) resolve this to the IP ad- dress for the gateway node arweave-gateway.net. When the gate- way receives a HTTP request with a non-default hostname, e.g. www.",
      "mycustomsite.com instead of www.arweave-gateway.net, the gate- way will query the DNS records for www.mycustomsite.com and the arweavetx TXT record will tell the node which transaction to serve. 2. TLS A signi\ufb01cant share of browsers disallow the use of native cryptographic functions for web pages accessed without TLS. This means Arweave applications cannot use hashing functions, signing, or veri\ufb01cation fea- tures without integrating support for TLS. Not supporting TLS there- fore potentially limits the usefulness of Arweave permaweb apps, while also leaving these applications vulnerable to man-in-the-middle (MITM) attacks. Given that transactions are signed, a direct MITM attack is di\ufb03cult, but the lack of encryption also opens a class of novel at- tack vectors.",
      "One example includes the possibility of intercepting and manipulating the price endpoint, which could be exploited to re- turn unacceptably low prices- possibly causing a users transactions to fail, or high prices- which might result in the users wallet being overcharged for a transaction. Gateway operators generate and maintain TLS certi\ufb01cates for the gateway client. For example using ACME 7 for Lets Encrypt, though other TLS systems can be used. On initial gateway setup, a wildcard certi\ufb01cate for the gateways domain can be requested and generated. This allows for tra\ufb03c to access the gateway over HTTPSTLS on the gateways apex domain, as well as single level subdomains (e.g. gateway.com and subdomain.gateway.com, but not sub.subdomain. gateway.com), in turn allowing for browser sandboxing. When a browser requests a transaction from the gateway, for example direct to https:label.gateway.com6BdL...6VzZ where label is a Base32 pseudo-unique address derived from the transaction ID.",
      "As the transaction is now being served from a subdomain, the browsers same-origin policy is invoked and the web page is restricted to that sandbox, giving it its own secure browser context. Support for DNS  TLS has been implemented to ensure backwards com- patibility with these technologies. In addition to these traditional systems, Arweave can also integrate with decentralised name systems. Use Cases The Arweave protocols o\ufb00ering of truly reliable, immutable, and decentral- ised data storage provides signi\ufb01cant utility to a wide range of potential use cases. Some of the unique bene\ufb01ts o\ufb00ered by the Arweave protocol when addressing real world use cases include:  Providing a reliable archive of record. Once data is added to the blockweave, it cannot be removed or altered, either intentionally or unintentionally. Authenticity. The blockweave o\ufb00ers proof of existence of a speci\ufb01c piece of data at a certain point in time, based on the associated trans- actions veri\ufb01able, reliable timestamp.",
      "Provenance. Each transaction is linked permanently to the previous transaction from the same wallet, meaning that end-users can con- sistently verify the true origin of the data inside any transaction by wallet address, including that of decentralised applications hosted on the Arweave network. Decentralised application hosting. The blockweave ensures more re- liable access to applications than any centralised application-hosting platform, which commonly negatively impact application integrity5. Incentivised data storing and serving. Arweaves unique mechanism design robustly encourages the rapid serving of data to all participants in the network, including end-users (see section 3.4 for further details). Example Applications There is a wide array of live, decentralised applications running on the Ar- weave network today, each bene\ufb01ting from the unique advantages detailed in the previous subsection of this paper.",
      "Here, we will brie\ufb02y describe a small number of these decentralised applications:  ArBoard61. A decentralised discussion board allowing users to create discussion topics within subcategories, to which other users can reply, vote on popular threads, and view the edit history of any post. All of this functionality is powered by ArQL, with no external dependencies or computation. AskWeave44. An Arweave community-created application hosted on the blockweave, o\ufb00ering a free-form QA-style forum functionality. Users can ask and answer questions, and tip their favourite responses using AR tokens directly within the application itself. Weavemail66. A decentralised, robust, permanent email client hosted on the Arweave network. In addition to sending immutable email on the blockweave itself, users may also use this as a method to securely send and receive AR tokens with other Weavemail users. ArweaveID43.",
      "Another Arweave community-created application, this enables users to claim a unique username linked to their Arweave wal- let address. Most interestingly, a number of other community de- velopers have integrated ArweaveID into their own permaweb applica- tions, including the aforementioned AskWeave, meaning that users can utilise the unsiloed feature of permaweb data (i.e. that the block- weave can act as a single, universal data source for any permaweb application). In this instance, this means that users can have one universal username for all permaweb applications, without having to register for individual user accounts on every individual platform. Future Work Succinct Proofs of Access The primary scalability challenge presented by the current Arweave archi- tecture is the necessity for pushing all transaction data to all nodes during the writing of the data into the network.",
      "While the current architecture of the system allows for approximately 310 million typically-sized pieces of Arweave data to be stored per year (as of block 223,280), substantial im- provements are possible in this area. The core Arweave development team plans to o\ufb00er these improvements to the community by introducing a new optional data access proof mechanism: Succinct Proofs of Access. Transactions secured by the Succinct Proof of Access mechanism replace the direct embedding of data within the blockweave with storing only Merkle roots of said data. This introduces a number of trade-o\ufb00s. While it allows for consensus about the entry of data into the blockweave to be acquired without all participating nodes receiving a full copy of the data (if, for example, they do not have excess storage capacity with which to store said data), it also removes the proof of distribution guarantees presented by traditional blockweave data structures.",
      "This allows transactions to contain massive quantities of data, but loses the certainty that said data was ever fully seeded to the network. Succinct Proofs of Access to transaction data can be provided by foreign nodes during block con\ufb01rmation with a high degree of con\ufb01dence, by simply then transferring a series of Merkle paths through the Merkle tree generated for the dataset. This has the advantage of allowing vast amounts of data to be added to the network quickly, but does not provide the same data avail- ability guarantees as traditional Proof of Access. Subsequently, Succinct Proofs of Access will be provided as an optional feature for those with large datasets that require long-term storage with high levels of redundancy, but do not require that certainty be achieved about the data ever being fully publicly available. Wallet Logs Further improvements are also planned for the system of Wallet Lists in the Arweave network.",
      "Wallet Lists (a mechanism for maintaining accounts, balances, and avoiding replay attacks) currently represent a high proportion of the data storage overhead in the Arweave network. Each wallet ever used in the block weave is represented in the Wallet List in each block. It is planned that these wallet lists will be replaced with append-only wallet logs. By appending a new entry to the wallet storage data structure each time a balance is changed, the storage complexity drops from Oblocks wallets to Otxs  blocks. Further, the wallet log mechanism does not materially a\ufb00ect wallet data look-up times, as the log structure can be e\ufb00ectively compacted once every given number of blocks. This compacting mechanism keeps the size of the structure manageable relative to the number of wallets in the network. Further, this mechanism also allows for O1 veri\ufb01cation of new block contents.",
      "Fast Find As the Arweave network incentivises all network participants to store as much of the blockweave as possible, the issue of rapid data location in a de- centralised web is quickly diminished when compared to typical distributed hash table-based approaches. By ensuring that there are large numbers of replications of each piece of data in the network (approximately 750 at the time of writing) the game of \ufb01nding data turns from \ufb01nding a needle in a haystack to \ufb01nding some hay in a barn, since replications are so abundant in the Arweave network. Nonetheless, at some point optimisation of data location times may be desirable. We expect that this is likely to take the form of modi\ufb01ed AIIA agents that reward players not just for providing fast access to data they store locally, but also for re-directing participants to nodes that are able to provide the data.",
      "As in typical AIIA meta-games, each node in the network is sel\ufb01shly in- centivised to incentivise the expression of this pro-social behaviour in other nodes. The implementation of each independent AIIA agent will de\ufb01ne ex- actly how participants maintain tables for routing requests of other nodes towards likely holders of the data they are requesting. Nonetheless, it is expected that eventually these agents will closely resemble next-hop routing layers, similar to the structure of the existing internet. Conclusion In conclusion, we have presented the Arweave family of protocols: a modular system for achieving permanent replication and recall of knowledge and his- tory, backed by sustainable economic mechanisms. As well as promoting the perpetual storage of information through a novel storage endowment, Ar- weave also promotes the distribution of this information through adaptable and robust o\ufb00-chain incentive mechanisms.",
      "On top of the primary Arweave protocol layer, we have also presented an out- line of the permaweb: a permanent, decentralised, and resilient web. Unlike the traditional web, all content in the permaweb is immutable, timestamped, and cryptographically signed (ensuring strong authorship properties). The permaweb is currently in its infancy. It has been live for 500 days, and is growing at an approximate rate of 2,500 web applications and pages per day. Further to the presented description of the current protocol implementation, we have also outlined areas of potential future work, as well as adaptive mechanism designs to encour these developments. 1 Mustafa Akgul and Melih Kirlidog. Internet censorship in turkey. In- ternet Policy Review, 4(2):122, 2015. 2 Eitan Altman, Rachid El-Azouzi, Yezekael Hayel, and Hamidou Tem- bine. The evolution of transport protocols: An evolutionary game per- spective. Computer Networks, 53(10):17511759, 2009. 3 Anderson, Moore, Nagaraja, and Ozment.",
      "Incentives and information security. In Nisan et al. 50. 4 Farhad Anklesaria, Mark McCahill, Paul Lindner, David Johnson, Daniel Torrey, and B Albert. The internet gopher protocol (a distrib- uted document search and retrieval protocol). University of Minesota Microcomputer and Workstation Networks Center, Spring, 1993. 5 Arweave. Avoiding consumer lock-ins with decentralised web. avoiding-consumer-lock-in-with-the-decentralised-web-c618f28241ab, 2019. 6 Fernando Baez and Alfred J MacAdam. A universal history of the destruction of books: From ancient Sumer to modern Iraq. Atlas New York, 2008. 7 R. Barnes, J. Ho\ufb00man-Andrews, D. McCarney, and J. Kasten. Automatic certi\ufb01cate management environment (acme). https: ietf-wg-acme.github.ioacmedraft-ietf-acme-acme.html, 2019. 8 Tobias Baumann, Thore Graepel, and John Shawe-Taylor. Adaptive mechanism design: Learning to promote cooperation. arXiv preprint arXiv:1806.04067, 2018. 9 D. Bayer, S. Haber, and W. S. Stornetta.",
      "Improving the e\ufb03ciency and reliability of digital time-stamping. In R. Capocelli, A. De Santis, and U. Vaccaro, editors, Sequences II, pages 329334. Springer, 1992. 10 Barry L Bayus. An analysis of product lifetimes in a technologically dynamic industry. Management Science, 44(6):763775, 1998. 11 Jacob D Bekenstein. Universal upper bound on the entropy-to-energy ratio for bounded systems. Physical Review D, 23(2):287, 1981. 12 J Benet. Ipfs content addressed, versioned, system (draft QmV9tSDx9UiPeWExXEeH6aoDvmihvx6jD5eLb4jbTaKGps. 13 A Berman. Bitmex research \ufb01nds potential bug in syncing of ethereum parity full node. bitmex-research-finds-potential-bug-in-syncing-of-ethereum-parity-full-node, 2019. 14 Bitcoin genesis block. 4a5e1e4baab89f3a32518a88c31bc87f618f76673e2cc77ab2127b7afdeda33b, 2009. 15 Rene Carmona and Francois Delarue. Probabilistic Theory of Mean Field Games with Applications I-II. Springer, 2018. 16 Anton-Hermann Chroust. Socratesa source problem.",
      "The New Schol- asticism, 19(1):4872, 1945. 17 D Clark. Intel rechisels tablet moores law. intel-rechisels-the-tablet-on-moores-law, 2015. 18 Bram Cohen. Incentives build robustness in bittorrent. In Workshop on Economics of Peer-to-Peer systems, volume 6, pages 6872, 2003. 19 Matt Corallo. Compact block relay. bip 152, 2017. 20 C. Decker and R. Wattnehofer. Information propagation in the bitcoin network. In IEEE P2P 2013 Proceedings, 2013. 21 Robert P Dellavalle, Eric J Hester, Lauren F Heilig, Amanda L Drake, Je\ufb00W Kuntzman, Marla Graber, and Lisa M Schilling. Going, going, gone: Lost internet references, 2003. 22 Bianca M Federico. The patent o\ufb03ce \ufb01re of 1836. J. Pat. O\ufb00. Socy, 19:804, 1937. 23 Steve Forbes. Web of deception: Misinformation on the Internet. In- formation Today, Inc., 2002. 24 Anne Frank and Storm Jameson. Anne Franks diary. Vallentine, Mitchell, 1958. 25 Henrik Frystyk, Tim Berners-Lee, and Roy T Fielding. Hypertext trans- fer protocol  http1.0.",
      "RFC 1945, RFC Editor, 1996. 26 Pawel Garbacki, Dick HJ Epema, and Maarten Van Steen. An amort- ized tit-for-tat protocol for exchanging bandwidth instead of content in p2p networks. In First International Conference on Self-Adaptive and Self-Organizing Systems (SASO 2007), pages 119128. IEEE, 2007. 27 GoldZeus. Weaver. https:github.comGoldZeusweaver, 2019. 28 Olivier Gueant, Jean-Michel Lasry, and Pierre-Louis Lions. Mean \ufb01eld games and applications. In Paris-Princeton lectures on mathematical \ufb01nance 2010, pages 205266. Springer, 2011. 29 S. Haber and W. S. Stornetta. How to time-stamp a digital document. Journal of Cryptology, 3:99111, 1991. 30 Zhu Han, Dusit Niyato, Walid Saad, Tamar Baar, and Are Hjrungnes. Game Theory in Wireless and Communication Networks: Theory, Mod- els, and Applications. Cambridge University Press, 2011. 31 Adam Hayes. A cost of production model for bitcoin. Available at SSRN 2580904, 2015. 32 Ethan Heilman, Alison Kendler, Aviv Zohar, and Sharon Goldberg.",
      "Eclipse attacks on bitcoins peer-to-peer network. In 24th USENIX Security Symposium (USENIX Security 15), pages 129144, 2015. 33 Lambertus Hesselink, Sergei S Orlov, and Matthew C Bashaw. Holo- graphic data storage systems. Proceedings of the IEEE, 92(8):1231 1280, 2004. 34 D Johnston. Norths ex-secretary tells altering memos. north-s-ex-secretary-tells-of-altering-memos.html, 1989. 35 B Kahle. Fire update: Lost many cameras, boxes. hurt. scanning-center-fire-please-help-rebuild, 2013. 36 N Kuznetsov. As russian censorship increases, is a decentralized web the answer? as-russian-censorship-increases-is-a-decentralized-web-the-answer, 2019. 37 A Lafrance. The internets dark ages. comtechnologyarchive201510raiders-of-the-lost-web 409210, 2015. 38 J Langston. Digital collections. https:www.loc.gov, 2019. 39 J Langston. With a hello, microsoft and uw demonstrate \ufb01rst fully automated dna data storage. innovation-storieshello-data-dna-storage, 2019. 40 Sangho Lee and Jong Kim.",
      "Early \ufb01ltering of ephemeral malicious ac- counts on twitter. Computer Communications, 54:4857, 2014. 41 Birmingham Public Libraries. Notes on the History of the Birmingham Public Libraries: 1861-1961. Birmingham Public Libraries, 1962. 42 Ziyao Liu, Nguyen Cong Luong, Wenbo Wang, Dusit Niyato, Ping Wang, Ying-Chang Liang, and Dong In Kim. A survey on applica- tions of game theory in blockchain. arXiv preprint arXiv:1902.10865, 2019. 43 Lyner. Arweaveid. fGUdNmXFmflBMGI2f9vD7KzsrAc1s1USQgQLgAVT0W0, 2019. 44 Lyner. Askweave. HhIjOjxgHYXJU5RVjRYfAR017vbZdujbCSlaA8NQ20U, 2019. 45 Yuval Marcus, Ethan Heilman, and Sharon Goldberg. Low-resource eclipse attacks on ethereums peer-to-peer network. IACR Cryptology ePrint Archive, 2018:236, 2018. 46 Microsoft. Photodna. https:www.microsoft.comen-usphotodna, 2019. 47 S. Nakamoto. Bitcoin: A peer-to-peer electronic cash system. http: www.bitcoin.orgbitcoin.pdf, 2008. 48 The National Archives.",
      "Investigation into forged documents discovered amongst authentic public records: Documents purporting to have been created by members of the british government and members of the brit- ish armed services relating to leading nazis \ufb01gures and axis power gov- ernments. https:discovery.nationalarchives.gov.ukdetails rC16525, 2007. 49 Mehdi Nikkhah, Aman Mangal, Constantine Dovrolis, and Roch Guerin. A statistical exploration of protocol adoption. IEEEACM Transactions on Networking, 25(5):28582871, 2017. 50 Nisan, Roughgarden, Tardos, and Vazirani, editors. Algorithmic game theory. Cambridge University Press, 2007. 51 G Orwell. Nineteen Eighty-Four. Secker  Warburg, 1949. 52 A Pinar Ozisik, Gavin Andresen, George Bissias, Amir Houmansadr, and Brian Levine. Graphene: A new protocol for block propagation us- ing set reconciliation. In Data Privacy Management, Cryptocurrencies and Blockchain Technology, pages 420428. Springer, 2017. 53 Steve Phelps, Peter McBurney, and Simon Parsons.",
      "Evolutionary mech- anism design: a review. Autonomous agents and multi-agent systems, 21(2):237264, 2010. 54 Michael Piatek, Tomas Isdal, Thomas Anderson, Arvind Krish- namurthy, and Arun Venkataramani. Do incentives build robustness in bittorrent. In Proc. of NSDI, volume 7, 2007. 55 Michael Piatek, Tomas Isdal, Tom Anderson, Arvind Krishnamurthy, and Arun Venkataramani. Building bit-tyrant, a (more) strategic bit- torrent client. login, 32(4):813, 2007. 56 J; Rydning J Reinsel, D; Gantz. Data age 2025: The di- gitization world from edge core. seagate.comfileswww-contentour-storytrendsfiles idc-seagate-dataage-whitepaper.pdf, 2018. 57 Jonathan Rose. The holocaust and the book: destruction and preserva- tion. Univ of Massachusetts Press, 2008. 58 T. Roughgarden. Twenty lectures on algorithmic game theory. Cam- bridge University Press, 2016. 59 Muhammad Saad, Je\ufb00rey Spaulding, Laurent Njilla, Charles Kamh- oua, Sachin Shetty, DaeHun Nyang, and Aziz Mohaisen.",
      "Exploring the attack surface of blockchain: A systematic overview. arXiv preprint arXiv:1904.03487, 2019. 60 M Sa\ufb01. Whatsapp deleting 2m accounts a month to stop fake news. whatsapp-deleting-two-million-accounts-per-month-to-stop-fake-news, 2019. 61 sergejmueller. Arboard. pvmiu4SZKQGWAYjrLWzE_mI70u1-v8zIzQ8WaxIYURk, 2019. 62 A Souppouris. The internet archive is now home to 10 peta- bytes of data. internet-archive-total-data-2012-ten-petabytes, 2012. 63 Margaret Sullivan. Were changes to sanders article stealth editing. The New York Times, 2016. 64 tevador. Randomx. https:github.comtevadorRandomX, 2019. 65 Dave Thaler and B Aboba. Rfc 5218: What makes for a successful protocol?, 2008. 66 S Williams. Weavemail. oEhzHOE2o9uZbi6O9cQatzhiHtc2EdFBGCQdqHsK-o4, 2019. 67 Gavin Wood et al. Ethereum: A secure decentralised generalised trans- action ledger. Ethereum project yellow paper, 151:132, 2014. 68 Xueyang Xu, Z Morley Mao, and J Alex Halderman.",
      "Internet censorship in china: Where does the \ufb01ltering occur? In International Conference on Passive and Active Network Measurement, pages 133142. Springer, 2011. 69 Jingyu Zhang, Mindaugas Gecevi\u02c7cius, Martynas Beresna, and Peter G Kazansky. 5d data storage by ultrafast laser nanostructuring in glass. In CLEO: Science and Innovations, pages CTh5D9. Optical Society of America, 2013. Appendices 10.1 Hard Drive Capacities and MTBF Over Time Please see the following Arweave transaction: 10.2 Anatomies This section will describe a range of data structures and their contents. 10.2.1 Block Data Structure  Transactions: A list of zero to many transactions (see 10.2.4 Trans- action Data Structure). Block Size: The sum of the number of bytes of the data in each transaction in Transactions. Timestamp: The decimal representation of the Unix Timestamp when mining the block started. Reward Address: Up to 32 bytes.",
      "If this matches a Wallet Address in the previous blocks Wallet List, then this Wallet Address will get the reward. Previous Block: The Independent Hash of the previous block. Height: The zero-based sequential number of this block in the Block Weave. Weave Size: The sum of the Block Size of this block and the Weave Size of the previous block. Storage Endowment: The number of Winston in the endowment (see 3.2.3). Hash List: A list of all the Independent Hashes starting from the previous block down to the Genesis Block  Hash List Merkle Root: The SHA-384 hash of the concatenation of the previous blocks Hash List Merkle Root and the previous blocks Independent Hash. For the Genesis Block, the value is empty. Ba- sically, this is the root of a completely unbalanced Merkle tree for all Independent Hashes from the Genesis Block up to the previous block. Wallet List: A list of all wallets ever received any AR, so called Active Wallets, after applying the transactions in this block.",
      "The list is ordered by Wallet Address. Wallet Address: The SHA-256 hash of the wallets public key. Last Transaction: The ID of the last mined transaction created by this wallet, i.e. the last mined transaction where the SHA-256 hash of Owner for the transaction is the Wallet Address. Balance: The current balance of AR for the wallet in Winston. Its adjusted by these rules: The block contains a transaction where the SHA-256 hash of Owner is matching the Wallet Address. The balance is then reduced by the Reward of the transaction. Negative balance is not allowed. The block contains one or more transactions where the Tar- get is matching the Wallet Address. The balance is then increased by the sum of the Quantity values of these trans- actions. If this is the \ufb01rst time this Wallet Address receives any AR, its entry in the Wallet List will be created according to these rules:  The balance will be set to the transactions Quantity minus 25 AR.",
      "This represents the Wallet Creation Fee which is spam \ufb01lter for active wallets. The wallet will only be created if the quantity is 25 AR or more. The transactions are applied in the order they are spe- ci\ufb01ed in the block. The Reward Address is matching the Wallet Address. The balance is then increased by the mining reward for this block (see 3.2.3). Block Data Segment: A SHA-384 hash of the concatenation of the following:  Independent Hash of the previous block. Dependent Hash of the previous block. The decimal representation of Timestamp. The decimal representation of Last Retarget. The decimal representation of Height. Concatenation of every value in Hash List. The Serialised Representation of Wallet List. Reward Address. The decimal representation of Storage Endowment. The Serialised Representation of this blocks Recall Block (see 2.1 The Recall Block, Proof of Access, and the Blockweave). Concatenation of the Serialised Representation of each transac- tion in Transactions.",
      "Hash List Merkle Root  Dependent Hash: The SHA-384 hash of the concatenation of Block Data Segment and Nonce. Nonce: A\ufb00ects the Dependent Hash and must be chosen so that the Dependent Hash ful\ufb01ls the Di\ufb03culty. Up to 512 bytes. Di\ufb03culty: The number the PoW hash must be greater than. The value is inherited from the previous block except if this is a retarget block where the value may increase or decrease proportionally to the ratio between the target time and the actual time. Last Retarget: The Timestamp of the last block where Retarget happened. If this is a retarget block, then the value should be this blocks Timestamp. Cumulative Di\ufb03culty: The expected number of hashes tried by the network, summed up over previous blocks of this fork. Independent Hash: The Deep Hash (see 10.2.2 Deep Hash) of a list of the following:  Nonce. Previous Block. The decimal representation of Timestamp. The decimal representation of Last Retarget. The decimal representation of Di\ufb03culty.",
      "The decimal representation of Cumulative Di\ufb03culty. The decimal representation of Height. Dependent Hash. Hash List Merkle Root. The concatenation of all transaction IDs of the transactions in Transactions. The Serialised Representation of Wallet List. Reward Address or unclaimed if empty. Empty list. The decimal representation of the Storage Endowment. The decimal representation of Weave Size. The decimal representation of Block Size. Serialised Representation: concatenation of the following:  Nonce. Previous Block. The decimal representation of Timestamp. The decimal representation of Last Retarget. The decimal representation of Di\ufb03culty. The decimal representation of Height. Hash. Indep Hash. The concatenation of the Serialised Representation of the trans- actions in Transactions sorted by Transaction ID. The concatenation of each element of Hash List. The Serialised Representation of Wallet List. Reward Address. Decimal representation of Weave Size.",
      "10.2.2 Deep Hash Deep Hash is a hash algorithm which takes a nested list of values as input and produces a 384 bit hash, where a change of any value or the structure will a\ufb00ect the hash. deep_hash(List) when is_list(List) - hash_bin_or_list ( List). INTERNAL hash_bin_or_list (Bin) when is_binary(Bin) - Tag  \"blob\", ( integer_to_binary (byte_size(Bin))) binary , hash_bin (( hash_bin(Tag))binary , (hash_bin(Bin)) binary ); hash_bin_or_list (List) when is_list(List) - Tag  \"list\", ( integer_to_binary (length(List))) binary , hash_list(List , hash_bin(Tag)). hash_list(, Acc) - Acc; hash_list(Head  List, Acc) - HashPair  Accbinary , ( hash_bin_or_list (Head)) binary , NewAcc  hash_bin(HashPair), hash_list(List , NewAcc). hash_bin(Bin) when is_binary(Bin) - hash_sha384(Bin). Listing 1: Deep Hash reference implementation in Erlang 10.2.3 Blockshadow Data Structure A Blockshadow is a slimmed-down version of the full Block where the re- moved data can be reconstructed from other data.",
      "This makes the block- shadow tiny compared to the full block, which makes it fast and cheap to propagate between nodes in the network. See 2.3 Blockshadows. A blockshadow is the same as the block with these di\ufb00erences:  Transactions: Instead of the list of transactions, the blockshadow has a list of the transaction IDs. When the blockshadow of a new block is propagated though the network, the receiving node is likely to have the transactions in its mempool, making it possible to reconstruct the full list of transactions from the transaction IDs. Hash List: The Hash List is trimmed down to only include the \ufb01rst 50 entries. The full Hash List can be reconstructed by taking the Hash List of the previous full block and pre-pending the previous blocks independent hash. Wallet List: The Wallet List is not included. It can be reconstructed by the Wallet List of the previous full block and the transactions of this block.",
      "10.2.4 Transaction Data Structure  Data: Between 0 and 10,485,760 bytes of arbitrary data. Owner: The public key of the RSA key-pair signing this transaction. Quantity: Amount of Winston to send to another wallet. Target: The Wallet Address of the recipient of the Winston speci\ufb01ed by quantity. No validation may be performed of the validity. Up to 32 bytes. Reward: The amount of Winston payed by Owner which will go to the Storage Endowment (see section 3.2.3. Tags: A list of any number of tags where its Serialised Representation is up to 2048 bytes. Tag: A key-value pair for arbitrary metadata. Name: The key. Unlimited number of whole bytes of data. Value: The value. Unlimited number of whole bytes of data. Serialised Representation: A concatenation of the Name and Value. Serialised Representation: A concatenation of each tags Seri- alised Representation. TXanchor: TXowners last processed transaction ID or the independent hash of one of the last 50 blocks. Empty for the \ufb01rst transaction.",
      "Signature Data Segment: A concatenation of the following \ufb01elds in the following order:  Owner. Target. Data. The decimal representation of Quantity. The decimal representation of Reward. Last TX. The Serialised Representation of Tags. Signature: The RSA-SHA256 signature of Signature Data Segment for the RSA key-pair where the public key is Owner. ID: SHA-256 hash of the Signature. Serialised Representation: The concatenation of the following:  ID. Last TX. Owner. The Serialised Representation of Tags. Target. The decimal representation of Quantity. Data. Signature. The decimal representation of Reward."
    ],
    "word_count": 20650,
    "page_count": 67
  },
  "ARB": {
    "chunks": [
      "Arbitrum Nitro: A Second-Generation Optimistic Rollup Lee Bousfield, Rachel Bousfield, Chris Buckland, Ben Burgess, Joshua Colvin, Edward W. Felten, Steven Goldfeder, Daniel Goldman, Braden Huddleston, Harry Kalodner, Frederico Arnaud Lacs, Harry Ng, Aman Sanghi, Tristan Wilson, Valeria Yermakova, and Tsahi Zidenberg Offchain Labs, Inc. Abstract We present Arbitrum Nitro, a second-generation Layer 2 blockchain protocol. Nitro provides higher throughput, faster finality, and more efficient dispute resolution than previous rollups.",
      "Nitro achieves these properties through several design principles: separating sequencing of trans- actions from deterministic execution; combining existing Ethereum emulation software with extensions to enable cross-chain functionalities; compiling separately for exe- cution versus proving, so that execution is fast and prov- ing is structured and machine-independent; and settling transaction results to the underlying Layer 1 chain using an optimistic rollup protocol based on interactive fraud proofs. Introduction In previous work, we described Arbitrum 9, a system and protocol for improving the performance and scala- bility of smart contracts. This paper describes Arbitrum Nitro, a significantly improved design that offers advan- tages over the original, including greater efficiency, re- duced latency, stronger liveness guarantees, and better incentive compatibility. Properties of Arbitrum Nitro Nitro supports execution of smart contracts.",
      "The system is implemented as a Layer 2 on top of Ethereum 14, although in principle it could be implemented on any blockchain system that supports at least basic smart contract functionality. Nitro provides an Ethereum- compatible chain: Nitro runs smart contract applications deployed in Ethereum Virtual Machine (EVM) code, and Nitro nodes support the same API as common Ethereum nodes. The Nitro protocol guarantees both safety and progress for the Layer 2 chain, assuming that (1) the un- derlying Ethereum chain is safe and live, and (2) at least one participant in the Nitro protocol is behaving honestly. The protocol is termed optimistic because execution is more efficient when parties behave according to their in- centives. A variant of Nitro, called AnyTrust, provides lower cost in exchange for an additional trust assumption. The main body of this paper describes regular Nitro, and the differences in AnyTrust are described in Section 7.",
      "Nitro has been deployed on the Arbitrum One chain, with Ethereum as the underlying Layer 1, since August 31, 2022. Nitros source code is available at https: github.comoffchainlabsnitro and its submod- ules. Design Approach Nitros design has four distinctive features, which we will use to organize the presentation. Sequencing followed by deterministic execution: Nitro handles submitted transactions in two stages. First, it puts transactions into the sequence in which they will be processed, and commits to that se- quence. Second, it applies a deterministic state tran- sition function to each transaction, in sequence. Geth at the core: The core execution and state maintenance functions in Nitro are handled by code from the open source go-ethereum (geth) pack- age, which is the most popular Ethereum execution layer node software. By compiling in that geth code as a library, Nitro ensures that its execution and state are highly compatible with Ethereums.",
      "Separate execution from proving: Nitro compiles the code of its state transition function for two tar- gets. The code is compiled for native execution when used in ordinary operation in a Nitro node. The same code is compiled to portable web assem- bly (wasm)13 code for use in the fraud proof protocol if needed. This dual-target approach as- sures that execution is fast, while proving is based on structured, machine-independent code. Optimistic rollup with interactive fraud proofs: Building on the original Arbitrum 9 design, Nitro uses an improved optimistic rollup protocol based on an optimized dissection-based interactive fraud proof protocol. Structure of the paper The remainder of the paper is structured as follows. Sec- tion 2 introduces the sequencer and the deterministic state transition function. Section 3 describes the struc- ture of the Nitro software, and the affordances it provides to support a Layer 2 chain.",
      "Section 4 describes the struc- ture and derivation of the code used for proving execu- tion results. Section 5 presents the protocol used to assert execution results, and Section 6 describes the challenge sub-protocol, which resolves any disputes about those results. AnyTrust, an extension of Nitro using an exter- nal data availability committee, is described in Section 7. Section 8 concludes and suggests future directions. Sequencing Followed by Deterministic Execution Processing of submitted transactions in Nitro occurs in two phases. First, a component called the Sequencer puts the transactions into an ordering and commits to the or- dering. Second, the transactions are consumed, in se- quence, by the deterministic State Transition Function. The process is illustrated in Figure 1. Submitted transactions may or may not be valid. For example, they may lack a valid signature, or they may be garbage data.",
      "An honest Sequencer will make its best ef- fort to discard submitted transactions that are invalid, but the protocol makes no assumptions about whether trans- actions in the Sequencers output are valid. Executing the State Transition Function on an invalid transaction will simply discard that transaction. The Sequencer The Sequencer is trusted only to order incoming trans- actions honestly, according to a first-come, first-served policy.1 At present the Sequencer is a centralized com- ponent operated by Offchain Labs, but in the future we intend to transition to a committee-based sequencer us- ing a fair distributed sequencing protocol 11, 10. 1In principle the Sequencer could implement any transaction order- ing policy. The first-come, first-served policy is easy to implement and minimizes latency. The Sequencer does not have the power to prevent the chain from making progress, nor to prevent the inclusion of any particular transaction.",
      "The Sequencer publishes its transaction ordering in two ways. First, it publishes a real-time feed of the se- quenced transactions, which any party can subscribe to. The feed represents the Sequencers promise to record transactions finally in a particular order. The Sequencer has the power to keep its promises, so any deviation from the promised sequence would be due to malfunction or malice by the Sequencer, or to a deep reorganization of the Layer 1 chain. Second, the Sequencer posts its transaction sequence as Ethereum calldata. The Sequencer collects a batch of consecutive transactions, compresses it using a general- purpose compression algorithm (currently brotli 1), and passes the result to the Nitro chains Inbox contract, which runs on L1 Ethereum. These batches represent the final and authoritative transaction ordering, so that once the Sequencers transaction to the Inbox has final- ity on Ethereum, the Nitro chains transaction sequence is final.",
      "The Delayed Inbox Although most user transactions will be submitted directly to the Sequencer and included in one of the Sequencers batches, there is another way to submit transactions, through the Delayed Inbox. This has two purposes. First, it allows transactions to be sub- mitted by L1 Ethereum contracts, which cannot gener- ate the digital signatures needed to submit a transaction through the Sequencer. Second, it provides a backup method for anyone to submit a transaction in case the Sequencer starts censoring valid transactions. A transaction is added to the Delayed Inbox by call- ing a method on the Nitro chains inbox contracts. The contracts keep a queue of timestamped transactions. The Sequencer can include in its sequence the first message in the delayed inbox queue.",
      "An honest Sequencer will do this after a brief delay, which is long enough to ensure that the arrival of that message in the delayed inbox will not be wiped out by a reorganization of the L1 chain typically a 10-minute delay. However, if a message has been in the delayed inbox for at least a threshold time period (currently 24 hours), anyone can force that message to be included next in the chains inbox, thereby guaranteeing its execution. This forced inclusion step prevents censorship by the Se- quencer, but would only be needed due to the Sequencer being malicious or having a long downtime. Section 3.2.3 presents more details about the role of the Delayed Inbox. Figure 1: Processing of transactions in Nitro. The sequencer establishes an ordering on transactions, and publishes the order as a real-time feed and as compressed data batches on the L1 chain.",
      "Sequenced transactions are processed one at a time by a deterministic state transition function, which updates the chain state and produces L2 blocks. These blocks are later settled to the L1 chain. Deterministic execution After incoming transactions have been sequenced, they are processed by the execution phase of Nitro, by us- ing the chains State Transition Function (STF). The STF takes as input a state (which is the root hash of an Ethereum state tree 14), and an incoming message, which is usually a single transaction. The STFs output is an updated state and a new Ethereum-compatible block header to be appended to the Nitro chain. The STF is fully deterministic, so that the outcome of executing the STF on a transaction depends only on the transactions data and the state before the transaction. Because of this, the outcome of a transaction T depends only on the genesis state of the Nitro chain, the sequence of transactions preceding T, and T itself.",
      "Because of this determinism, an honest party can de- termine the full state and history of the chain, given only the transaction sequence, or given a confirmed state of the chain at some point in the past and the transaction se- quence since then. Nodes need not communicate, and no consensus is necessary among them, in order to agree on the correct state and history, because this depends only on the transaction sequence which is visible to all. Nitro does have a rollup sub-protocol (discussed in Section 5) to confirm the transaction results to the L1 Ethereum chain. That sub-protocol does not decide the result of transactions but only confirms and records the result that was already known to honest protocol partici- pants. Software Architecture: Geth at the Core The second key design idea in Nitro is geth at the core. Here geth refers to go-ethereum, the most common execution layer node software for Ethereum.",
      "As its name would suggest, go-ethereum is written in Go 7, as is almost all of Nitro. The software that makes up a Nitro node can be thought of as built in three main layers, which are shown in Figure 2. The base layer is the core of geththe parts of geth that emulate the execution of EVM contracts and maintain the data structures that make up the Ethereum state. Nitro compiles in this code as a library, with a few minor modifications to add nec- essary hooks. The middle layer, which we call ArbOS, is cus- tom software that provides additional functions as- sociated with Layer 2 functionality, such as decom- pressing and parsing the Sequencers data batches, accounting for Layer 1 gas costs and collecting fees to reimburse for them, and supporting cross-chain bridge functionalities such as deposits of Ether and tokens from L1 and withdrawals of the same back to L1. Figure 2: High-level structure of the Nitro code, showing major components.",
      "The boundary of the State Transition Function is shown in orange. The top layer consists of node software, mostly drawn from geth. This handles connections and in- coming RPC requests from clients and provides the other top-level functionality required to operate an Ethereum-compatible blockchain node. Because the top and bottom layers rely heavily on code from geth, this structure has been dubbed a geth sand- wich.2 The State Transition Function consists of the bottom geth layer, and a portion of the middle ArbOS layer. In particular, the STF is a designated function in the source code, and implicitly includes all of the code called by that function. The STF takes as input the bytes of a transac- tion received in the inbox, and has access to a modifiable copy of the Ethereum state tree. Executing the STF may modify the state, and at the end will emit the header of a new block (in Ethereums block header format) which will be appended to the Nitro chain.",
      "ArbOS ArbOS is a software layer that implements functionality that is necessary and convenient for managing a Layer 2 chain. This includes bookkeeping functions, cross-chain communication, and L2-specific fee tracking and collec- 2Strictly speaking, geth plays the role of the bread in the sandwich, and ArbOS is the filling, but this sandwich is named for the bread. tion. Portions of ArbOS are included in the State Transi- tion Function. 3.1.1 State Representation All state of a Layer 2 Nitro chain is stored in Ethereums Merkle Patricia state trie data structure. This includes the state of ArbOS, which is modified as part of the State Transition Function. ArbOS encodes its state in the storage slots of a spe- cial Ethereum account whose private key is unknown.",
      "The specific slots used are chosen to satisfy the following goals  keep all ArbOS state in the storage of a single Ethereum account,  allow sub-components of ArbOS to manage their state separately, without collisions,  maintain reasonable locality within the same sub- component, and  avoid constraining future additions to the state. The state is organized as a hierarchy of nested spaces where each space is a mapping from 256-bit index to 256-bit value, with all values implicitly initial- ized to zero. This structure is mapped onto a single flat 256-bit to 256-bit key-value store, which is the storage of the special Ethereum contract. Each space is associated with a key. The key of the root space is zero, and the key of a subspace named n within a space with key k is H(kn) where H is Ethereums standard Keccak256 hashfunction. This scheme ensures that spaces keys do not collide.",
      "Within the space with key k, the item with index i is stored at location H(ki) in the underlying flat stor- age, where H is a locality-preserving hashfunction. The function H(x) hashes all but the last 8 bits of x, trun- cates the result to 248 bits, then appends the last 8 bits of x. This ensures that contiguous groups of 256 indices are kept contiguous by the hashfunction, while ensuring that the function is collision-resistant.3 The use of this locality-preserving hashfunction will reduce the cost of state accesses when Ethereum switches to a state repre- sentation that rewards contiguity. Cross-chain Interaction One of the roles of ArbOS is to support secure cross- chain calls in both directions between Nitro and Layer 1 Ethereum. An account on one layer can send a trans- action to the other chain, and that transaction will be executed asynchronously.",
      "In this section we describe the Outbox, which supports calls from a Nitro chain to Ethereum, and two mechanisms, the Inbox and Retryable Tickets, which support calls from Ethereum to a Nitro chain. 3.2.1 Address Aliasing When a Layer 1 Ethereum contract submits a transaction to a Nitro chain, the question arises of what sender ad- dress should be attached to the transaction when it runs on Nitro. It is tempting to simply use the L1 address of the sending contract, but there could be a contract at the same address on the Nitro chain, and if so the two con- tracts would be indistinguishable to a call recipient on Nitro, which would allow either one to impersonate the other on the Nitro chain. This is potentially dangerous. To avoid this, the address of an L1 sender at ad- dress A on Layer 1 is presented on the Nitro chain as f(A)  (A C) mod 2160, where C is a specified odd constant.",
      "Because all Ethereum addresses, and all other Nitro addresses, are generated by hashing some data (the exact data depending on how the address originated), it is infeasible to generate a collision between an aliased address and another Nitro address. Nitro software trans- lates addresses in either direction as needed, when inter- acting with Ethereum. 3H loses a few bits of collision resistance compared to H, requiring a factor of 24 less effort to find a collision by brute force search, but this reduction is acceptable. 3.2.2 The Outbox Nitros Outbox system allows for arbitrary L2 to L1 con- tract calls; i.e., messages initiated on L2 which are even- tually executed on L1. Given the security properties in- herent to optimistic rollups, an outgoing messages L1 execution can only take place after its messages dispute period passes and its Rollup Block is confirmed (as de- scribed in Section 5).",
      "Logically, an L2 to L1 message is like a ticket that is created on L2 and can later be redeemed at L1 to cause a specified transaction call to occur at L1. The recipient of that transaction call can verify that it is an authorized L2 to L1 message call, and can confirm the L2 sender and data of the call. This functionality is sufficient to support secure transfers of ETH, tokens, or other forms of value from L2 to L1. The asynchronous ticket model is needed for safety. Messages must be asynchronous because they cannot be redeemed until an RBlock that includes them has been confirmed; and redemption is done per-ticket and not in strict order because redemption of a particular ticket could require executing arbitrary code which may be very expensive in L1 gas or not even possible. L2 to L1 messages are initiated via an L2 transaction that calls a special ArbSys precompile that is part of Ar- bOS.",
      "ArbOS serializes the L2 senders address, amount of ETH provided with the call, L1 destination address, and calldata, and the result becomes an L2 to L1 mes- sage. Part of the state asserted by an RBlock is the root hash of a Merkle tree of all L2 to L1 messages in the chains history. When an RBlock is confirmed, this root hash is updated in the chains Outbox contract on L1; at this point, a user can call the Outbox contract with a Merkle proof of inclusion of a message to redeem it. The L1 Out- box contract tracks which messages have been success- fully redeemed, so that each message can be redeemed at most once. ArbOS uses an efficient representation to support in- cremental computation of the Merkle tree root while re- quiring only logarithmic storage. Any Merkle tree can be decomposed into a minimal set of completely full binary subtrees, of decreasing size. ArbOS remembers only the size of the overall Merkle tree, along with the root hashes of these full subtrees.",
      "The addition of a new leaf to the tree will result in a state with exactly one completely full subtree that did not previously exist. ArbOS emits an L2 EVM event containing the hash of the newly created subtree. Every inclusion proof in a Merkle tree version consists of a set of these subtree hashes, and a client who wants to create a proof can use standard event searches to find the L2 events containing the hashes needed for their proof. (The Nitro node API includes support for constructing these proofs automatically.) 3.2.3 The Inbox The Inbox, which is managed by a set of Layer 1 Ethereum contracts, is responsible for recording mes- sages (typically transactions) sent to a Nitro chain. The Delayed Inbox receives messages that are submitted on Layer 1, while the main Inbox receives messages sent by the Sequencer as well as merging in messages from the Delayed Inbox. The Delayed Inbox is a set of Layer 1 Ethereum con- tracts that accept messages to be delivered to the Nitro chain.",
      "This is an alternative to submitting via the Se- quencer. The Delayed Inbox is the only way for Layer 1 contracts to submit messages, because Layer 1 contracts cannot sign messages or submit them to the Sequencer. It also provides a way for any user to submit a message without relying on the Sequencer, in case the Sequencer is unavailable or misbehaving. The Delayed Inbox is logically a queue. It tracks the number of messages submitted to it and a hash-chain commitment to the contents of those messages. These messages will eventually be copied into the main Inbox as described below. The Sequencer submits its data batches directly to the Inbox contracts. Each batch contains a compressed se- quence of transactions, along with a directive to include a specified number of messages from the head of the De- layed Inbox. The main input loop of the ArbOS will con- sume these Sequencer batches in order.",
      "A well-behaved Sequencer will include Delayed In- box messages after a short delay, which is long enough to minimize the risk that a reorganization of the Layer 1 chain will cause an included message to disappear or change. The current Sequencer implementation includes Delayed Inbox messages after ten minutes. If the Se- quencer fails to include a Delayed Inbox message within a fixed interval4, any party can call the Inbox to force inclusion of the message, which occurs by forcing a Se- quencer batch including the message(s) into the Inbox. The ability to submit a message to the Delayed Inbox and force its inclusion without relying on the Sequencer supports Nitros guarantee of liveness. 3.2.4 Retryable Tickets Layer 1 contracts can submit transactions to a Ni- tro chain, but those transactions necessarily run asyn- chronously on the Nitro chain, so the submitting Layer 1 transaction cannot see whether they succeed.",
      "This poses problems for applications such as token bridging which 4Setting this parameter reflects a tradeoff between the desire for prompt inclusion, and the desire to avoid unexpected behavior if the Sequencer experiences downtime. Currently it is set to 24 hours, but we expect the value to be reduced as the perceived risk of Sequencer downtime diminishes. require a Layer 1 contract to ensure that a deposit trans- action runs at Layer 2. If the deposit transaction fails, for example due to changes in gas prices, the Layer 1 bridge contract cannot know this until much later, and user funds could be lost or stranded in the meantime. To support this and other use cases, Nitro includes a retryable ticket system, which allows a transaction sub- mitted from Layer 1 to be designated as retryable, mean- ing that if the transaction fails, ArbOS creates a retryable ticket for the transaction.",
      "If the transaction had ETH call- value attached, ArbOS escrows that callvalue, associated with the ticket. A later transaction can redeem the ticket by providing funds for its gas. The retry will run with the original sender, callvalue, and data, with the only differ- ence being the gas parameters and who is paying for the gas. If a retry fails, the ticket remains in the retry buffer, and can be retried again. (If the retry succeeds, the ticket is removed.) After a fixed interval, currently one week, an unredeemed ticket expires and will be automatically deleted by ArbOS. If the deleted ticket had callvalue es- crowed by ArbOS, that callvalue is refunded. The submitter of a retryable transaction must pay a submission fee, which will be refunded to the submit- ter if the initial execution of the transaction succeeds, or paid to ArbOS if the initial execution fails and a retryable ticket is made.",
      "The submission fee is meant to cover the cost of keeping the ticket in ArbOSs storage until the tickets expiration time. The submission fee depends on the size of the transaction and is determined, for each submission, by a Layer 1 contract, to ensure that Layer 1 submitters know exactly what the fee will be. 3.2.5 Token Bridge Nitros cross-chain messaging affordances can be used to create a Token Bridge, an application that allows for the effective transfer of assets between the Ethereum and Nitro chains. The Offchain Labs team has implemented and released a Token Bridge informally referred to as canonical, though the Nitro core protocol grants it no special recognition or affordances; it is effectively an ap- plication like any other.",
      "(Note that, similarly, Nitro has no natively recognized notion of tokens nor of any par- ticular token standard, much like Ethereum.) At its core, the Token Bridge offers the ability to de- posit (transfer from Ethereum to Nitro) and withdraw (transfer from Nitro to Ethereum) fungible tokens. To deposit n tokens, a transaction is sent to Ethereum which carries out two operations: it sends n tokens to an L1 contract (known as a Token Gateway), and creates a retryable transaction (Section 3.2.4) that mints n tokens of an L2-counterpart contract. The two token contracts are counterparts, due to the guarantee that a holder of L2 token can carry out a withdrawal: a withdrawal of m tokens is initiated via an L2 transaction which burns m tokens on L2 and creates a L2 to L1 message (Sec- tion 3.2.2) directing that the L1 Token Gateway release m tokens on L1. Upon confirmation, the message can be executed in the Outbox, which releases the m tokens from escrow.",
      "By default, tokens are bridged via the Standard Gate- way contracts. When going through the Standard Gate- way, a token gets its L2 counterpart deployed on L2 at a deterministically generated address (via the CRE- ATE2 EVM opcode). The token contract deployed on L2 is a StandardArbERC20  an OpenZeppelin 12 ERC20 contract with additional affordances to mintburn from the bridge contracts, along with callback hook af- fordances. Alternatively, to use a different contract as its L2 counterpart, an L1 token contract can register itself to any other custom gateway. Gateway Router contracts are responsible for tracking the mapping of L1 tokens to their Gateways (which in turn map them to their L2 counterpart tokens). Many additional token bridge features are theoreti- cally possible, including non-fungible token bridging, atomic swaps for fast L2 to L1 withdrawals, and bridging tokens natively deployed on L2 back to L1.",
      "Several inde- pendent services offer enhanced bridging functionalities, typically building on the canonical bridge. Gas and Fees Like many blockchains, Arbitrum collects fees from each transaction, to cover the costs of operating the chain, align incentives, and ration resources when demand is high. Fees are charged and collected in chain-specific gas. For clarity we will use the term NitroGas to de- note Layer 2 gas on a Nitro chain, and L1Gas to denote Layer 1 gas on Ethereum. Each EVM instruction costs the same number of gas units on both chains; for exam- ple, the MULMOD instruction costs 8 NitroGas on Nitro and 8 L1Gas on Ethereum. Each transaction requires some amount of NitroGas, depending on the resources used by the transaction. The price of NitroGas is equal to the current basefee which varies algorithmically as described below. NitroGas prices and NitroGas payments are denominated in ETH.",
      "A Nitro transaction specifies a gas limit, which is the maximum amount of NitroGas it will be allowed to con- sume. If the transaction tries to consume more NitroGas than its limit, the transaction fails but it must pay for the NitroGas it used. A transaction also specifies the maxi- mum basefee it is willing to pay.The transaction will not run (and therefore will not consume NitroGas) if the cur- rent basefee is above the transactions maximum. To- gether these rules ensure that a transactions NitroGas spending cannot be more than the product of its gas limit and maximum basefee. By signing a transaction, the user is authorizing a deduction from its ETH account for gas costs of up to this amount, and Nitro respects this limit. This approach preserves the user experience of Ethereum, allowing developers and users to use standard tools and wallets.",
      "3.3.1 L2 Gas Metering and Pricing Like Ethereum, Nitro tracks the usage of NitroGas and dynamically adjusts its basefee based on usage, so that when demand exceeds the sustainable capacity of the chain, the basefee increases until demand and capacity come back into balance. The sustainable capacity of the chain is reflected in a chains speed limit parameter, which reflects the maxi- mum sustainable throughput of the chain, based on prac- tical engineering considerations. NitroGas usage is al- lowed to exceed the speed limit over short periods, but the pricing algorithm must ensure that average NitroGas usage does not exceed the speed limit over an extended period. Unlike Ethereum, Nitro has variable time between blocks, so Nitros basefee adjustment algorithm oper- ates at a one-second granularity rather than one-block as on Ethereum.",
      "Additionally, in Nitro the sequencer attaches timestamps to transactions, so ArbOS must be prepared to handle a large number of transactions with the same timestamp, or a large amount of NitroGas re- quested by transactions with the same timestamp. By contrast, Ethereum limits the L1Gas usage in a single block to twice Ethereums speed limit. Nitros gas metering algorithm tracks a backlog B, which is updated as follows. If a transaction consumes G NitroGas, B BG. If T seconds elapse, B max(B TS,0), where S is the speed limit. Intuitively, B tracks how far behind the sustainable speed limit the chain has been during the current burst of usage. Nitros basefee is then calculated as F(B)  F0emax(0,\u03b2(BB0)) where F0 is the minimum basefee, and B0 is a toler- ance parameter. The scale factor \u03b2 is chosen so that a 12-second period with gas usage double the speed limit would multiply the basefee by a factor of 9 8, yield- ing \u03b2  102S.",
      "This matches the rate of increase that Ethereum would see if it experienced a 12-second block at double its speed limit. The exponential growth of the basefee, as a function of backlog, guarantees that the backlog is bounded in prac- tice. If the demand curve is unchanging, and if demand exceeds the speed limit at the minimum basefee, then the basefee will equilibrate at the level where demand equals the speed limit, and the backlog will be constant and log- arithmic in the equilibrium price. 3.3.2 L1 Data Metering and Pricing In addition to Layer 2 resources, a transaction also uses some resources on Layer 1 Ethereum. These must be included in the transactions total gas cost, so that costs can be recovered and incentives aligned. Although these L1 resources are charged in NitroGas, these L1 charges are not included when tracking the backlog, because they do not reflect consumption of the Nitro chains own re- sources.",
      "The relevant costs are incurred by the Sequencer when it submits Ethereum transactions to post data batches on Layer 1 and perform associated bookkeeping. In practice this will typically be the largest component of cost on an Ethereum-based Nitro chain. There are two main challenges in pricing these re- sources. First, it is not obvious how to apportion the costs of a batch among the transactions that comprise it. The posted data is compressed using a general-purpose com- pression algorithm 1 whose effectiveness depends on patterns shared in common across the transactions in a batch. Ideally we would charge less for transactions that contribute more to the compressibility of the batch, but there is no obvious and efficient way to determine how much a particular transaction contributed to overall com- pressibility.",
      "Instead, we will approximate, as described below.5 The second challenge is that the L1 fee assessed to a transaction must be known to ArbOS when the transac- tion is sequencedto use information that is available only later would violate the determinism property of the State Transition Function. But at the time a transaction is sequenced, the cost of the batch eventual batch in which it will be posted is not known. The eventual cost will de- pend on the L1 basefee at the future time when the batch is posted, and on the remaining contents of the batch (which affect the size and compressibility of the batch), but neither is known when the transaction is sequenced. So we cannot hope to charge a transaction for its actual L1 posting costs, because they are not yet known when the charge must be assessed.",
      "Nitro addresses these challenges by determining two things: (1) for each transaction, the estimated relative footprint of that transaction, measured in data units, and (2) at each point in time, a fee per data unit. 5AnyTrust chains raise additional issues, which are discussed in Section 7. Apportioning Costs Among Transactions To appor- tion cost among transactions, we approximate the com- pressibility of each transaction by applying the Brotli compressor, at its lowest compression  cheapest compu- tation level, to each transaction, and multiplying the size of the result by 16.6 We use Brotli on its fastest setting in order to reduce the computational load, because this computation is done inside the State Transition Function and is essentially an overhead cost for the chain. The size of this compressed data is an approximation to the size of the same transaction if compressed with the more aggressive compressor used to build Sequencer batches.",
      "This in turn is a rough approximation to the transactions contribution to the compressibility of the entire batch. More accurate approximations are possible, but we do not know of a better approximation that is fast enough. Determining Cost Per Data Unit One might think, naively, that the cost per data unit should just be equal to the L1 basefee, because that is what the Sequencer pays to post data. But this is not a viable approach, for at least two reasons. First, ArbOS has no way of directly measuring the L1 basefee, and we do not trust the Se- quencer to report the L1 basefee, because the Sequencer receives more payment if the basefee is higher. Second, because the number of units charged to a transaction is only an approximation to its overall data footprint, the total number of units charged is not directly proportional to the Sequencers costs.",
      "Data is priced using an adaptive algorithm that is de- signed to serve two main goals: to minimize the long- run difference between data fees collected and the Se- quencers data costs7, and secondarily to avoid sudden fluctuations in the data price. To do this, the pricer tracks:  an amount owed to the Sequencer,  a reimbursement fund, which receives all of the funds charged to transactions for L1 fees,  a count of recent data units, to which the number of data units in each transaction is added, and  the current L1 data unit price, in wei. The pricer varies the L1 data unit price adaptively, based on this data. 6We multiply by 16 because Ethereum charges 16 gas per byte for most data, so the number of data units is measured on a scale similar to the gas cost of data on Ethereum. 7In practice it is convenient to allow data to be posted by batch posters other than the Sequencer, and to direct reimbursement for a batch to the batch poster who posted it.",
      "The deployed system supports this, but in the main text we will assume that the Sequencer is the only batch poster, to simplify the exposition. When the Sequencer posts a batch to the L1 inbox, this causes the L1 inbox to insert a batch posting re- port transaction into the chains delayed inbox. After a delay, this transaction will be processed by the pricer, as follows. 1. The pricer computes the cost of posting the reported batch, and adds that amount to the amount owed to the Sequencer. 2. The pricer computes a number of data units as- signed to this update, as Uupd  U TupdTprev TTprev , where U is the count of recent data units, T is the current time, Tupd is the time when the update occurred, and Tprev is the time when the previous update occurred. Uupd is subtracted from U. 3. The pricer pays the Sequencer, from the reimburse- ment fund, the minimum of what the Sequencer is owed and the balance of the reimbursement fund.",
      "The amount paid is deducted from the reimburse- ment fund and from the amount owed to the Se- quencer. 4. The pricer computes the current surplus S, which is the reimbursement fund balance, minus the amount owed to the Sequencer. (The surplus may be nega- tive.) It then computes the derivative of the surplus as D  SSprev Uupd . 5. The pricer computes the derivative goal as D  E , where E is an equilibration constant. This is the derivative that must hold on average in order for the surplus to reach zero after E more data units are processed. 6. The pricer computes a change in the price as P  (D D) Uupd \u03b1Uupd where \u03b1 is a smoothing parameter. 7. The pricer updates the price to P  max(0,Pprev  P). This algorithm should cause the Sequencers long- term reimbursements to be nearly equal to its long-term costs. We can also add a small per-unit reward, payable to an arbitrary address, to cover any other small pay- ments needed for infrastructure or operations.",
      "Compiling for execution versus proving One of the challenges in designing a practical rollup sys- tem is the tension between wanting the system to perform well in ordinary execution, versus being able to reliably prove the results of execution. Nitro resolves this by us- ing the same source code for both execution and proving, but compiling it to different targets for the two cases. When compiling the Nitro node software for execu- tion, the ordinary Go compiler is used, producing native code for the target architecture, which of course will be different for different node deployments. (The node soft- ware is distributed in source code form, and as a Docker image containing a compiled binary.) Separately, the portion of the code that is the State Transition Function is compiled by the Go compiler to WebAssembly (wasm), which is a typed, portable ma- chine code format. The wasm code then goes through a simple transformation into a format we call WAVM, which is detailed below.",
      "If there is a dispute about the correct result of computing the STF, it is resolved by an interactive fraud proof protocol (described in Section 5) with reference to the WAVM code. WAVM The wasm format has many features that make it a good vehicle for fraud proofs  it is portable, structured, well- specified, designed for controlled execution of untrusted code, and has reasonably good tools and support  but it needs a few modifications to do the job completely. We have defined a slightly modified version of wasm, which we call WAVM. A simple transformation stage turns the wasm code from the compiler into WAVM code suitable for proving. WAVM differs from wasm in three main ways. First, WAVM removes some features of wasm that are not gen- erated by the Go compiler; the transformation phase ver- ifies that these features are not present. Second, WAVM restricts a few features of wasm.",
      "For example, WAVM does not contain floating-point instruc- tions, so the transformer replaces floating-point instruc- tions with calls to the Berkeley SoftFloat library 8.8 WAVM does not contain nested control flow, so the trans- former flattens control flow constructs, turning control flow instructions into jumps. Some wasm instructions take a variable amount of time to execute, which we avoid in WAVM by transforming them into constructs using fixed cost instructions. These transformations sim- plify proving. Third, WAVM adds a few opcodes to enable interac- tion with the blockchain environment. For example, new instructions allow the WAVM code to read and write the chains global state, to get the next message from the chains inbox, or to signal a successful end to executing the State Transition Function. 8We use software floating-point to reduce the risk of floating-point incompatibilities between architectures.",
      "No floating-point is used by the core Nitro functions, but the Go runtime uses some floating-point. 4.1.1 The ReadPreImage Instruction The most interesting new instruction is ReadPreImage which takes as input a hash H and an offset I, and returns the word of data at offset I in the preimage of H (and the number of bytes returned, which is zero if I is at or after the end of the preimage). Of course, it is not feasible in general to produce a preimage from an arbitrary hash. For safety, the ReadPreImage instruction can only be used in a context where the preimage is publicly known9, and where the size of the preimage is known to be less than a fixed upper bound of about 110 kbytes. As an example, the state of a Nitro chain is maintained in Ethereums state tree format, which is organized as a Merkle tree. Nodes of the tree are stored in a database, indexed by the Merkle hash of the node.",
      "In Nitro, the state tree is kept outside of the STFs storage, with the STF only knowing the root hash of the tree. Given the hash of a tree node, the STF can recover the tree nodes contents by using ReadPreImage, relying on the fact that the full contents of the tree are publicly known and that nodes in the Ethereum state tree will always be smaller than the upper bound on preimage size. In this manner, the STF is able to arbitrarily read and write to the state tree, despite only storing its root hash. The only other use of ReadPreImage is to fetch the contents of recent L2 block headers, given the header hash. This is safe because the block headers are publicly known and have bounded size. This hash oracle trick of storing the Merkle hash of a data structure, and relying on protocol participants to store the full structure and thereby support fetch-by-hash of the contents, originated in the original Arbitrum de- sign 9.",
      "WAVM Modules WAVM also allows the virtual machine to compose mul- tiple wasm binaries, called modules. Each module main- tains its own code, globals, and memory. Modules can call other modules via the CrossModuleCall WAVM instruction, and the callee can read and write to the callers memory in order to pass data between them. This allows the bootloader written in Rust, the State Transi- tion Function written in Go, and various libraries written in C to all run in the same WAVM machine. Without the module system, Gos memory management would interfere with Cs, but the module system allows them to maintain their own separate memories. 9In this context, publicly known information is information that can be derived or recovered efficiently by any honest party, assuming that the full history of the L1 Ethereum chain is available. For con- venience, a hash preimage can also be supplied by a third party such as a public server, and the correctness of the supplied value is easily verified.",
      "One-Step Proofs The WAVM instruction set is designed so that it is pos- sible to verify a one-step proof covering execution of a single WAVM instruction. Given the hash of a before state, the hash of an after state, and a bounded-size wit- ness, an Ethereum contract can verify that executing a single instruction from a state with the before hash will yield a state with the after hash. For proving purposes, the hash of a WAVM state is computed as a certain Merkle hash over the state of the WAVMwasm virtual machine, as described in more de- tail in Section 6.1.3. Optimistic Rollup Protocol The rollup protocol is Nitros method for confirming L2 chain states and associated data on the L1 Ethereum chain. This is useful for contracts on the L1 chain, and for parties who dont want to bother interacting with the L2 chain.",
      "But L2 users typically wont wait for L1 confirmationinstead they will rely on the determinis- tic State Transition Function which allows transaction results to be derived from the recorded transaction se- quence. The rollup protocol produces a chain of Rollup Blocks (RBlocks), which are not the same as L2 blocks. In brief, an RBlock typically encapsulates a sequence of L2 blocks, so that RBlocks are much less numerous than L2 blocks. RBlock boundaries need not be, and usually are not, aligned with the boundaries of Sequencer batches. An RBlock includes:  an L2 block number,  a header hash for the L2 block with that number,  the number of incoming messages10 consumed by the chain as of that L2 block,  a digest of the outbox messages output by the chain in that L2 block and earlier,  a pointer to a predecessor RBlock, and  additional bookkeeping information as needed to track the RBlocks state in the protocol described below.",
      "Initially an RBlock just represents a claim by some party that the RBlocks data is correct. Eventually every such claim will either be confirmed by the protocol, or re- jected and then pruned off of the RBlock chain. The set of confirmed RBlocks will form a single chain starting 10Here messages refers to individual transactions, or similar com- munications to the L2 chain, not to Sequencer batches. with the genesis RBlock, and growing over time. In gen- eral, the RBlock chain will consist of a single chain of confirmed blocks, possibly followed by a tree of uncon- firmed RBlocks. Each RBlock is said to be valid if either (a) the RBlock has been confirmed, or (b) all of the following are true:  the RBlocks L2 block number, header hash, num- ber of incoming messages, and digest of message output all represent correct execution of the chain,  any siblings of the RBlock that are older (i.e., were created earlier) are invalid, and  the predecessor RBlock is valid.",
      "By definition, the set of valid RBlocks will form a sin- gle chain, which has the set of confirmed RBlocks as a prefix. A party can stake on a particular RBlock, representing the partys assertion that that RBlock is valid. Because validity implies the validity of the predecessor, the party is also asserting that the predecessor of that RBlock, and the chain of predecessors back to the genesis RBlock, are all valid. The Common Case Parties will be aware that for reasons described below, staking on an invalid RBlock will likely lead to loss of the stake, so if all parties follow their incentives, only valid RBlocks will be created. Those valid RBlocks will form a single chain that extends the chain of confirmed RBlocks. If an RBlock B is confirmed, and B has a single child, and that child is valid, and the time since the child was posted is greater than a defined challenge period C, then the child can be confirmed.",
      "It follows that in the common case where parties follow their incentives as de- scribed above, if an RBlock is posted at time T, it will be confirmed at T C. Challenges If two parties do stake on separate successors of the same RBlock, those parties can be put into a challenge. The party staked on the older successor will be defending the feasibility of the L2 block number in the older successor, and the correctness of the header hash, incoming mes- sage count, and output digest associated with that block number in the older successor. The other party will be trying to establish that one of those items is incorrect. The challenge sub-protocol is described in detail be- low. Within the overall rollup protocol, its job is to iden- tify one of the two contending parties as having made a false claim (either in its staking on one of the two RBlocks, or at some point in the challenge sub-protocol). The losing party has its stake removed from all RBlocks.",
      "Half of the losers stake is given to the winner, and the other half is added to a public goods fund.11 The challenge sub-protocol guarantees that a party whose initial claim is valid can always win the chal- lenge by making a valid claim at every stage of the chal- lenge.12 It follows that an honest party (i.e., one who always makes valid claims) will win every challenge. Because the honest party will eventually engage in chal- lenges against every party who disagrees with it, the hon- est party will eventually eliminate all disagreeing parties, and the overall protocol can then make progress. The Challenge Sub-Protocol We describe the challenge protocol in two stages. First, we will describe a simplified version of the protocol that is correct but less efficient. Then, we will describe en- hancements to improve efficiency. To simplify the expo- sition, we will ignore some corner cases that are handled by the real protocol.",
      "The challenge protocol can be viewed as a game be- tween two parties, Alice and Bob, with an Ethereum con- tract acting as a referee who checks the players moves for validity and keeps track of the game state. The game includes a chess clock timer for each player. Each players timer is initially set equal to the challenge period. When it is a players turn to move, that players clock is ticking down. When a player makes a valid move, its timer is paused and its opponents timer is resumed. If a players timer reaches zero, that player forfeits the challenge. Basic Challenge Protocol The basic challenge protocol operates in three phases. First, a dispute over block results is repeatedly bisected, reducing it to a dispute over the creation of a single block.",
      "Second, the single-block dispute is converted into a dis- pute over some number of steps of wavm computation to generate that block, and that dispute is repeatedly bi- sected, reducing it to a dispute over execution of a single 11The public goods fund is used to benefit the entire user community, independent of the interests of the challenge participants, so we can assume that if the stake amount is S, then the challenge has a negative- sum outcome for the participants of S 12If both parties in a challenge make false claims, the protocol doesnt care who wins the challenge. One party will be identified as a liar, and the other party will survive the challenge despite its lies. However, a party staked on an invalid RBlock will eventually get into a challenge against an honest party, which it will lose. The correctness argument for the overall protocol does not assume that the winner of a challenge is honest, it only assumes that there exists an honest party. wavm instruction.",
      "Third, Alice must produce a one-step proof to prove her claim about execution of that single wavm instruction. If Alice can produce a valid one-step proof at the end of this process, Alice wins the challenge; otherwise Bob wins. 6.1.1 Phase 1: Bisecting Over Blocks This phase happens in a sequence of rounds. At the be- ginning of each round, Alice and Bob agree on a start state SB at some block number B, and they disagree on the end state SNB at block number B  N, for some N  1. Alice is now required to claim what the state SM is at the midpoint block M  BN 2 . Next, Bob must say whether he agrees or disagrees with Alices claimed midpoint state SM. Now there are two cases: 1. If Bob agrees with Alices midpoint state, the pro- tocol has identified a smaller dispute: Alice and Bob agree on SM but disagree about SBN  SMN, where N  N N 2 . 2.",
      "If Bob disagrees with Alices midpoint state, the protocol has identified a smaller dispute: Alice and Bob agree on SB but disagree about SBN, where N  N 2 . In both cases the size of the dispute has been cut roughly in half. The same procedure is repeated, cutting repeat- edly in half, until N  1. This requires at most log2 N rounds. 6.1.2 Phase 2: Bisecting Over Instructions At the beginning of this phase, Alice and Bob disagree about a single Nitro blockthey agree about the blocks and state before that block, but disagree about the con- tents of the block or the state after the block or both. This means they are disagreeing about the result of a sin- gle invocation of the State Transition Function. Alice must say how many wavm instructions are ex- ecuted by that invocation. Suppose she says there were K instructions.",
      "Alice and Bob are now in disagreement about the result of executing K instructions: they agree about the initial state but disagree about the state after the execution of K instructions. The protocol now mimics the phase 1 bisection pro- tocol, except that now the bisection is over instructions rather than blocks, and the states are states of the wavm virtual machine as it is executing the State Transition Function. After at most log2 Krounds of this bisection, Alice and Bob will have a dispute over the execution of a single wavm instruction. 6.1.3 Phase 3: One-step Proof At the beginning of this phase, Alice and Bob agree on a hash of the wavm VM state, but disagree about the hash of the wavm VM state after executing one more wavm instruction. The state hashes are computed by organizing the VM state into a tree, and computing the Merkle hash of the tree.",
      "Alice must call a one-step proof verification contract on Ethereum, passing it a witness that causes the contract to accept her claim about the single step of execution. The verification contract is written so that (assuming that the preimage of the before hash is known) it is feasible to find a successful witness if and only if execution of a single instruction can take the VM from a state with the before hash to a state with the after hash. In our implementation, the witness consists of a par- tial expansion of the Merkle tree representing the before state, and the verifier uses the partially expanded state tree to read the next instruction, emulate the instructions execution, compute the Merkle root hash of the resulting state, and compare this to the after state hash. The one-step verification contract is written, and the wavm instruction set is customized, so that it is always possible to verify a valid witness using a feasible amount of Ethereum gas.",
      "If Alice produces a valid one-step proof, Alice wins the challenge. Otherwise, Bob wins the challenge. Efficiency Improvements In the basic protocol, the bisection phase of a dispute occurs in alternating steps: the asserter bisects their as- sertion, then the challenger chooses one side of the bi- section to challenge, then the asserter bisects, then the challenger asserts, and so on. Each two-step cycle cuts the number of instructions in the dispute in half. A prac- tical implementation does d-way dissection rather than binary bisection, but the principle is the same. We can cut the number of steps by another factor of two, by requiring a party who responds to their oppo- nents dissection to not only identify which of the d seg- ments it is challenging, but to also offer a dissection of that segment into d subsegments, ending in a different state than asserted by the other party.",
      "These two steps together reduce the number of steps from roughly 2log2(NK) to roughly logd(NK), an im- provement by a factor of 2log2 d. 6.2.1 Choosing d An implementation of this protocol must choose the dis- section degree d. The overall cost of dispute resolution is the number of rounds logd(NK), multiplied by the cost per round, which is proportional to \u03b1 d for some \u03b1, be- cause an on-chain transaction has a cost that is a constant plus a term proportional to the amount of data posted in the transaction. In addition, each step of the dispute res- olution protocol may impose a constant amount of delay on execution of the contract; we absorb the total cost of this delay into the constant term \u03b1. The optimal value of d is then the value that minimizes the total cost (\u03b1d)log(NK) logd .",
      "We find the optimum by dif- ferentiating the cost with respect to d and setting the re- sult to zero, with the result that cost is minimized for the value of d satisfying d(ln(d)1)  \u03b1, independent of N and K. Given a specific value of \u03b1, the optimal d can be found numerically. Correctness To demonstrate that the protocol is still correct, we must show that a truthful party can always win the dispute, whether that party is an asserter or a challenger. First we show that a truthful asserter can win the dis- pute. If Alice makes a truthful assertion, and Bob chal- lenges it, Bob will have to propose an alternative asser- tion, which will necessarily be false because it must dif- fer from Alices truthful assertion. When Bob d-sects his false assertion, at least one of the resulting assertions will be false. Alice can challenge a false assertion, offering as an alternative a true assertion, and so on. At each stage Alice can make true assertions, thereby forcing Bob to make false assertions.",
      "Second we show that a truthful challenger can win the dispute. If Alice initially makes a false assertion, the truthful challenger Bob can offer a true assertion as his alternative, d-secting it into smaller true assertions. Al- ice will have to challenge one of those true assertions, offering an alternative that is necessarily false. This al- lows Bob to respond again with a true assertion, and so on. At each stage Bob can make true assertions, thereby forcing Alice to make false assertions. It follows that a truthful party can always win a dis- pute, and a lying party can always be forced to lose. AnyTrust: Nitro with External Data Availability This section describes AnyTrust, a variant of Nitro that lowers costs by accepting a mild trust assumption. AnyTrust support is included in the Nitro code base, with the AnyTrust feature enabled or disabled by a configura- tion switch.",
      "Correctness of the Arbitrum protocol requires that all Arbitrum nodes have access to the data of every L2 transaction in the Arbitrum chains inbox. As described above, standard Nitro provides data access by posting the data (in batched, compressed form) on L1 Ethereum as calldata. The Ethereum gas to pay for this is the largest component of cost in Nitro. AnyTrust relies instead on an external Data Availabil- ity Committee (hereafter, the Committee) to store data and provide the data on demand. The Committee has N members, of which AnyTrust assumes at least two are honest. This means that if N 1 Committee members promise to provide access to some data, at least one of the promising parties must be honest, ensuring that the data will be available so that the overall Arbitrum proto- col can function correctly. In AnyTrust, the Sequencer avoids posting the data of its batches on the L1 chain.",
      "Instead, for each batch it posts a Data Availability Certificate, containing the hash of the data, which proves that batch data with that hash is available from the Committee, assuming at least two Committee members are honest. Keysets A Keyset specifies the public keys of Committee mem- bers and the number of signatures required for a Data Availability Certificate to be valid. Keysets make Com- mittee membership changes possible and provide Com- mittee members the ability to change their keys. A Keyset contains  the number of Committee members, and  for each Committee member, a BLS public key, and  the number of Committee signatures required. Keysets are identified by their hashes. An L1 KeysetManager contract maintains a list of cur- rently valid Keysets. The L2 chains Owner can add or remove Keysets from this list. When a Keyset be- comes valid, the KeysetManager contract emits an L1 Ethereum event containing the Keysets hash and full contents.",
      "This allows the contents to be recovered later by anyone, given only the Keyset hash. Although the API does not limit the number of Keysets that can be valid at the same time, normally only one Keyset will be valid. Data Availability Certificates A central concept in AnyTrust is the Data Availability Certificate (hereafter, a DACert). A DACert contains:  the hash of a data block, and  an expiration time, and Figure 3: Processing of transactions under AnyTrust. Rather than posting data batches to the L1 chain, the sequencer sends batches to the Data Availability Committee, and posts the resulting Data Availability Certificate to the L1 chain instead of the full data. proof that a sufficient number of Committee mem- bers have signed the (hash, expiration time) pair, consisting of  the hash of the Keyset used in signing, and  a bitmap saying which Committee members signed, and  a BLS aggregated signature 4 (over the BLS12-381 curve 3) proving that those par- ties signed.",
      "Because of the 2-of-N trust assumption, a DACert consti- tutes proof that the blocks data (i.e., the preimage of the hash in the DACert) will be available from at least one honest Committee member, at least until the expiration time. In ordinary (non-AnyTrust) Nitro, the Arbitrum se- quencer posts data blocks on the L1 chain as calldata. The hashes of the data blocks are committed by the L1 Inbox contract, allowing the data to be reliably read by L2 code. AnyTrust gives the sequencer two ways to post a data block on L1: it can post the full data as above, or it can post a DACert proving availability of the data. The L1 inbox contract will reject any DACert that uses an invalid Keyset; the other aspects of DACert validity are checked by L2 code. The L2 code that reads data from the inbox reads a full-data block as in ordinary Nitro.",
      "If it sees a DACert instead, it checks the validity of the DACert, with ref- erence to the Keyset specified by the DACert (which is known to have been valid at the time it was posted, be- cause the L1 Inbox verified that). The L2 code verifies that:  the number of signers is at least the number required by the Keyset, and  the aggregated signature is valid for the claimed signers, and  the expiration time is at least two weeks after the current L2 timestamp. If the DACert is invalid, the L2 code discards the DACert and behaves as if it had received an empty batch. If the DACert is valid, the L2 code reads the data block, which is guaranteed to be available because the DACert is valid. Data Availability Servers Committee members run Data Availability Server (DAS) software. The DAS exposes two APIs:  The Sequencer API, which is meant to be called only by the Arbitrum chains Sequencer, is a JSON- RPC interface allowing the Sequencer to submit data blocks to the DAS for storage.",
      "Deployments will typically block access to this API from callers other than the Sequencer. The REST API, which is meant to be available to the world, is a RESTful HTTP(S) based protocol that allows data blocks to be fetched by hash. This API is fully cacheable, and deployments may use a caching proxy or CDN to increase scale and protect against DoS attacks. Only Committee members have reason to support the Se- quencer API. We expect others to run the REST API, typ- ically by mirroring other REST API servers, and that is helpful. The DAS software, depending on configuration op- tions, can store its data in local files, or in a BadgerDB 6 database, or on Amazon S3, or redundantly across mul- tiple backing stores. The software also supports optional caching in memory (using Bigcache 2) or in a Redis 5 instance.",
      "Sequencer-Committee Interaction When the Nitro sequencer produces a data batch that it wants to post using the Committee, it sends the batchs data, along with an expiration time (normally three weeks in the future) via RPC to all Committee members in parallel. Each Committee member stores the data in its backing store, indexed by the datas hash. Then the member signs the (hash, expiration time) pair using its BLS key, and returns the signature to the sequencer. Once the Sequencer has collected enough signatures, it can aggregate the signatures and create a valid DACert for the (hash, expiration time) pair. The Sequencer then posts that DACert to the L1 inbox contract, making it available to the AnyTrust chain software at L2. If the Sequencer fails to collect enough signatures within a reasonable time, it can abandon the attempt to use the Committee, and fall back to rollup by posting the full data directly to the L1 chain, as it would do in a non-AnyTrust chain.",
      "The L2 software can understand both data posting formats (via DACert or via full data) and will handle each one correctly. AnyTrust and L1 Pricing By substantially reducing the amount of L1 data required for the same number of transactions, AnyTrust leads to much lower prices for transaction data. The same L1 pricing algorithm (described in Section 3.3.2) is used as for a normal Nitro chain, however under AnyTrust the Sequencers data spending is much lower (paying for posting of data availability certificates rather than full data), so the pricing algorithm will assess a much lower price per data unit on user transactions. If the Se- quencer on an AnyTrust chain does fall back to posting full data on Layer 1, it will then report higher spending and the data price will rise to compensate. No changes are needed in the pricing algorithm; only the outcome will differ.",
      "Conclusion By using the design described above, Arbitrum Nitro achieves high throughput, with trustless guarantees of safety and liveness, in a system achievable and deployed today. The current Nitro code is available at https: github.comoffchainlabsnitro. We will con- tinue to evolve Nitro to increase performance and reduce cost. 1 Alakuijala, J., Farruggia, A., Ferragina, P., Kliuch- nikov, E., Obryk, R., Szabadka, Z., Vandevenne, L.: Brotli: A general-purpose data compressor. ACM Transactions on Information Systems (TOIS) 37(1), 130 (2018) 2 Allegro Tech: BigCache https:github.com allegrobigcache 3 Barreto, P.S., Kim, H.Y., Lynn, B., Scott, M.: Ef- ficient algorithms for pairing-based cryptosystems. In: Annual international cryptology conference. pp. 354369. Springer (2002) 4 Boneh, D., Lynn, B., Shacham, H.: Short signa- tures from the weil pairing. In: International con- ference on the theory and application of cryptology and information security. pp. 514532.",
      "Springer (2001) 5 Carlson, J.: Redis in action. Simon and Schuster (2013) 6 Dgraph: BadgerDB dgraph-iobadger 7 Donovan, A.A., Kernighan, B.W.: The Go pro- gramming language. Addison-Wesley Professional (2015) 8 Hauser, J.R.: Berkeley softfloat release 3e (2018) 9 Kalodner, H., Goldfeder, S., Chen, X., Weinberg, S.M., Felten, E.W.: Arbitrum: Scalable, private smart contracts. In: 27th USENIX Security Sym- posium. pp. 13531370 (2018) 10 Kelkar, M., Deb, S., Long, S., Juels, A., Kannan, S.: Themis: Fast, strong order-fairness in byzantine consensus. Cryptology ePrint Archive (2021) 11 Kelkar, M., Zhang, F., Goldfeder, S., Juels, A.: Order-fairness for byzantine consensus. In: Annual International Cryptology Conference. pp. 451480. Springer (2020) 12 OpenZeppelin project: OpenZeppelin Con- tracts, openzeppelin-contracts 13 WebAssembly Working Group: WebAssembly 14 Wood, G.: Ethereum: A secure decentralised gen- eralised transaction ledger. Ethereum Project Yel- low Paper 151, 132 (2014)"
    ],
    "word_count": 11116,
    "page_count": 16
  },
  "ATOM": {
    "chunks": [
      "Liberty House Club A Parallel Binance Chain to Enable Smart Contracts _NOTE: This document is under development.",
      "Please check regularly for updates!_  Table of Contents - Motivation(motivation) - Design Principles(design-principles) - Consensus and Validator Quorum(consensus-and-validator-quorum)  Proof of Staked Authority(proof-of-staked-authority)  Validator Quorum(validator-quorum)  Security and Finality(security-and-finality)  Reward(reward) - Token Economy(token-economy)  Native Token(native-token)  Other Tokens(other-tokens) - Cross-Chain Transfer and Communication(cross-chain-transfer-and-communication)  Cross-Chain Transfer(cross-chain-transfer)  BC to BSC Architecture(bc-to-bsc-architecture)  BSC to BC Architecture(bsc-to-bc-architecture)  Timeout and Error Handling(timeout-and-error-handling)  Cross-Chain User Experience(cross-chain-user-experience)  Cross-Chain Contract Event(cross-chain-contract-event) - Staking and Governance(staking-and-governance)  Staking on BC(staking-on-bc)  Rewarding(rewarding)  Slashing(slashing) - Relayers(relayers)  BSC Relayers(bsc-relayers)  Oracle Relayers(oracle-relayers) - Outlook(outlook)  Motivation After its mainnet community launch(https:www.binance.comenblog327334696200323072Binance-DEX-Launches-on-Binance-Chain-Invites-Further-Community-Development) in April 2019, Binance Chain(https:www.binance.org) has exhibited its high speed and large throughput design.",
      "Binance Chains primary focus, its native decentralized application(https:en.wikipedia.orgwikiDecentralized_application) (dApp) Binance DEX(https:www.binance.orgtrade), has demonstrated its low-latency matching with large capacity headroom by handling millions of trading volume in a short time. Flexibility and usability are often in an inverse relationship with performance. The concentration on providing a convenient digital asset issuing and trading venue also brings limitations. Binance Chain's most requested feature is the programmable extendibility, or simply the Smart Contract(https:en.wikipedia.orgwikiSmart_contract) and Virtual Machine functions. Digital asset issuers and owners struggle to add new decentralized features for their assets or introduce any sort of community governance and activities. Despite this high demand for adding the Smart Contract feature onto Binance Chain, it is a hard decision to make.",
      "The execution of a Smart Contract may slow down the exchange function and add non-deterministic factors to trading. If that compromise could be tolerated, it might be a straightforward idea to introduce a new Virtual Machine specification based on Tendermint(https:tendermint.comcore), based on the current underlying consensus protocol and major RPC(https:docs.binance.orgapi-referencenode-rpc.html) implementation of Binance Chain. But all these will increase the learning requirements for all existing dApp communities, and will not be very welcomed. We propose a parallel blockchain of the current Binance Chain to retain the high performance of the native DEX blockchain and to support a friendly Smart Contract function at the same time. Design Principles After the creation of the parallel blockchain into the Binance Chain ecosystem, two blockchains will run side by side to provide different services.",
      "The new parallel chain will be called Binance Smart Chain (short as BSC for the below sections), while the existing mainnet remains named Binance Chain (short as BC for the below sections). Here are the design principles of BSC: 1. Standalone Blockchain: technically, BSC is a standalone blockchain, instead of a layer-2 solution. Most BSC fundamental technical and business functions should be self-contained so that it can run well even if the BC stopped for a short period. 2. Ethereum Compatibility: The first practical and widely-used Smart Contract platform is Ethereum. To take advantage of the relatively mature applications and community, BSC chooses to be compatible with the existing Ethereum mainnet. This means most of the dApps, ecosystem components, and toolings will work with BSC and require zero or minimum changes; BSC node will require similar (or a bit higher) hardware specification and skills to run and operate.",
      "The implementation should leave room for BSC to catch up with further Ethereum upgrades. 3. Staking Involved Consensus and Governance: Staking-based consensus is more environmentally friendly and leaves more flexible option to the community governance. Expectedly, this consensus should enable better network performance over proof-of-work(https:en.wikipedia.orgwikiProof_of_work) blockchain system, i.e., faster blocking time and higher transaction capacity. 4. Native Cross-Chain Communication: both BC and BSC will be implemented with native support for cross-chain communication among the two blockchains. The communication protocol should be bi-directional, decentralized, and trustless. It will concentrate on moving digital assets between BC and BSC, i.e., BEP2(https:github.combinance-chainBEPsblobmasterBEP2.md) tokens, and eventually, other BEP tokens introduced later.",
      "The protocol should care for the minimum of other items stored in the state of the blockchains, with only a few exceptions. Consensus and Validator Quorum Based on the above design principles, the consensus protocol of BSC is to fulfill the following goals: 1. Blocking time should be shorter than Ethereum network, e.g. 5 seconds or even shorter. 2. It requires limited time to confirm the finality of transactions, e.g. around 1-min level or shorter. 3. There is no inflation of native token: BNB, the block reward is collected from transaction fees, and it will be paid in BNB. 4. It is compatible with Ethereum system as much as possible. 5. It allows modern proof-of-stake(https:en.wikipedia.orgwikiProof_of_stake) blockchain network governance. Proof of Staked Authority Although Proof-of-Work (PoW) has been recognized as a practical mechanism to implement a decentralized network, it is not friendly to the environment and also requires a large size of participants to maintain the security.",
      "Ethereum and some other blockchain networks, such as MATIC Bor(https:github.commaticnetworkbor), TOMOChain(https:tomochain.com), GoChain(https:gochain.io), xDAI(https:xdai.io), do use Proof-of-Authority(PoA)(https:en.wikipedia.orgwikiProof_of_authority) or its variants in different scenarios, including both testnet and mainnet. PoA provides some defense to 51 attack, with improved efficiency and tolerance to certain levels of Byzantine players (malicious or hacked). It serves as an easy choice to pick as the fundamentals. Meanwhile, the PoA protocol is most criticized for being not as decentralized as PoW, as the validators, i.e. the nodes that take turns to produce blocks, have all the authorities and are prone to corruption and security attacks. Other blockchains, such as EOS and Lisk both, introduce different types of Delegated Proof of Stake (DPoS)(https:en.bitcoinwiki.orgwikiDPoS) to allow the token holders to vote and elect the validator set.",
      "It increases the decentralization and favors community governance. BSC here proposes to combine DPoS and PoA for consensus, so that: 1. Blocks are produced by a limited set of validators 2. Validators take turns to produce blocks in a PoA manner, similar to Ethereums Clique(https:eips.ethereum.orgEIPSeip-225) consensus design 3. Validator set are elected in and out based on a staking based governance  Validator Quorum In the genesis stage, a few trusted nodes will run as the initial Validator Set. After the blocking starts, anyone can compete to join as candidates to elect as a validator. The staking status decides the top 21 most staked nodes to be the next validator set, and such an election will repeat every 24 hours. BNB is the token used to stake for BSC.",
      "In order to remain as compatible as Ethereum and upgradeable to future consensus protocols to be developed, BSC chooses to rely on the BC for staking management (Please refer to the below Staking and Governance(staking-and-governance) section). There is a dedicated staking module for BSC on BC. It will accept BSC staking from BNB holders and calculate the highest staked node set. Upon every UTC midnight, BC will issue a verifiable ValidatorSetUpdate cross-chain message to notify BSC to update its validator set. While producing further blocks, the existing BSC validators check whether there is a ValidatorSetUpdate message relayed onto BSC periodically. If there is, they will update the validator set after an epoch period, i.e. a predefined number of blocking time. For example, if BSC produces a block every 5 seconds, and the epoch period is 240 blocks, then the current validator set will check and update the validator set for the next epoch in 1200 seconds (20 minutes).",
      "Security and Finality Given there are more than \u00bdN1 validators are honest, PoA based networks usually work securely and properly. However, there are still cases where certain amount Byzantine validators may still manage to attack the network, e.g. through the Clone Attack(https:arxiv.orgpdf1902.10244.pdf). To secure as much as BC, BSC users are encouraged to wait until receiving blocks sealed by more than \u2154N1 different validators. In that way, the BSC can be trusted at a similar security level to BC and can tolerate less than \u2153N Byzantine validators. With 21 validators, if the block time is 5 seconds, the \u2154N1 different validator seals will need a time period of (\u2154211)5  75 seconds. Any critical applications for BSC may have to wait for \u2154N1 to ensure a relatively secure finality. However, besides such arrangement, BSC does introduce Slashing logic to penalize Byzantine validators for double signing or inavailability, which will be covered in the Staking and Governance section later.",
      "This Slashing logic will expose the malicious validators in a very short time and make the Clone Attack very hard or extremely non-beneficial to execute. With this enhancement, \u00bdN1 or even fewer blocks are enough as confirmation for most transactions. Reward All the BSC validators in the current validator set will be rewarded with transaction fees in BNB. As BNB is not an inflationary token, there will be no mining rewards as what Bitcoin and Ethereum network generate, and the gas fee is the major reward for validators. As BNB is also utility tokens with other use cases, delegators and validators will still enjoy other benefits of holding BNB. The reward for validators is the fees collected from transactions in each block. Validators can decide how much to give back to the delegators who stake their BNB to them, in order to attract more staking.",
      "Every validator will take turns to produce the blocks in the same probability (if they stick to 100 liveness), thus, in the long run, all the stable validators may get a similar size of the reward. Meanwhile, the stakes on each validator may be different, so this brings a counter-intuitive situation that more users trust and delegate to one validator, they potentially get less reward. So rational delegators will tend to delegate to the one with fewer stakes as long as the validator is still trustful (insecure validator may bring slashable risk). In the end, the stakes on all the validators will have less variation. This will actually prevent the stake concentration and winner wins forever problem seen on some other networks. Some parts of the gas fee will also be rewarded to relayers for Cross-Chain communication. Please refer to the Relayers(relayers) section below. Token Economy BC and BSC share the same token universe for BNB and BEP2 tokens. This defines: 1.",
      "The same token can circulate on both networks, and flow between them bi-directionally via a cross-chain communication mechanism. 2. The total circulation of the same token should be managed across the two networks, i.e. the total effective supply of a token should be the sum of the tokens total effective supply on both BSC and BC. 3. The tokens can be initially created on BSC in a similar format as ERC20 token standard, or on BC as a BEP2, then created on the other. There are native ways on both networks to link the two and secure the total supply of the token. Native Token BNB will run on BSC in the same way as ETH runs on Ethereum so that it remains as native token for both BSC and BC. This means, in addition to BNB is used to pay most of the fees on Binance Chain and Binance DEX, BNB will be also used to: 1. pay fees to deploy smart contracts on BSC 2. stake on selected BSC validators, and get corresponding rewards 3.",
      "perform cross-chain operations, such as transfer token assets across BC and BSC  Seed Fund Certain amounts of BNB will be burnt on BC and minted on BSC during its genesis stage. This amount is called Seed Fund to circulate on BSC after the first block, which will be dispatched to the initial BC-to-BSC Relayer(described in later sections) and initial validator set introduced at genesis. These BNBs are used to pay transaction fees in the early stage to transfer more BNB from BC onto BSC via the cross-chain mechanism.",
      "The BNB cross-chain transfer is discussed in a later section, but for BC to BSC transfer, it is generally to lock BNB on BC from the source address of the transfer to a system-controlled address and unlock the corresponding amount from special contract to the target address of the transfer on BSC, or reversely, when transferring from BSC to BC, it is to lock BNB from the source address on BSC into a special contract and release locked amount on BC from the system address to the target address. The logic is related to native code on BC and a series of smart contracts on BSC. Other Tokens BC supports BEP2 tokens and upcoming BEP8 tokens(https:github.combinance-chainBEPspull69), which are native assets transferrable and tradable (if listed) via fast transactions and sub-second finality.",
      "Meanwhile, as BSC is Ethereum compatible, it is natural to support ERC20 tokens on BSC, which here is called BEP2E (with the real name to be introduced by the future BEPs,it potentially covers BEP8 as well). BEP2E may be Enhanced by adding a few more methods to expose more information, such as token denomination, decimal precision definition and the owner address who can decide the Token Binding across the chains. BSC and BC work together to ensure that one token can circulate in both formats with confirmed total supply and be used in different use cases. Token Binding BEP2 tokens will be extended to host a new attribute to associate the token with a BSC BEP2E token contract, called Binder, and this process of association is called Token Binding. Token Binding can happen at any time after BEP2 and BEP2E are ready. The token owners of either BEP2 or BEP2E dont need to bother about the Binding, until before they really want to use the tokens on different scenarios.",
      "Issuers can either create BEP2 first or BEP2E first, and they can be bound at a later time. Of course, it is encouraged for all the issuers of BEP2 and BEP2E to set the Binding up early after the issuance. A typical procedure to bind the BEP2 and BEP2E will be like the below: 1. Ensure both the BEP2 token and the BEP2E token both exist on each blockchain, with the same total supply. BEP2E should have 3 more methods than typical ERC20 token standard:  symbol(): get token symbol  decimals(): get the number of the token decimal digits  owner(): get BEP2E contract owners address. This value should be initialized in the BEP2E contract constructor so that the further binding action can verify whether the action is from the BEP2E owner. 2. Decide the initial circulation on both blockchains. Suppose the total supply is S, and the expected initial circulating supply on BC is K, then the owner should lock S-K tokens to a system controlled address on BC. 3.",
      "Equivalently, K tokens is locked in the special contract on BSC, which handles major binding functions and is named as TokenHub. The issuer of the BEP2E token should lock the K amount of that token into TokenHub, resulting in S-K tokens to circulate on BSC. Thus the total circulation across 2 blockchains remains as S. 4. The issuer of BEP2 token sends the bind transaction on BC. Once the transaction is executed successfully after proper verification:  It transfers S-K tokens to a system-controlled address on BC. A cross-chain bind request package will be created, waiting for Relayers to relay. 5. BSC Relayers will relay the cross-chain bind request package into TokenHub on BSC, and the corresponding request and information will be stored into the contract. 6. The contract owner and only the owner can run a special method of TokenHub contract, ApproveBind, to verify the binding request to mark it as a success.",
      "It will confirm:  the token has not been bound;  the binding is for the proper symbol, with proper total supply and decimal information;  the proper lock are done on both networks; 10. Once the ApproveBind method has succeeded, TokenHub will mark the two tokens are bounded and share the same circulation on BSC, and the status will be propagated back to BC. After this final confirmation, the BEP2E contract address and decimals will be written onto the BEP2 token as a new attribute on BC, and the tokens can be transferred across the two blockchains bidirectionally. If the ApproveBind fails, the failure event will also be propagated back to BC to release the locked tokens, and the above steps can be re-tried later.",
      "Cross-Chain Transfer and Communication Cross-chain communication is the key foundation to allow the community to take advantage of the dual chain structure:  users are free to create any tokenization, financial products, and digital assets on BSC or BC as they wish  the items on BSC can be manually and programmingly traded and circulated in a stable, high throughput, lighting fast and friendly environment of BC  users can operate these in one UI and tooling ecosystem. Cross-Chain Transfer The cross-chain transfer is the key communication between the two blockchains. Essentially the logic is: 1. the transfer-out blockchain will lock the amount from source owner addresses into a system controlled addresscontracts; 2. the transfer-in blockchain will unlock the amount from the system controlled addresscontracts and send it to target addresses. The cross-chain transfer package message should allow the BSC Relayers and BC Oracle Relayers to verify: 1.",
      "Enough amount of token assets are removed from the source address and locked into a system controlled addressescontracts on the source blockchain. And this can be confirmed on the target blockchain. 2. Proper amounts of token assets are released from a system controlled addressescontracts and allocated into target addresses on the target blockchain. If this fails, it can be confirmed on source blockchain, so that the locked token can be released back (may deduct fees). 3. The sum of the total circulation of the token assets across the 2 blockchains are not changed after this transfer action completes, no matter if the transfer succeeds or not. !cross-chain(.assetscross-chain.png) The architecture of cross-chain communication is as in the above diagram. To accommodate the 2 heteroid systems, communication handling is different in each direction. BC to BSC Architecture BC is a Tendermint-based, instant finality blockchain.",
      "Validators with at least \u2154N1 of the total voting power will co-sign each block on the chain. So that it is practical to verify the block transactions and even the state value via Block Header and Merkle Proof verification. This has been researched and implemented as Light-Client Protocol, which are intensively discussed in the Ethereum(https:github.comethereumwikiwikiLight-client-protocol) community, studied and implemented for Cosmos inter-chain communication(https:github.comcosmosicsbloba4173c91560567bdb7cc9abee8e61256fc3725e9specics-007-tendermint-clientREADME.md). BC-to-BSC communication will be verified in an on-chain light client implemented via BSC Smart Contracts (some of them may be pre-compiled). After some transactions and state change happen on BC, if a transaction is defined to trigger cross-chain communication,the Cross-chain package message will be created and BSC Relayers will pass and submit them onto BSC as data into the \"build-in system contracts\".",
      "The build-in system contracts will verify the package and execute the transactions if it passes the verification. The verification will be guaranteed with the below design: 1. BC blocking status will be synced to the light client contracts on BSC from time to time, via block header and pre-commits, for the below information:  block and app hash of BC that are signed by validators  current validatorset, and validator set update 2. the key-value from the blockchain state will be verified based on the Merkle Proof and information from above 1. After confirming the key-value is accurate and trustful, the build-in system contracts will execute the actions corresponding to the cross-chain packages. Some examples of such packages that can be created for BC-to-BSC are: 1. Bind: bind the BEP2 tokens and BEP2E 2. Transfer: transfer tokens after binding, this means the circulation will decrease (be locked) from BC and appear in the target address balance on BSC 3.",
      "Error Handling: to handle any timeoutfailure event for BSC-to-BC communication 4. Validatorset update of BSC To ensure no duplication, proper message sequence and timely timeout, there is a Channel concept introduced on BC to manage any types of the communication. For relayers, please also refer to the below Relayers section. BSC to BC Architecture BSC uses Proof of Staked Authority consensus protocol, which has a chance to fork and requires confirmation of more blocks. One block only has the signature of one validator, so that it is not easy to rely on one block to verify data from BSC. To take full advantage of validator quorum of BC, an idea similar to many Bridge (https:github.compoanetworkpoa-bridge)or Oracle blockchains is adopted: 1. The cross-chain communication requests from BSC will be submitted and executed onto BSC as transactions. The execution of the transanction wil emit Events, and such events can be observed and packaged in certain Oracle onto BC.",
      "Instead of Block Headers, Hash and Merkle Proof, this type of Oracle package directly contains the cross-chain information for actions, such as sender, receiver and amount for transfer. 2. To ensure the security of the Oracle, the validators of BC will form anothe quorum of Oracle Relayers. Each validator of the BC should run a dedicated process as the Oracle Relayer. These Oracle Relayers will submit and vote for the cross-chain communication package, like Oracle, onto BC, using the same validator keys. Any package signed by more than \u2154N1 Oracle Relayers voting power is as secure as any block signed by \u2154N1 of the same quorum of validators voting power. By using the same validator quorum, it saves the light client code on BC and continuous block updates onto BC. Such Oracles also have Oracle IDs and types, to ensure sequencing and proper error handling. Timeout and Error Handling There are scenarios that the cross-chain communication fails.",
      "For example, the relayed package cannot be executed on BSC due to some coding bug in the contracts. Timeout and error handling logics are used in such scenarios. For the recognizable user and system errors or any expected exceptions, the two networks should heal themselves. For example, when BC to BSC transfer fails, BSC will issue a failure event and Oracle Relayers will execute a refund on BC; when BSC to BC transfer fails, BC will issue a refund package for Relayer to relay in order to unlock the fund. However, unexpected error or exception may still happen on any step of the cross-chain communication. In such a case, the Relayers and Oracle Relayers will discover that the corresponding cross-chain channel is stuck in a particular sequence. After a Timeout period, the Relayers and Oracle Relayers can request a SkipSequence transaction, the stuck sequence will be marked as Unexecutable.",
      "A corresponding alerts will be raised, and the community has to discuss how to handle this scenario, e.g. payback via the sponsor of the validators, or event clear the fund during next network upgrade. Cross-Chain User Experience Ideally, users expect to use two parallel chains in the same way as they use one single chain. It requires more aggregated transaction types to be added onto the cross-chain communication to enable this, which will add great complexity, tight coupling, and maintenance burden. Here BC and BSC only implement the basic operations to enable the value flow in the initial launch and leave most of the user experience work to client side UI, such as wallets. E.g. a great wallet may allow users to sell a token directly from BSC onto BCs DEX order book, in a secure way. Cross-Chain Contract Event Cross-Chain Contract Event (CCCE) is designed to allow a smart contract to trigger cross-chain transactions, directly through the contract code.",
      "This becomes possible based on: 1. Standard system contracts can be provided to serve operations callable by general smart contracts; 2. Standard events can be emitted by the standard contracts; 3. Oracle Relayers can capture the standard events, and trigger the corresponding cross-chain operations; 4. Dedicated, code-managed address (account) can be created on BC and accessed by the contracts on the BSC, here it is named as Contract Address on BC (CAoB). Several standard operations are implemented: 1. BSC to BC transfer: this is implemented in the same way as normal BSC to BC transfer, by only triggered via standard contract. The fund can be transferred to any addresses on BC, including the corresponding CAoB of the transfer originating contract. 2. Transfer on BC: this is implemented as a special cross-chain transfer, while the real transfer is from CAoB to any other address (even another CAoB). 3. BC to BSC transfer: this is implemented as two-pass cross-chain communication.",
      "The first is triggered by the BSC contract and propagated onto BC, and then in the second pass, BC will start a normal BC to BSC cross-chain transfer, from CAoB to contract address on BSC. A special note should be paid on that the BSC contract only increases balance upon any transfer coming in on the second pass, and the error handling in the second pass is the same as the normal BC to BSC transfer. 4. IOC (Immediate-Or-Cancel) Trade Out: the primary goal of transferring assets to BC is to trade. This event will instruct to trade a certain amount of an asset in CAoB into another asset as much as possible and transfer out all the results, i.e. the left the source and the traded target tokens of the trade, back to BSC. BC will handle such relayed events by sending an Immediate-Or-Cancel, i.e. IOC order onto the trading pairs, once the next matching finishes, the result will be relayed back to BSC, which can be in either one or two assets. 5.",
      "Auction Trade Out: Such event will instruct BC to send an auction order to trade a certain amount of an asset in CAoB into another asset as much as possible and transfer out all the results back to BSC at the end of the auction. Auction function is upcoming on BC. There are some details for the Trade Out: 1. both can have a limit price (absolute or relative) for the trade; 2. the end result will be written as cross-chain packages to relay back to BSC; 3. cross-chain communication fees may be charged from the asset transferred back to BSC; 4. BSC contract maintains a mirror of the balance and outstanding orders on CAoB. No matter what error happens during the Trade Out, the final status will be propagated back to the originating contract and clear its internal state. With the above features, it simply adds the cross-chain transfer and exchange functions with high liquidity onto all the smart contracts on BSC.",
      "It will greatly add the application scenarios on Smart Contract and dApps, and make 1 chain 1 chain  2 chains. Staking and Governance Proof of Staked Authority brings in decentralization and community involvement. Its core logic can be summarized as the below. You may see similar ideas from other networks, especially Cosmos and EOS. 1. Token holders, including the validators, can put their tokens bonded into the stake. Token holders can delegate their tokens onto any validator or validator candidate, to expect it can become an actual validator, and later they can choose a different validator or candidate to re-delegate their tokenssup1sup. 2. All validator candidates will be ranked by the number of bonded tokens on them, and the top ones will become the real validators. 3. Validators can share (part of) their blocking reward with their delegators. 4. Validators can suffer from Slashing, a punishment for their bad behaviors, such as double sign andor instability. 5.",
      "There is an unbonding period for validators and delegators so that the system makes sure the tokens remain bonded when bad behaviors are caught, the responsible will get slashed during this period. Staking on BC Ideally, such staking and reward logic should be built into the blockchain, and automatically executed as the blocking happens. Cosmos Hub, who shares the same Tendermint consensus and libraries with Binance Chain, works in this way. BC has been preparing to enable staking logic since the design days. On the other side, as BSC wants to remain compatible with Ethereum as much as possible, it is a great challenge and efforts to implement such logic on it. This is especially true when Ethereum itself may move into a different Proof of Stake consensus protocol in a short (or longer) time. In order to keep the compatibility and reuse the good foundation of BC, the staking logic of BSC is implemented on BC: 1.",
      "The staking token is BNB, as it is a native token on both blockchains anyway 2. The staking, i.e. token bond and delegation actions and records for BSC, happens on BC. 3. The BSC validator set is determined by its staking and delegation logic, via a staking module built on BC for BSC, and propagated every day UTC 00:00 from BC to BSC via Cross-Chain communication. 4. The reward distribution happens on BC around every day UTC 00:00. Rewarding Both the validator update and reward distribution happen every day around UTC 00:00. This is to save the cost of frequent staking updates and block reward distribution. This cost can be significant, as the blocking reward is collected on BSC and distributed on BC to BSC validators and delegators. (Please note BC blocking fees will remain rewarding to BC validators only.) A deliberate delay is introduced here to make sure the distribution is fair: 1.",
      "The blocking reward will not be sent to validator right away, instead, they will be distributed and accumulated on a contract; 2. Upon receiving the validator set update into BSC, it will trigger a few cross-chain transfers to transfer the reward to custody addresses on the corresponding validators. The custody addresses are owned by the system so that the reward cannot be spent until the promised distribution to delegators happens. 3. In order to make the synchronization simpler and allocate time to accommodate slashing, the reward for N day will be only distributed in N2 days. After the delegators get the reward, the left will be transferred to validators own reward addresses. Slashing Slashing is part of the on-chain governance, to ensure the malicious or negative behaviors are punished. BSC slash can be submitted by anyone. The transaction submission requires slash evidence and cost fees but also brings a larger reward when it is successful. So far there are two slashable cases.",
      "Double Sign It is quite a serious error and very likely deliberate offense when a validator signs more than one block with the same height and parent block. The reference protocol implementation should already have logic to prevent this, so only the malicious code can trigger this. When Double Sign happens, the validator should be removed from the Validator Set right away. Anyone can submit a slash request on BC with the evidence of Double Sign of BSC, which should contain the 2 block headers with the same height and parent block, sealed by the offending validator. Upon receiving the evidence, if the BC verifies it to be valid: 1. The validator will be removed from validator set by an instance BSC validator set update Cross-Chain update; 2. A predefined amount of BNB would be slashed from the self-delegated BNB of the validator; Both validator and its delegators will not receive the staking rewards. 3.",
      "Part of the slashed BNB will allocate to the submitters address, which is a reward and larger than the cost of submitting slash request transaction 4. The rest of the slashed BNB will allocate to the other validators custody addresses, and distributed to all delegators in the same way as blocking reward. Inavailability The liveness of BSC relies on everyone in the Proof of Staked Authority validator set can produce blocks timely when it is their turn. Validators can miss their turn due to any reason, especially problems in their hardware, software, configuration or network. This instability of the operation will hurt the performance and introduce more indeterministic into the system. There can be an internal smart contract responsible for recording the missed blocking metrics of each validator. Once the metrics are above the predefined threshold, the blocking reward for validator will not be relayed to BC for distribution but shared with other better validators.",
      "In such a way, the poorly-operating validator should be gradually voted out of the validator set as their delegators will receive less or none reward. If the metrics remain above another higher level of threshold, the validator will be dropped from the rotation, and this will be propagated back to BC, then a predefined amount of BNB would be slashed from the self-delegated BNB of the validator. Both validators and delegators will not receive their staking rewards. Governance Parameters There are many system parameters to control the behavior of the BSC, e.g. slash amount, cross-chain transfer fees. All these parameters will be determined by BSC Validator Set together through a proposal-vote process based on their staking. Such the process will be carried on BC, and the new parameter values will be picked up by corresponding system contracts via a cross-chain communication. Relayers Relayers are responsible to submit Cross-Chain Communication Packages between the two blockchains.",
      "Due to the heterogeneous parallel chain structure, two different types of Relayers are created. BSC Relayers Relayers for BC to BSC communication referred to as BSC Relayers, or just simply Relayers. Relayer is a standalone process that can be run by anyone, and anywhere, except that Relayers must register themselves onto BSC and deposit a certain refundable amount of BNB. Only relaying requests from the registered Relayers will be accepted by BSC. The package they relay will be verified by the on-chain light client on BSC. The successful relay needs to pass enough verification and costs gas fees on BSC, and thus there should be incentive reward to encourage the community to run Relayers. Incentives There are two major communication types: 1. Users triggered Operations, such as token bind or cross chain transfer. Users must pay additional fee to as relayer reward. The reward will be shared with the relayers who sync the referenced blockchain headers.",
      "Besides, the reward won't be paid the relayers' accounts directly. A reward distribution mechanism will be brought in to avoid monopolization. 2. System Synchronization, such as delivering refund package(caused by failures of most oracle relayers), special blockchain header synchronization(header contains BC validatorset update), BSC staking package. System reward contract will pay reward to relayers' accounts directly. If some Relayers have faster networks and better hardware, they can monopolize all the package relaying and leave no reward to others. Thus fewer participants will join for relaying, which encourages centralization and harms the efficiency and security of the network. Ideally, due to the decentralization and dynamic re-election of BSC validators, one Relayer can hardly be always the first to relay every message. But in order to avoid the monopolization further, the rewarding economy is also specially designed to minimize such chance: 1.",
      "The reward for Relayers will be only distributed in batches, and one batch will cover a number of successful relayed packages. 2. The reward a Relayer can get from a batch distribution is not linearly in proportion to their number of successful relayed packages. Instead, except the first a few relays, the more a Relayer relays during a batch period, the less reward it will collect. Oracle Relayers Relayers for BSC to BC communication are using the Oracle model, and so-called Oracle Relayers. Each of the validators must, and only the ones of the validator set, run Oracle Relayers. Each Oracle Relayer watches the blockchain state change. Once it catches Cross-Chain Communication Packages, it will submit to vote for the requests. After Oracle Relayers from \u2154 of the voting power of BC validators vote for the changes, the cross-chain actions will be performed.",
      "Oracle Replayers should wait for enough blocks to confirm the finality on BSC before submitting and voting for the cross-chain communication packages onto BC. The cross-chain fees will be distributed to BC validators together with the normal BC blocking rewards. Such oracle type relaying depends on all the validators to support. As all the votes for the cross-chain communication packages are recorded on the blockchain, it is not hard to have a metric system to assess the performance of the Oracle Relayers. The poorest performer may have their rewards clawed back via another Slashing logic introduced in the future. Outlook It is hard to conclude for Binance Chain, as it has never stopped evolving. The dual-chain strategy is to open the gate for users to take advantage of the fast transferring and trading on one side, and flexible and extendable programming on the other side, but it will be one stop along the development of Binance Chain.",
      "Here below are the topics to look into so as to facilitate the community better for more usability and extensibility: 1. Add different digital asset model for different business use cases 2. Enable more data feed, especially DEX market data, to be communicated from Binance DEX to BSC 3. Provide interface and compatibility to integrate with Ethereum, including its further upgrade, and other blockchain 4. Improve client side experience to manage wallets and use blockchain more conveniently ------ 1: BNB business practitioners may provide other benefits for BNB delegators, as they do now for long term BNB holders."
    ],
    "word_count": 6133,
    "page_count": 1
  },
  "AVAX": {
    "chunks": [
      "Avalanche Platform 20200630 Kevin Sekniqi, Daniel Laine, Stephen Buttolph, and Emin Gun Sirer Abstract. This paper provides an architectural overview of the \ufb01rst release of the Avalanche platform, codenamed Avalanche Borealis. For details on the economics of the native token, labeled AVAX, we guide the reader to the accompanying token dynamics paper 2. Disclosure: The information described in this paper is preliminary and subject to change at any time. Furthermore, this paper may contain forward-looking statements.1 Git Commit: 7497e4a4ba0a1ea2dc2a111bc6deefbf3023708e Introduction This paper provides an architectural overview of the Avalanche platform. The key focus is on the three key di\ufb00erentiators of the platform: the engine, the architectural model, and the governance mechanism. Avalanche Goals and Principles Avalanche is a high-performance, scalable, customizable, and secure blockchain platform.",
      "It targets three broad use cases:  Building application-speci\ufb01c blockchains, spanning permissioned (private) and permissionless (public) deployments. Building and launching highly scalable and decentralized applications (Dapps). Building arbitrarily complex digital assets with custom rules, covenants, and riders (smart assets). 1 Forward-looking statements generally relate to future events or our future performance. This includes, but is not limited to, Avalanches projected performance; the expected development of its business and projects; execution of its vision and growth strategy; and completion of projects that are currently underway, in development or otherwise under consideration. Forward-looking statements represent our managements beliefs and assumptions only as of the date of this presentation. These statements are not guarantees of future performance and undue reliance should not be placed on them.",
      "Such forward-looking statements necessarily involve known and unknown risks, which may cause actual performance and results in future periods to di\ufb00er materially from any projections expressed or implied herein. Avalanche undertakes no obligation to update forward-looking statements. Although forward-looking statements are our best prediction at the time they are made, there can be no assurance that they will prove to be accurate, as actual results and future events could di\ufb00er materially. The reader is cautioned not to place undue reliance on forward-looking statements. Kevin Sekniqi, Daniel Laine, Stephen Buttolph, and Emin Gun Sirer The overarching aim of Avalanche is to provide a unifying platform for the creation, transfer, and trade of digital assets. By construction, Avalanche possesses the following properties: Scalable Avalanche is designed to be massively scalable, robust, and e\ufb03cient.",
      "The core consensus engine is able to support a global network of potentially hundreds of millions of internet-connected, low and high- powered devices that operate seamlessly, with low latencies and very high transactions per second. Secure Avalanche is designed to be robust and achieve high security. Classical consensus protocols are designed to withstand up to f attackers, and fail completely when faced with an attacker of size f  1 or larger, and Nakamoto consensus provides no security when 51 of the miners are Byzantine. In contrast, Avalanche provides a very strong guarantee of safety when the attacker is below a certain threshold, which can be parametrized by the system designer, and it provides graceful degradation when the attacker exceeds this threshold. It can uphold safety (but not liveness) guarantees even when the attacker exceeds 51. It is the \ufb01rst permissionless system to provide such strong security guarantees.",
      "Decentralized Avalanche is designed to provide unprecedented decentralization. This implies a commitment to multiple client implementations and no centralized control of any kind. The ecosystem is designed to avoid divisions between classes of users with di\ufb00erent interests. Crucially, there is no distinction between miners, developers, and users. Governable and Democratic AVAX is a highly inclusive platform, which enables anyone to connect to its network and participate in validation and \ufb01rst-hand in governance. Any token holder can have a vote in selecting key \ufb01nancial parameters and in choosing how the system evolves. Interoperable and Flexible Avalanche is designed to be a universal and \ufb02exible infrastructure for a multitude of blockchainsassets, where the base AVAX is used for security and as a unit of account for exchange. The system is intended to support, in a value-neutral fashion, many blockchains to be built on top.",
      "The platform is designed from the ground up to make it easy to port existing blockchains onto it, to import balances, to support multiple scripting languages and virtual machines, and to meaningfully support multiple deployment scenarios. Outline The rest of this paper is broken down into four major sections. Section 2 outlines the details of the engine that powers the platform. Section 3 discusses the architectural model behind the platform, including subnetworks, virtual machines, bootstrapping, membership, and staking. Section 4 explains the governance model that enables dynamic changes to key economic parameters. Finally, in Section 5 explores various peripheral topics of interest, including potential optimizations, post-quantum cryptography, and realistic adversaries.",
      "Avalanche Platform 20200630 Naming Convention The name of the platform is Avalanche, and is typically referred to as the Avalanche platform, and is interchangeablesynonymous with the Avalanche network, or  simply  Avalanche. Codebases will be released using three numeric identi\ufb01ers, labeled v.0-9.0-9.0-100, where the \ufb01rst number identi\ufb01es major releases, the second number identi\ufb01es minor releases, and the third number identi\ufb01es patches. The \ufb01rst public release, codenamed Avalanche Borealis, is v. 1.0.0. The native token of the platform is called AVAX. The family of consensus protocols used by the Avalanche platform is referred to as the Snow family. There are three concrete instantiations, called Avalanche, Snowman, and Frosty. The Engine Discussion of the Avalanche platform begins with the core component which powers the platform: the consensus engine. Background Distributed payments and  more generally  computation, require agreement between a set of machines.",
      "Therefore, consensus protocols, which enable a group of nodes to achieve agreement, lie at the heart of blockchains, as well as almost every deployed large-scale industrial distributed system. The topic has received extensive scrutiny for almost \ufb01ve decades, and that e\ufb00ort, to date, has yielded just two families of protocols: classical consensus protocols, which rely on all-to-all communication, and Nakamoto consensus, which relies on proof-of-work mining coupled with the longest-chain-rule. While classical consensus protocols can have low latency and high throughput, they do not scale to large numbers of participants, nor are they robust in the presence of membership changes, which has relegated them mostly to permissioned, mostly static deployments. Nakamoto consensus protocols 5, 7, 4, on the other hand, are robust, but su\ufb00er from high con\ufb01rmation latencies, low throughput, and require constant energy expenditure for their security.",
      "The Snow family of protocols, introduced by Avalanche, combine the best properties of classical con- sensus protocols with the best of Nakamoto consensus. Based on a lightweight network sampling mechanism, they achieve low latency and high throughput without needing to agree on the precise membership of the system. They scale well from thousands to millions of participants with direct participation in the consen- sus protocol. Further, the protocols do not make use of PoW mining, and therefore avoid its exorbitant energy expenditure and subsequent leak of value in the ecosystem, yielding lightweight, green, and quiescent protocols. Mechanism and Properties The Snow protocols operate by repeated sampling of the network. Each node polls a small, constant-sized, randomly chosen set of neighbors, and switches its proposal if a supermajority supports a di\ufb00erent value. Samples are repeated until convergence is reached, which happens rapidly in normal operations.",
      "We elucidate the mechanism of operation via a concrete example. First, a transaction is created by a user and sent to a validating node, which is a node participating in the consensus procedure. It is then propagated out to other nodes in the network via gossiping. What happens if that user also issues a con\ufb02icting Kevin Sekniqi, Daniel Laine, Stephen Buttolph, and Emin Gun Sirer transaction, that is, a doublespend? To choose amongst the con\ufb02icting transactions and prevent the double- spend, every node randomly selects a small subset of nodes and queries which of the con\ufb02icting transactions the queried nodes think is the valid one. If the querying node receives a supermajority response in favor of one transaction, then the node changes its own response to that transaction. Every node in the network repeats this procedure until the entire network comes to consensus on one of the con\ufb02icting transactions.",
      "Surprisingly, while the core mechanism of operation is quite simple, these protocols lead to highly desirable system dynamics that make them suitable for large-scale deployment. Permissionless, Open to Churn, and Robust. The latest slew of blockchain projects employ classical consensus protocols and therefore require full membership knowledge. Knowing the entire set of par- ticipants is su\ufb03ciently simple in closed, permissioned systems, but becomes increasingly hard in open, decentralized networks. This limitation imposes high security risks to existing incumbents employing such protocols. In contrast, Snow protocols maintain high safety guarantees even when there are well- quanti\ufb01ed discrepancies between the network views of any two nodes. Validators of Snow protocols enjoy the ability to validate without continuous full membership knowledge. They are, therefore, robust and highly suitable for public blockchains.",
      "Scalable and Decentralized A core feature of the Snow family is its ability to scale without incurring fundamental tradeo\ufb00s. Snow protocols can scale to tens of thousands or millions of nodes, without dele- gation to subsets of validators. These protocols enjoy the best-in-class system decentralization, allowing every node to fully validate. First-hand continuous participation has deep implications for the security of the system. In almost every proof-of-stake protocol that attempts to scale to a large participant set, the typical mode of operation is to enable scaling by delegating validation to a subcommittee. Natu- rally, this implies that the security of the system is now precisely as high as the corruption cost of the subcommittee. Subcommittees are furthermore subject to cartel formation. In Snow-type protocols, such delegation is not necessary, allowing every node operator to have a \ufb01rst- hand say in the system, at all times.",
      "Another design, typically referred to as state sharding, attempts to provide scalability by parallelizing transaction serialization to independent networks of validators. Unfortunately, the security of the system in such a design becomes only as high as the easiest corruptible independent shard. Therefore, neither subcommittee election nor sharding are suitable scaling strategies for crypto platforms. Adaptive. Unlike other voting-based systems, Snow protocols achieve higher performance when the adversary is small, and yet highly resilient under large attacks. Asynchronously Safe. Snow protocols, unlike longest-chain protocols, do not require synchronicity to operate safely, and therefore prevent double-spends even in the face of network partitions. In Bitcoin, for example, if synchronicity assumption is violated, it is possible to operate to independent forks of the Bitcoin network for prolonged periods of time, which would invalidate any transactions once the forks heal. Low Latency.",
      "Most blockchains today are unable to support business applications, such as trading or daily retail payments. It is simply unworkable to wait minutes, or even hours, for con\ufb01rmation of transactions. Therefore, one of the most important, and yet highly overlooked, properties of consensus protocols is the time to \ufb01nality. Snow protocols reach \ufb01nality typically in 1 second, which is signi\ufb01cantly lower than both longest-chain protocols and sharded blockchains, both of which typically span \ufb01nality to a matter of minutes. Avalanche Platform 20200630  High Throughput. Snow protocols, which can build a linear chain or a DAG, reach thousands of transac- tions per second (5000 tps), while retaining full decentralization. New blockchain solutions that claim high TPS typically trade o\ufb00decentralization and security and opt for more centralized and insecure consensus mechanisms. Some projects report numbers from highly controlled settings, thus misreporting true performance results.",
      "The reported numbers for AVAX are taken directly from a real, fully imple- mented Avalanche network running on 2000 nodes on AWS, geo-distributed across the globe on low-end machines. Higher performance results (10,000) can be achieved through assuming higher bandwidth provisioning for each node and dedicated hardware for signature veri\ufb01cation. Finally, we note that the aforementioned metrics are at the base-layer. Layer-2 scaling solutions immediately augment these results considerably. Comparative Charts of Consensus Table 1 describes the di\ufb00erences between the three known families of consensus protocols through a set of 8 critical axes.",
      "Nakamoto Classical Snow Robust (Suitable for Open Settings) Highly Decentralized (Allows Many Validators) Low Latency and Quick Finality (Fast Transaction Con\ufb01rmation) High Throughput (Allows Many Clients) Lightweight (Low System Requirements) Quiescent (Not Active When No Decisions Performed) Safety Parameterizable (Beyond 51 Adversarial Presence) Highly Scalable Table 1. Comparative chart between the three known families of consensus protocols. Avalanche, Snowman, and Frosty all belong to the Snow family. Platform Overview In this section, we provide an architectural overview of the platform and discuss various implementation details. The Avalanche platform cleanly separates three concerns: chains (and assets built on top), execution environments, and deployment. Architecture Subnetworks A subnetwork, or subnet, is a dynamic set of validators working together to achieve consensus on the state of a set of blockchains.",
      "Each blockchain is validated by one subnet, and a subnet can validate arbitrarily many blockchains. A validator may be a member of arbitrarily many subnets. A subnet decides who may enter it, and may require that its constituent validators have certain properties. The Avalanche platform supports the creation and operation of arbitrarily many subnets. In order to create a new subnet or to join a subnet, one must pay a fee denominated in AVAX. Kevin Sekniqi, Daniel Laine, Stephen Buttolph, and Emin Gun Sirer The subnet model o\ufb00ers a number of advantages:  If a validator doesnt care about the blockchains in a given subnet, it will simply not join that subnet. This reduces network tra\ufb03c, as well as the computational resources required of validators. This is in contrast to other blockchain projects, in which every validator must validate every transaction, even those they dont care about. Since subnets decide who may enter them, one can create private subnets.",
      "That is, each blockchain in the subnet is validated only by a set of trusted validators. One can create a subnet where each validator has certain properties. For example, one could create a subnet where each validator is located in a certain jurisdiction, or where each validator is bound by some real-world contract. This may be beni\ufb01cial for compliance reasons. There is one special subnet called the Default Subnet. It is validated by all validators. (That is, in order to validate any subnet, one must also validate the Default Subnet.) The Default Subnet validates a set of pre-de\ufb01ned blockchains, including the blockchain where AVAX lives and is traded. Virtual Machines Each blockchain is an instance of a Virtual Machine (VM.) A VM is a blueprint for a blockchain, much like a class is a blueprint for an object in an object-oriented programming language. The interface, state and behavior of a blockchain is de\ufb01ned by the VM that the blockchain runs.",
      "The following properties of a blockchain, and other, are de\ufb01ned by a VM:  The contents of a block  The state transition that occurs when a block is accepted  The APIs exposed by the blockchain and their endpoints  The data that is persisted to disk We say that a blockchain uses or runs a given VM. When creating a blockchain, one speci\ufb01es the VM it runs, as well as the genesis state of the blockchain. A new blockchain can be created using a pre-existing VM, or a developer can code a new one. There can be arbitrarily many blockchains that run the same VM. Each blockchain, even those running the same VM, is logically independent from others and maintains its own state. Bootstrapping The \ufb01rst step in participating in Avalanche is bootstrapping. The process occurs in three stages: connection to seed anchors, network and state discovery, and becoming a validator. Seed Anchors Any networked system of peers that operates without a permissioned (i.e.",
      "hard-coded) set of identities requires some mechanism for peer discovery. In peer-to-peer \ufb01le sharing networks, a set of trackers are used. In crypto networks, a typical mechanism is the use of DNS seed nodes (which we refer Avalanche Platform 20200630 to as seed anchors), which comprise a set of well-de\ufb01ned seed-IP addresses from which other members of the network can be discovered. The role of DNS seed nodes is to provide useful information about the set of active participants in the system. The same mechanism is employed in Bitcoin Core 1, wherein the srcchainparams.cpp \ufb01le of the source code holds a list of hard-coded seed nodes. The di\ufb00erence between BTC and Avalanche is that BTC requires just one correct DNS seed node, while Avalanche requires a simple majority of the anchors to be correct. As an example, a new user may choose to bootstrap the network view through a set of well established and reputable exchanges, any one of which individually are not trusted.",
      "We note, however, that the set of bootstrap nodes does not need to be hard-coded or static, and can be provided by the user, though for ease of use, clients may provide a default setting that includes economically important actors, such as exchanges, with which clients wish to share a world view. There is no barrier to become a seed anchor, therefore a set of seed anchors can not dictate whether a node may or may not enter the network, since nodes can discover the latest network of Avalanche peers by attaching to any set of seed anchors. Network and State Discovery Once connected to the seed anchors, a node queries for the latest set of state transitions. We call this set of state transitions the accepted frontier. For a chain, the accepted frontier is the last accepted block. For a DAG, the accepted frontier is the set of vertices that are accepted, yet have no accepted children.",
      "After collecting the accepted frontiers from the seed anchors, the state transitions that are accepted by a majority of the seed anchors is de\ufb01ned to be accepted. The correct state is then extracted by synchronizing with the sampled nodes. As long as there is a majority of correct nodes in the seed anchor set, then the accepted state transitions must have been marked as accepted by at least one correct node. This state discovery process is also used for network discovery. The membership set of the network is de\ufb01ned on the validator chain. Therefore, synchronizing with the validator chain allows the node to discover the current set of validators. The validator chain will be discussed further in the next section. Sybil Control and Membership Consensus protocols provide their security guarantees under the assumption that up to a threshold number of members in the system could be adversarial.",
      "A Sybil attack, wherein a node cheaply \ufb02oods the network with malicious identities, can trivially invalidate these guarantees. Fundamentally, such an attack can only be deterred by trading o\ufb00presence with proof of a hard-to-forge resource 3. Past systems have explored the use of Sybil deterrence mechanisms that span proof-of-work (PoW), proof-of-stake (PoS), proof-of-elapsed-time (POET), proof-of-space-and-time (PoST), and proof-of-authority (PoA). At their core, all of these mechanisms serve an identical function: they require that each participant have some skin in the game in the form of some economic commitment, which in turn provides an economic barrier against misbehavior by that participant. All of them involve a form of stake, whether it is in the form of mining rigs and hash power (PoW), disk space (PoST), trusted hardware (POET), or an approved identity (PoA). This stake forms the basis of an economic cost that participants must bear to acquire a voice.",
      "For instance, in Bitcoin, the ability to contribute valid blocks is directly proportional to the hash-power of the proposing participant. Unfortunately, there has also been substantial confusion between consensus protocols Kevin Sekniqi, Daniel Laine, Stephen Buttolph, and Emin Gun Sirer versus Sybil control mechanisms. We note that the choice of consensus protocols is, for the most part, orthogonal to the choice of the Sybil control mechanism. This is not to say that Sybil control mechanisms are drop-in-replacements for each other, since a particular choice might have implications about the underlying guarantees of the consensus protocol. However, the Snow family can be coupled with many of these known mechanisms, without signi\ufb01cant modi\ufb01cation. Ultimately, for security and to ensure that the incentives of participants are aligned for the bene\ufb01t of the network, AVAX choose PoS to the core Sybil control mechanism.",
      "Some forms of stake are inherently centralized: mining rig manufacturing (PoW), for instance, is inherently centralized in the hands of a few people with the appropriate know-how and access to the dozens of patents required for competitive VLSI manufacturing. Furthermore, PoW mining leaks value due to the large yearly miner subsidies. Similarly, disk space is most abundantly owned by large datacenter operators.Further, all sybil control mechanisms that accrue ongoing costs, e.g. electricity costs for hashing, leak value out of the ecosystem, not to mention destroy the environment. This, in turn, reduces the feasibility envelope for the token, wherein an adverse price move over a small time frame can render the system inoperable. Proof-of-work inherently selects for miners who have the connections to procure cheap electricity, which has little to do with the miners ability to serialize transactions or their contributions to the overall ecosystem.",
      "Among these options, we choose proof-of-stake, because it is green, accessible, and open to all. We note, however, that while the AVAX uses PoS, the Avalanche network enables subnets to be launched with PoW and PoS. Staking is a natural mechanism for participation in an open network because it enables a direct economic argument: the probability of success of an attack is directly proportional to a well-de\ufb01ned monetary cost function. In other words, the nodes that stake are economically motivated to not engage in behavior that might hurt the value of their stake. Additionally, this stake does not incur any additional upkeep costs (other then the opportunity cost of investing in another asset), and has the property that, unlike mining equipment, is fully consumed if used in a catastrophic attack. For PoW operations, mining equipment can be simply reused or  if the owner decides to  entirely sold back to the market.",
      "A node wishing to enter the network can freely do so by \ufb01rst putting up a stake that is immobilized during the duration of participation in the network. The user determines the amount duration of the stake. Once accepted, a stake cannot be reverted. The main goal is to ensure that nodes substantially share the same mostly stable view of the network. We anticipate setting the minimum staking time on the order of a week. Unlike other systems that also propose a PoS mechanism, AVAX does not make usage of slashing, and therefore all stake is returned when the staking period expires. This prevents unwanted scenarios such as a client software or hardware failure leading to a loss of coins. This dovetails with our design philosophy of building predictable technology: the staked tokens are not at risk, even in the presence of software or hardware \ufb02aws. In Avalanche, a node that wants to participate issues a special stake transaction to the validator chain.",
      "Staking transactions name an amount to stake, the staking key of the participant that is staking, the duration, and the time that validation will start. Once the transaction is accepted, the funds will be locked until the end of the staking period. The minimal allowed amount is decided and enforced by the system. The stake amount placed by a participant has implications for both the amount of in\ufb02uence the participant has in the Avalanche Platform 20200630 consensus process, as well as the reward, as discussed later. The speci\ufb01ed staking duration, must be between \u03b4min and \u03b4max, the minimum and maximum timeframes for which any stake can be locked. As with the staking amount, the staking period also has implications for the reward in the system. Loss or theft of the staking key cannot lead to asset loss, as the staking key is used only in the consensus process, not for asset transfer.",
      "Smart Contracts in AVAX At launch Avalanche supports standard Solidity-based smart contracts through the Ethereum virtual ma- chine (EVM). We envision that the platform will support a richer and more powerful set of smart contract tools, including:  Smart contracts with o\ufb00-chain execution and on-chain veri\ufb01cation. Smart contracts with parallel execution. Any smart contracts that do not operate on the same state in any subnet in Avalanche will be able to execute in parallel. An improved Solidity, called Solidity. This new language will support versioning, safe mathematics and \ufb01xed point arithmetic, an improved type system, compilation to LLVM, and just-in-time execution. If a developer requires EVM support but wants to deploy smart contracts in a private subnet, they can spin-up a new subnet directly. This is how Avalanche enables functionality-speci\ufb01c sharding through the subnets.",
      "Furthermore, if a developer requires interactions with the currently deployed Ethereum smart contracts, they can interact with the Athereum subnet, which is a spoon of Ethereum. Finally, if a developer requires a di\ufb00erent execution environment from the Ethereum virtual machine, they may choose to deploy their smart contract through a subnet that implements a di\ufb00erent execution environment, such as DAML or WASM. Subnets can support additional features beyond VM behavior. For example, subnets can enforce performance requirements for bigger validator nodes that hold smart contracts for longer periods of time, or validators that hold contract state privately. Governance and The AVAX Token The AVAX Native Token Monetary Policy The native token, AVAX, is capped-supply, where the cap is set at 720, 000, 000 tokens, with 360, 000, 000 tokens available on mainnet launch.",
      "However, unlike other capped-supply tokens which bake the rate of minting perpetually, AVAX is designed to react to changing economic conditions. In par- ticular, the objective of AVAXs monetary policy is to balance the incentives of users to stake the token versus using it to interact with the variety of services available on the platform. Participants in the platform collectively act as a decentralized reserve bank. The levers available on Avalanche are staking rewards, fees, and airdrops, all of which are in\ufb02uenced by governable parameters. Staking rewards are set by on-chain gov- ernance, and are ruled by a function designed to never surpass the capped supply. Staking can be induced by increasing fees or increasing staking rewards. On the other hand, we can induce increased engagement with the Avalanche platform services by lowering fees, and decreasing the staking reward.",
      "Kevin Sekniqi, Daniel Laine, Stephen Buttolph, and Emin Gun Sirer Uses Payments True decentralized peer-to-peer payments are largely an unrealized dream for the industry due to the current lack of performance from incumbents. AVAX is as powerful and easy to use as payments using Visa, allowing thousands of transactions globally every second, in a fully trustless, decentralized manner. Furthermore, for merchants worldwide, AVAX provides a direct value proposition over Visa, namely lower fees. Staking: Securing the System On the Avalanche platform, sybil control is achieved via staking. In order to validate, a participant must lock up coins, or stake. Validators, sometimes referred to as stakers, are compensated for their validation services based on staking amount and staking duration, amongst other properties. The chosen compensation function should minimize variance, ensuring that large stakers do not disproportionately receive more compensation.",
      "Participants are also not subject to any luck factors, as in PoW mining. Such a reward scheme also discourages the formation of mining or staking pools enabling truly decentralized, trustless participation in the network. Atomic swaps Besides providing the core security of the system, the AVAX token serves as the universal unit of exchange. From there, the Avalanche platform will be able to support trustless atomic swaps natively on the platform enabling native, truly decentralized exchanges of any type of asset directly on Avalanche. Governance Governance is critical to the development and adoption of any platform because  as with all other types of systems  Avalanche will also face natural evolution and updates. AVAX provides on-chain governance for critical parameters of the network where participants are able to vote on changes to the network and settle network upgrade decisions democratically.",
      "This includes factors such as the minimum staking amount, minting rate, as well as other economic parameters. This enables the platform to e\ufb00ectively perform dy- namic parameter optimization through a crowd oracle. However, unlike some other governance platforms out there, Avalanche does not allow unlimited changes to arbitrary aspects of the system. Instead, only a pre-determined number of parameters can be modi\ufb01ed via governance, rendering the system more predictable and increasing safety. Further, all governable parameters are subject to limits within speci\ufb01c time bounds, introducing hysteresis, and ensuring that the system remains predictable over short time ranges. A workable process for \ufb01nding globally acceptable values for system parameters is critical for decentral- ized systems without custodians. Avalanche can use its consensus mechanism to build a system that allows anyone to propose special transactions that are, in essence, system-wide polls.",
      "Any participating node may issue such proposals. Nominal reward rate is an important parameter that a\ufb00ects any currency, whether digital or \ufb01at. Unfor- tunately, cryptocurrencies that \ufb01x this parameter might face various issues, including de\ufb02ation or in\ufb02ation. To that end, the nominal reward rate is subject to governance, within pre-established boundaries. This will allow token holders to choose on whether AVAX is eventually capped, uncapped, or even de\ufb02ationary. Avalanche Platform 20200630 Transaction fees, denoted by the set F, are also subject to governance. F is e\ufb00ectively a tuple which de- scribes the fees associated with the various instructions and transactions. Finally, staking times and amounts are also governable. The list of these parameters is de\ufb01ned in Figure 1. : Staking amount, denominated in AVAX. This value de\ufb01nes the minimal stake required to be placed as bond before participating in the system.",
      "\u03b4min : The minimal amount of time required for a node to stake into the system. \u03b4max : The maximal amount of time a node can stake. \u03c1 : (\u03c0, \u03c4\u03b4min) R : Reward rate function, also referred to as minting rate, determines the reward a participant can claim as a function of their staking amount given some number of \u03c0 publicly disclosed nodes under its ownership, over a period of \u03c4 consecutive \u03b4min timeframes, such that \u03c4\u03b4min \u03b4max. F : the fee structure, which is a set of governable fees parameters that specify costs to various transactions. Fig. 1. Key non-consensus parameters used in Avalanche. All notation is rede\ufb01ned upon \ufb01rst use. In line with the principle of predictability in a \ufb01nancial system, governance in AVAX has hysteresis, meaning that changes to parameters are highly dependent on their recent changes. There are two limits associated with each governable parameter: time and range.",
      "Once a parameter is changed using a governance transaction, it becomes very di\ufb03cult to change it again immediately and by a large amount. These di\ufb03culty and value constraints relax as more time passes since the last change. Overall, this keeps the system from changing drastically over a short period of time, allowing users to safely predict system parameters in the short term, while having strong control and \ufb02exibility for the long term. Discussion Optimizations Pruning Many blockchain platforms, especially those implementing Nakamoto consensus such as Bitcoin, su\ufb00er from perpetual state growth. This is because  by protocol  they have to store the entire history of transactions. However, in order for a blockchain to grow sustainably, it must be able to prune old history. This is especially important for blockchains that support high performance, such as Avalanche. Pruning is simple in the Snow family.",
      "Unlike in Bitcoin (and similar protocols), where pruning is not possible per the algorithmic requirements, in AVAX nodes do not need to maintain parts of the DAG that are deep and highly committed. These nodes do not need to prove any past history to new bootstrapping nodes, and therefore simply have to store active state, i.e. the current balances, as well as uncommitted transactions. Client Types Avalanche can support three di\ufb00erent types of clients: archival, full, and light. Archival nodes store the entire history of the AVAX subnet, the staking subnet, and the smart contract subnet, all the Kevin Sekniqi, Daniel Laine, Stephen Buttolph, and Emin Gun Sirer way to genesis, meaning that these nodes serve as bootstrapping nodes for new incoming nodes. Additionally these nodes may store the full history of other subnets for which they choose to be validators. Archival nodes are typically machines with high storage capabilities that are paid by other nodes when downloading old state.",
      "Full nodes, on the other hand, participate in validation, but instead of storing all history, they simply store the active state (e.g. current UTXO set). Finally, for those that simply need to interact securely with the network using the most minimal amount of resources, Avalanche supports light clients which can prove that some transaction has been committed without needing to download or synchronize history. Light clients engage in the repeated sampling phase of the protocol to ensure safe commitment and network wide consensus. Therefore, light clients in Avalanche provide the same security guarantees as full nodes. Sharding Sharding is the process of partitioning various system resources in order to increase performance and reduce load. There are various types of sharding mechanisms.",
      "In network sharding, the set of participants is divided into separate subnetworks as to reduce algorithmic load; in state sharding, participants agree on storing and maintaining only speci\ufb01c subparts of the entire global state; lastly, in transaction sharding, participants agree to separate the processing of incoming transactions. In Avalanche Borealis, the \ufb01rst form of sharding exists through the subnetworks functionality. For example, one may launch a gold subnet and another real-estate subnet. These two subnets can exist entirely in parallel. The subnets interact only when a user wishes to buy real-estate contracts using their gold holdings, at which point Avalanche will enable an atomic swap between the two subnets. Concerns Post Quantum Cryptography Post-quantum cryptography has recently gained widespread attention due to the advances in the development of quantum computers and algorithms.",
      "The concern with quantum computers is that they can break some of the currently deployed cryptographic protocols, speci\ufb01cally digital signatures. The Avalanche network model enables any number of VMs, so it supports a quantum-resistant virtual machine with a suitable digital signature mechanism. We anticipate several types of digital signature schemes to be deployed, including quantum resistant RLWE-based signatures. The consensus mechanism does not assume any kind of heavy crypto for its core operation. Given this design, it is straightforward to extend the system with a new virtual machine that provides quantum secure cryptographic primitives. Realistic Adversaries The Avalanche paper 6 provides very strong guarantees in the presence of a powerful and hostile adversary, known as a round-adaptive adversary in the full point-to-point model.",
      "In other terms, the adversary has full access to the state of every single correct node at all times, knows the random choices of all correct nodes, as well as can update its own state at any time, before and after the correct node has the chance to update its own state. E\ufb00ectively, this adversary is all powerful, except for the ability to directly update the state of a correct node or modify the communication between correct nodes. Nonetheless, in reality, such an adversary is purely theoretical since practical implementations of the strongest possible adversary are limited at statistical approximations of the network state. Therefore, in practice, we expect worst-case-scenario attacks to be di\ufb03cult to deploy. Avalanche Platform 20200630 Inclusion and Equality A common problem in permissionless currencies is that of the rich getting richer.",
      "This is a valid concern, since a PoS system that is improperly implemented may in fact allow wealth generation to be disproportionately attributed to the already large holders of stake in the system. A simple example is that of leader-based consensus protocols, wherein a subcommittee or a designated leader collects all the rewards during its operation, and where the probability of being chosen to collect rewards is proportional to the stake, accruing strong reward compounding e\ufb00ects. Further, in systems such as Bitcoin, there is a big get bigger phenomenon where the big miners enjoy a premium over smaller ones in terms of fewer orphans and less lost work. In contrast, Avalanche employs an egalitarian distribution of minting: every single participant in the staking protocol is rewarded equitably and proportionally based on stake. By enabling very large numbers of people to participate \ufb01rst-hand in staking, Avalanche can accommodate millions of people to participate equally in staking.",
      "The minimum amount required to participate in the protocol will be up for governance, but it will be initialized to a low value to encourage wide participation. This also implies that delegation is not required to participate with a small allocation. Conclusion In this paper, we discussed the architecture of the Avalanche platform. Compared to other platforms today, which either run classical-style consensus protocols and therefore are inherently non-scalable, or make usage of Nakamoto-style consensus that is ine\ufb03cient and imposes high operating costs, the Avalanche is lightweight, fast, scalable, secure, and e\ufb03cient. The native token, which serves for securing the network and paying for various infrastructural costs is simple and backwards compatible. AVAX has capacity beyond other proposals to achieve higher levels of decentralization, resist attacks, and scale to millions of nodes without any quorum or committee election, and hence without imposing any limits to participation.",
      "Besides the consensus engine, Avalanche innovates up the stack, and introduces simple but important ideas in transaction management, governance, and a slew of other components not available in other plat- forms. Each participant in the protocol will have a voice in in\ufb02uencing how the protocol evolves at all times, made possible by a powerful governance mechanism. Avalanche supports high customizability, allowing nearly instant plug-and-play with existing blockchains. 1. Bitcoin: bitcoinbitcoin (Oct 2018), https:github.combitcoinbitcoin 2. Buttolph, Moin, Sekniqi, Sirer, E.G.: Avalanche token paper token dynamics (2019), 3. Douceur, J.R.: The sybil attack. In: International Workshop on Peer-to-Peer Systems. pp. 251260. Springer (2002) 4. Eyal, I., Gencer, A.E., Sirer, E.G., van Renesse, R.: Bitcoin-ng: A scalable blockchain protocol. In: 13th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2016, Santa Clara, CA, USA, March 16-18, 2016. pp.",
      "4559 (2016), https:www.usenix.orgconferencensdi16technical-sessionspresentationeyal 5. Nakamoto, S.: Bitcoin: A peer-to-peer electronic cash system (2008) 6. Rocket, T.: Snow\ufb02ake to Avalanche: A novel metastable consensus protocol family for cryptocurrencies. IPFS (2018), Kevin Sekniqi, Daniel Laine, Stephen Buttolph, and Emin Gun Sirer 7. Wood, G.: Ethereum: A secure decentralised generalised transaction ledger (2014)"
    ],
    "word_count": 6382,
    "page_count": 14
  },
  "AXS": {
    "chunks": [
      "Axie Infinity bars Axie Infinity search circle-xmark  Ctrl k Axie Infinity Axie Infinity Gameplay chevron-right Axie Infinity Shards - AXS chevron-right Community chevron-right Decentralized Organization Key Metrics chevron-right Technology chevron-right Roadmap and completed milestones Team Partners chevron-up chevron-down gitbook Powered by GitBook xmark block-quote On this page chevron-down copy Copy chevron-down Axie Infinity Official Axie Infinity Whitepaper, last updated November 2021 hashtag Axie Infinity Universe Axie Infinity is a universe filled with fierce, collectible creatures called Axies. Axie features a player-owned economy where players have complete ownership of their digital assets and can buy, sell, and trade them just like physical trading cards and collectibles. The Axie universe is always expanding through new games and experiences.",
      "Many of these experiences will allow players to compete with each other using complex strategies and tactics to attain top rankings or be rewarded with coveted resources. Others will have them complete quests, defeat bosses, and unlock in-depth storylines. The number of Axie combination is almost infinite. Each Axie has a variety of parts based on a unique genetic code. These genes correspond to body parts which can be interpreted in different ways by different experiences. For example, the first generation of Axie games have all featured systems where Axies can play cards or use moves in battle determined by their body parts. hashtag Community  Economy All art assets and Axie genetic data can be easily accessed by 3rd parties, allowing community developers to build their own tools and experiences in the Axie Infinity universe. While still in early access, Axie is ranked the 1 Ethereum game by daily, weekly, and monthly active users.",
      "Players have spent over 1M ETH (4B USD) so far in the Axie universe. While Axie is a fun game, itx27;s also taken on characteristics of a social network and earning platform due to the strong community and opportunity to truly own resources earned within the games. A key difference between Axie and a traditional game is that Blockchain economic design unlocks the ability to have complex player-owned economies and reward players who are able to reach advanced levels of skill. Players are able to have fun and work towards ambitious goals while simultaneously earning potential resources that will have real monetary value due to an open economic system and demand from other players - Play-and-Earn.",
      "Currently, players can gain more resources to advance in the game by: Competing in Battles to win leaderboard prizes as well as Smooth Love Potions (SLP) Breeding Axies to produce new ones with particular Body Part combinations Collecting and speculating on rare Axies such as Mystics and Origins Creating art and content On-boarding newcomers to the Axie universe hashtag Mission Wex27;re here to create an ecosystem of amazing Axie gaming experiences with community and player-owned economies as foundational pillars. By doing this we will deliver property rights to all users of the internet, starting with gamers. Axie is a digital nation where people globally come together to play, work, and make lifelong connections. When we started this journey 4 years we werenx27;t sure where wex27;d end up but we believed that if we asked the right questions we could build something with you that had never been seen before. We asked: What if we built a game that could onboard the world to Web3?",
      "What if we could make crypto nostalgic,educational, and immersive? What if a game could be built by developers and players working together through aligned incentives? Welcome to the Axie revolution. Disclaimer: Please note that anything written in this white paper should not be taken as financial advice. Axie is a bleeding-edge game thatx27;s incorporating unfinished, risky, and highly experimental technology. Development priorities, roadmaps, and features are subject to radical overhaul based on research, traction, feedback from the community, and a myriad of other factors. Next Gameplay chevron-right Last updated 3 years ago Was this helpful? Axie Infinity Universe Community  Economy Mission Was this helpful?"
    ],
    "word_count": 654,
    "page_count": 1
  },
  "BAND": {
    "chunks": [
      "GitHub - bandprotocolwhitepaper-legacy:  DEPRECATED Whitepaper v1.0 - v2.0 of Band Protocol Skip to content Navigation Menu Toggle navigation Sign in Appearance settings Platform AI CODE CREATION GitHub Copilot Write better code with AI GitHub Spark Build and deploy intelligent apps GitHub Models Manage and compare prompts MCP Registry New Integrate external tools DEVELOPER WORKFLOWS Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes APPLICATION SECURITY GitHub Advanced Security Find and fix vulnerabilities Code security Secure your code as you build Secret protection Stop leaks before they start EXPLORE Why GitHub Documentation Blog Changelog Marketplace View all features Solutions BY COMPANY SIZE Enterprises Small and medium teams Startups Nonprofits BY USE CASE App Modernization DevSecOps DevOps CICD View all use cases BY INDUSTRY Healthcare Financial services Manufacturing Government View all industries View all solutions Resources EXPLORE BY TOPIC AI Software Development DevOps Security View all topics EXPLORE BY TYPE Customer stories Events  webinars Ebooks  reports Business insights GitHub Skills SUPPORT  SERVICES Documentation Customer support Community forum Trust center Partners Open Source COMMUNITY GitHub Sponsors Fund open source developers PROGRAMS Security Lab Maintainer Community Accelerator Archive Program REPOSITORIES Topics Trending Collections Enterprise ENTERPRISE SOLUTIONS Enterprise platform AI-powered developer platform AVAILABLE ADD-ONS GitHub Advanced Security Enterprise-grade security features Copilot for Business Enterprise-grade AI features Premium Support Enterprise-grade 247 support Pricing Search or jump to...",
      "Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up Appearance settings Resetting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert  message  This repository was archived by the owner on Sep 3, 2019. It is now read-only.",
      "bandprotocol whitepaper-legacy Public archive Notifications You must be signed in to change notification settings Fork Star  DEPRECATED Whitepaper v1.0 - v2.0 of Band Protocol bandprotocol.com stars forks Branches Tags Activity Star Notifications You must be signed in to change notification settings Code Issues Pull requests Actions Projects Security Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights bandprotocolwhitepaper-legacy master Branches Tags Go to file Code Open more actions menu Folders and files Name Name Last commit message Last commit date Latest commit History 31 Commits .gitignore .gitignore Makefile Makefile README.md README.md paper.bib paper.bib paper.tex paper.tex plasma.pdf plasma.pdf tcr.pdf tcr.pdf View all files Repository files navigation README Band Protocol Whitepaper This is the Band Protocol Whitepaper - latest PDF release .",
      "make to build PDF whitepaper from source make clean to clean up files generated from running make About  DEPRECATED Whitepaper v1.0 - v2.0 of Band Protocol bandprotocol.com Topics blockchain whitepaper bandprotocol Resources Readme Uh oh! There was an error while loading. Please reload this page . Activity Custom properties Stars stars Watchers watching Forks forks Report repository Releases v2.0.0 (TCR) Latest Aug 9, 2018  3 releases Packages No packages published Contributors Uh oh! There was an error while loading. Please reload this page . Languages 99.7 Makefile 0.3 Footer copy; 2026 GitHub, Inc. Footer navigation Terms Privacy Security Status Community Docs Contact Manage cookies Do not share my personal information You cant perform that action at this time."
    ],
    "word_count": 600,
    "page_count": 1
  },
  "BTC": {
    "chunks": [
      "Bitcoin: A Peer-to-Peer Electronic Cash System Satoshi Nakamoto satoshingmx.com www.bitcoin.org Abstract. A purely peer-to-peer version of electronic cash would allow online payments to be sent directly from one party to another without going through a financial institution. Digital signatures provide part of the solution, but the main benefits are lost if a trusted third party is still required to prevent double-spending. We propose a solution to the double-spending problem using a peer-to-peer network. The network timestamps transactions by hashing them into an ongoing chain of hash-based proof-of-work, forming a record that cannot be changed without redoing the proof-of-work. The longest chain not only serves as proof of the sequence of events witnessed, but proof that it came from the largest pool of CPU power. As long as a majority of CPU power is controlled by nodes that are not cooperating to attack the network, they'll generate the longest chain and outpace attackers.",
      "The network itself requires minimal structure. Messages are broadcast on a best effort basis, and nodes can leave and rejoin the network at will, accepting the longest proof-of-work chain as proof of what happened while they were gone. Introduction Commerce on the Internet has come to rely almost exclusively on financial institutions serving as trusted third parties to process electronic payments. While the system works well enough for most transactions, it still suffers from the inherent weaknesses of the trust based model. Completely non-reversible transactions are not really possible, since financial institutions cannot avoid mediating disputes. The cost of mediation increases transaction costs, limiting the minimum practical transaction size and cutting off the possibility for small casual transactions, and there is a broader cost in the loss of ability to make non-reversible payments for non- reversible services. With the possibility of reversal, the need for trust spreads.",
      "Merchants must be wary of their customers, hassling them for more information than they would otherwise need. A certain percentage of fraud is accepted as unavoidable. These costs and payment uncertainties can be avoided in person by using physical currency, but no mechanism exists to make payments over a communications channel without a trusted party. What is needed is an electronic payment system based on cryptographic proof instead of trust, allowing any two willing parties to transact directly with each other without the need for a trusted third party. Transactions that are computationally impractical to reverse would protect sellers from fraud, and routine escrow mechanisms could easily be implemented to protect buyers. In this paper, we propose a solution to the double-spending problem using a peer-to-peer distributed timestamp server to generate computational proof of the chronological order of transactions.",
      "The system is secure as long as honest nodes collectively control more CPU power than any cooperating group of attacker nodes. Transactions We define an electronic coin as a chain of digital signatures. Each owner transfers the coin to the next by digitally signing a hash of the previous transaction and the public key of the next owner and adding these to the end of the coin. A payee can verify the signatures to verify the chain of ownership. The problem of course is the payee can't verify that one of the owners did not double-spend the coin. A common solution is to introduce a trusted central authority, or mint, that checks every transaction for double spending. After each transaction, the coin must be returned to the mint to issue a new coin, and only coins issued directly from the mint are trusted not to be double-spent.",
      "The problem with this solution is that the fate of the entire money system depends on the company running the mint, with every transaction having to go through them, just like a bank. We need a way for the payee to know that the previous owners did not sign any earlier transactions. For our purposes, the earliest transaction is the one that counts, so we don't care about later attempts to double-spend. The only way to confirm the absence of a transaction is to be aware of all transactions. In the mint based model, the mint was aware of all transactions and decided which arrived first. To accomplish this without a trusted party, transactions must be publicly announced 1, and we need a system for participants to agree on a single history of the order in which they were received. The payee needs proof that at the time of each transaction, the majority of nodes agreed it was the first received. Timestamp Server The solution we propose begins with a timestamp server.",
      "A timestamp server works by taking a hash of a block of items to be timestamped and widely publishing the hash, such as in a newspaper or Usenet post 2-5. The timestamp proves that the data must have existed at the time, obviously, in order to get into the hash. Each timestamp includes the previous timestamp in its hash, forming a chain, with each additional timestamp reinforcing the ones before it. Block Item Item Hash Block Item Item Hash Transaction Owner 1's Public Key Owner 0's Signature Hash Transaction Owner 2's Public Key Owner 1's Signature Hash Verify Transaction Owner 3's Public Key Owner 2's Signature Hash Verify Owner 2's Private Key Owner 1's Private Key Sign Sign Owner 3's Private Key Proof-of-Work To implement a distributed timestamp server on a peer-to-peer basis, we will need to use a proof- of-work system similar to Adam Back's Hashcash 6, rather than newspaper or Usenet posts.",
      "The proof-of-work involves scanning for a value that when hashed, such as with SHA-256, the hash begins with a number of zero bits. The average work required is exponential in the number of zero bits required and can be verified by executing a single hash. For our timestamp network, we implement the proof-of-work by incrementing a nonce in the block until a value is found that gives the block's hash the required zero bits. Once the CPU effort has been expended to make it satisfy the proof-of-work, the block cannot be changed without redoing the work. As later blocks are chained after it, the work to change the block would include redoing all the blocks after it. The proof-of-work also solves the problem of determining representation in majority decision making. If the majority were based on one-IP-address-one-vote, it could be subverted by anyone able to allocate many IPs. Proof-of-work is essentially one-CPU-one-vote.",
      "The majority decision is represented by the longest chain, which has the greatest proof-of-work effort invested in it. If a majority of CPU power is controlled by honest nodes, the honest chain will grow the fastest and outpace any competing chains. To modify a past block, an attacker would have to redo the proof-of-work of the block and all blocks after it and then catch up with and surpass the work of the honest nodes. We will show later that the probability of a slower attacker catching up diminishes exponentially as subsequent blocks are added. To compensate for increasing hardware speed and varying interest in running nodes over time, the proof-of-work difficulty is determined by a moving average targeting an average number of blocks per hour. If they're generated too fast, the difficulty increases. Network The steps to run the network are as follows: New transactions are broadcast to all nodes. Each node collects new transactions into a block.",
      "Each node works on finding a difficult proof-of-work for its block. When a node finds a proof-of-work, it broadcasts the block to all nodes. Nodes accept the block only if all transactions in it are valid and not already spent. Nodes express their acceptance of the block by working on creating the next block in the chain, using the hash of the accepted block as the previous hash. Nodes always consider the longest chain to be the correct one and will keep working on extending it. If two nodes broadcast different versions of the next block simultaneously, some nodes may receive one or the other first. In that case, they work on the first one they received, but save the other branch in case it becomes longer. The tie will be broken when the next proof- of-work is found and one branch becomes longer; the nodes that were working on the other branch will then switch to the longer one.",
      "Block Prev Hash Nonce Block Prev Hash Nonce New transaction broadcasts do not necessarily need to reach all nodes. As long as they reach many nodes, they will get into a block before long. Block broadcasts are also tolerant of dropped messages. If a node does not receive a block, it will request it when it receives the next block and realizes it missed one. Incentive By convention, the first transaction in a block is a special transaction that starts a new coin owned by the creator of the block. This adds an incentive for nodes to support the network, and provides a way to initially distribute coins into circulation, since there is no central authority to issue them. The steady addition of a constant of amount of new coins is analogous to gold miners expending resources to add gold to circulation. In our case, it is CPU time and electricity that is expended. The incentive can also be funded with transaction fees.",
      "If the output value of a transaction is less than its input value, the difference is a transaction fee that is added to the incentive value of the block containing the transaction. Once a predetermined number of coins have entered circulation, the incentive can transition entirely to transaction fees and be completely inflation free. The incentive may help encourage nodes to stay honest. If a greedy attacker is able to assemble more CPU power than all the honest nodes, he would have to choose between using it to defraud people by stealing back his payments, or using it to generate new coins. He ought to find it more profitable to play by the rules, such rules that favour him with more new coins than everyone else combined, than to undermine the system and the validity of his own wealth. Reclaiming Disk Space Once the latest transaction in a coin is buried under enough blocks, the spent transactions before it can be discarded to save disk space.",
      "To facilitate this without breaking the block's hash, transactions are hashed in a Merkle Tree 725, with only the root included in the block's hash. Old blocks can then be compacted by stubbing off branches of the tree. The interior hashes do not need to be stored. A block header with no transactions would be about 80 bytes. If we suppose blocks are generated every 10 minutes, 80 bytes  6  24  365  4.2MB per year. With computer systems typically selling with 2GB of RAM as of 2008, and Moore's Law predicting current growth of 1.2GB per year, storage should not be a problem even if the block headers must be kept in memory. Block Block Block Header (Block Hash) Prev Hash Nonce Hash01 Hash0 Hash1 Hash2 Hash3 Hash23 Root Hash Hash01 Hash2 Hash23 Block Header (Block Hash) Root Hash Transactions Hashed in a Merkle Tree After Pruning Tx0-2 from the Block Prev Hash Nonce Hash3 Simplified Payment Verification It is possible to verify payments without running a full network node.",
      "A user only needs to keep a copy of the block headers of the longest proof-of-work chain, which he can get by querying network nodes until he's convinced he has the longest chain, and obtain the Merkle branch linking the transaction to the block it's timestamped in. He can't check the transaction for himself, but by linking it to a place in the chain, he can see that a network node has accepted it, and blocks added after it further confirm the network has accepted it. As such, the verification is reliable as long as honest nodes control the network, but is more vulnerable if the network is overpowered by an attacker. While network nodes can verify transactions for themselves, the simplified method can be fooled by an attacker's fabricated transactions for as long as the attacker can continue to overpower the network.",
      "One strategy to protect against this would be to accept alerts from network nodes when they detect an invalid block, prompting the user's software to download the full block and alerted transactions to confirm the inconsistency. Businesses that receive frequent payments will probably still want to run their own nodes for more independent security and quicker verification. Combining and Splitting Value Although it would be possible to handle coins individually, it would be unwieldy to make a separate transaction for every cent in a transfer. To allow value to be split and combined, transactions contain multiple inputs and outputs. Normally there will be either a single input from a larger previous transaction or multiple inputs combining smaller amounts, and at most two outputs: one for the payment, and one returning the change, if any, back to the sender.",
      "It should be noted that fan-out, where a transaction depends on several transactions, and those transactions depend on many more, is not a problem here. There is never the need to extract a complete standalone copy of a transaction's history. Transaction Hash01 Hash2 Hash3 Hash23 Block Header Merkle Root Prev Hash Nonce Block Header Merkle Root Prev Hash Nonce Block Header Merkle Root Prev Hash Nonce Merkle Branch for Tx3 Longest Proof-of-Work Chain 10. Privacy The traditional banking model achieves a level of privacy by limiting access to information to the parties involved and the trusted third party. The necessity to announce all transactions publicly precludes this method, but privacy can still be maintained by breaking the flow of information in another place: by keeping public keys anonymous. The public can see that someone is sending an amount to someone else, but without information linking the transaction to anyone.",
      "This is similar to the level of information released by stock exchanges, where the time and size of individual trades, the \"tape\", is made public, but without telling who the parties were. As an additional firewall, a new key pair should be used for each transaction to keep them from being linked to a common owner. Some linking is still unavoidable with multi-input transactions, which necessarily reveal that their inputs were owned by the same owner. The risk is that if the owner of a key is revealed, linking could reveal other transactions that belonged to the same owner. 11. Calculations We consider the scenario of an attacker trying to generate an alternate chain faster than the honest chain. Even if this is accomplished, it does not throw the system open to arbitrary changes, such as creating value out of thin air or taking money that never belonged to the attacker.",
      "Nodes are not going to accept an invalid transaction as payment, and honest nodes will never accept a block containing them. An attacker can only try to change one of his own transactions to take back money he recently spent. The race between the honest chain and an attacker chain can be characterized as a Binomial Random Walk. The success event is the honest chain being extended by one block, increasing its lead by 1, and the failure event is the attacker's chain being extended by one block, reducing the gap by -1. The probability of an attacker catching up from a given deficit is analogous to a Gambler's Ruin problem. Suppose a gambler with unlimited credit starts at a deficit and plays potentially an infinite number of trials to try to reach breakeven.",
      "We can calculate the probability he ever reaches breakeven, or that an attacker ever catches up with the honest chain, as follows 8: p  probability an honest node finds the next block q  probability the attacker finds the next block qz  probability the attacker will ever catch up from z blocks behind q z if pq q p if pq Identities Transactions Trusted Third Party Counterparty Public Identities Transactions Public New Privacy Model Traditional Privacy Model Given our assumption that p  q, the probability drops exponentially as the number of blocks the attacker has to catch up with increases. With the odds against him, if he doesn't make a lucky lunge forward early on, his chances become vanishingly small as he falls further behind. We now consider how long the recipient of a new transaction needs to wait before being sufficiently certain the sender can't change the transaction.",
      "We assume the sender is an attacker who wants to make the recipient believe he paid him for a while, then switch it to pay back to himself after some time has passed. The receiver will be alerted when that happens, but the sender hopes it will be too late. The receiver generates a new key pair and gives the public key to the sender shortly before signing. This prevents the sender from preparing a chain of blocks ahead of time by working on it continuously until he is lucky enough to get far enough ahead, then executing the transaction at that moment. Once the transaction is sent, the dishonest sender starts working in secret on a parallel chain containing an alternate version of his transaction. The recipient waits until the transaction has been added to a block and z blocks have been linked after it.",
      "He doesn't know the exact amount of progress the attacker has made, but assuming the honest blocks took the average expected time per block, the attacker's potential progress will be a Poisson distribution with expected value: z q To get the probability the attacker could still catch up now, we multiply the Poisson density for each amount of progress he could have made by the probability he could catch up from that point: k! q p zk if kz if kz Rearranging to avoid summing the infinite tail of the distribution... 1q p zk Converting to C code... include math.h double AttackerSuccessProbability(double q, int z) double p  1.0 - q; double lambda  z  (q  p); double sum  1.0; int i, k; for (k  0; k  z; k) double poisson  exp(-lambda); for (i  1; i  k; i) poisson  lambda  i; sum - poisson  (1 - pow(q  p, z - k)); return sum; Running some results, we can see the probability drop off exponentially with z.",
      "q0.1 z0 P1.0000000 z1 P0.2045873 z2 P0.0509779 z3 P0.0131722 z4 P0.0034552 z5 P0.0009137 z6 P0.0002428 z7 P0.0000647 z8 P0.0000173 z9 P0.0000046 z10 P0.0000012 q0.3 z0 P1.0000000 z5 P0.1773523 z10 P0.0416605 z15 P0.0101008 z20 P0.0024804 z25 P0.0006132 z30 P0.0001522 z35 P0.0000379 z40 P0.0000095 z45 P0.0000024 z50 P0.0000006 Solving for P less than 0.1... P  0.001 q0.10 z5 q0.15 z8 q0.20 z11 q0.25 z15 q0.30 z24 q0.35 z41 q0.40 z89 q0.45 z340 12. Conclusion We have proposed a system for electronic transactions without relying on trust. We started with the usual framework of coins made from digital signatures, which provides strong control of ownership, but is incomplete without a way to prevent double-spending. To solve this, we proposed a peer-to-peer network using proof-of-work to record a public history of transactions that quickly becomes computationally impractical for an attacker to change if honest nodes control a majority of CPU power.",
      "The network is robust in its unstructured simplicity. Nodes work all at once with little coordination. They do not need to be identified, since messages are not routed to any particular place and only need to be delivered on a best effort basis. Nodes can leave and rejoin the network at will, accepting the proof-of-work chain as proof of what happened while they were gone. They vote with their CPU power, expressing their acceptance of valid blocks by working on extending them and rejecting invalid blocks by refusing to work on them. Any needed rules and incentives can be enforced with this consensus mechanism. W. Dai, \"b-money,\" http:www.weidai.combmoney.txt, 1998. H. Massias, X.S. Avila, and J.-J. Quisquater, \"Design of a secure timestamping service with minimal trust requirements,\" In 20th Symposium on Information Theory in the Benelux, May 1999. S. Haber, W.S. Stornetta, \"How to time-stamp a digital document,\" In Journal of Cryptology, vol 3, no 2, pages 99-111, 1991. D. Bayer, S.",
      "Haber, W.S. Stornetta, \"Improving the efficiency and reliability of digital time-stamping,\" In Sequences II: Methods in Communication, Security and Computer Science, pages 329-334, 1993. S. Haber, W.S. Stornetta, \"Secure names for bit-strings,\" In Proceedings of the 4th ACM Conference on Computer and Communications Security, pages 28-35, April 1997. A. Back, \"Hashcash - a denial of service counter-measure,\" R.C. Merkle, \"Protocols for public key cryptosystems,\" In Proc. 1980 Symposium on Security and Privacy, IEEE Computer Society, pages 122-133, April 1980. W. Feller, \"An introduction to probability theory and its applications,\" 1957."
    ],
    "word_count": 3452,
    "page_count": 9
  },
  "COMP": {
    "chunks": [
      "6142019 Compound Whitepaper - Google Docs Compound: The Money Market Protocol Version 1.0 February 2019 Authors Robert Leshner, Geoffrey Hayes Abstract In this paper we introduce a decentralized protocol which establishes money markets with algorithmically set interest rates based on supply and demand, allowing users to frictionlessly exchange the time value of Ethereum assets.",
      "Contents 1 Introduction 2 The Compound Protocol 2.1 Supplying Assets 2.1.1 Primary Use Cases 2.2 Borrowing Assets 2.2.1 Collateral Value 2.2.2 Risk  Liquidation 2.2.3 Primary Use Cases 2.3 Interest Rate Model 2.3.1 Liquidity Incentive Structure 3 Implementation  Architecture 3.1 cToken Contracts 3.2 Interest Rate Mechanics 3.2.1 Market Dynamics 3.2.2 Borrower Dynamics 3.3 Borrowing 3.4 Liquidation 3.5 Price Feeds 3.6 Comptroller 3.7 Governance 4 Summary 6142019 Compound Whitepaper - Google Docs Introduction The market for cryptocurrencies and digital blockchain assets has developed into a vibrant ecosystem of investors, speculators, and traders, exchanging thousands 1 of blockchain assets. Unfortunately, the sophistication of financial markets hasnt followed: participants have little capability of trading the time value of assets.",
      "Interest rates fill the gap between people with surplus assets they cant use, and people without assets (that have a productive or investment use); trading the time value of assets benefits both parties, and creates non-zero-sum wealth. For blockchain assets, two major flaws exist today: Borrowing mechanisms are extremely limited, which contributes to mispriced assets (e.g. scamcoins with unfathomable valuations, because theres no way to short them). Blockchain assets have negative yield, resulting from significant storage costs and risks (both on-exchange and off-exchange), without natural interest rates to offset those costs. This contributes to volatility, as holding is disincentivized. Centralized exchanges (including Bitfinex, Poloniex...) allow customers to trade blockchain assets on margin, with borrowing markets built into the exchange.",
      "These are trust-based systems (you have to trust that the exchange wont get hacked, abscond with your assets, or incorrectly close out your position), are limited to certain customer groups, and limited to a small number of (the most mainstream) assets. Finally, balances and positions are virtual; you cant move a position on-chain, for example to use borrowed Ether or tokens in a smart contract or ICO, making these facilities inaccessible to dApps 2. Peer to peer protocols facilitate collateralized and uncollateralized loans between market participants directly. Unfortunately, decentralization forces significant costs and frictions onto users; in every protocol reviewed, lenders are required to post, manage, and (in the event of collateralized loans) supervise loan offers and active loans, and loan fulfillment is often slow  asynchronous (loans have to be funded, which takes time) 3-6.",
      "In this paper, we introduce a decentralized system for the frictionless borrowing of Ethereum tokens without the flaws of existing approaches, enabling proper money markets to function, and creating a safe positive-yield approach to storing assets. The Compound Protocol Compound is a protocol on the Ethereum blockchain that establishes money markets, which are pools of assets with algorithmically derived interest rates, based on the supply and demand for the asset. Suppliers (and borrowers) of an asset interact directly with the protocol, earning (and paying) a floating interest rate, without having to negotiate terms such as maturity, interest rate, or collateral with a peer or counterparty.",
      "6142019 Compound Whitepaper - Google Docs Each money market is unique to an Ethereum asset (such as Ether, an ERC-20 stablecoin such as Dai, or an ERC-20 utility token such as Augur), and contains a transparent and publicly-inspectable ledger, with a record of all transactions and historical interest rates. Supplying Assets Unlike an exchange or peer-to-peer platform, where a users assets are matched and lent to another user, the Compound protocol aggregates the supply of each user; when a user supplies an asset, it becomes a fungible resource. This approach offers significantly more liquidity than direct lending; unless every asset in a market is borrowed (see below: the protocol incentivizes liquidity), users can withdraw their assets at any time, without waiting for a specific loan to mature. Assets supplied to a market are represented by an ERC-20 token balance (cToken), which entitles the owner to an increasing quantity of the underlying asset.",
      "As the money market accrues interest, which is a function of borrowing demand, cTokens become convertible into an increasing amount of the underlying asset. In this way, earning interest is as simple as holding a ERC-20 cToken. 2.1.1 Primary Use Cases Individuals with long-term investments in Ether and tokens (HODLers) can use a Compound money market as a source of additional returns on their investment. For example, a user that owns Augur can supply their tokens to the Compound protocol, and earn interest (denominated in Augur) without having to manage their asset, fulfill loan requests or take speculative risks. dApps, machines, and exchanges with token balances can use the Compound protocol as a source of monetization and incremental returns by sweeping balances; this has the potential to unlock entirely new business models for the Ethereum ecosystem.",
      "Borrowing Assets Compound allows users to frictionlessly borrow from the protocol, using cTokens as collateral, for use anywhere in the Ethereum ecosystem. Unlike peer-to-peer protocols, borrowing from Compound simply requires a user to specify a desired asset; there are no terms to negotiate, maturity dates, or funding periods; borrowing is instant and predictable. Similar to supplying an asset, each money market has a floating interest rate, set by market forces, which determines the borrowing cost for each asset. 2.2.1 Collateral Value Assets held by the protocol (represented by ownership of a cToken) are used as collateral to borrow from the protocol. Each market has a collateral factor, ranging from 0 to 1, that represents the portion of the underlying asset value that can be borrowed. Illiquid, small-cap assets have low collateral factors; they do not make good collateral, while liquid, high-cap assets have high collateral 6142019 Compound Whitepaper - Google Docs factors.",
      "The sum of the value of an accounts underlying token balances, multiplied by the collateral factors, equals a users borrowing capacity . Users are able to borrow up to, but not exceeding, their borrowing capacity, and an account can take no action (e.g. borrow, transfer cToken collateral, or redeem cToken collateral) that would raise the total value of borrowed assets above their borrowing capacity; this protects the protocol from default risk. 2.2.2 Risk  Liquidation If the value of an accounts borrowing outstanding exceeds their borrowing capacity, a portion of the outstanding borrowing may be repaid in exchange for the users cToken collateral, at the current market price minus a liquidation discount ; this incentives an ecosystem of arbitrageurs to quickly step in to reduce the borrowers exposure, and eliminate the protocols risk. The proportion eligible to be closed, a close factor , is the portion of the borrowed asset that can be repaid, and ranges from 0 to 1, such as 25.",
      "The liquidation process may continue to be called until the users borrowing is less than their borrowing capacity. Any Ethereum address that possesses the borrowed asset may invoke the liquidation function, exchanging their asset for the borrowers cToken collateral. As both users, both assets, and prices are all contained within the Compound protocol, liquidation is frictionless and does not rely on any outside systems or order-books.",
      "2.2.3 Primary Use Cases The ability to seamlessly hold new assets (without selling or rearranging a portfolio) gives new superpowers to dApp consumers, traders and developers: Without having to wait for an order to fill, or requiring off-chain behavior, dApps can borrow tokens to use in the Ethereum ecosystem, such as to purchase computing power on the Golem network Traders can finance new ICO investments by borrowing Ether, using their existing portfolio as collateral Traders looking to short a token can borrow it, send it to an exchange and sell the token, profiting from declines in overvalued tokens Interest Rate Model Rather than individual suppliers or borrowers having to negotiate over terms and rates, the Compound protocol utilizes an interest rate model that achieves an interest rate equilibrium, in each money market, based on supply and demand.",
      "Following economic theory, interest rates (the price of money) should increase as a function of demand; when demand is low, interest rates 6142019 Compound Whitepaper - Google Docs should be low, and vise versa when demand is high. The utilization ratio U for each market a unifies supply and demand into a single variable: orrows (Cash orrows ) U a  B a  B The demand curve is codified through governance and is expressed as a function of utilization. As an example, borrowing interest rates may resemble the following: orrowing Interest Rate 2.5  U a  2 The interest rate earned by suppliers is implicit , and is equal to the borrowing interest rate, multiplied by the utilization rate. 2.3.1 Liquidity Incentive Structure The protocol does not guarantee liquidity; instead, it relies on the interest rate model to incentivize it.",
      "In periods of extreme demand for an asset, the liquidity of the protocol (the tokens available to withdraw or borrow) will decline; when this occur, interest rates rise, incentivizing supply, and disincentivizing borrowing. Implementation  Architecture At its core, a Compound money market is a ledger that allows Ethereum accounts to supply or borrow assets, while computing interest, a function of time. The protocols smart contracts will be publicly accessible and completely free to use for machines, dApps and humans. cToken Contracts Each money market is structured as a smart contract that implements the ERC-20 token specification. Users balances are represented as cToken balances; users can mint(uint amountUnderlying) cTokens by supplying assets to the market, or redeem(uint amount) cTokens for the underlying asset.",
      "The price (exchange rate) between cTokens and the underlying asset increases over time, as interest is accrued by borrowers of the asset, and is equal to: xchangeRate cTokenSupplya underlyingBalance totalBorrowBalance  reserves As the markets total borrowing balance increases (as a function of borrower interest accruing), the exchange rate between cTokens and the underlying asset increases. Function ABI Description mint(uint256 amountUnderlying) Transfers an underlying asset into the market, updates msg.senders cToken balance. 6142019 Compound Whitepaper - Google Docs redeem(uint256 amount) redeemUnderlying(uint256 amountUnderlying) Transfers an underlying asset out of the market, updates msg.senders cToken balance. borrow(uint amount) Checks msg.sender collateral value, and if sufficient, transfers the underlying asset out of the market to msg.sender, and updates msg.senders borrow balance.",
      "repayBorrow(uint amount) repayBorrowBehalf(address account, uint amount) Transfers the underlying asset into the market, updates the borrowers borrow balance. liquidate(address borrower, address collateralAsset, uint closeAmount) Transfers the underlying asset into the market, updates the borrowers borrow balance, then transfers cToken collateral from the borrower to msg.sender Table 2. ABI and summary of primary cToken smart contract functions Interest Rate Mechanics Compound money markets are defined by an interest rate, applied to all borrowers uniformly, which adjust over time as the relationship between supply and demand changes. The history of each interest rate, for each money market, is captured by an Interest Rate Index , which is calculated each time an interest rate changes, resulting from a user minting, redeeming, borrowing, repaying or liquidating the asset.",
      "3.2.1 Market Dynamics Each time a transaction occurs, the Interest Rate Index for the asset is updated to compound the interest since the prior index, using the interest for the period, denominated by r  t, calculated using a per-block interest rate: ndex ndex a,n  I a,(n1)  (  r  t The markets total borrowing outstanding is updated to include interest accrued since the last index: otalBorrowBalance otalBorrowBalance a,n  t a,(n1)  (  r  t And a portion of the accrued interest is retained (set aside) as reserves, determined by a reserveFactor , ranging from 0 to 1: eserves eserves otalBorrowBalance eserveFactor) a  r a,(n1)  t a,(n1)  (  t  r 3.2.2 Borrower Dynamics A borrowers balance, including accrued interest, is simply the ratio of the current index divided by the index when the users balance was last checkpointed. 6142019 Compound Whitepaper - Google Docs The balance for each borrower address in the cToken is stored as an account checkpoint .",
      "An account checkpoint is a Solidity tuple uint256 balance, uint256 interestIndex . This tuple describes the balance at the time interest was last applied to that account. Borrowing A user who wishes to borrow and who has sufficient balances stored in Compound may call borrow(uint amount) on the relevant cToken contract. This function call checks the users account value, and given sufficient collateral, will update the users borrow balance, transfer the tokens to the users Ethereum address, and update the money markets floating interest rate. Borrows accrue interest in the exact same fashion as balance interest was calculated in section 3.2; a borrower has the right to repay an outstanding loan at any time, by calling repayBorrow(uint amount) which repays the outstanding balance.",
      "Liquidation If a users borrowing balance exceeds their total collateral value (borrowing capacity) due to the value of collateral falling, or borrowed assets increasing in value, the public function liquidate(address target, address collateralAsset, address borrowAsset, uint closeAmount) can be called, which exchanges the invoking users asset for the borrowers collateral, at a slightly better than market price. Price Feeds A Price Oracle maintains the current exchange rate of each supported asset; the Compound protocol delegates the ability to set the value of assets to a committee which pools prices from the top 10 exchanges. These exchange rates are used to determine borrowing capacity and collateral requirements, and for all functions which require calculating the value equivalent of an account. Comptroller The Compound protocol does not support specific tokens by default; instead, markets must be whitelisted.",
      "This is accomplished with an admin function, supportMarket(address market, address interest rate model) that allows users to begin interacting with the asset. In order to borrow an asset, there must be a valid price from the Price Oracle; in order to use an asset as collateral, there must be a valid price and a collateralFactor. Each function call is validated through a policy layer, referred to as the Comptroller ; this contract validates collateral and liquidity, before allowing a user action to proceed. 6142019 Compound Whitepaper - Google Docs Governance Compound will begin with centralized control of the protocol (such as choosing the interest rate model per asset), and over time, will transition to complete community and stakeholder control.",
      "The following rights in the protocol are controlled by the admin: The ability to list a new cToken market The ability to update the interest rate model per market The ability to update the oracle address The ability to withdraw the reserve of a cToken The ability to choose a new admin, such as a DAO controlled by the community; because this DAO can itself choose a new admin, the administration has the ability to evolve over time, based on the decisions of the stakeholders Summary Compound creates properly functioning money markets for Ethereum assets Each money market has interest rates that are determined by the supply and demand of the underlying asset; when demand to borrow an asset grows, or when supply is removed, interest rates increase, incentivizing additional liquidity Users can supply tokens to a money market to earn interest, without trusting a central party Users can borrow a token (to use, sell, or re-lend) by using their balances in the protocol as collateral 1 Cryptocurrency Market Capitalizations.",
      "https:coinmarketcap.com 2 Bitfixex Margin Funding Guide. https:support.bitfinex.com 3 ETHLend White Paper. https:github.comETHLend 4 Ripio White Paper. https:ripiocredit.network 5 Lendroid White Paper. https:lendroid.com 6 dYdX White Paper. https:whitepaper.dydx.exchange 7 Fred Ehrsam: The Decentralized Business Model. https:blog.coinbase.com"
    ],
    "word_count": 2573,
    "page_count": 8
  },
  "CRV": {
    "chunks": [
      "StableSwap - e\ufb03cient mechanism for Stablecoin liquidity Michael Egorov November 10, 2019 Abstract StableSwap provides a mechanism to create cross-markets for stable- coins in a way which could be called Uniswap with leverage. It is a fully autonomous market-maker for stablecoins with very minimal price slip- page, as well as an e\ufb03cient \ufb01at savings account for liquidity providers on the other side. This is a brief version which doesnt show all the details (most notably, the StableSwap invariant). Introduction Stablecoins become very popular recently: custodial USDC, USDT, BUSD, PAX, TrueUSD, as well as decentralized DAI. They however (especially decen- tralized ones) have a problem of price stability and liquidity. This is especially painful for DeFi arbitrage.",
      "For example, when MakerDAO decreased its stabil- ity fee to 5.5, many users of Compound (which had the interest rate of 11 at the time) preferred to stay there because theyve taken the loan in DAI, and converting between DAI and USDC is an expensive task. At the same time, many DeFi users are willing to load their stablecoins up for lending in order to earn 5 APR, as it is much more than what traditional banking o\ufb00ers. They, however, would be uncomfortable giving same money to trading \ufb01rms who promise pro\ufb01ts. In this work, I introduce StableSwap - automated liquidity provider for sta- blecoins. On the demand side, it o\ufb00ers a Uniswap-like automated exchange with very low price slippage (typically 100 times smaller).",
      "On the supply side, it o\ufb00ers a multi-stablecoin savings account which, according to simulation, can bring 300 APR, according to simulations assuming that traders will arbitrage between the smart contract and existing exchanges, taking into account their trading volumes and prices for stablecoins for the past half a year. This happens with no middleman being responsible for the trading, e.g. no exchange owners, no orderbooks, no human market makers. How it works First of all, imagine a liquidity provider which has constant price. If you have two coins X and Y , for example, selling dx of coin X will lead to buying dy  dx of coin Y . This can be generalized for any number of coins Xi having a linear invariant: xi  const. The price is determined as dxidxj which is, in this case, always exactly 1. This doesnt work in a \ufb02uctuating market unless the price is adjusted all the time. It can be done with price oracles, but it has risks and not very decentralized. Its possible to do better.",
      "Uniswap, Bancor and Kyber work with inherently volatile and price-unstable markets, so they do it di\ufb00erently. They adjust prices in such a way that the portfolio (which is usually just two coins) is rebalances (so that value of coin X and Y in the liquidity pool, when expressed in the same currency, is the same). It appears, that this is given automatically when you keep product of quantities of coins in the liquidity pool constant: xy  const. Moreover, it is possible to generalize this invariant to any number of coins with any rations, as was brilliantly done by Balancer:  const. While this is suitable for assets like ETH and tokens, its not very well working for something which is meant to be stable. The problem is that the price slippage is enormous, and one should provide enormous funds to keep a meaningful liquidity. On the \ufb02ip side, if one for example loads DAI and USDC into Uniswaps liquidity pool, the returns will be tiny (perhaps, several percent per year).",
      "For StableSwap, there was a middle-ground invariant found (Fig. 1). As expected, the price (equal to derivative) only slightly deviates from 1 when number of coins is closed to balance. x () 10.0 12.5 15.0 17.5 20.0 y () Uniswap invariant (xy  const) Constant price  1.0 (x  y  const) Stableswap invariant Figure 1: Comparison of StableSwap invariant with Uniswap (constant-product) and constant price invariants. The portfolio consists of coins X and Y which have the ideal price of 1.0. There are x  5 and y  5 coins loaded up initially. As x decreases, y increases, and the price is the derivative dydx. The price slippage (Fig. 2) is much smaller, if compared to constant-product invariant. The StableSwap invariant has an ampli\ufb01cation coe\ufb03cient parameter: the lower it is, the closer the invariant is to the constant product. When calculating slippage, we use a practical value of A  100. This is somewhat comparable to using Uniswap with 100x leverage.",
      "dx () price Stableswap Uniswap Figure 2: Price slippage: Uniswap invariant (dashed line) vs Stableswap (solid line) If the price appears to be shifted from equilibrium point (1.0), the invariant starts operating in a suboptimal point, still however providing some liquidity (in most cases larger than constant-product invariant, if optimal A was correctly found). At any price, this invariant, just like a constant-product one, would provide some liquidity (unlike the constant-sum invariant). Constructing the StableSwap invariant As depicted in Fig. 1, the constant-price invariant forms a straight line (or a hypersurface if having more than two coins). A constant-product invariant forms a hyperbola. The price is a slope of the line on the graph.",
      "We are looking for some invari- ant which is relatively \ufb02at near balance (price changes slowly, the graph is very close to the straight line, likely a zoomed in hyperbola), however shifting to- wards the constant-product invariant as the portfolio becomes more imbalanced (e.g. closer to the axes). Here are constant-sum (constant-price) and constant-product invariants gen- eralized for n coins, enumerated by i: xi  D; xi  The constant D has a meaning of total amount of coins when they have an equal price. Lets imagine what would an ampli\ufb01ed invariant be. It should have a small curvature to have a low price slippage. A zero slippage invariant would cor- respond to in\ufb01nite leverage. However, the zero-slippage invariant is a constant- price, or constant-sum one! Hence, assuming that constant-product has a zero leverage, and constant sum has an in\ufb01nite leverage, lets construct something in between. Lets denote the leverage \u03c7.",
      "If we multiply the constant-sum invari- ant by \u03c7 and add it to the constant-product one, we will have an invariant which is constant-product when \u03c7  0, and constant-sum when \u03c7  : the property we are looking for. However, \u03c7 should ideally be a dimensionless parameter, not depending on numbers of coins we have. Therefore, lets multiply the constant-sum invariant by \u03c7Dn1 and add to the second invariant: \u03c7Dn1 X xi  xi  \u03c7Dn  If this equation holds at all times, we will have trades with a leverage \u03c7. How- ever, it wouldnt support prices going far from the ideal price 1.0. The invariant should support any prices (so that we have some liquidity at all times). In order to do so, we make \u03c7 dynamic. When the portfolio is in a perfect balance, its equal to a constant A, however falls o\ufb00to 0 when going out of balance: \u03c7  A Q xi (Dn)n .",
      "Substituting this to the leveraged invariant above, we come to the StableSwap invariant: Ann X xi  D  ADnn  Dn1 nn Q xi When a portfolio of coins xi is loaded up, we need to calculate D, and we need to hold this equation true when we perform trades (e.g. swap xi into xj). That is done by \ufb01nding an iterative, converging solution either for D, or for xj when all other variables are known. Simulations and performance The performance of the algorithm was evaluated and optimized assuming provid- ing liquidity for 3 stablecoins (DAI, USDC and USTD) taking price feeds from Coinbase Pro (DAIUSDC), Binance (USDCUSDT) and HitBtc (USDTDAI) over the period of 6 months (May - October 2019). The simulations assumed the total liquidity in the contract of 30000. Trades were only done if there was enough volume in the price change. The results were the following:  Optimial ampli\ufb01cation coe\ufb03cient (leverage): A  85;  Optimal fee: 0.06 per trade;  Liquidity provider pro\ufb01t at optimal parameters: 312 APR.",
      "Implementation Multi-stablecoin contract was implemented in Vyper. Solutions of the equations which use the stableswap invariant were obtained iteratiely inside the smart con- tract itself, using only integer arithmetics. Browser UI (Fig. 3) was implemented in pure client-side Javascript. Other applications Apart from liquidity for stablecoins, the same method can be applied for provid- ing liquidity to interest-bearing assets (cDAI) and tokenized stake for stakeable cryptocurrencies. In my opinion, the method is an important part of future DeFi infrastructure. Applying this method to stablecoins can get it battle-tested, and to increase usability of decentralized (non-custodial) stablecoins. Figure 3: Stableswap UI"
    ],
    "word_count": 1424,
    "page_count": 6
  },
  "DOT": {
    "chunks": [
      "This is translated from PolkaDotPaper(https:polkadot.networkPolkaDotPaper.pdf) in the Summer of 2017."
    ],
    "word_count": 10,
    "page_count": 1
  },
  "EGLD": {
    "chunks": [
      "MultiversX A Highly Scalable Public Blockchain via Adaptive State Sharding and Secure Proof of Stake Technical whitepaper  release 2  revision 2 Updated: renaming to MultiversX June 19, 2019 - The MultiversX Team AbstractThe advent of secure public blockchains through Bitcoin and later Ethereum, has brought forth a notable degree of interest and capital influx, providing the premise for a global wave of permissionless innovation. Despite lofty promises, creating a decentralized, secure and scalable public blockchain has proved to be a strenuous task. This paper proposes MultiversX, a novel architecture which goes beyond state of the art by introducing a genuine state sharding scheme for practical scalability, eliminating energy and computational waste while ensuring distributed fairness through a Secure Proof of Stake (SPoS) consensus.",
      "Having a strong focus on security, MultiversX network is built to ensure resistance to known security problems like Sybil attack, Nothing at Stake attack and others. In an ecosystem that strives for interconnectivity, our solution for smart contracts offers an EVM compliant engine to ensure interoperability by design. Preliminary simulations and testnet results reflect that Mul- tiversX exceeds Visas average throughput and achieves an im- provement beyond three orders of magnitude or 1000x compared to the existing viable approaches, while drastically reducing the costs of bootstrapping and storage to ensure long term sustainability. I Introduction 1 General aspects Cryptocurrency and smart contract platforms such as Bit- coin and Ethereum have sparked considerable interest and have become promising solutions for electronic payments, decentralized applications and potential digital stores of value.",
      "However, when compared to their centralized counterparts in key metrics 1, the current state of affairs suggests that present public blockchain iterations exhibit severe limitations, particularly with respect to scalability, hindering their main- stream adoption and delaying public use. In fact, it has proved extremely challenging to deal with the current engi- neering boundaries imposed by the trade-offs in the blockchain trilemma paradigm 2. Several solutions have been proposed, but few of them have shown significant and viable results. Thus, in order to solve the scalability problem, a complete rethinking of public blockchain infrastructures was required.",
      "2 Defining the challenges Several challenges must be addressed properly in the pro- cess of creating an innovative public blockchain solution designed to scale:  Full decentralization - Eliminating the need for any trusted third party, hence removing any single point of failure;  Robust security - Allowing secure transactions and preventing any attacks based on known attack vectors;  High scalability - Enabling the network to achieve a performance at least equal to the centralized counterpart, as measured in TPS;  Efficiency - Performing all network services with mini- mal energy and computational requirements;  Bootstrapping and storage enhancement - Ensuring a competitive cost for data storage and synchronization;  Cross-chain interoperability - Enforced by design, per- mitting unlimited communication with external services.",
      "Starting from the above challenges, weve created Multi- versX as a complete rethinking of public blockchain infras- tructure, specifically designed to be secure, efficient, scalable and interoperable. MultiversX main contribution rests on two cornerstone building blocks: 1) A genuine State Sharding approach: effectively parti- tioning the blockchain and account state into multiple shards, handled in parallel by different participating validators; 2) Secure Proof of Stake consensus mechanism: an improved variation of Proof of Stake (PoS) that ensures long term security and distributed fairness, while elimi- nating the need for energy intensive PoW algorithms. 3 Adaptive State Sharding MultiversX proposes a dynamically adaptive sharding mech- anism that enables shard computation and reorganizing based on necessity and the number of active network nodes.",
      "The reassignment of nodes in the shards at the beginning of each epoch is progressive and nondeterministic, inducing no temporary liveness penalties. Adaptive state sharding comes with additional challenges compared to the static model. One of the key-points resides in how shard-splitting and shard- merging is done to prevent overall latency penalties introduced by the synchronizationcommunication needs when the shard number changes. Latency, in this case, is the communication overhead required by nodes, in order to retrieve the new state, once their shard address space assignment has been modified. MultiversX proposes a solution for this problem below, but first some notions have to be defined: users and nodes. Users are external actors and can be identified by an unique account address; nodes are computersdevices in the Multi- versX network that run our protocol. Notions like users, nodes, addresses will be further described in chapter II.1 - Entities.",
      "MultiversX solves this challenge by: 1) Dividing the account address space in shards, using a binary tree which can be built with the sole requirement of knowing the exact number of shards in a certain epoch. Using this method, the accumulated latency is reduced and the network liveness is improved in two ways. First, thanks to the designed model, the dividing of the account address space is predetermined by hierarchy. Hence, there is no split overhead, meaning that one shard breaks into two shards, each of them keeping only one half of the previous address space in addition to the associated state. Second, the latency is reduced through the state redundancy mechanism, as the merge is prepared by retaining the state in the sibling nodes. 2) Introducing a technique of balancing the nodes in each shard, to achieve overall architecture equilibrium. This technique ensures a balanced workload and reward for each node in the network.",
      "3) Designing a built-in mechanism for automatic transac- tion routing in the corresponding shards, considerably reduces latency as a result. The routing algorithm is de- scribed in chapter IV.4 - MultiversX sharding approach. 4) In order to achieve considerable improvements with respect to bootstrapping and storage, MultiversX makes use of a shard pruning mechanism. This ensures sus- tainability of our architecture even with a throughput of tens of thousands of transactions per second (TPS). 4 Secure Proof of Stake (SPoS) We introduce a Secure Proof of Stake consensus mecha- nism, that expands on Algorands 3 idea of a random se- lection mechanism, differentiating itself through the following aspects: 1) MultiversX introduces an improvement which reduces the latency allowing each node in the shard to determine the members of the consensus group (block proposer and validators) at the beginning of a round.",
      "This is possible because the randomization factor r is stored in every block and is created by the block proposer using a BLS signature 4 on the previous r. 2) The block proposer is the validator in the consensus group whos hash of the public key and randomization factor is the smallest. In contrast to Algorands 3 ap- proach, where the random committee selection can take up to 12 seconds, in MultiversX the time necessary for random selection of the consensus group is considerably reduced (estimated under 100 ms) excluding network latency. Indeed, there is no communication requirement for this random selection process, which enables Mul- tiversX to have a newly and randomly selected group that succeeds in committing a new block to the ledger in each round. The tradeoff for this enhancement relies on the premise that an adversary cannot adapt faster than the rounds time frame and can choose not to propose the block.",
      "A further improvement on the security of the randomness source, would be the use of verifiable delay functions (VDFs) in order to prevent any tampering possibilities of the randomness source until it is too late. Currently, the research in VDFs is still ongoing - there only a few working (and poorly tested) VDF implementations. 3) In addition to the stake factor generally used in PoS architectures as a sole decision input, MultiversX refines its consensus mechanism by adding an additional weight factor called rating. The nodes probability to be selected in the consensus group takes into consideration both stake and rating. The rating of a block proposer is recal- culated at the end of each epoch, except in cases where slashing should occur, when the actual rating decrease is done instantly, adding another layer of security by promoting meritocracy.",
      "4) A modified BLS multisignature scheme 5 with 2 communication rounds is used by the consensus group for block signing 5) MultiversX considers formal verification for the critical protocol implementations (e.g. SPoS consensus mecha- nism) in order to validate the correctness of our algo- rithms. II Architecture Overview 1 Entities There are two main entities in MultiversX: users and nodes. Users, each holding a (finite) number of public  private (Pksk) key pairs (e.g. in one or multiple wallet apps), use the MultiversX network to deploy signed transactions for value transfers or smart contracts execution. They can be identified by one of their account addresses (derived from the public key). The nodes are represented by the devices that form the MultiversX network and can be passive or actively engaged in processing tasks. Eligible validators are active participants in MultiversX network.",
      "Specifically, they are responsible for running consensus, adding blocks, maintaining the state and being rewarded for their contribution. Each eligible validator can be uniquely identified by a public key constructed through Fig. 1: Relations between MultiversX entities a derivation of the address that staked the necessary amount and the node id. Relations between entities in the MultiversX protocol are shown in Fig. 1. Furthermore, the network is divided into smaller units called shards. An eligible validator is assigned to a shard based on an algorithm that keeps the nodes evenly distributed across shards, depending on the tree level. Each shard contains a randomly selected consensus group. Any block proposer is responsible to aggregate transactions into a new block. The validators are responsible to either reject, or approve the proposed block, thereby validating it and committing it to the blockchain.",
      "2 Intrinsic token MultiversX grants access to the usage of its network through intrinsic utility token called eGold, in short EGLD. All costs for processing transactions, running smart contracts and rewards for various contributions to the network will be paid in EGLD. References to fees, payments or balances are assumed to be in EGLD. 3 Threat model MultiversX assumes a byzantine adversarial model, where at least 2 3n1 of the eligible nodes in a shard are honest. The protocol permits the existence of adversaries that have stake or good rating, delay or send conflicting messages, compromise other nodes, have bugs or collude among themselves, but as long as 2 3n1 of the eligible validators in a shard are honestnot compromised, the protocol can achieve consensus. The protocol assumes highly adaptive adversaries, which however cannot adapt faster than a rounds timeframe.",
      "The computational power of an adversary is bounded, therefore the cryptographic assumptions granted by the security level of the chosen primitives hold firmly within the complexity class of problems solvable by a Turing machine in polynomial time. The network of honest nodes is assumed to form a well connected graph and the propagation of their messages is done in a bounded time . Attack vectors prevention 1) Sybil attacks: mitigated through the stake locking when joining the network. This way the generation of new identities has a cost equal to the minimum stake; 2) Nothing at stake: removed through the need of multiple signatures, not just from proposer, and the stake slashing. The reward per block compared to the stake locked will discourage such behavior; 3) Long range attacks: mitigated by our pruning mech- anism, the use of a randomly selected consensus group every round (and not just a single proposer) and stake locking.",
      "On top of all these, our pBFT consensus algo- rithm ensures finality; 4) DDoS attacks: the consensus group is randomly sam- pled every round (few seconds); the small time frame making DDoS almost impossible. Other attack vectors we have taken into consideration are: shard takeover attack, transaction censorship, double spend, bribery attacks, etc. 4 Chronology In MultiversX network, the timeline is split into epochs and rounds. The epochs have a fixed duration, set to one day (can be modified as the architecture evolves), at the end of which the shards reorganization and pruning is triggered. The epochs are further divided into rounds, lasting for a fixed timeframe. A new consensus group is randomly selected per shard in each round, that can commit a maximum of one block in the shards ledger. New validators can join the network by locking their stake, as presented in chapter V.2 - Secure Proof of Stake.",
      "They are added to the unassigned node pool in the current epoch e, are assigned to the waiting list of a shard at the beginning of epoch e  1, but can only become eligible validators to participate in consensus and get rewarded in the next epoch e  2. The timeline aspects are further detailed in section IX.1. III Related Work MultiversX was designed upon and inspired by the ideas from Ethereum 6, Omniledger 7, Zilliqa 8, Algorand 3 and ChainSpace 9. Our architecture goes beyond state of the art and can be seen as an augmentation of the existing models, improving the performance while focusing to achieve a better nash equilibrium state between security, scalability and decentralization. 1 Ethereum Much of Ethereums 6 success can be attributed to the introduction of its decentralized applications layer through EVM 10, Solidity 11 and Web3j 12. While Dapps have been one of the core features of ethereum, scalability has proved a pressing limitation.",
      "Considerable research has been put into solving this problem, however results have been negligible up to this point. Still, few promising improvements are being proposed: Casper 13 prepares an update that will replace the current Proof of Work (PoW) consensus with a Proof of Stake (PoS), while Plasma based side-chains and sharding are expected to become available in the near future, alleviating Ethereums scalability problem at least partially 14. Compared to Ethereum, MultiversX eliminates both energy and computational waste from PoW algorithms by imple- menting a SPoS consensus while using transaction processing parallelism through sharding. 2 Omniledger Omniledger 7 proposes a novel scale-out distributed ledger that preserves long term security under permission-less op- eration. It ensures security and correctness by using a bias- resistant public-randomness protocol for choosing large, statis- tically representative shards that process transactions.",
      "To com- mit transactions atomically across shards, Omniledger intro- duces Atomix, an efficient cross-shard commit protocol. The concept is a two-phase client-driven lockunlock protocol that ensures that nodes can either fully commit a transaction across shards, or obtain rejection proofs to abort and unlock the state affected by partially completed transactions. Om- niledger also optimizes performance via parallel intra-shard transaction processing, ledger pruning via collectively-signed state blocks, and low-latency trust-but-verify validation for low-value transactions. The consensus used in Omniledger is a BFT variation, named ByzCoinX, that increases performance and robustness against DoS attacks. Compared to Omniledger, MultiversX has an adaptive ap- proach on state sharding, a faster random selection of the consensus group and an improved security by replacing the validators set after every round (a few seconds) not after every epoch (1 day).",
      "3 Zilliqa Zilliqa 8 is the first transaction-sharding architecture that allows the mining network to process transactions in parallel and reach a high throughput by dividing the mining network into shards. Specifically, its design allows a higher transaction rate as more nodes are joining the network. The key is to ensure that shards process different transactions, with no overlaps and therefore no double-spending. Zilliqa uses pBFT 15 for consensus and PoW to establish identities and prevent Sybil attacks. Compared to Zilliqa, MultiversX pushes the limits of shard- ing by using not only transaction sharding but also state shard- ing. MultiversX completely eliminates the PoW mechanism and uses SPoS for consensus. Both architectures are building their own smart contract engine, but MultiversX aims not only for EVM compliance, so that SC written for Ethereum will run seamlessly on our VM, but also aims to achieve interoperability between blockchains.",
      "4 Algorand Algorand 3 proposes a public ledger that keeps the con- venience and efficiency of centralized systems, without the inefficiencies and weaknesses of current decentralized imple- mentations. The leader and the set of verifiers are randomly chosen, based on their signature applied to the last blocks quantity value. The selections are immune to manipulations and unpredictable until the last moment. The consensus relies on a novel message-passing Byzantine Agreement that enables the community and the protocol to evolve without hard forks. Compared to Algorand, MultiversX doesnt have a single blockchain, instead it increases transactions throughput using sharding. MultiversX also improves on Algorands idea of ran- dom selection by reducing the selection time of the consensus group from over 12 seconds to less than a second, but assumes that the adversaries cannot adapt within a round.",
      "5 Chainspace Chainspace 9 is a distributed ledger platform for high integrity and transparent processing of transactions. It uses language agnostic and privacy-friendly smart contracts for extensibility. The sharded architecture allows a linearly scal- able transaction processing throughput using S-BAC, a novel distributed atomic commit protocol that guarantees consistency and offers high auditability. Privacy features are implemented through modern zero knowledge techniques, while the consen- sus is ensured by BFT. Compared to Chainspace, where the TPS decreases with each node added in a shard, MultiversX approach is not influenced by the number of nodes in a shard, because the con- sensus group has a fixed size. A strong point for Chainspace is the approach for language agnostic smart contracts, while MultiversX focuses on building an abstraction layer for EVM compliance. Both projects use different approaches for state sharding to enhance performance.",
      "However, MultiversX goes a step further by anticipating the blockchain size problem in high throughput architectures and uses an efficient pruning mechanism. Moreover, MultiversX exhibits a higher resistance to sudden changes in node population and malicious shard takeover by introducing shard redundancy, a new feature for sharded blockchains. IV Scalability via Adaptive State Sharding 1 Why sharding Sharding was first used in databases and is a method for dis- tributing data across multiple machines. This scaling technique can be used in blockchains to partition states and transaction processing, so that each node would process only a fraction of all transactions in parallel with other nodes. As long as there is a sufficient number of nodes verifying each transaction so that the system maintains high reliability and security, then splitting a blockchain into shards will allow it to process many transactions in parallel, and thus greatly improving transaction throughput and efficiency.",
      "Sharding promises to increase the throughput as the validator network expands, a property that is referred to as horizontal scaling. 2 Sharding types A comprehensive and thorough introduction 16 empha- sizes the three main types of sharding: network sharding, transaction sharding and state sharding. Network sharding handles the way the nodes are grouped into shards and can be used to optimize communication, as message propagation inside a shard can be done much faster than propagation to the entire network. This is the first challenge in every sharding approach and the mechanism that maps nodes to shards has to take into consideration the possible attacks from an attacker that gains control over a specific shard. Transaction sharding handles the way the transactions are mapped to the shards where they will be processed. In an account-based system, the transactions could be assigned to shards based on the senders address. State sharding is the most challenging approach.",
      "In contrast to the previously described sharding mechanisms, where all nodes store the entire state, in state- sharded blockchains, each shard maintains only a portion of the state. Every transaction handling accounts that are in different shards, would need to exchange messages and update states in different shards. In order to increase resiliency to malicious attacks, the nodes in the shards have to be reshuffled from time to time. However, moving nodes between shards introduces synchronization overheads, that is, the time taken for the newly added nodes to download the latest state. Thus, it is imperative that only a subset of all nodes should be redistributed during each epoch, to prevent down times during the synchronization process. 3 Sharding directions Some sharding proposals attempt to only shard transactions 8 or only shard state 17, which increases transactions throughput, either by forcing every node to store lots of state data or to be a supercomputer 2.",
      "Still, more recently, at least one claim has been made about successfully performing both transaction and state sharding, without compromising on storage or processing power 13. But sharding introduces some new challenges like: single- shard takeover attack, cross-shard communication, data avail- ability and the need of an abstraction layer that hides the shards. However, conditional on the fact that the above problems are addressed correctly, state sharding brings con- siderable overall improvements: transaction throughput will increase significantly due to parallel transaction processing and transaction fees will be considerably reduced. Two main criterias widely considered to be obstacles transforming into advantages and incentives for mainstream adoption of the blockchain technology.",
      "4 MultiversX sharding approach While dealing with the complexity of combining network, transaction and state sharding, MultiversX approach was designed with the following goals in mind: 1) Scalability without affecting availability: Increasing or decreasing the number of shards should affect a negligibly small vicinity of nodes without causing down- times, or minimizing them while updating states; 2) Dispatching and instant traceability: Finding out the destination shard of a transaction should be determinis- tic, trivial to calculate, eliminating the need for commu- nication rounds; 3) Efficiency and adaptability: The shards should be as balanced as possible at any given time. Method Description To calculate an optimum number of shards Nsh in epoch ei1 (Nsh,i1), we have defined one threshold coefficient for the number of transactions in a block, \u03b8T X.",
      "Variable optN represents the optimal number of nodes in a shard, \u03f5sh is a positive number and represents the number of nodes a shard can vary by. totalNi is the total number of nodes (eligible validators, nodes in the waiting lists and newly added nodes in the node pool) on all shards in epoch ei, while NT XB,i is the average number of transactions in a block on all shards in epoch ei. Nsh,0 will be considered as 1. The total number of shards Nsh,i1 will change if the number of nodes totalNi in the network changes and if the blockchain utilization needs it: if the number of nodes increases above a threshold nSplit from one epoch to another and the average number of transactions per block is greater than the threshold number of transactions per block NT XB,i  \u03b8T X or if the number of nodes decreases below a threshold nMerge as shown in function ComputeShardsN.",
      "1: function COMPUTESHARDSN(totalNi1, Nsh,i) nSplit (Nsh,i  1) (optN  \u03f5sh) nMerge (Nsh,i 1) a Nsh,i1 Nsh,i if (totalNi1  nSplit and NT XB,i  \u03b8T X) then Nsh,i1 totalNi1(optN  \u03f5sh) else if totalNi1  nMerge then Nsh,i1 totalNi1(optN) return Nsh,i1 From one epoch to another, there is a probability that the number of active nodes changes. If this aspect influences the number of shards, anyone can calculate the two masks m1 and m2, used in transaction dispatching. 1: function COMPUTEM1ANDM2(Nsh) n math.ceil(log2Nsh) m1 (1  n) 1 m2 (1  (n 1)) 1 return m1, m2 As the main goal is to increase the throughput beyond thousands of transactions per second and to diminish the cross-shard communication, MultiversX proposes a dispatch- ing mechanism which determines automatically the shards involved in the current transaction and routes the transaction accordingly. The dispatcher will take into consideration the account address (addr) of the transaction senderreceiver.",
      "The result is the number of the shard (shard) the transaction will be dispatched to. 1: function COMPUTESHARD(Nsh, addr, m1, m2) shard (addr and m1) if shard  Nsh then shard (addr and m2) return shard The entire sharding scheme is based on a binary tree structure that distributes the account addresses, favors the scalability and deals with the state transitions. A representation of the tree can be seen in Fig. 2. The presented tree structure is merely a logical represen- tation of the account address space used for a deterministic mapping; e.g. shard allocation, sibling computation etc. The leaves of the binary tree represent the shards with their ID number. Starting from root (nodeshard 0), if there is only one shardleaf (a), all account addresses are mapped to this one and all transactions will be executed here. Further on, if the formula for Nsh dictates the necessity of 2 shards (b), the address space will be split in equal parts, according to the last bits in the address.",
      "Sometimes, the tree can also become unbalanced (c) if Nsh is not a power of 2. This case only affects the leaves on the Fig. 2: Example of a sharding tree structure last level. The structure will become balanced again when the number of shards reaches a power of 2. The unbalancing of the binary tree causes the shards located in the lowest level to have half the address space of nodes of a shard located one level higher, so it can be argued that the active nodes allocated to these shards will have a lower fee income - block rewards are not affected. However, this problem is solved by having a third of each shard nodes redistributed randomly each epoch (detailed in the Chronology section) and having a balanced distribution of nodes according to the tree level.",
      "Looking at the tree, starting from any leaf and going through branches towards the root, the encoding from branches represents the last n bits of the account addresses that will have their associated originating transactions processed by that leafshard. Going the other way around, from root to leaf, the information is related to the evolution of the structure, sibling shards, the parent shard from where they split. Using this hierarchy, the shard that will split when Nsh increases or the shards that will merge when Nsh decreases can easily be calculated. The entire state sharding mechanism benefits from this structure by always keeping the address and the associated state within the same shard. Knowing Nsh, any node can follow the redistribution pro- cess without the need of communication. The allocation of IDs for the new shards is incremental and reducing the number of shards involves that the higher numbered shards will be removed.",
      "For example, when going from Nsh to Nsh- 1, two shards will be merged, the shard to be removed is the highest numbered shard (shmergeNsh-1). Finding the shard number that shmerge will be merged with is trivial.",
      "According to the tree structure, the resulting shard has the siblings number: 1: function COMPUTESIBLING(shmerge, n) sibling (shmerge xor (1  (n 1))) return sibling For shard redundancy, traceability of the state transitions and fast scaling, it is important to determine the sibling and parent of a generic shard with number p: 1: function COMPUTEPARENTSIBLINGS(n, p, Nsh) mask1 1  (n 1) mask2 1  (n 2) sibling (p xor mask1) parent min(p, sibling) if sibling Nsh then sibling (p xor mask2) sibling2 (sibling xor mask1) parent min(p, sibling) if sibling2 Nsh then sibling is a shard return parent, sibling, NULL else sibling is a subtree with shards (sibling, sibling2) return parent, sibling, sibling2 else sibling is a shard return parent, sibling, NULL Shard redundancy On blockchain, state sharding is susceptible to shard failure when there is an insufficient number of online nodes in a shard or the distribution is localized geographically.",
      "In the unlikely case when one shard fails (either the shard cannot be contacted - all nodes are offline, or consensus cannot be reached - more than 1 3 of nodes are not responding), there is a high risk that the entire architecture relies only on super- full nodes 2, which fully download every block of every shard, fully verifying everything. As displayed in Fig. 3, our protocol has a protection mechanism that introduces a tradeoff in the state holding structure by enforcing the shards from the last tree level to also hold the state from their siblings. This mechanism reduces the communication and eliminates the bootstrapping when sibling shards are merging since they already have the data. Context switching To preserve security in sharded public blockchains, context switching becomes crucial 7. This refers to the realloca- tion of the active nodes between shards on a fixed time interval by some random criteria.",
      "In MultiversX approach, the context switching represents a security improvement, but also increases the complexity required to maintain consis- tency between multiple states. The state transition has the Fig. 3: Shard redundancy across epochs biggest footprint on performance since the movement of active nodes requires to resync the state, blockchain and transactions alongside the eligible nodes in the new shard. At the start of each epoch, in order to maintain liveness, only less than of these nodes will be uniformly re-distributed across shards. This mechanism is highly effective against forming malicious groups. 5 Notarization (Meta) chain All network and global data operations (node joining the network, node leaving the network, eligible validator lists computation, nodes assignment to the shards waiting lists, consensus agreement on a block in a specific shard challenges for invalid blocks will be notarized in the metachain.",
      "The metachain consensus is run by a different shard that com- municates with all other shards and facilitates cross-shard operations. Every round of every epoch, the metachain receives block headers from the other shards and, if necessary, proofs for the challenges of the invalid blocks. This information will be aggregated into blocks on the metachain on which consensus has to be run. Once the blocks are validated in the consensus group, shards can request information about blocks, miniblocks (see chapter VII), eligible validators, nodes in waiting lists etc., in order to securely process cross-shard transactions. Further details about the cross-shard transaction execution, communication between shards and metachain will be presented in Chapter VII Cross-shard transaction process- ing. V Consensus via Secure Proof of Stake 1 Consensus Analysis The first blockchain consensus algorithm based on Proof of Work (PoW), is used in Bitcoin, Ethereum and other blockchain platforms.",
      "In Proof of Work each node is required to solve a mathematical puzzle (hard to calculate but easy to verify). And the first node that finishes the puzzle will collect the reward 18. Proof of Work mechanisms successfully prevent double-spending, DDoS and Sybil attacks at the cost of high energy consumption. Proof of Stake (PoS) is a novel and more efficient con- sensus mechanism proposed as an alternative to the intensive energy and computational use in Proof of Work consensus mechanisms. PoS can be found in many new architectures like Cardano 19 and Algorand 3 or can be used in next version of Ethereum. In PoS the node that proposes the next block is selected by a combination of stake (wealth), randomness andor age. It mitigates the PoW energy problem but also puts two important issues on the table: the Nothing at Stake attack and a higher centralization risk. Proof of Meme as envisioned in Constellation 20, is an algorithm based on the nodes historical participation on the network.",
      "Its behaviour is stored in a matrix of weights in the blockchain and supports changes over time. Also, it allows new nodes to gain trust by building up reputation. The main drawback regarding Sybil attacks is alleviated through the NetFlow algorithm. Delegated Proof of Stake (DPoS) found in Bitshares 21, Steemit 22 and EOS 23 is a hybrid between Proof of Authority and Proof of Stake in which the few nodes respon- sible for deploying new blocks are elected by stakeholders. Although it has a high throughput, the model is susceptible to human related social problems such as bribing and corruption. Also, a small number of delegates makes the system prone to DDoS attacks and centralization. 2 Secure Proof of Stake (SPoS) MultiversXs approach to consensus is made by combining random validators selection, eligibility through stake and rating, with an optimal dimension for the consensus group.",
      "The algorithm is described in the steps below: 1) Each node ni is defined as a tuple of public key (Pk), rating (default is 0) and the locked stake. If ni wishes to participate in the consensus, it has to first register through a smart contract, by sending a transaction that contains an amount equal to the minimum required stake and other information (Pks, a public key derived from Pk and nodeid that will be used for the signing process in order not to use a real wallet address). 2) The node ni joins the node pool and waits for the shard assignment at the end of the current epoch e. The shard assignment mechanism creates a new set of nodes containing all the nodes that joined in epoch e and all the nodes that need to be reshuffled (less than 3 of every shard). All nodes in this set will be reassigned to the waiting lists of shards. Wj represents js shard waiting list and Nsh represents the number of shards. A node also has a secret key sk that by nature is not to be made public.",
      "ni  (Pki, ratingi, stakei) ni Wj, 0 j  Nsh 3) At the end of the epoch in which it has joined, the node will be moved to the list of eligible nodes (Ej) of a shard j, where e is the current epoch. ni Wj,e1 ni Wj,e, ni Ej,e 4) Each node from the list Ej can be selected as part of an optimally dimensioned consensus group (in terms of se- curity and communication), by a deterministic function, based on the randomness source added to the previous block, the round r and a set of variation parameters. The random number, known to all shard nodes through gossip, cannot be predicted before the block is actually signed by the previous consensus group. This property makes it a good source of randomness and prevents highly adaptive malicious attacks. We define a selection function to return the set of chosen nodes (consensus group) Nchosen with the first being the block proposer, that takes following parameters: E, r and sigr1 - the previous block signature.",
      "Nchosen  f(E, r, sigr1), where Nchosen E 5) The block will be created by the block proposer and the validators will co-sign it based on a modified practical Byzantine Fault Tolerance (pBFT). 6) If, for any reason, the block proposer did not create a block during its allocated time slot (malicious, offline, etc.), round r will be used together with the randomness source from the last block to select a new consensus group. If the current block proposer acts in a malicious way, the rest of the group members apply a negative feedback to change its rating, decreasing or even cancelling out the chances that this particular node will be selected again. The feedback function for the block proposer (ni) in round number r, with parameter ratingModifier Z is computed as: feedbackfunction  ff(ni, ratingModifier, r) When ratingModifier  0, slashing occurs so the node ni loses its stake.",
      "The consensus protocol remains safe in the face of DDoS attacks by having a high number of possible validators from the list E (hundreds of nodes) and no way to predict the order of the validators before they are selected. To reduce the communication overhead that comes with an increased number of shards, a consensus will be run on a composite block. This composite block is formed by:  Ledger block: the block to be added into the shards ledger, having all intra shard transactions and cross shard transactions for which confirmation proof was received;  Multiple mini-blocks: each of them holding cross shard transactions for a different shard; The consensus will be run only once, on the composite block containing both intra- and cross-shard transactions. After consensus is reached, the block header of each shard is sent to the metachain for notarization.",
      "VI Cryptographic Layer 1 Signature Analysis Digital signatures are cryptographic primitives used to achieve information security by providing several properties like message authentication, data integrity and non-repudiation 24. Most of the schemes used for existing blockchain platforms rely on the discrete logarithm (DL) problem: one-way expo- nentiation function y \u03b1ymod p. It is scientifically proven that calculating the discrete logarithm with base is hard 25. Elliptic curve cryptography (ECC) uses a cyclic group of points instead of a cyclic group of integers. The scheme reduces the computational effort, such that for key lengths of only 160 - 256 bits, ECC provides same security level that RSA, Elgamal, DSA and others provide for key lengths of 1024 - 3072 bits (see Table 1 24).",
      "The reason why ECC provides a similar security level for much smaller parameter lengths is because existing attacks on elliptic curve groups are weaker than the existing integer DL attacks, the complexity of such algorithms require on average p steps to solve. This means that an elliptic curve using a prime p of 256 bit length provides on average a security of 2128 steps needed to break it 24. Both Ethereum and Bitcoin use curve cryptography, with the ECDSA signing algorithm. The security of the algorithm is very dependent on the random number generator, because if the generator does not produce a different number on each query, the private key can be leaked 26. Another digital signature scheme is EdDSA, a Schnorr variant based on twisted Edwards curves that support fast arithmetic 27.",
      "In contrast to ECDSA, it is provably non- malleable, meaning that starting from a simple signature, it is impossible to find another set of parameters that defines the same point on the elliptic curve 28, 29. Additionally, EdDSA doesnt need a random number generator because it Algorithm Family Crypto systems Security level (bit) Integer factorization Discrete logarithm DH, DSA, Elgamal Elliptic curves ECDH, ECDSA Symmetric AES, 3DES TABLE 1: Bit lengths of public-key algorithms for different security levels uses a nonce, calculated as the hash of the private key and the message, so the attack vector of a broken random number generator that can reveal the private key is avoided. Schnorr signature variants are gaining more attention 8, 30 due to a native multi-signature capability and being provably secure in the random oracle model 31.",
      "A multi- signature scheme is a combination of a signing and verification algorithms, where multiple signers, each with their own private and public keys, can sign the same message, producing a single signature 32, 33. This signature can then be checked by a verifier which has access to the message and the public keys of the signers. A sub-optimal method would be to have each node calculate his own signature and then concatenate all results in a single string. However, such an approach is unfeasible as the generated string size grows with the number of signers. A practical solution would be to aggregate the output into a single fixed size signature, independent of the number of participants. There have been multiple proposals of such schemes, most of them are susceptible to rogue-key (cancellation) attacks. One solution for this problem would be to introduce a step where each signer needs to prove possession of the private key associated with its public key 34.",
      "Bellare and Neven 35 (BN) proposed a secure multi- signature scheme without a proof of possession, in the plain public key model, under the discrete logarithm assumption 31. The participants commit first to their share Ri by prop- agating its hash to all other signers so they cannot calculate a function of it. Each signer computes a different challenge for their partial signature. However, this scheme sacrifices the public key aggregation. In this case, the verification of the aggregated signature, requires the public key from each signer. A recent paper by Gregory Maxwell et al. 29 proposes another multi-signature scheme in the plain public key model 36, under the one more discrete logarithm assumption (OMDL). This approach improves the previous scheme 35 by reducing the communication rounds from 3 to 2, reintroducing the key aggregation with a higher complexity cost.",
      "BLS 4 is another interesting signature scheme, from the Weil pairing, which bases its security on the Computational Diffie-Hellman assumption on certain elliptic curves and gen- erates short signatures. It has several useful properties like batch verification, signature aggregation, public key aggrega- tion, making BLS a good candidate for threshold and multi- signature schemes. Dan Boneh, Manu Drijvers and Gregory Neven recently proposed a BLS multi-signature scheme 5, using ideas from the previous work of 35, 30 to provide the scheme with defenses against rogue key attacks. The scheme supports efficient verification with only two pairings needed to verify a multi-signature and without any proof of knowledge of the secret key (works in the plain public key model). Another advantage is that the multi-signature can be created in only two communication rounds. For traceability and security reasons, a consensus based on a reduced set of validators requires the public key from each signer.",
      "In this context, our analysis concludes that the most appropriate multi-signature scheme for block signing in MultiversX is BLS multi-signature 5, which is faster overall than the other options due to only two communication rounds. 2 Block signing in MultiversX For block signing, MultiversX uses curve cryptography based on the BLS multi-signature scheme over the bn256 bilinear group, which implements the Optimal Ate pairing over a 256-bit Barreto Naehrig curve. The bilinear pairing is defined e : g0  g1 gt where g0, g1 and gt are elliptic curves of prime order p defined by bn256, and e is a bilinear map (i.e. pairing function). Let G0 and G1 be generators for g0 and g1. Also, let H0 be a hashing function that produces points on the curve g0: H0 : M g0 where M is the set of all possible binary messages of any length.",
      "The signing scheme used by MultiversX employs a second hasing function as well, with parameters known by all signers: H1 : M Zp Each signer i has its own privatepublic key pair (ski, Pki), where ski is randomly chosen from Zp. For each key pair, the property Pki  ski  G1 holds. Let L  Pk1, Pk2, ..., Pkn be the set of public keys of all possible signers during a specific round which, in the case of MultiversX, is the set of public keys of all the nodes in the consensus group. Below, the two stages of block signing process is presented: signing and verification. Practical signing - Round 1 The leader of the consensus group creates a block with transactions, then signs and broadcasts this block to the consensus group members. Practical signing - Round 2 Each member of the consensus group (including the leader) who receives the block must validate it, and if found valid, it signs it with BLS and then sends the signature to the leader: Sigi  ski H0(m) where Sigi is a point on g0.",
      "Practical signing - Round 3 The leader waits to receive the signatures for a specific timeframe. If it does not receive at least 2 3  n  1 signatures in that timeframe, the consensus round is aborted. But if the leader does receive 2 3  n  1 or more valid signatures, it uses them to generate the aggregated signature: SigAgg  H1(Pki)  Sigi  Bi where SigAgg is a point on g0. The leader then adds the aggregated signature to the block together with the selected signers bitmap B, where a 1 indicates that the corresponding signer in the list L had its signature added to the aggregated signature SigAgg. Practical verification Given the list of public keys L, the bitmap for the signers B, the aggregated signature SigAgg, and a message m (block), the verifier computes the aggregated public key: PkAgg  H1(Pki)  Pki  Bi The result, PkAgg, is a point on g1. The final verification is e(G1, SigAgg)  e(PkAgg, H0(m)) where e is the pairing function.",
      "VII Cross-shard Execution For an in depth example of how the cross-shard transactions are being executed and how the communication between shards and the metachain occurs, we are simplifying the entire process to just two shards and the metachain. Assuming that a user generates a transaction from his wallet, which has an address in shard 0 and wants to send EGLD to another user that has a wallet with an address in shard 1, the steps depicted in Fig. 4 are required for processing the cross-shard transaction. As mentioned in chapter V - Consensus via Secure Proof of Stake, the blocks structure is represented by a block Header that contains information about the block (block nonce, round, proposer, validators timestamp etc), and a list of miniblocks for each shard that contain the actual transactions inside.",
      "Every miniblock contains all transactions that have either the sender in the current shard and the receiver in another shard or the sender in a different shard and the destination in the current shard. In our case, for a block in shard 0, there will normally be 3 miniblocks:  miniblock 0: containing the intrashard transactions for shard 0  miniblock 1: containing cross-shard transactions with the sender in shard 0 and destination in shard 1  miniblock 2: containing cross-shard transactions with sender in shard 1 and destination in shard 0. These transactions were already processed in the sender shard 1 and will be finalized after the processing also in the current shard. There is no limitation on the number of miniblocks with the same sender and receiver in one block. Meaning multiple miniblocks with the same sender and receiver can appear in the same block.",
      "1 Processing Currently the atomic unit of processing in cross-shard execution is a miniblock: either all the transactions of the miniblock are processed at once or none and the miniblocks execution will be retried in the next round. Our cross-shard transaction strategy uses an asynchronous model. Validation and processing is done first in senders shard and then in receivers shard. Transactions are first dispatched in the senders shard, as it can fully validate any transaction initiated from the account in this shard  mainly the current balance. Afterwards, in the receivers shard, the nodes only need proof of execution offered by metachain, do signature verification and check for replay attack and finally update the balance for the receiver, adding the amount from the transaction. Shard 0 processes both intra-shard transactions in miniblock 0 and a set of cross-shard transactions that have addresses from shard 1 as a receiver in miniblock 1.",
      "The block header and miniblocks are sent to the metachain. The metachain notarizes the block from shard 0, by creating a new metachain block (metablock) that contains the following information about each miniblock: sender shard ID, receiver shard ID, miniblock hash. Fig. 4: Cross-shard transaction processing Shard 1 fetches the hash of miniblock 1 from metablock, requests the miniblock from shard 0, parses the transaction list, requests missing transactions (if any), executes the same miniblock 1 in shard 1 and sends to the metachain resulting block. After notarization the cross transaction set can be considered finalized. The next diagram shows the number of rounds required for a transaction to be finalized. The rounds are considered between the first inclusion in a miniblock until the last miniblock is notarised. VIII Smart Contracts The execution of smart contracts is a key element in all future blockchain architectures.",
      "Most of the existing solutions avoid to properly explain the transactions and data dependency. This context leads to the following two scenarios: 1) When there is no direct correlation between smart con- tract transactions, as displayed in Fig. 5, any architecture can use out of order scheduling. This means there are no additional constraints on the time and place (shard) where a smart contract is executed. 2) The second scenario refers to the parallelism induced by the transactions that involve correlated smart contracts 37. This case, reflected in Fig. 6, adds additional pressure on the performance and considerably increases the complexity. Basically there must be a mechanism to ensure that contracts are executed in the right order and on the right place (shard). To cover this aspect, MultiversX protocol proposes a solution that assigns and moves the smart contract to the same shard where their static dependencies reside.",
      "This way most, if not all SC calls will have dependencies in the same shard and no cross-shard lockingunlocking will be needed. MultiversX focuses on the implementation of the Multi- versX Virtual Machine, an EVM compliant engine. The EVM Fig. 5: Independent transaction processing under simple smart contracts that can be executed out of order Fig. 6: Mechanism for correlated smart contracts that can be executed only sequentially Fig. 7: Abstraction Layer for Smart Contracts compliance is extremely important for adoption purposes, due to the large number of smart contracts built on Ethereums platform. The MultiversX Virtual Machines implementation will hide the underlying architecture isolating the smart contract de- velopers from system internals ensuring a proper abstraction layer, as displayed in Fig. 7. In MultiversX, cross chain interoperability can be imple- mented by using an adapter mechanism at the Virtual Machine level as proposed by Cosmos 38.",
      "This approach requires spe- cialized adapters and an external medium for communication between adapter SC for each chain that will interoperate with MultiversX. The value exchange will be operated using some specialized smart contracts acting as asset custodians, capable of taking custody of adapted chain native tokens and issuing MultiversX native tokens. 1 VM Infrastructure MultiversX builds its VM infrastructure on top of the K Framework, which is an executable semantic framework where programming languages, calculi, as well as type systems or formal analysis tools can be defined 39. The greatest advantage of using the K framework is that with it, smart contract languages can be unambiguously de- fined, eliminating the potential for unspecified behavior and bugs that are hard to detect. The K Framework is executable, in the sense that the seman- tic specifications of languages can be directly used as working interpreters for the languages in question.",
      "More specifically, one can either run programs against the specifications using the K Framework core implementation directly, or one can generate an interpreter in several programming languages. These are also referred to as backends. For the sake of execution speed and ease of interoperability, MultiversX uses its own custom-built K Framework backend. 2 Smart contract languages One great advantage of the K Framework is that one can generate an interpreter for any language defined in K, without the need for additional programming. This also means that interpreters produced this way are correct-by-construction. There are several smart contract languages specified in the K Framework already, or with their specifications under de- velopment. MultiversX Network will support three low-level languages: IELE VM, KEVM, and WASM. IELE VM is an intermediate-level language, in the style of LLVM, but adapted for the blockchain.",
      "It was built directly in K, no other specification or implementation of it exists outside of the K framework 40. Its purpose is to be human readable, fast, and to overcome some limita- tions of EVM. MultiversX uses a slightly altered version of IELE - most changes are related to account address management. Smart contract developers can program in IELE directly, but most will choose to code in Solidity and then use a Solidity to IELE compiler, as can be seen in Fig. 8. KEVM is a version of the Ethereum Virtual Machine (EVM), written in K 41. Certain vulnerabilities of EVM are fixed in the K version, or the vulnerable features are left out entirely. Web Assembly (WASM) is a binary instruction format for a stack-based virtual machine, which can be used for running smart contracts. A WASM infrastructure enables developers to write smart contracts in CC, Rust, C, and others. Having a language specification and generating the inter- preter is only half of the challenge.",
      "The other half is integrating the generated interpreter with the MultiversX network. We have built a common VM interface, that enables us to plug in any VM into an MultiversX node as shown in Fig. 9. Each VM then has an adapter that implements this interface. Each contract is saved as bytecode of the VM for which it was compiled and runs on its corresponding VM. 3 Support for formal modelling and verification Because the smart contract languages are formally defined in K Framework, it is possible to perform formal verification of smart contracts written in these languages. To do this, it Fig. 8: MultiversX VM execution Fig. 9: MultiversX VM components is necessary to also formally model their requirements, which can also be performed using the K Framework 42. 4 Smart contracts on the sharded architecture Smart contracts on sharded architectures are still in the early stages of research and development and pose serious challenges.",
      "Protocols like Atomix 7 or S-BAC 9 represent a starting point. Dynamic smart contract dependencies cannot be resolved by moving the SCs into the same shard, as at deployment time, not all the dependencies can be calculated. Solution currently research in the space: 1) A locking mechanism that allows the atomic execution of smart contract from different shards, ensures that the involved SCs will be either all executed at the same time, or none at all. This requires multiple interaction messages and synchronization between consensuses of different shards. 9 2) Cross-shard contract yanking proposal for Ethereum 2.0 would move that smart contract code and data into the caller shard at the execution time. Atomic execution is not needed, but the locking mechanism is mandatory on the moved SC, which would block the execution of SC for other transactions. The locking mechanism is simpler, but it needs to transfer the whole internal state of the SC.",
      "43 Following Ethereums model, MultiversX has the following transaction types: 1) SC construction and deployment: transactions receiver address is empty and data field contains the smart contract code as byte array; 2) SC method invoking: transaction has a non empty re- ceiver address and that address has an associated code; 3) Payment transactions: transaction has a non empty re- ceiver and that address does not have code. MultiversX approach to this problem is to use asyn- chronous cross-shard execution model in case of smart con- tracts. The user creates a smart contract execution transaction. If the smart contract is not in the current shard, the transaction is treated as a payment transaction, the value of the transaction is subtracted from the sender account and it is added to the block where the sender shard resides, into a miniblock with the destination shard where the receiver account is. The transaction is notarized by metachain, then processed by the destination shard.",
      "In the destination shard, the transaction is treated as SC method invoking, as the receiver address is a smart contract which exists in this shard. For the smart contract call a temporary account which shadows the sender account is created, with the balance from the transaction value and the smart contract is called. After the execution, the smart contract might return results which affects a number of accounts from different shards. All the results, which affect in-shard accounts are executed in the same round. For those accounts which are not in the shard where the smart contract was executed, transactions called Smart Contract Results will be created, saving the smart contract execution output for each of these accounts. SCR miniblocks are created for each destination shard. These miniblocks are notarized the same way as cross-shard transactions by metachain, then processed by the respective shards, where the accounts resides.",
      "In case one smart contract calls dynamically another smart contract from another shard, this call is saved as an intermediate result and treated the same as for accounts. The solution has multiple steps and the finalization of a cross-shard smart contract call will need at least 5 rounds, but it does not need locking and state movement across shards. IX Bootstrapping and Storage 1 Timeline division Proof of Stake systems tend to generally divide timeline into epochs and each epoch into smaller rounds 19. The timeline and terminology may differ between architectures but most of them use a similar approach. Epochs In MultiversX Protocol, each epoch has a fixed duration, initially set to 24 hours (might suffer updates after sev- eral testnet confirmation stages). During this timeframe, the configuration of the shards remains unchanged. The system adapts to scalability demands between epochs by modifying the number of shards.",
      "To prevent collusion, after an epoch, the configuration of each shard needs to change. While reshuffling all nodes between shards would provide the highest security level, it would affect the systems liveness by introducing additional latency due to bootstrapping. For this reason, at the end of each epoch, less than 1 3 of the eligible validators, belonging to a shard will be redistributed non-deterministically and uniformly to the other shards waiting lists. Only prior to the start of a new epoch, the validator distribution to shards can be determined, without additional communication as displayed in Fig. 10.",
      "The node shuffling process runs in multiple steps: 1) The new nodes registered in the current epoch ei land in the unassigned node pool until the end of the current epoch; 2) Less than 1 3 of the nodes in every shard are randomly selected to be reshuffled and are added to the assigned node pool; 3) The new number of shards Nsh,i1 is computed based on the number of nodes in the network ki and network usage; 4) Nodes previously in all shards waiting lists, that are cur- rently synchronized, are added to the eligible validators lists; 5) The newly added nodes from the unassigned node pool are uniformly random distributed across all shards waiting lists during epoch ei1; 6) The reshuffled nodes from the assigned node pool are redistributed with higher ratios to shards waiting lists that will need to split in the next epoch ei2. Rounds Each round has a fixed time duration of 5 seconds (might suffer updates after several testnet confirmation stages).",
      "During each round, a new block can be produced within every shard by a randomly selected set of block validators (including one block proposer). From one round to another the set is changed using the eligible nodes list, as detailed in the chapter IV. As described before, the reconfiguration of shards within epochs and the arbitrary selection of validators within rounds discourages the creation of unfair coalitions, diminishes the possibility of DDoS and bribery attacks while maintaining decentralization and a high transactions throughput. 2 Pruning A high throughput will lead to a distributed ledger that rapidly grows in size and increases bootstrapping cost (timestorage), as highlighted in section XI.1. This cost can be addressed by using efficient pruning algorithms, that can summarize the blockchains full state in a more condensed structure. The pruning mechanism is similar to the stable checkpoints in pBFT 15 and compresses the entire ledger state.",
      "MultiversX protocol makes use of an efficient pruning algorithm 7 detailed below. Let us consider that e is the current epoch and a is the current shard: 1) the shard nodes keep track of the account balances of e in a Merkle tree 44; 2) at the end of each epoch, the block proposer creates a state block sb(a, e), which stores the hash of the Merkle trees root in the blocks header and the balances in the blocks body; 3) validators verify and run consensus on sb(a, e); 4) if consensus is reached, the block proposer will store sb(a, e) in the shards ledger, making it the genesis block for epoch e  1; 5) at the end of epoch e  1, nodes will drop the body of sb(a, e) and all blocks preceding sb(a, e). Using this mechanism, the bootstrapping of the new nodes should be very efficient. Actually, they start only from the last valid state block and compute only the following blocks instead of its full history.",
      "X Security Evaluation 1 Randomness source MultiversX makes use of random numbers in its opera- tion e.g. for the random sampling of block proposer and validators into consensus groups and the shuffling of nodes between shards at the end of an epoch. Because these features contribute to MultiversX security guarantees, it is therefore Fig. 10: Shuffling the nodes at the end of each epoch important to make use of random numbers that are provably unbiasable and unpredictable. In addition to these properties, the generation of random numbers also needs to be efficient so that it can be used in a scalable and high throughput blockchain architecture. These properties can be found in some asymmetric cryptog- raphy schemes, like the BLS signing scheme. One important property of BLS is that using the same private key to sign the same message always produces the same results.",
      "This is similar to what is achieved using ECDSA with deterministic k generation and is due to the scheme not using any random parameters: sig  sk  H(m) where H is a hashing function that hashes to points on the used curve and sk is the private key. 2 Randomness creation in MultiversX One random number is created in every round, and added by the block proposer to every block in the blockchain. This ensures that the random numbers are unpredictable, as each random number is the signature of a different block proposer over the previous randomness source. The creation of random numbers is detailed below as part of one consensus round: 1) New consensus group is selected using the randomness source from the previous block header. Consensus group is formed by a block proposer and validators. 2) The block proposer signs the previous randomness source with BLS, adds the signature to the proposed block header as new randomness source, then broadcasts this block to the consensus group.",
      "3) Each member of the consensus group validates the randomness source as part of block validation, and sends their block signature to the block proposer. 4) Block proposer aggregates the validators block signa- tures and broadcasts the block with the aggregated block signature and the new randomness source to the whole shard. The evolution of randomness source in each round can be seen as an unbiasable and verifiable blockchain, where each new random number can be linked to and verified against the previous random number. 3 K block finality scheme The signed block at round n is final, if and only if blocks n  1, n  2, ..., n  k are signed. Furthermore, a final block cannot be reverted. The metachain notarizes only final blocks to ensure that a fork in one shard does not affect other shards. Shards only take into consideration the final metachain blocks, in order to not be affected if the metachain forks.",
      "Finality and correctness is verified at block creation and at block validation as well. The chosen k parameter is 1 and this ensures forks of maximum 2 blocks length. The probability that a malicious super majority ( 2 3  n  1) is selected in the shard for the same round in the same consensus is 109, even if 33 of the nodes from the shard are malicious. In that case they can propose a block and sign it - lets call it block m, but it will not be notarized by the metachain. The metachain notarizes block m, only if block m  1 is built on top of it. In order to create block m1 the next consensus group has to agree with block m. Only a malicious group will agree with block m, so the next group must have a malicious super majority again. As the random seed for group selection cannot be tampered with, the probability of selecting one more malicious super majority group is 109 (5.38  1010, to be exact).",
      "The probability of signing two consecutive malicious blocks equals with selecting two subgroups with at least ( 2 3 n1) members from the malicious group consequently. The probability for this is 1018. Furthermore, the consequently selected groups must be colluding, otherwise the blocks will not be signed. 4 Fisherman challenge When one invalid block is proposed by a malicious majority, the shard state root is tampered with an invalid result (after including invalid changes to the state tree). By providing the combined merkle proof for a number of accounts, an honest node could raise a challenge with a proof. The honest nodes will provide the block of transactions, the previous reduced merkle tree with all affected accounts before applying the challenged block and the smart contract states, thus demon- strating the invalid transaction  state. If a challenge with the proof is not provided in the bounded time frame, the block is considered valid.",
      "The cost of one invalid challenge is the entire stake of the node which raised the challenge. The metachain detects the inconsistency, either an invalid transaction, or an invalid state root, through the presented challenges and proofs. This can be traced and the consensus group can be slashed. At the same time the challenger can be rewarded with part of the slashed amount. Another problem is when a malicious group hides the invalid block from other nodes - non-malicious ones. However, by making it mandatory for the current consensus to propagate the produced block to the sibling shard and to the observer nodes, the data cannot be hidden anymore. The communication overhead is further reduced by sending only the intrashard miniblock to the sibling shard. The cross shard miniblocks are always sent on different topics accessible by interested nodes. In the end, challenges can be raised by multiple honest nodes. Another security pro- tection is given by the setup of P2P topics.",
      "The communication from one shard toward the metachain is done through a defined set of topics  channels, which can be listened to by any honest validator - the metachain will not accept any other messages from other channels. This solution introduces some delay in the metachain only in case of challenges, which are very low in number and highly improbable since if detected (high probability of being detected) the nodes risk their entire stake. 5 Shard reorganization After each epoch, less than 1 3  n of the nodes from each shard are redistributed uniformly and non-deterministically across the other shards, to prevent collusion. This method adds bootstrapping overhead for the nodes that were redistributed, but doesnt affect liveness as shuffled nodes do not participate in the consensus in the epoch they have been redistributed. The pruning mechanism will decrease this time to a feasible amount, as explained in section IX.2.",
      "6 Consensus group selection After each round a new set of validators are selected using the random seed of the last commited block, current round and the eligible nodes list. In case of network desynchronization due to the delays in message propagation, the protocol has a recovery mechanism, and takes into consideration both the round r and the randomness seed from the last committed block in order to select new consensus groups every round. This avoids forking and allows synchronization on last block. The small time window (round time) in which the validators group is known, minimizes the attack vectors. 7 Node rating Beside stake, the eligible validators rating influences the chances to be selected as part of the consensus group. If the block proposer is honest and its block gets committed to the blockchain, it will have its rating increased, otherwise, its rating will be decreased.",
      "This way, each possible validator is incentivized to be honest, run the most up-to-date client software version, increase its service availability and thus ensuring the network functions as designed. 8 Shard redundancy The nodes that were distributed in sibling shards on the trees lowest level (see section IV.4) keep track of each others blockchain data and application state. By introducing the concept of shard redundancy, when the number of nodes in the network decreases, some of the sibling shards will need to be merged. The targeted nodes will instantly initiate the process of shard merging. XI Understanding the real problems 1 Centralized vs Decentralized Blockchain was initially instantiated as an alternative to the centralized financial system of systems 45. Even if the freedom and anonymity of distributed architectures remains an undisputed advantage, the performance has to be analyzed at a global scale in a real-world environment.",
      "The most relevant metric measuring performance is transac- tions per second (TPS), as seen in Table 2. A TPS comparison of traditional centralized systems with decentralized novel architectures that were validated as trusted and efficient on a large scale, reflects an objective yet unsettling reality 46, 47, 48, 49. Archi- tecture Type Dispersion (average) (max limit) VISA Distributed virtualization Centralized Paypal Distributed virtualization Centralized Ripple Private Blockchain Permissioned Private Blockchain Mixed Ethereum Public Blockchain Decentralized Bitcoin Public Blockchain Decentralized TABLE 2: Centralized vs Decentralized TPS comparison The scalability of blockchain architectures is a critical but still unsolved problem. Take, for instance, the example determining the data storage and bootstrapping implications of current blockchain architectures suddenly functioning at Visa level throughput.",
      "By performing such exercises, the magnitude of multiple secondary problems becomes obvious (see Fig. 11). XII The blockchain performance paradigm The process of designing distributed architectures on blockchain faces several challenges, perhaps one of the most challenging being the struggle to maintain operability under contextual pressure conditions. The main components that determine the performance pressure are:  complexity  system size  transaction volume Complexity The first element that limits the system performance, is the consensus protocol. A more complicated protocol determines a bigger hotspot. In PoW consensus architectures a big perfor- mance penalty is induced by the mining complexity that aims to keep the system decentralized and ASIC resilient 50. To overrun this problem PoS makes a trade-off, simplifies the network management by concentrating the computing power to a subset of the network, but yields more complexity on the control mechanism.",
      "System size Expanding the number of nodes in existing validated archi- tectures forces a serious performance degradation and induces a higher computational price that must be paid. Sharding seems to be a good approach, but the shard size plays a major role. Smaller shards are agile but more likely to be affected by malicious groups, bigger shards are safer, but their reconfiguration affects the system liveness. Transaction volume With a higher relevance compared to the others, the last item on the list represents the transaction processing performance. In order to correctly measure the impact of this criteria, this must be analyzed considering the following two standpoints:  C1 transaction throughput - how many transactions a system can process per time unit, known as TPS, an output of a system 51;  C2 transaction finality - how fast one particular trans- action is processed, referring to the interval between its launch and its finalization - an input to output path. C1.",
      "Transactionthroughput in single chain architectures is very low and can be increased by using workarounds such as sidechain 52. In a sharded architecture like ours, the transaction throughput is influenced by the number of shards, the computing capabilities of the validatorsblock proposers and the messaging infrastructure 8. In general, as displayed in Fig. 13, this goes well to the public, but despite the importance of the metric, it provides only a fragmented view. Fig. 11: Storage Estimation - Validated distributed architectures working at an average of VISA TPS Fig. 13: Transaction throughput C2. Transaction finality - A more delicate aspect that emphasizes that even if the system may have a throughput of 1000 TPS, it may take a while to process a particular transac- tion.",
      "Beside the computing capabilities of the validatorsblock proposers and the messaging infrastructure, the transaction finality is mainly affected by the dispatching algorithm (when the decision is made) and the routing protocol (where should the transaction be executed). Most of the existing state of the art architectures refuse to mention this aspect but from a user standpoint this is extremely important. This is displayed in Fig. 14, where the total time required to execute a certain transaction from start to end is considered. In MultiversX, the dispatching mechanism (detailed in sec- tion V) allows an improved time to finality by routing the transactions directly to the right shard, mitigating the overall delays. XIII Conclusion 1 Performance Performance tests and simulations, presented in Fig. 12, reflect the efficiency of the solution as a highly scalable Fig. 14: Transaction finality distributed ledger.",
      "As more and more nodes join the network our sharding approach shows a linearly increasing throughput. The chosen consensus model involves multiple communication rounds, thus the result is highly influenced by the network quality (speed, latency, availability). Simulations using our testnet using worldwide network speed averages, at its maxi- mum theoretical limit, suggest MultiversX exceeds the average VISA level with just 2 shards, and approaches peak VISA level with 16 shards. 2 Ongoing and future research Our team is constantly re-evaluating and improving Mul- tiversX design, in an effort to make this one of the most compelling public blockchain architectures; solving scalability via adaptive state sharding, while maintaining security and high energy efficiency through a secure Proof of Stake consen- sus mechanism. Some of our next directions of improvement include: 1) Reinforcement learning: we aim to increase the ef- ficiency of the sharding process by allocating the fre- Fig.",
      "12: Network throughput measured in transactions per seconds with a global network speed of 8 MBs quently trading clients in the same shard to reduce the overall cost; 2) AI supervision: create an AI supervisor that detects malicious behavioral patterns; it is still uncertain how this feature can be integrated in the protocol without disrupting the decentralization; 3) Reliability as a consensus factor: the existing protocol weighs between stake and rating but we plan to add reliability, as a metric that should be computed in a distributed manner after applying a consensus protocol on previously submitted blocks from the very recent history; 4) Cross-chain interoperability: implements and con- tribute to standards like those initiated by the De- centralized Identity Foundation 53 or the Blockchain Interoperability Alliance 54; 5) Privacy preserving transactions: use Zero-Knowledge Succinct Non-Interactive Argument of Knowledge 55 to protect the identity of the participants and offer auditing capabilities while preserving the privacy.",
      "3 Overall Conclusions MultiversX is the first highly scalable public blockchain that uses the newly proposed Secure Proof of Stake algorithm in a genuine state-sharded architecture to achieve VISA level throughput and confirmation times of seconds. MultiversX novel approach on adaptive state sharding improves on Om- niledgers proposal increasing security and throughput, while the built-in automatic transaction routing and state redundancy mechanisms considerably reduce latencies. By using a shard pruning technique the bootstrapping and storage costs are also considerably reduced compared to other approaches. The newly introduced Secure Proof of Stake consensus algorithm ensures distributed fairness and improves on Algorands idea of random selection, reducing the time needed for the random selection of the consensus group from 12 seconds to 100 ms.",
      "Our method of combining state sharding and the very efficient Secure Proof of Stake consensus algorithm has shown promising results in our initial estimations, validated by our latest testnet results. 1 G. Hileman Rauchs, 2017 Global Cryptocurrency Benchmarking Study, Social Science Research Network, Rochester, NY, SSRN Scholarly Paper ID 2965436, Apr. 2017. Online. Available: 2 The Ethereum Wiki - Sharding FAQ, 2018, original-date: 2014-02- 14T23:05:17Z. Online. Available: https:github.comethereumwiki wikiSharding-FAQ 3 Y. Gilad, R. Hemo, S. Micali, G. Vlachos, and N. Zeldovich, Algorand: Scaling Byzantine Agreements for Cryptocurrencies, in Proceedings of the 26th Symposium on Operating Systems Principles, ser. SOSP 17. New York, NY, USA: ACM, 2017, pp. 5168. Online. Available: 4 D. Boneh, B. Lynn, and H. Shacham, Short signatures from the weil pairing, in Advances in Cryptology  ASIACRYPT 01, LNCS. Springer, 2001, pp. 514532. 5 D. Boneh, M. Drijvers, and G.",
      "Neven, Compact multi-signatures for smaller blockchains, in Advances in Cryptology  ASIACRYPT 2018, ser. Lecture Notes in Computer Science, vol. 11273. Springer, 2018, pp. 435464. 6 V. Buterin, Ethereum: Next-Generation Smart Contract Decentralized Application Platform, 2013. Online. Available: https: www.ethereum.orgpdfsEthereumWhitePaper.pdf 7 E. Kokoris-Kogias, P. Jovanovic, L. Gasser, N. Gailly, E. Syta, and B. Ford, OmniLedger: A Secure, Scale-Out, Decentralized Ledger via Sharding, Tech. Rep. 406, 2017. Online. Available: 8 The ZILLIQA Technical Whitepaper, 2017. Online. Available: 9 M. Al-Bassam, A. Sonnino, S. Bano, D. Hrycyszyn, and G. Danezis, Chainspace: A Sharded Smart Contracts Platform, arXiv:1708.03778 cs, Aug. 2017, arXiv: 1708.03778. Online. Available: http: arxiv.orgabs1708.03778 10 G. Wood, Ethereum: Secure Decentralised Generalised Transaction Ledger, 2017. Online. Available: github.ioyellowpaperpaper.pdf 11 Solidity  Solidity 0.4.21 documentation. Online.",
      "Available: 12 web3j, 2018. Online. Available: https:github.comweb3j 13 Casper, 2018. Online. Available: http:ethresear.chccasper 14 The State of Ethereum Scaling, March 2018  Highlights from EthCC on Plasma Cash, Minimum Viable Plasma, and More... Medium, 2018. Online. Available: https:medium.comloom-network the-state-of-ethereum-scaling-march-2018-74ac08198a36 15 M. Castro and B. Liskov, Practical Byzantine Fault Tolerance, Proceedings Third Symposium Operating Systems Design and Implementation, ser. OSDI 99. Berkeley, CA, USA: USENIX Association, 1999, 173186. Online. Available: 16 Y. Jia, Op Ed: The Many Faces of Sharding for Blockchain Scalability, 2018. Online. Available: https:bitcoinmagazine.com articlesop-ed-many-faces-sharding-blockchain-scalability 17 Using Merklix tree shard block validation Deadalnixs den, 2016. Online. Available: https:www.deadalnix.me20161106 using-merklix-tree-to-shard-block-validation 18 S. Nakamoto, Bitcoin: A Peer-to-Peer Electronic Cash System, p.",
      "9, 2008. 19 Why we are building Cardano - Introduction. Online. Available: 20 Constellation - a blockchain microservice operating system - White Paper, 2017, original-date: 2018-01-05T20:42:05Z. Online. Available: 21 Bitshares Delegated Proof-of-Stake Consensus, 2014. Online. Available: delegated-proof-of-stake-consensus 22 dantheman, DPOS Consensus Algorithm - The Missing White Paper, May 2017. Online. Available: https:steemit.comdposdantheman dpos-consensus-algorithm-this-missing-white-paper 23 EOS.IO Technical White Paper v2, 2018, original-date: 2017- 06-06T07:55:17Z. Online. Available: DocumentationblobmasterTechnicalWhitePaper.md 24 C. Paar and J. Pelzl, Understanding Cryptography: A Textbook for Students and Practitioners. Berlin Heidelberg: Springer-Verlag, 2010. Online. Available: www.springer.comgpbook9783642041006 25 C. Schnorr, Efficient signature generation by smart cards, Journal of Cryptology, vol. 4, pp. 161174, Jan. 1991. 26 K.",
      "Michaelis, Meyer, Schwenk, Randomly Failed! The State of Randomness in Current Java Implementations, in Topics Cryptology CT-RSA 2013, ser. Lecture Notes Computer Science. Springer, Berlin, Heidelberg, Feb. 2013, pp. 129144. Online. Available: https:link.springer.comchapter10.1007 978-3-642-36095-4 9 27 D. J. Bernstein, P. Birkner, M. Joye, T. Lange, and C. Peters, Twisted Edwards Curves, Progress Cryptology AFRICACRYPT 2008, ser. Lecture Notes in Computer Science. Springer, Berlin, Heidelberg, Jun. 2008, 389405. Online. Available: https: link.springer.comchapter10.1007978-3-540-68164-9 26 28 A. Poelstra, Schnorr Signatures are Non-Malleable in the Random Oracle Model, 2014. Online. Available: https:download.wpsoftware. netbitcoinwizardryschnorr-mall.pdf 29 C. Decker and R. Wattenhofer, Bitcoin Transaction Malleability and MtGox, arXiv:1403.6676 cs, vol. 8713, pp. 313326, 2014, arXiv: 1403.6676. Online. Available: http:arxiv.orgabs1403.6676 30 G. Maxwell, A. Poelstra, Y. Seurin, and P.",
      "Wuille, Simple Schnorr Multi-Signatures with Applications to Bitcoin, Tech. Rep. 068, 2018. Online. Available: https:eprint.iacr.org2018068 31 Y. Seurin, Exact Security Schnorr-Type Signatures Random Oracle Model, Advances Cryptology EUROCRYPT 2012, ser. Lecture Notes in Computer Science. Springer, Berlin, Heidelberg, Apr. 2012, pp. 554571. Online. Available: 32 K. Itakura and K. Nakamura, A public-key cryptosystem suitable for digital multisignatures, 1983. 33 S. Micali, Ohta, Reyzin, Accountable-subgroup Multisignatures: Extended Abstract, Proceedings ACM Conference on Computer and Communications Security, ser. CCS 01. New York, NY, USA: ACM, 2001, pp. 245254. Online. Available: http:doi.acm.org10.1145501983.502017 34 T. Ristenpart and S. Yilek, The Power of Proofs-of-Possession: Securing Multiparty Signatures against Rogue-Key Attacks, Advances in Cryptology - EUROCRYPT 2007, ser. Lecture Notes in Computer Science. Springer, Berlin, Heidelberg, May 2007, pp. 228245. Online.",
      "Available: https:link.springer.comchapter10.1007 978-3-540-72540-4 13 35 M. Bellare and G. Neven, Multi-signatures in the Plain public-Key Model and a General Forking Lemma, in Proceedings of the 13th ACM Conference on Computer and Communications Security, ser. CCS 06. New York, NY, USA: ACM, 2006, pp. 390399. Online. Available: http:doi.acm.org10.11451180405.1180453 36 D.-P. Le, A. Bonnecaze, and A. Gabillon, Multisignatures as Secure as the Diffie-Hellman Problem in the Plain Public-Key Model, in Pairing-Based Cryptography  Pairing 2009, ser. Lecture Notes in Computer Science. Springer, Berlin, Heidelberg, Aug. 2009, pp. 3551. Online. Available: https:link.springer.comchapter10.1007 978-3-642-03298-1 3 37 T. Dickerson, P. Gazzillo, M. Herlihy, and E. Koskinen, Adding Concurrency Smart Contracts, Proceedings Symposium on Principles of Distributed Computing, ser. PODC 17. New York, NY, USA: ACM, 2017, pp. 303312. Online. Available: 38 J. Kwon and E.",
      "Buchman, Cosmos Network - Internet of Blockchains, 2017. Online. Available: https:cosmos.networkwhitepaper 39 G. Ros,u and T. F. S, erbanuta, An overview of the k semantic frame- work, The Journal of Logic and Algebraic Programming, vol. 79, no. 6, pp. 397434, 2010. 40 T. Kasampalis, D. Guth, B. Moore, T. Serbanuta, V. Serbanuta, D. Fi- laretti, G. Rosu, and R. Johnson, Iele: An intermediate-level blockchain language designed and implemented using formal semantics, Tech. Rep., 2018. 41 E. Hildenbrandt, M. Saxena, X. Zhu, N. Rodrigues, P. Daian, D. Guth, and G. Rosu, Kevm: A complete semantics of the ethereum virtual machine, Tech. Rep., 2017. 42 How Formal Verification Smart Contracts Works RV Blog. Online. Available: https:runtimeverification.comblog how-formal-verification-of-smart-contracts-works 43 Cross-shard contract yanking. Online. Available: https:ethresear. chtcross-shard-contract-yanking1450 44 R. C.",
      "Merkle, A Certified Digital Signature, in Advances in Cryptology  CRYPTO 89 Proceedings, ser. Lecture Notes in Computer Science. Springer, New York, NY, Aug. 1989, pp. 218238. Online. Available: 45 A. Veysov and M. Stolbov, Financial System Classification: From Conventional Dichotomy to a More Modern View, Social Science Research Network, Rochester, NY, SSRN Scholarly Paper ID 2114842, Jul. 2012. Online. Available: https:papers.ssrn.comabstract2114842 46 XRP Digital Asset Payments. Online. Available: 47 Visa Annual Report 2017, 2018. Online. Avail- able: https:s1.q4cdn.com050606653filesdoc financialsannual2017 Visa-2017-Annual-Report.pdf 48 PayPal Reports Fourth Quarter Full Year Results (NASDAQ:PYPL), 2018. Online. Available: paypal-corp.comreleasedetail.cfm?releaseid1055924 49 M. Schwarz, Crypto Transaction Speeds 2018 - All the Major Cryptocurrencies, 2018. Online. Available: https:www.abitgreedy.",
      "comtransaction-speed 50 The Ethereum Wiki Mining, 2018, original-date: 2014-02- 14T23:05:17Z. Online. Available: https:github.comethereumwiki wikiMininghttps:github.comethereumwiki 51 Transaction throughput. Online. Available: https:docs.oracle.com cdE17276 01htmlprogrammer referencetransapp throughput.html 52 W. Martino, M. Quaintance, and S. Popejoy, Chainweb: A Proof- of-Work Parallel-Chain Architecture for Massive Throughput, 2018. Online. Available: http:kadena.iodocschainweb-v15.pd 53 DIF Decentralized Identity Foundation. Online. Available: 54 H. World, Blockchain Interoperability Alliance: ICON Aion Wanchain, Dec. 2017. Online. Available: blockchain-interoperability-alliance-icon-x-aion-x-wanchain-8aeaafb3ebdd 55 S. Goldwasser, S. Micali, and C. Rackoff, The Knowledge Complexity of Interactive Proof-systems, in Proceedings of the Seventeenth Annual ACM Symposium on Theory of Computing, ser. STOC 85. York, NY, USA: ACM, 1985, pp. 291304. Online. Available:"
    ],
    "word_count": 13657,
    "page_count": 19
  },
  "ETH": {
    "chunks": [
      "When Satoshi Nakamoto first set the Bitcoin blockchain into motion in January 2009, he was simultaneously introducing two radical and untested concepts. The first is the \"bitcoin\", a decentralized peer-to-peer online currency that maintains a value without any backing, intrinsic value or central issuer. So far, the \"bitcoin\" as a currency unit has taken up the bulk of the public attention, both in terms of the political aspects of a currency without a central bank and its extreme upward and downward volatility in price. However, there is also another, equally important, part to Satoshi's grand experiment: the concept of a proof of work-based blockchain to allow for public agreement on the order of transactions. Bitcoin as an application can be described as a first-to-file system: if one entity has 50 BTC, and simultaneously sends the same 50 BTC to A and to B, only the transaction that gets confirmed first will process.",
      "There is no intrinsic way of determining from two transactions which came earlier, and for decades this stymied the development of decentralized digital currency. Satoshi's blockchain was the first credible decentralized solution. And now, attention is rapidly starting to shift toward this second part of Bitcoin's technology, and how the blockchain concept can be used for more than just money. Commonly cited applications include using on-blockchain digital assets to represent custom currencies and financial instruments (\"colored coins\"), the ownership of an underlying physical device (\"smart property\"), non-fungible assets such as domain names (\"Namecoin\") as well as more advanced applications such as decentralized exchange, financial derivatives, peer-to-peer gambling on-blockchain identity reputation systems. Another important area of inquiry is \"smart contracts\" - systems which automatically move digital assets according to arbitrary pre-specified rules.",
      "For example, one might have a treasury contract of the form \"A can withdraw up to X currency units per day, B can withdraw up to Y per day, A and B together can withdraw anything, and A can shut off B's ability to withdraw\". The logical extension of this is decentralized autonomous organizations (DAOs) - long-term smart contracts that contain the assets and encode the bylaws of an entire organization. What Ethereum intends to provide is a blockchain with a built-in fully fledged Turing-complete programming language that can be used to create \"contracts\" that can be used to encode arbitrary state transition functions, allowing users to create any of the systems described above, as well as many others that we have not yet imagined, simply by writing up the logic in a few lines of code. Page 1 ethereum.org Ethereum: A Next-Generation Smart Contract and Decentralized Application Platform. By Vitalik Buterin (2014).",
      "History Bitcoin As A State Transition System Mining Merkle Trees Alternative Blockchain Applications Scripting Ethereum Ethereum Accounts Messages and Transactions Ethereum State Transition Function Code Execution Blockchain and Mining Applications Token Systems Financial derivatives Identity and Reputation Systems Decentralized File Storage Decentralized Autonomous Organizations Further Applications Miscellanea And Concerns Modified GHOST Implementation Fees Computation And Turing-Completeness Currency And Issuance Mining Centralization Scalability Putting It All Together: Decentralized Applications Conclusion References and Further Reading Page 2 ethereum.org Page 3 ethereum.org History The concept of decentralized digital currency, as well as alternative applications like property registries, has been around for decades.",
      "The anonymous e-cash protocols of the 1980s and the 1990s, mostly reliant on a cryptographic primitive known as Chaumian blinding, provided a currency with a high degree of privacy, but the protocols largely failed to gain traction because of their reliance on a centralized intermediary. In 1998, Wei Dai's b-money became the first proposal to introduce the idea of creating money through solving computational puzzles as well as decentralized consensus, but the proposal was scant on details as to how decentralized consensus could actually be implemented. In 2005, Hal Finney introduced a concept of \"reusable proofs of work\", a system which uses ideas from b-money together with Adam Back's computationally difficult Hashcash puzzles to create a concept for a cryptocurrency, but once again fell short of the ideal by relying on trusted computing as a backend.",
      "Because currency is a first-to-file application, where the order of transactions is often of critical importance, decentralized currencies require a solution to decentralized consensus. The main roadblock that all pre-Bitcoin currency protocols faced is the fact that, while there had been plenty of research on creating secure Byzantine-fault-tolerant multiparty consensus systems for many years, all of the protocols described were solving only half of the problem. The protocols assumed that all participants in the system were known, and produced security margins of the form \"if N parties participate, then the system can tolerate up to N4 malicious actors\". The problem is, however, that in an anonymous setting such security margins are vulnerable to sybil attacks, where a single attacker creates thousands of simulated nodes on a server or botnet and uses these nodes to unilaterally secure a majority share.",
      "The innovation provided by Satoshi is the idea of combining a very simple decentralized consensus protocol, based on nodes combining transactions into a \"block\" every ten minutes creating an ever-growing blockchain, with proof of work as a mechanism through which nodes gain the right to participate in the system. While nodes with a large amount of computational power do have proportionately greater influence, coming up with more computational power than the entire network combined is much harder than simulating a million nodes. Despite the Bitcoin blockchain model's crudeness and simplicity, it has proven to be good enough, and would over the next five years become the bedrock of over two hundred currencies and protocols around the world.",
      "Page 4 ethereum.org Bitcoin As A State Transition System From a technical standpoint, the Bitcoin ledger can be thought of as a state transition system, where there is a \"state\" consisting of the ownership status of all existing bitcoins and a \"state transition function\" that takes a state and a transaction and outputs a new state which is the result. In a standard banking system, for example, the state is a balance sheet, a transaction is a request to move X from A to B, and the state transition function reduces the value in A's account by X and increases the value in B's account by X. If A's account has less than X in the first place, the state transition function returns an error.",
      "Hence, one can formally define: APPLY(S,TX)  S' or ERROR In the banking system defined above: APPLY( Alice: 50, Bob: 50 ,\"send 20 from Alice to Bob\")   Alice: 30, Bob: 70  But: APPLY( Alice: 50, Bob: 50 ,\"send 70 from Alice to Bob\")  ERROR The \"state\" in Bitcoin is the collection of all coins (technically, \"unspent transaction outputs\" or UTXO) that have been minted and not yet spent, with each UTXO having a denomination and an owner (defined by a 20-byte address which is essentially a cryptographic public key1). A transaction contains one or more inputs, with each input containing a reference to an existing UTXO and a cryptographic signature produced by the private key associated with the owner's address, and one or more outputs, with each output containing a new UTXO to be added to the state. Page 5 ethereum.org The state transition function APPLY(S,TX)  S' can be defined roughly as follows: For each input in TX: If the referenced UTXO is not in S, return an error.",
      "If the provided signature does not match the owner of the UTXO, return an error. If the sum of the denominations of all input UTXO is less than the sum of the denominations of all output UTXO, return an error. Return S with all input UTXO removed and all output UTXO added. The first half of the first step prevents transaction senders from spending coins that do not exist, the second half of the first step prevents transaction senders from spending other people's coins, and the second step enforces conservation of value. In order to use this for payment, the protocol is as follows. Suppose Alice wants to send 11.7 BTC to Bob. First, Alice will look for a set of available UTXO that she owns that totals up to at least 11.7 BTC. Realistically, Alice will not be able to get exactly 11.7 BTC; say that the smallest she can get is 64212. She then creates a transaction with those three inputs and two outputs.",
      "The first output will be 11.7 BTC with Bob's address as its owner, and the second output will be the remaining 0.3 BTC \"change\", with the owner being Alice herself. Mining If we had access to a trustworthy centralized service, this system would be trivial to implement; it could simply be coded exactly as described. However, with Bitcoin we are trying to build a decentralized currency system, so we will need to combine the state transition system with a consensus system in order to ensure that everyone agrees on the order of transactions. Bitcoin's decentralized consensus process requires nodes in the network to continuously attempt to produce packages of transactions called \"blocks\". The network is intended to produce roughly one block every ten minutes, with each block containing a timestamp, a nonce, a reference to (ie. hash of) the Page 6 ethereum.org previous block and a list of all of the transactions that have taken place since the previous block.",
      "Over time, this creates a persistent, ever-growing, \"blockchain\" that constantly updates to represent the latest state of the Bitcoin ledger. The algorithm for checking if a block is valid, expressed in this paradigm, is as follows: Check if the previous block referenced by the block exists and is valid Check that the timestamp of the block is greater than that of the previous block2 and less than 2 hours into the future. Check that the proof of work on the block is valid. Let S0 be the state at the end of the previous block. Suppose TX is the block's transaction list with n transactions. For all i in 0...n-1, setSi1  APPLY(Si,TXi) If any application returns an error, exit and return false. Return true, and register Sn as the state at the end of this block Essentially, each transaction in the block must provide a state transition that is valid.",
      "Note that the state is not encoded in the block in any way; it is purely an abstraction to be remembered by the validating node and can only be (securely) computed for any block by starting from the genesis state and sequentially applying every transaction in every block. Additionally, note that the order in which the miner includes transactions into the block matters; if there are two transactions A and B in a block such that B spends a UTXO created by A, then the block will be valid if A comes before B but not otherwise. The interesting part of the block validation algorithm is the concept of \"proof of work\": the condition is that the SHA256 hash of every block, treated as a 256-bit number, must be less than a dynamically adjusted target, which as of the time of this writing is approximately 2190. The purpose of this is to make block creation computationally \"hard\", thereby preventing sybil attackers from remaking the entire blockchain in their favor.",
      "Because SHA256 is designed to be a completely unpredictable pseudorandom function, the only way to create a valid block is simply trial and error, repeatedly incrementing the nonce and seeing if the new hash matches. At the current target of 2192, this means an average of 264 tries; in general, the target is recalibrated by the network every 2016 blocks so that on average a new block is produced by some node in the network every ten minutes. In order to compensate miners for this computational work, the miner of every block is entitled to include a transaction giving themselves 25 BTC out of nowhere. Additionally, if any transaction has a higher total denomination in its inputs than in its outputs, the difference also goes to the miner as a \"transaction fee\". Incidentally, this is also the only mechanism by which BTC are issued; the genesis state contained no coins at all.",
      "Page 7 ethereum.org In order to better understand the purpose of mining, let us examine what happens in the event of a malicious attacker. Since Bitcoin's underlying cryptography is known to be secure, the attacker will target the one part of the Bitcoin system that is not protected by cryptography directly: the order of transactions. The attacker's strategy is simple: Send 100 BTC to a merchant in exchange for some product (preferably a rapid-delivery digital good) Wait for the delivery of the product Produce another transaction sending the same 100 BTC to himself Try to convince the network that his transaction to himself was the one that came first. Once step (1) has taken place, after a few minutes some miner will include the transaction in a block, say block number 270000. After about one hour, five more blocks will have been added to the chain after that block, with each of those blocks indirectly pointing to the transaction and thus \"confirming\" it.",
      "At this point, the merchant will accept the payment as finalized and deliver the product; since we are assuming this is a digital good, delivery is instant. Now, the attacker creates another transaction sending the 100 BTC to himself. If the attacker simply releases it into the wild, the transaction will not be processed; miners will attempt to run APPLY(S,TX) and notice that TX consumes a UTXO which is no longer in the state. So instead, the attacker creates a \"fork\" of the blockchain, starting by mining another version of block 270000 pointing to the same block 269999 as a parent but with the new transaction in place of the old one. Because the block data is different, this requires redoing the proof of work. Furthermore, the attacker's new version of block 270000 has a different hash, so the original blocks 270001 to 270005 do not \"point\" to it; thus, the original chain and the attacker's new chain are completely separate. The rule is that in a fork the longest blockchain (ie.",
      "the one backed by the largest quantity of proof of work) is taken to be the truth, and so legitimate miners will work on the 270005 chain while the attacker alone is working on the 270000 chain. In order for the attacker to make his blockchain the longest, he would need to have more computational power than the rest of the network combined in order to catch up (hence, \"51 attack\"). Page 8 ethereum.org Merkle Trees Left: it suffices to present only a small number of nodes in a Merkle tree to give a proof of the validity of a branch. Right: any attempt to change any part of the Merkle tree will eventually lead to an inconsistency somewhere up the chain. An important scalability feature of Bitcoin is that the block is stored in a multi-level data structure.",
      "The \"hash\" of a block is actually only the hash of the block header, a roughly 200-byte piece of data that contains the timestamp, nonce, previous block hash and the root hash of a data structure called the Merkle tree storing all transactions in the block. A Merkle tree is a type of binary tree, composed of a set of nodes with a large number of leaf nodes at the bottom of the tree containing the underlying data, a set of intermediate nodes where each node is the hash of its two children, and finally a single root node, also formed from the hash of its two children, representing the \"top\" of the tree. The purpose of the Merkle tree is to allow the data in a block to be delivered piecemeal: a node can download only the header of a block from one source, the small part of the tree relevant to them from Page 9 ethereum.org another source, and still be assured that all of the data is correct.",
      "The reason why this works is that hashes propagate upward: if a malicious user attempts to swap in a fake transaction into the bottom of a Merkle tree, this change will cause a change in the node above, and then a change in the node above that, finally changing the root of the tree and therefore the hash of the block, causing the protocol to register it as a completely different block (almost certainly with an invalid proof of work). The Merkle tree protocol is arguably essential to long-term sustainability. A \"full node\" in the Bitcoin network, one that stores and processes the entirety of every block, takes up about 15 GB of disk space in the Bitcoin network as of April 2014, and is growing by over a gigabyte per month. Currently, this is viable for some desktop computers and not phones, and later on in the future only businesses and hobbyists will be able to participate.",
      "A protocol known as \"simplified payment verification\" (SPV) allows for another class of nodes to exist, called \"light nodes\", which download the block headers, verify the proof of work on the block headers, and then download only the \"branches\" associated with transactions that are relevant to them. This allows light nodes to determine with a strong guarantee of security what the status of any Bitcoin transaction, and their current balance, is while downloading only a very small portion of the entire blockchain. Alternative Blockchain Applications The idea of taking the underlying blockchain idea and applying it to other concepts also has a long history.",
      "In 2005, Nick Szabo came out with the concept of \"secure property titles with owner authority\", a document describing how \"new advances in replicated database technology\" will allow for a blockchain-based system for storing a registry of who owns what land, creating an elaborate framework including concepts such as homesteading, adverse possession and Georgian land tax. However, there was unfortunately no effective replicated database system available at the time, and so the protocol was never implemented in practice. After 2009, however, once Bitcoin's decentralized consensus was developed a number of alternative applications rapidly began to emerge: Namecoin - created in 2010, Namecoin is best described as a decentralized name registration database.",
      "In decentralized protocols like Tor, Bitcoin and BitMessage, there needs to be some way of identifying accounts so that other people can interact with them, but in all existing solutions the only kind identifier available pseudorandom hash like1LW79wp5ZBqaHW1jL5TCiBCrhQYtHagUWy. Ideally, one would like to be able to have an account with a name like \"george\". However, the problem is that if one person can create an account named \"george\" then someone else can use the same process to register \"george\" for themselves as well and impersonate them. The only solution is a first-to-file paradigm, where the first registrant succeeds and the second fails - a problem perfectly suited for the Bitcoin consensus protocol. Namecoin is the oldest, and most successful, implementation of a name registration system using such an idea.",
      "Colored coins - the purpose of colored coins is to serve as a protocol to allow people to create their own digital currencies - or, in the important trivial case of a currency with one unit, digital tokens, Page 10 ethereum.org on the Bitcoin blockchain. In the colored coins protocol, one \"issues\" a new currency by publicly assigning a color to a specific Bitcoin UTXO, and the protocol recursively defines the color of other UTXO to be the same as the color of the inputs that the transaction creating them spent (some special rules apply in the case of mixed-color inputs). This allows users to maintain wallets containing only UTXO of a specific color and send them around much like regular bitcoins, backtracking through the blockchain to determine the color of any UTXO that they receive.",
      "Metacoins - the idea behind a metacoin is to have a protocol that lives on top of Bitcoin, using Bitcoin transactions to store metacoin transactions but having a different state transition function, APPLY'. Because the metacoin protocol cannot prevent invalid metacoin transactions from appearing in the Bitcoin blockchain, a rule is added that if APPLY'(S,TX) returns an error, the protocol defaults to APPLY'(S,TX)  S. This provides an easy mechanism for creating an arbitrary cryptocurrency protocol, potentially with advanced features that cannot be implemented inside of Bitcoin itself, but with a very low development cost since the complexities of mining and networking are already handled by the Bitcoin protocol. Thus, in general, there are two approaches toward building a consensus protocol: building an independent network, and building a protocol on top of Bitcoin.",
      "The former approach, while reasonably successful in the case of applications like Namecoin, is difficult to implement; each individual implementation needs to bootstrap an independent blockchain, as well as building and testing all of the necessary state transition and networking code. Additionally, we predict that the set of applications for decentralized consensus technology will follow a power law distribution where the vast majority of applications would be too small to warrant their own blockchain, and we note that there exist large classes of decentralized applications, particularly decentralized autonomous organizations, that need to interact with each other. The Bitcoin-based approach, on the other hand, has the flaw that it does not inherit the simplified payment verification features of Bitcoin.",
      "SPV works for Bitcoin because it can use blockchain depth as a proxy for validity; at some point, once the ancestors of a transaction go far enough back, it is safe to say that they were legitimately part of the state. Blockchain-based meta-protocols, on the other hand, cannot force the blockchain not to include transactions that are not valid within the context of their own protocols. Hence, a fully secure SPV meta-protocol implementation would need to backward scan all the way to the beginning of the Bitcoin blockchain to determine whether or not certain transactions are valid. Currently, all \"light\" implementations of Bitcoin-based meta-protocols rely on a trusted server to provide the data, arguably a highly suboptimal result especially when one of the primary purposes of a cryptocurrency is to eliminate the need for trust. Scripting Even without any extensions, the Bitcoin protocol actually does facilitate a weak version of a concept of \"smart contracts\".",
      "UTXO in Bitcoin can be owned not just by a public key, but also by a more complicated script expressed in a simple stack-based programming language. In this paradigm, a transaction spending that UTXO must provide data that satisfies the script. Indeed, even the basic public key ownership mechanism is Page 11 ethereum.org implemented via a script: the script takes an elliptic curve signature as input, verifies it against the transaction and the address that owns the UTXO, and returns 1 if the verification is successful and 0 otherwise. Other, more complicated, scripts exist for various additional use cases. For example, one can construct a script that requires signatures from two out of a given three private keys to validate (\"multisig\"), a setup useful for corporate accounts, secure savings accounts and some merchant escrow situations.",
      "Scripts can also be used to pay bounties for solutions to computational problems, and one can even construct a script that says something like \"this Bitcoin UTXO is yours if you can provide an SPV proof that you sent a Dogecoin transaction of this denomination to me\", essentially allowing decentralized cross-cryptocurrency exchange. However, the scripting language as implemented in Bitcoin has several important limitations: Lack of Turing-completeness - that is to say, while there is a large subset of computation that the Bitcoin scripting language supports, it does not nearly support everything. The main category that is missing is loops. This is done to avoid infinite loops during transaction verification; theoretically it is a surmountable obstacle for script programmers, since any loop can be simulated by simply repeating the underlying code many times with an if statement, but it does lead to scripts that are very space-inefficient.",
      "For example, implementing an alternative elliptic curve signature algorithm would likely require 256 repeated multiplication rounds all individually included in the code. Value-blindness - there is no way for a UTXO script to provide fine-grained control over the amount that can be withdrawn. For example, one powerful use case of an oracle contract would be a hedging contract, where A and B put in 1000 worth of BTC and after 30 days the script sends 1000 worth of BTC to A and the rest to B. This would require an oracle to determine the value of 1 BTC in USD, but even then it is a massive improvement in terms of trust and infrastructure requirement over the fully centralized solutions that are available now. However, because UTXO are all-or-nothing, the only way to achieve this is through the very inefficient hack of having many UTXO of varying denominations (eg. one UTXO of 2k for every k up to 30) and having the oracle pick which UTXO to send to A and which to B.",
      "Lack of state - UTXO can either be spent or unspent; there is no opportunity for multi-stage contracts or scripts which keep any other internal state beyond that. This makes it hard to make multi-stage options contracts, decentralized exchange offers or two-stage cryptographic commitment protocols (necessary for secure computational bounties). It also means that UTXO can only be used to build simple, one-off contracts and not more complex \"stateful\" contracts such as decentralized organizations, and makes meta-protocols difficult to implement. Binary state combined with value-blindness also mean that another important application, withdrawal limits, is impossible. Blockchain-blindness - UTXO are blind to blockchain data such as the nonce and previous hash. This severely limits applications in gambling, and several other categories, by depriving the scripting language of a potentially valuable source of randomness.",
      "Page 12 ethereum.org Thus, we see three approaches to building advanced applications on top of cryptocurrency: building a new blockchain, using scripting on top of Bitcoin, and building a meta-protocol on top of Bitcoin. Building a new blockchain allows for unlimited freedom in building a feature set, but at the cost of development time and bootstrapping effort. Using scripting is easy to implement and standardize, but is very limited in its capabilities, and meta-protocols, while easy, suffer from faults in scalability. With Ethereum, we intend to build a generalized framework that can provide the advantages of all three paradigms at the same time.",
      "Ethereum The intent of Ethereum is to merge together and improve upon the concepts of scripting, altcoins and on-chain meta-protocols, and allow developers to create arbitrary consensus-based applications that have the scalability, standardization, feature-completeness, ease of development and interoperability offered by these different paradigms all at the same time. Ethereum does this by building what is essentially the ultimate abstract foundational layer: a blockchain with a built-in Turing-complete programming language, allowing anyone to write smart contracts and decentralized applications where they can create their own arbitrary rules for ownership, transaction formats and state transition functions. A bare-bones version of Namecoin can be written in two lines of code, and other protocols like currencies and reputation systems can be built in under twenty.",
      "Smart contracts, cryptographic \"boxes\" that contain value and only unlock it if certain conditions are met, can also be built on top of our platform, with vastly more power than that offered by Bitcoin scripting because of the added powers of Turing-completeness, value-awareness, blockchain-awareness and state. Ethereum Accounts In Ethereum, the state is made up of objects called \"accounts\", with each account having a 20-byte address and state transitions being direct transfers of value and information between accounts. An Ethereum account contains four fields: The nonce, a counter used to make sure each transaction can only be processed once The account's current ether balance The account's contract code, if present The account's storage (empty by default) \"Ether\" is the main internal crypto-fuel of Ethereum, and is used to pay transaction fees.",
      "In general, there are two types of accounts: externally owned accounts, controlled by private keys, and contract accounts, controlled by their contract code. An externally owned account has no code, and one can send messages from an externally owned account by creating and signing a transaction; in a contract account, every time the Page 13 ethereum.org contract account receives a message its code activates, allowing it to read and write to internal storage and send other messages or create contracts in turn. Messages and Transactions \"Messages\" in Ethereum are somewhat similar to transactions in Bitcoin, but with three important differences. First, an Ethereum message can be created either by an external entity or a contract, whereas a Bitcoin transaction can only be created externally. Second, there is an explicit option for Ethereum messages to contain data.",
      "Finally, the recipient of an Ethereum message, if it is a contract account, has the option to return a response; this means that Ethereum messages also encompass the concept of functions. The term \"transaction\" is used in Ethereum to refer to the signed data package that stores a message to be sent from an externally owned account. Transactions contain the recipient of the message, a signature identifying the sender, the amount of ether and the data to send, as well as two values called STARTGAS and GASPRICE. In order to prevent exponential blowup and infinite loops in code, each transaction is required to set a limit to how many computational steps of code execution it can spawn, including both the initial message and any additional messages that get spawned during execution. STARTGAS is this limit, and GASPRICE is the fee to pay to the miner per computational step.",
      "If transaction execution \"runs out of gas\", all state changes revert - except for the payment of the fees, and if transaction execution halts with some gas remaining then the remaining portion of the fees is refunded to the sender. There is also a separate transaction type, and corresponding message type, for creating a contract; the address of a contract is calculated based on the hash of the account nonce and transaction data. An important consequence of the message mechanism is the \"first class citizen\" property of Ethereum - the idea that contracts have equivalent powers to external accounts, including the ability to send message and create other contracts.",
      "This allows contracts to simultaneously serve many different roles: for example, one might have a member of a decentralized organization (a contract) be an escrow account (another contract) between an paranoid individual employing custom quantum-proof Lamport signatures (a third contract) and a co-signing entity which itself uses an account with five keys for security (a fourth contract). The strength of the Ethereum platform is that the decentralized organization and the escrow contract do not need to care about what kind of account each party to the contract is. Page 14 ethereum.org Ethereum State Transition Function The Ethereum state transition function, APPLY(S,TX) - S' can be defined as follows: Check if the transaction is well-formed (ie. has the right number of values), the signature is valid, and the nonce matches the nonce in the sender's account. If not, return an error. Calculate the transaction fee as STARTGAS  GASPRICE, and determine the sending address from the signature.",
      "Subtract the fee from the sender's account balance and increment the sender's nonce. If there is not enough balance to spend, return an error. Initialize GAS  STARTGAS, and take off a certain quantity of gas per byte to pay for the bytes in the transaction. Transfer the transaction value from the sender's account to the receiving account. If the receiving account does not yet exist, create it. If the receiving account is a contract, run the contract's code either to completion or until the execution runs out of gas. If the value transfer failed because the sender did not have enough money, or the code execution ran out of gas, revert all state changes except the payment of the fees, and add the fees to the miner's account. Otherwise, refund the fees for all remaining gas to the sender, and send the fees paid for gas consumed to the miner.",
      "Page 15 ethereum.org For example, suppose that the contract's code is: if !contract.storagemsg.data0: contract.storagemsg.data0  msg.data1 Note that in reality the contract code is written in the low-level EVM code; this example is written in Serpent, our high-level language, for clarity, and can be compiled down to EVM code. Suppose that the contract's storage starts off empty, and a transaction is sent with 10 ether value, 2000 gas, 0.001 ether gasprice, and two data fields:  2, 'CHARLIE' 3. The process for the state transition function in this case is as follows: Check that the transaction is valid and well formed. Check that the transaction sender has at least 2000  0.001  2 ether. If it is, then subtract 2 ether from the sender's account. Initialize gas  2000; assuming the transaction is 170 bytes long and the byte-fee is 5, subtract 850 so that there is 1150 gas left. Subtract 10 more ether from the sender's account, and add it to the contract's account. Run the code.",
      "In this case, this is simple: it checks if the contract's storage at index 2 is used, notices that it is not, and so it sets the storage at index 2 to the value CHARLIE. Suppose this takes 187 gas, so the remaining amount of gas is 1150 - 187  963 Add 963  0.001  0.963 ether back to the sender's account, and return the resulting state. If there was no contract at the receiving end of the transaction, then the total transaction fee would simply be equal to the provided GASPRICE multiplied by the length of the transaction in bytes, and the data sent alongside the transaction would be irrelevant. Additionally, note that contract-initiated messages can assign a gas limit to the computation that they spawn, and if the sub-computation runs out of gas it gets reverted only to the point of the message call. Hence, just like transactions, contracts can secure their limited computational resources by setting strict limits on the sub-computations that they spawn.",
      "Page 16 ethereum.org Code Execution The code in Ethereum contracts is written in a low-level, stack-based bytecode language, referred to as \"Ethereum virtual machine code\" or \"EVM code\". The code consists of a series of bytes, where each byte represents an operation. In general, code execution is an infinite loop that consists of repeatedly carrying out the operation at the current program counter (which begins at zero) and then incrementing the program counter by one, until the end of the code is reached or an error or STOP or RETURN instruction is detected. The operations have access to three types of space in which to store data: The stack, a last-in-first-out container to which 32-byte values can be pushed and popped Memory, an infinitely expandable byte array The contract's long-term storage, a keyvalue store where keys and values are both 32 bytes. Unlike stack and memory, which reset after computation ends, storage persists for the long term.",
      "The code can also access the value, sender and data of the incoming message, as well as block header data, and the code can also return a byte array of data as an output. The formal execution model of EVM code is surprisingly simple. While the Ethereum virtual machine is running, its full computational state can be defined by the tuple (block_state, transaction, message, code, memory, stack, pc, gas), where block_state is the global state containing all accounts and includes balances and storage. Every round of execution, the current instruction is found by taking the pc-th byte of code, and each instruction has its own definition in terms of how it affects the tuple. For example, ADD pops two items off the stack and pushes their sum, reduces gas by 1 and increments pc by 1, and SSTORE pushes the top two items off the stack and inserts the second item into the contract's storage at the index specified by the first item, as well as reducing gas by up to 200 and incrementing pc by 1.",
      "Although there are many ways to optimize Ethereum via just-in-time compilation, a basic implementation of Ethereum can be done in a few hundred lines of code. Page 17 ethereum.org Blockchain and Mining The Ethereum blockchain is in many ways similar to the Bitcoin blockchain, although it does have some differences. The main difference between Ethereum and Bitcoin with regard to the blockchain architecture is that, unlike Bitcoin, Ethereum blocks contain a copy of both the transaction list and the most recent state. Aside from that, two other values, the block number and the difficulty, are also stored in the block. The block validation algorithm in Ethereum is as follows: Check if the previous block referenced exists and is valid.",
      "Check that the timestamp of the block is greater than that of the referenced previous block and less than 15 minutes into the future Check that the block number, difficulty, transaction root, uncle root and gas limit (various low-level Ethereum-specific concepts) are valid. Check that the proof of work on the block is valid. Let S0 be the STATE_ROOT of the previous block. Let TX be the block's transaction list, with n transactions. For all in in 0...n-1, setSi1  APPLY(Si,TXi). If any applications returns an error, or if the total gas consumed in the block up until this point exceeds the GASLIMIT, return an error. Let S_FINAL be Sn, but adding the block reward paid to the miner. Check if S_FINAL is the same as the STATE_ROOT. If it is, the block is valid; otherwise, it is not valid. Page 18 ethereum.org The approach may seem highly inefficient at first glance, because it needs to store the entire state with each block, but in reality efficiency should be comparable to that of Bitcoin.",
      "The reason is that the state is stored in the tree structure, and after every block only a small part of the tree needs to be changed. Thus, in general, between two adjacent blocks the vast majority of the tree should be the same, and therefore the data can be stored once and referenced twice using pointers (ie. hashes of subtrees). A special kind of tree known as a \"Patricia tree\" is used to accomplish this, including a modification to the Merkle tree concept that allows for nodes to be inserted and deleted, and not just changed, efficiently. Additionally, because all of the state information is part of the last block, there is no need to store the entire blockchain history - a strategy which, if it could be applied to Bitcoin, can be calculated to provide 5-20x savings in space. Applications In general, there are three types of applications on top of Ethereum.",
      "The first category is financial applications, providing users with more powerful ways of managing and entering into contracts using their money. This includes sub-currencies, financial derivatives, hedging contracts, savings wallets, wills, and ultimately even some classes of full-scale employment contracts. The second category is semi-financial applications, where money is involved but there is also a heavy non-monetary side to what is being done; a perfect example is self-enforcing bounties for solutions to computational problems. Finally, there are applications such as online voting and decentralized governance that are not financial at all. Token Systems On-blockchain token systems have many applications ranging from sub-currencies representing assets such as USD or gold to company stocks, individual tokens representing smart property, secure unforgeable coupons, and even token systems with no ties to conventional value at all, used as point systems for incentivization.",
      "Token systems are surprisingly easy to implement in Ethereum. The key point to understand is that all a currency, or token systen, fundamentally is is a database with one operation: subtract X units from A and give X units to B, with the proviso that (i) X had at least X units before the transaction and (2) the transaction is approved by A. All that it takes to implement a token system is to implement this logic into a contract. Page 19 ethereum.org The basic code for implementing a token system in Serpent looks as follows: from  msg.sender to  msg.data0 value  msg.data1 if contract.storagefrom  value: contract.storagefrom  contract.storagefrom  value contract.storageto  contract.storageto  value This is essentially a literal implementation of the \"banking system\" state transition function described further above in this document.",
      "A few extra lines of code need to be added to provide for the initial step of distributing the currency units in the first place and a few other edge cases, and ideally a function would be added to let other contracts query for the balance of an address. But that's all there is to it. Theoretically, Ethereum-based token systems acting as sub-currencies can potentially include another important feature that on-chain Bitcoin-based meta-currencies lack: the ability to pay transaction fees directly in that currency. The way this would be implemented is that the contract would maintain an ether balance with which it would refund ether used to pay fees to the sender, and it would refill this balance by collecting the internal currency units that it takes in fees and reselling them in a constant running auction. Users would thus need to \"activate\" their accounts with ether, but once the ether is there it would be reusable because the contract would refund it each time.",
      "Financial derivatives and Stable-Value Currencies Financial derivatives are the most common application of a \"smart contract\", and one of the simplest to implement in code. The main challenge in implementing financial contracts is that the majority of them require reference to an external price ticker; for example, a very desirable application is a smart contract that hedges against the volatility of ether (or another cryptocurrency) with respect to the US dollar, but doing this requires the contract to know what the value of ETHUSD is. The simplest way to do this is through a \"data feed\" contract maintained by a specific party (eg. NASDAQ) designed so that that party has the ability to update the contract as needed, and providing an interface that allows other contracts to send a message to that contract and get back a response that provides the price. Given that critical ingredient, the hedging contract would look as follows: Wait for party A to input 1000 ether.",
      "Wait for party B to input 1000 ether. Record the USD value of 1000 ether, calculated by querying the data feed contract, in storage, say this is x. After 30 days, allow A or B to \"ping\" the contract in order to send x worth of ether (calculated by querying the data feed contract again to get the new price) to A and the rest to B. Page 20 ethereum.org Such a contract would have significant potential in crypto-commerce. One of the main problems cited about cryptocurrency is the fact that it's volatile; although many users and merchants may want the security and convenience of dealing with cryptographic assets, they many not wish to face that prospect of losing 23 of the value of their funds in a single day.",
      "Up until now, the most commonly proposed solution has been issuer-backed assets; the idea is that an issuer creates a sub-currency in which they have the right to issue and revoke units, and provide one unit of the currency to anyone who provides them (offline) with one unit of a specified underlying asset (eg. gold, USD). The issuer then promises to provide one unit of the underlying asset to anyone who sends back one unit of the crypto-asset. This mechanism allows any non-cryptographic asset to be \"uplifted\" into a cryptographic asset, provided that the issuer can be trusted. In practice, however, issuers are not always trustworthy, and in some cases the banking infrastructure is too weak, or too hostile, for such services to exist. Financial derivatives provide an alternative. Here, instead of a single issuer providing the funds to back up an asset, a decentralized market of speculators, betting that the price of a cryptographic reference asset will go up, plays that role.",
      "Unlike issuers, speculators have no option to default on their side of the bargain because the hedging contract holds their funds in escrow. Note that this approach is not fully decentralized, because a trusted source is still needed to provide the price ticker, although arguably even still this is a massive improvement in terms of reducing infrastructure requirements (unlike being an issuer, issuing a price feed requires no licenses and can likely be categorized as free speech) and reducing the potential for fraud. Page 21 ethereum.org Identity and Reputation Systems The earliest alternative cryptocurrency of all, Namecoin, attempted to use a Bitcoin-like blockchain to provide a name registration system, where users can register their names in a public database alongside other data. The major cited use case is for a DNS system, mapping domain names like \"bitcoin.org\" (or, in Namecoin's case, \"bitcoin.bit\") to an IP address.",
      "Other use cases include email authentication and potentially more advanced reputation systems. Here is the basic contract to provide a Namecoin-like name registration system on Ethereum: if !contract.storagetx.data0: contract.storagetx.data0  tx.data1 The contract is very simple; all it is is a database inside the Ethereum network that can be added to, but not modified or removed from. Anyone can register a name with some value, and that registration then sticks forever. A more sophisticated name registration contract will also have a \"function clause\" allowing other contracts to query it, as well as a mechanism for the \"owner\" (ie. the first registerer) of a name to change the data or transfer ownership. One can even add reputation and web-of-trust functionality on top.",
      "Decentralized File Storage Over the past few years, there have emerged a number of popular online file storage startups, the most prominent being Dropbox, seeking to allow users to upload a backup of their hard drive and have the service store the backup and allow the user to access it in exchange for a monthly fee. However, at this point the file storage market is at times relatively inefficient; a cursory look at various existing solutions shows that, particularly at the \"uncanny valley\" 20-200 GB level at which neither free quotas nor enterprise-level discounts kick in, monthly prices for mainstream file storage costs are such that you are paying for more than the cost of the entire hard drive in a single month. Ethereum contracts can allow for the development of a decentralized file storage ecosystem, where individual users can earn small quantities of money by renting out their own hard drives and unused space can be used to further drive down the costs of file storage.",
      "The key underpinning piece of such a device would be what we have termed the \"decentralized Dropbox contract\". This contract works as follows. First, one splits the desired data up into blocks, encrypting each block for privacy, and builds a Merkle tree out of it. One then makes a contract with the rule that, every N blocks, the contract would pick a random index in the Merkle tree (using the previous block hash, accessible from contract code, as a source of randomness), and give X ether to the first entity to supply a transaction with a Page 22 ethereum.org simplified payment verification-like proof of ownership of the block at that particular index in the tree. When a user wants to re-download their file, they can use a micropayment channel protocol (eg.",
      "pay 1 szabo per 32 kilobytes) to recover the file; the most fee-efficient approach is for the payer not to publish the transaction until the end, instead replacing the transaction with a slightly more lucrative one with the same nonce after every 32 kilobytes. An important feature of the protocol is that, although it may seem like one is trusting many random nodes not to decide to forget the file, one can reduce that risk down to near-zero by splitting the file into many pieces via secret sharing, and watching the contracts to see each piece is still in some node's possession. If a contract is still paying out money, that provides a cryptographic proof that someone out there is still storing the file. Decentralized Autonomous Organizations The general concept of a \"decentralized organization\" is that of a virtual entity that has a certain set of members or shareholders which, perhaps with a 67 majority, have the right to spend the entity's funds and modify its code.",
      "The members would collectively decide on how the organization should allocate its funds. Methods for allocating a DAO's funds could range from bounties, salaries to even more exotic mechanisms such as an internal currency to reward work. This essentially replicates the legal trappings of a traditional company or nonprofit but using only cryptographic blockchain technology for enforcement. So far much of the talk around DAOs has been around the \"capitalist\" model of a \"decentralized autonomous corporation\" (DAC) with dividend-receiving shareholders tradable shares; alternative, perhaps described \"decentralized autonomous community\", would have all members have an equal share in the decision making and require 67 of existing members to agree to add or remove a member. The requirement that one person can only have one membership would then need to be enforced collectively by the group. A general outline for how to code a DO is as follows.",
      "The simplest design is simply a piece of self-modifying code that changes if two thirds of members agree on a change. Although code is theoretically immutable, one can easily get around this and have de-facto mutability by having chunks of the code in separate contracts, having address of which contracts to call stored in the modifiable storage. In a simple implementation of such a DAO contract, there would be three transaction types, distinguished by the data provided in the transaction: 0,i,K,Vto register a proposal with index i to change the address at storage index K to value V 0,i to register a vote in favor of proposal i 2,i to finalize proposal i if enough votes have been made The contract would then have clauses for each of these. It would maintain a record of all open storage changes, along with a list of who voted for them. It would also have a list of all members.",
      "When any storage Page 23 ethereum.org change gets to two thirds of members voting for it, a finalizing transaction could execute the change. A more sophisticated skeleton would also have built-in voting ability for features like sending a transaction, adding members and removing members, and may even provide for Liquid Democracy-style vote delegation (ie. anyone can assign someone to vote for them, and assignment is transitive so if A assigns B and B assigns C then C determines A's vote). This design would allow the DO to grow organically as a decentralized community, allowing people to eventually delegate the task of filtering out who is a member to specialists, although unlike in the \"current system\" specialists can easily pop in and out of existence over time as individual community members change their alignments. An alternative model is for a decentralized corporation, where any account can have zero or more shares, and two thirds of the shares are required to make a decision.",
      "A complete skeleton would involve asset management functionality, the ability to make an offer to buy or sell shares, and the ability to accept offers (preferably with an order-matching mechanism inside the contract). Delegation would also exist Liquid Democracy-style, generalizing the concept of a \"board of directors\". In the future, more advanced mechanisms for organizational governance may be implemented; it is at this point that a decentralized organization (DO) can start to be described as a decentralized autonomous organization (DAO). The difference between a DO and a DAO is fuzzy, but the general dividing line is whether the governance is generally carried out via a political-like process or an automatic process; a good intuitive test is the no common language criterion: can the organization still function if no two members spoke the same language?",
      "Clearly, a simple traditional shareholder-style corporation would fail, whereas something like the Bitcoin protocol would be much more likely to succeed. Robin Hansons futarchy, a mechanism for organizational governance via prediction markets, is a good example of what truly autonomous governance might look like. Note that one should not necessarily assume that all DAOs are superior to all DOs; automation is simply a paradigm that is likely to have have very large benefits in certain particular places and may not be practical in others, and many semi-DAOs are also likely to exist. Further Applications 1. Savings wallets. Suppose that Alice wants to keep her funds safe, but is worried that she will lose or someone will hack her private key. She puts ether into a contract with Bob, a bank, as follows: Alice alone can withdraw a maximum of 1 of the funds per day.",
      "Bob alone can withdraw a maximum of 1 of the funds per day, but Alice has the ability to make a transaction with her key shutting off this ability. Alice and Bob together can withdraw anything. Normally, 1 per day is enough for Alice, and if Alice wants to withdraw more she can contact Bob for help. If Alice's key gets hacked, she runs to Bob to move the funds to a new contract. If she loses her key, Bob will get the funds out eventually. If Bob turns out to be malicious, then she can turn off his ability to withdraw. Page 24 ethereum.org 2. Crop insurance. One can easily make a financial derivatives contract but using a data feed of the weather instead of any price index. If a farmer in Iowa purchases a derivative that pays out inversely based on the precipitation in Iowa, then if there is a drought, the farmer will automatically receive money and if there is enough rain the farmer will be happy because their crops would do well. 3. A decentralized data feed.",
      "For financial contracts for difference, it may actually be possible to decentralize the data feed via a protocol called \"SchellingCoin\". SchellingCoin basically works as follows: N parties all put into the system the value of a given datum (eg. the ETHUSD price), the values are sorted, and everyone between the 25th and 75th percentile gets one token as a reward. Everyone has the incentive to provide the answer that everyone else will provide, and the only value that a large number of players can realistically agree on is the obvious default: the truth. This creates a decentralized protocol that can theoretically provide any number of values, including the ETHUSD price, the temperature in Berlin or even the result of a particular hard computation. 4. Smart multi-signature escrow. Bitcoin allows multisignature transaction contracts where, for example, three out of a given five keys can spend the funds.",
      "Ethereum allows for more granularity; for example, four out of five can spend everything, three out of five can spend up to 10 per day, and two out of five can spend up to 0.5 per day. Additionally, Ethereum multisig is asynchronous - two parties can register their signatures on the blockchain at different times and the last signature will automatically send the transaction. 5. Cloud computing. The EVM technology can also be used to create a verifiable computing environment, allowing users to ask others to carry out computations and then optionally ask for proofs that computations at certain randomly selected checkpoints were done correctly. This allows for the creation of a cloud computing market where any user can participate with their desktop, laptop or specialized server, and spot-checking together with security deposits can be used to ensure that the system is trustworthy (ie. nodes cannot profitably cheat).",
      "Although such a system may not be suitable for all tasks; tasks that require a high level of inter-process communication, for example, cannot easily be done on a large cloud of nodes. Other tasks, however, are much easier to parallelize; projects like SETIhome, foldinghome and genetic algorithms can easily be implemented on top of such a platform. 6. Peer-to-peer gambling. Any number of peer-to-peer gambling protocols, such as Frank Stajano and Richard Clayton's Cyberdice, can be implemented on the Ethereum blockchain. The simplest gambling protocol is actually simply a contract for difference on the next block hash, and more advanced protocols can be built up from there, creating gambling services with near-zero fees that have no ability to cheat. 7. Prediction markets.",
      "Provided an oracle or SchellingCoin, prediction markets are also easy to implement, and prediction markets together with SchellingCoin may prove to be the first mainstream application of futarchy as a governance protocol for decentralized organizations. 8. On-chain decentralized marketplaces, using the identity and reputation system as a base. Page 25 ethereum.org Miscellanea And Concerns Modified GHOST Implementation The \"Greedy Heaviest Observed Subtree\" (GHOST) protocol is an innovation first introduced by Yonatan Sompolinsky and Aviv Zohar in December 2013. The motivation behind GHOST is that blockchains with fast confirmation times currently suffer from reduced security due to a high stale rate - because blocks take a certain time to propagate through the network, if miner A mines a block and then miner B happens to mine another block before miner A's block propagates to B, miner B's block will end up wasted and will not contribute to network security.",
      "Furthermore, there is a centralization issue: if miner A is a mining pool with 30 hashpower and B has 10 hashpower, A will have a risk of producing a stale block 70 of the time (since the other 30 of the time A produced the last block and so will get mining data immediately) whereas B will have a risk of producing a stale block 90 of the time. Thus, if the block interval is short enough for the stale rate to be high, A will be substantially more efficient simply by virtue of its size. With these two effects combined, blockchains which produce blocks quickly are very likely to lead to one mining pool having a large enough percentage of the network hashpower to have de facto control over the mining process.",
      "As described by Sompolinsky and Zohar, GHOST solves the first issue of network security loss by including stale blocks in the calculation of which chain is the \"longest\"; that is to say, not just the parent and further ancestors of a block, but also the stale children of the block's ancestors (in Ethereum jargon, \"uncles\") are added to the calculation of which block has the largest total proof of work backing it. To solve the second issue of centralization bias, we go beyond the protocol described by Sompolinsky and Zohar, and also allow stales to be registered into the main chain to receive a block reward: a stale block receives 93.75 of its base reward, and the nephew that includes the stale block receives the remaining 6.25. Transaction fees, however, are not awarded to uncles. Ethereum implements a simplified version of GHOST which only goes down five levels.",
      "Specifically, a stale block can only be included as an uncle by the 2nd to 5th generation child of its parent, and not any block with a more distant relation (eg. 6th generation child of a parent, or 3rd generation child of a grandparent). This was done for several reasons. First, unlimited GHOST would include too many complications into the calculation of which uncles for a given block are valid. Second, unlimited GHOST with compensation as used in Ethereum removes the incentive for a miner to mine on the main chain and not the chain of a public attacker. Finally, calculations show that five-level GHOST with incentivization is over 95 efficient even with a 15s block time, and miners with 25 hashpower show centralization gains of less than 3.",
      "Page 26 ethereum.org Fees Because every transaction published into the blockchain imposes on the network the cost of needing to download and verify it, there is a need for some regulatory mechanism, typically involving transaction fees, to prevent abuse. The default approach, used in Bitcoin, is to have purely voluntary fees, relying on miners to act as the gatekeepers and set dynamic minimums. This approach has been received very favorably in the Bitcoin community particularly because it is \"market-based\", allowing supply and demand between miners and transaction senders determine the price.",
      "The problem with this line of reasoning is, however, that transaction processing is not a market; although it is intuitively attractive to construe transaction processing as a service that the miner is offering to the sender, in reality every transaction that a miner includes will need to be processed by every node in the network, so the vast majority of the cost of transaction processing is borne by third parties and not the miner that is making the decision of whether or not to include it. Hence, tragedy-of-the-commons problems are very likely to occur. However, as it turns out this flaw in the market-based mechanism, when given a particular inaccurate simplifying assumption, magically cancels itself out. The argument is as follows. Suppose that: A transaction leads to k operations, offering the reward kR to any miner that includes it where R is set by the sender and k and R are (roughly) visible to the miner beforehand. An operation has a processing cost of C to any node (ie.",
      "all nodes have equal efficiency) There are N mining nodes, each with exactly equal processing power (ie. 1N of total) No non-mining full nodes exist. A miner would be willing to process a transaction if the expected reward is greater than the cost. Thus, the expected reward is kRN since the miner has a 1N chance of processing the next block, and the processing cost for the miner is simply kC. Hence, miners will include transactions where kRN  kC, or R  NC. Note that R is the per-operation fee provided by the sender, and is thus a lower bound on the benefit that the sender derives from the transaction, and NC is the cost to the entire network together of processing an operation. Hence, miners have the incentive to include only those transactions for which the total utilitarian benefit exceeds the cost.",
      "However, there are several important deviations from those assumptions in reality: The miner does pay a higher cost to process the transaction than the other verifying nodes, since the extra verification time delays block propagation and thus increases the chance the block will become a stale. There do exist non-mining full nodes. Page 27 ethereum.org The mining power distribution may end up radically inegalitarian in practice. Speculators, political enemies and crazies whose utility function includes causing harm to the network do exist, and they can cleverly set up contracts whose cost is much lower than the cost paid by other verifying nodes. Point 1 above provides a tendency for the miner to include fewer transactions, and point 2 increases NC; hence, these two effects at least partially cancel each other out.",
      "Points 3 and 4 are the major issue; to solve them we simply institute a floating cap: no block can have more operations than BLK_LIMIT_FACTOR times the long-term exponential moving average. Specifically: blk.oplimit  floor((blk.parent.oplimit  (EMAFACTOR - 1)  floor(parent.opcount  BLK_LIMIT_FACTOR))  EMA_FACTOR) BLK_LIMIT_FACTOR and EMA_FACTOR are constants that will be set to 65536 and 1.5 for the time being, but will likely be changed after further analysis. Computation And Turing-Completeness An important note is that the Ethereum virtual machine is Turing-complete; this means that EVM code can encode any computation that can be conceivably carried out, including infinite loops. EVM code allows looping in two ways. First, there is a JUMP instruction that allows the program to jump back to a previous spot in the code, and a JUMPI instruction to do conditional jumping, allowing for statements like while x  27: x  x  2.",
      "Second, contracts can call other contracts, potentially allowing for looping through recursion. This naturally leads to a problem: can malicious users essentially shut miners and full nodes down by forcing them to enter into an infinite loop? The issue arises because of a problem in computer science known as the halting problem: there is no way to tell, in the general case, whether or not a given program will ever halt. As described in the state transition section, our solution works by requiring a transaction to set a maximum number of computational steps that it is allowed to take, and if execution takes longer computation is reverted but fees are still paid. Messages work in the same way. To show the motivation behind our solution, consider the following examples: An attacker creates a contract which runs an infinite loop, and then sends a transaction activating that loop to the miner.",
      "The miner will process the transaction, running the infinite loop, and wait for it to run out of gas. Even though the execution runs out of gas and stops halfway through, the transaction is still valid and the miner still claims the fee from the attacker for each computational step. An attacker creates a very long infinite loop with the intent of forcing the miner to keep computing for such a long time that by the time computation finishes a few more blocks will have come out and it will not be possible for the miner to include the transaction to claim the fee. However, Page 28 ethereum.org the attacker will be required to submit a value for STARTGAS limiting the number of computational steps that execution can take, so the miner will know ahead of time that the computation will take an excessively large number of steps.",
      "An attacker sees a contract with code of some form like send(A,contract.storageA); contract.storageA  0, and sends a transaction with just enough gas to run the first step but not the second (ie. making a withdrawal but not letting the balance go down). The contract author does not need to worry about protecting against such attacks, because if execution stops halfway through the changes get reverted. A financial contract works by taking the median of nine proprietary data feeds in order to minimize risk. An attacker takes over one of the data feeds, which is designed to be modifiable via the variable-address-call mechanism described in the section on DAOs, and converts it to run an infinite loop, thereby attempting to force any attempts to claim funds from the financial contract to run out of gas. However, the financial contract can set a gas limit on the message to prevent this problem.",
      "The alternative to Turing-completeness is Turing-incompleteness, where JUMP and JUMPI do not exist and only one copy of each contract is allowed to exist in the call stack at any given time. With this system, the fee system described and the uncertainties around the effectiveness of our solution might not be necessary, as the cost of executing a contract would be bounded above by its size. Additionally, Turing-incompleteness is not even that big a limitation; out of all the contract examples we have conceived internally, so far only one required a loop, and even that loop could be removed by making 26 repetitions of a one-line piece of code. Given the serious implications of Turing-completeness, and the limited benefit, why not simply have a Turing-incomplete language? In reality, however, Turing-incompleteness is far from a neat solution to the problem.",
      "To see why, consider the following contracts: C0: call(C1) call(C1) C1: call(C2) call(C2) C2: call(C3) call(C3) C49: call(C50) call(C50) C50: (run one step of a program and record the change in storage) Now, send a transaction to A. Thus, in 51 transactions, we have a contract that takes up 250 computational steps. Miners could try to detect such logic bombs ahead of time by maintaining a value alongside each contract specifying the maximum number of computational steps that it can take, and calculating this for contracts calling other contracts recursively, but that would require miners to forbid contracts that create other contracts (since the creation and execution of all 50 contracts above could easily be rolled into a single contract). Another problematic point is that the address field of a message is a variable, so in general it may not even be possible to tell which other contracts a given contract will call ahead of time.",
      "Hence, all in all, we have a surprising conclusion: Turing-completeness is surprisingly easy to manage, and the lack of Page 29 ethereum.org Turing-completeness is equally surprisingly difficult to manage unless the exact same controls are in place - but in that case why not just let the protocol be Turing-complete? Currency And Issuance The Ethereum network includes its own built-in currency, ether, which serves the dual purpose of providing a primary liquidity layer to allow for efficient exchange between various types of digital assets and, more importantly, of providing a mechanism for paying transaction fees. For convenience and to avoid future argument (see the current mBTCuBTCsatoshi debate in Bitcoin), the denominations will be pre-labelled: 1: wei 1012: szabo 1015: finney 1018: ether This should be taken as an expanded version of the concept of \"dollars\" and \"cents\" or \"BTC\" and \"satoshi\".",
      "In the near future, we expect \"ether\" to be used for ordinary transactions, \"finney\" for microtransactions and \"szabo\" and \"wei\" for technical discussions around fees and protocol implementation. The issuance model will be as follows: Ether will be released in a currency sale at the price of 1337-2000 ether per BTC, a mechanism intended to fund the Ethereum organization and pay for development that has been used with success by a number of other cryptographic platforms. Earlier buyers will benefit from larger discounts. The BTC received from the sale will be used entirely to pay salaries and bounties to developers, researchers and projects in the cryptocurrency ecosystem. 0.099x the total amount sold will be allocated to early contributors who participated in development before BTC funding or certainty of funding was available, and another 0.099x will be allocated to long-term research projects. 0.26x the total amount sold will be allocated to miners per year forever after that point.",
      "Page 30 ethereum.org Issuance Breakdown The permanent linear supply growth model reduces the risk of what some see as excessive wealth concentration in Bitcoin, and gives individuals living in present and future eras a fair chance to acquire currency units, while at the same time discouraging depreciation of ether because the \"supply growth rate\" as a percentage still tends to zero over time. We also theorize that because coins are always lost over time due to carelessness, death, etc, and coin loss can be modeled as a percentage of the total supply per year, that the total currency supply in circulation will in fact eventually stabilize at a value equal to the annual issuance divided by the loss rate (eg. at a loss rate of 1, once the supply reaches 26X then 0.26X will be mined and 0.26X lost every year, creating an equilibrium).",
      "Group At launch After 1 year After 5 years Currency units 1.198X 1.458X 2.498X Purchasers 83.5 68.6 40.0 Early contributor distribution 8.26 6.79 3.96 Long-term endowment 8.26 6.79 3.96 Miners 17.8 52.0 Despite the linear currency issuance, just like with Bitcoin over time the supply growth rate nevertheless tends to zero. Page 31 ethereum.org Mining Centralization The Bitcoin mining algorithm basically works by having miners compute SHA256 on slightly modified versions of the block header millions of times over and over again, until eventually one node comes up with a version whose hash is less than the target (currently around 2190). However, this mining algorithm is vulnerable to two forms of centralization. First, the mining ecosystem has come to be dominated by ASICs (application-specific integrated circuits), computer chips designed for, and therefore thousands of times more efficient at, the specific task of Bitcoin mining.",
      "This means that Bitcoin mining is no longer a highly decentralized and egalitarian pursuit, requiring millions of dollars of capital to effectively participate in. Second, most Bitcoin miners do not actually perform block validation locally; instead, they rely on a centralized mining pool to provide the block headers. This problem is arguably worse: as of the time of this writing, the top two mining pools indirectly control roughly 50 of processing power in the Bitcoin network, although this is mitigated by the fact that miners can switch to other mining pools if a pool or coalition attempts a 51 attack. The current intent at Ethereum is to use a mining algorithm based on randomly generating a unique hash function for every 1000 nonces, using a sufficiently broad range of computation to remove the benefit of specialized hardware. Such a strategy will certainly not reduce the gain of centralization to zero, but it does not need to.",
      "Note that each individual user, on their private laptop or desktop, can perform a certain quantity of mining activity almost for free, paying only electricity costs, but after the point of 100 CPU utilization of their computer additional mining will require them to pay for both electricity and hardware. ASIC mining companies need to pay for electricity and hardware starting from the first hash. Hence, if the centralization gain can be kept to below this ratio, (E  H)  E, then even if ASICs are made there will still be room for ordinary miners. Additionally, we intend to design the mining algorithm so that mining requires access to the entire blockchain, forcing miners to store the entire blockchain and at least be capable of verifying every transaction.",
      "This removes the need for centralized mining pools; although mining pools can still serve the legitimate role of evening out the randomness of reward distribution, this function can be served equally well by peer-to-peer pools with no central control. It additionally helps fight centralization, by increasing the number of full nodes in the network so that the network remains reasonably decentralized even if most ordinary users prefer light clients. Page 32 ethereum.org Scalability One common concern about Ethereum is the issue of scalability. Like Bitcoin, Ethereum suffers from the flaw that every transaction needs to be processed by every node in the network. With Bitcoin, the size of the current blockchain rests at about 20 GB, growing by about 1 MB per hour. If the Bitcoin network were to process Visa's 2000 transactions per second, it would grow by 1 MB per three seconds (1 GB per hour, 8 TB per year).",
      "Ethereum is likely to suffer a similar growth pattern, worsened by the fact that there will be many applications on top of the Ethereum blockchain instead of just a currency as is the case with Bitcoin, but ameliorated by the fact that Ethereum full nodes need to store just the state instead of the entire blockchain history. The problem with such a large blockchain size is centralization risk. If the blockchain size increases to, say, 100 TB, then the likely scenario would be that only a very small number of large businesses would run full nodes, with all regular users using light SPV nodes. In such a situation, there arises the potential concern that the full nodes could band together and all agree to cheat in some profitable fashion (eg. change the block reward, give themselves BTC). Light nodes would have no way of detecting this immediately.",
      "Of course, at least one honest full node would likely exist, and after a few hours information about the fraud would trickle out through channels like Reddit, but at that point it would be too late: it would be up to the ordinary users to organize an effort to blacklist the given blocks, a massive and likely infeasible coordination problem on a similar scale as that of pulling off a successful 51 attack. In the case of Bitcoin, this is currently a problem, but there exists a blockchain modification suggested by Peter Todd which will alleviate this issue. In the near term, Ethereum will use two additional strategies to cope with this problem. First, because of the blockchain-based mining algorithms, at least every miner will be forced to be a full node, creating a lower bound on the number of full nodes. Second and more importantly, however, we will include an intermediate state tree root in the blockchain after processing each transaction.",
      "Even if block validation is centralized, as long as one honest verifying node exists, the centralization problem can be circumvented via a verification protocol. If a miner publishes an invalid block, that block must either be badly formatted, or the state Sn is incorrect. Since S0 is known to be correct, there must be some first state Si that is incorrect where Si-1 is correct. The verifying node would provide the index i, along with a \"proof of invalidity\" consisting of the subset of Patricia tree nodes needing to process APPLY(Si-1,TXi) - Si. Nodes would be able to use those nodes to run that part of the computation, and see that the Si generated does not match the Si provided. Another, more sophisticated, attack would involve the malicious miners publishing incomplete blocks, so the full information does not even exist to determine whether or not blocks are valid.",
      "The solution to this is a challenge-response protocol: verification nodes issue \"challenges\" in the form of target transaction indices, and upon receiving a node a light node treats the block as untrusted until another node, whether the miner or another verifier, provides a subset of Patricia nodes as a proof of validity. Page 33 ethereum.org Putting It All Together: Decentralized Applications The contract mechanism described above allows anyone to build what is essentially a command line application run on a virtual machine that is executed by consensus across the entire network, allowing it to modify a globally accessible state as its hard drive. However, for most people, the command line interface that is the transaction sending mechanism is not sufficiently user-friendly to make decentralization an attractive mainstream alternative.",
      "To this end, a complete decentralized application should consist of both low-level business-logic components, whether implemented entirely on Ethereum, using a combination of Ethereum and other systems (eg. a P2P messaging layer, one of which is currently planned to be put into the Ethereum clients) or other systems entirely, and high-level graphical user interface components. The Ethereum clients design is to serve as a web browser, but include support for a eth Javascript API object, which specialized web pages viewed in the client will be able to use to interact with the Ethereum blockchain. From the point of view of the traditional web, these web pages are entirely static content, since the blockchain and other decentralized protocols will serve as a complete replacement for the server for the purpose of handling user-initiated requests. Eventually, decentralized protocols, hopefully themselves in some fashion using Ethereum, may be used to store the web pages themselves.",
      "Conclusion The Ethereum protocol was originally conceived as an upgraded version of a cryptocurrency, providing advanced features such as on-blockchain escrow, withdrawal limits and financial contracts, gambling markets and the like via a highly generalized programming language. The Ethereum protocol would not \"support\" any of the applications directly, but the existence of a Turing-complete programming language means that arbitrary contracts can theoretically be created for any transaction type or application. What is more interesting about Ethereum, however, is that the Ethereum protocol moves far beyond just currency.",
      "Protocols and decentralized applications around decentralized file storage, decentralized computation and decentralized prediction markets, among dozens of other such concepts, have the potential to substantially increase the efficiency of the computational industry, and provide a massive boost to other peer-to-peer protocols by adding for the first time an economic layer. Finally, there is also a substantial array of applications that have nothing to do with money at all. The concept of an arbitrary state transition function as implemented by the Ethereum protocol provides for a platform with unique potential; rather than being a closed-ended, single-purpose protocol intended for a specific array of applications in data storage, gambling or finance, Ethereum is open-ended by design, and we believe that it is extremely well-suited to serving as a foundational layer for a very large number of both financial and non-financial protocols in the years to come.",
      "Page 34 ethereum.org Notes and Further Reading Notes A sophisticated reader may notice that in fact a Bitcoin address is the hash of the elliptic curve public key, and not the public key itself. However, it is in fact perfectly legitimate cryptographic terminology to refer to the pubkey hash as a public key itself. This is because Bitcoin's cryptography can be considered to be a custom digital signature algorithm, where the public key consists of the hash of the ECC pubkey, the signature consists of the ECC pubkey concatenated with the ECC signature, and the verification algorithm involves checking the ECC pubkey in the signature against the ECC pubkey hash provided as a public key and then verifying the ECC signature against the ECC pubkey. Technically, the median of the 11 previous blocks. Internally, 2 and \"CHARLIE\" are both numbers, with the latter being in big-endian base 256 representation. Numbers can be at least 0 and at most 2256-1.",
      "Further Reading Intrinsic value: https:tinyurl.comBitcoinMag-IntrinsicValue Smart property: https:en.bitcoin.itwikiSmart_Property Smart contracts: https:en.bitcoin.itwikiContracts B-money: http:www.weidai.combmoney.txt Reusable proofs of work: http:www.finney.orghalrpow Secure property titles with owner authority: http:szabo.best.vwh.netsecuretitle.html Bitcoin whitepaper: http:bitcoin.orgbitcoin.pdf Namecoin: https:namecoin.org Zooko's triangle: http:en.wikipedia.orgwikiZooko's_triangle Colored coins whitepaper: https:tinyurl.comcoloredcoin-whitepaper Mastercoin whitepaper: https:github.commastercoin-MSCspec Decentralized autonomous corporations, Bitcoin Magazine: https:tinyurl.comBootstrapping-DACs Simplified payment verification:https:en.bitcoin.itwikiScalabilitySimplifiedpaymentverification Merkle trees: http:en.wikipedia.orgwikiMerkle_tree Patricia trees: http:en.wikipedia.orgwikiPatricia_tree GHOST: http:www.cs.huji.ac.ilavivzpubs13btc_scalability_full.pdf StorJ and Autonomous Agents, Jeff Garzik: https:tinyurl.comstorj-agents Mike Hearn on Smart Property at Turing Festival: http:www.youtube.comwatch?vPu4PAMFPo5Y Page 35 ethereum.org Ethereum RLP: https:github.comethereumwikiwiki5BEnglish5D-RLP Ethereum Merkle Patricia trees: https:github.comethereumwikiwiki5BEnglish5D-Patricia-Tree Peter Todd on Merkle sum trees:http:sourceforge.netpbitcoinmailmanmessage31709140 Page 36 ethereum.org"
    ],
    "word_count": 13632,
    "page_count": 36
  },
  "FIL": {
    "chunks": [
      "Filecoin: A Decentralized Storage Network Protocol Labs July 19, 2017 Abstract The internet is in the middle of a revolution: centralized proprietary services are being replaced with decentralized open ones; trusted parties replaced with veri\ufb01able computation; brittle location addresses replaced with resilient content addresses; ine\ufb03cient monolithic services replaced with peer-to-peer algo- rithmic markets. Bitcoin, Ethereum, and other blockchain networks have proven the utility of decen- tralized transaction ledgers. These public ledgers process sophisticated smart contract applications and transact crypto-assets worth tens of billions of dollars. These systems are the \ufb01rst instances of internet- wide Open Services, where participants form a decentralized network providing useful services for pay, with no central management or trusted parties.",
      "IPFS has proven the utility of content-addressing by decentralizing the web itself, serving billions of \ufb01les used across a global peer-to-peer network. It lib- erates data from silos, survives network partitions, works o\ufb04ine, routes around censorship, and gives permanence to digital information. Filecoin is a decentralized storage network that turns cloud storage into an algorithmic market. The market runs on a blockchain with a native protocol token (also called Filecoin), which miners earn by providing storage to clients. Conversely, clients spend Filecoin hiring miners to store or distribute data. As with Bitcoin, Filecoin miners compete to mine blocks with sizable rewards, but Filecoin mining power is proportional to active storage, which directly provides a useful service to clients (unlike Bitcoin mining, whose usefulness is limited to maintaining blockchain consensus). This creates a powerful incen- tive for miners to amass as much storage as they can, and rent it out to clients.",
      "The protocol weaves these amassed resources into a self-healing storage network that anybody in the world can rely on. The network achieves robustness by replicating and dispersing content, while automatically detecting and repairing replica failures. Clients can select replication parameters to protect against di\ufb00erent threat models. The protocols cloud storage network also provides security, as content is encrypted end-to-end at the client, while storage providers do not have access to decryption keys. Filecoin works as an incentive layer on top of IPFS 1, which can provide storage infrastructure for any data. It is especially useful for decentralizing data, building and running distributed applications, and implementing smart contracts. This work: (a) Introduces the Filecoin Network, gives an overview of the protocol, and walks through several components in detail. (b) Formalizes decentralized storage network (DSN) schemes and their properties, then constructs File- coin as a DSN.",
      "(c) Introduces a novel class of proof-of-storage schemes called proof-of-replication, which allows proving that any replica of data is stored in physically independent storage. (d) Introduces a novel useful-work consensus based on sequential proofs-of-replication and storage as a measure of power. (e) Formalizes veri\ufb01able markets and constructs two markets, a Storage Market and a Retrieval Market, which govern how data is written to and read from Filecoin, respectively. (f) Discusses use cases, connections to other systems, and how to use the protocol. Note: Filecoin is a work in progress. Active research is under way, and new versions of this paper will appear at https:\ufb01lecoin.io. For comments and suggestions, contact us at research\ufb01lecoin.io. Contents Introduction Elementary Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Protocol Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Paper organization . .",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . De\ufb01nition of a Decentralized Storage Network Fault tolerance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Proof-of-Replication and Proof-of-Spacetime Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Proof-of-Replication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Proof-of-Spacetime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Practical PoRep and PoSt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Usage in Filecoin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Filecoin: a DSN Construction Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      ". . . . . . . . . . . . . Data Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Guarantees and Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Filecoin Storage and Retrieval Markets Veri\ufb01able Markets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Storage Market . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Retrieval Market . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Useful Work Consensus Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Filecoin Consensus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Smart Contracts Contracts in Filecoin . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      ". . . . . . . . . . . . . . Integration with other systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Future Work On-going Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Open Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Proofs and Formal Veri\ufb01cation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . List of Figures Sketch of the Filecoin Protocol. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Illustration of the Filecoin Protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Illustration of the underlying mechanism of PoSt.Prove . . . . . . . . . . . . . . . . . . . . . . Proof-of-Replication and Proof-of-Spacetime protocol sketches . . . . . . . . . . . . . . . . . . Data Structures in a DSN scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "Example execution of the Filecoin DSN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Description of the Put and Get Protocols in the Filecoin DSN . . . . . . . . . . . . . . . . . . Description of the Manage Protocol in the Filecoin DSN . . . . . . . . . . . . . . . . . . . . . Generic protocol for Veri\ufb01able Markets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Orders data structures for the Retrieval and Storage Markets . . . . . . . . . . . . . . . . . . Detailed Storage Market protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Detailed Retrieval Market protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Leader Election in the Expected Consensus protocol . . . . . . . . . . . . . . . . . . . . . . . Introduction Filecoin is a protocol token whose blockchain runs on a novel proof, called Proof-of-Spacetime, where blocks are created by miners that are storing data.",
      "Filecoin protocol provides a data storage and retrieval service via a network of independent storage providers that does not rely on a single coordinator, where: (1) clients pay to store and retrieve data, (2) Storage Miners earn tokens by o\ufb00ering storage (3) Retrieval Miners earn tokens by serving data. Elementary Components The Filecoin protocol builds upon four novel components. 1. Decentralized Storage Network (DSN): We provide an abstraction for network of independent storage providers to o\ufb00er storage and retrieval services (in Section 2). Later, we present the Filecoin protocol as an incentivized, auditable and veri\ufb01able DSN construction (in Section 4). 2. Novel Proofs-of-Storage: We present two novel Proofs-of-Storage (in Section 3): (1) Proof-of- Replication allows storage providers to prove that data has been replicated to its own uniquely dedicated physical storage.",
      "Enforcing unique physical copies enables a veri\ufb01er to check that a prover is not deduplicating multiple copies of the data into the same storage space; (2) Proof-of-Spacetime allows storage providers to prove they have stored some data throughout a speci\ufb01ed amount of time. 3. Veri\ufb01able Markets: We model storage requests and retrieval requests as orders in two decentralized veri\ufb01able markets operated by the Filecoin network (in Section 5). Veri\ufb01able markets ensure that payments are performed when a service has been correctly provided. We present the Storage Market and the Retrieval Market where miners and clients can respectively submit storage and retrieval orders. 4. Useful Proof-of-Work: We show how to construct a useful Proof-of-Work based on Proof-of- Spacetime that can be used in consensus protocols. Miners do not need to spend wasteful computation to mine blocks, but instead must store data in the network.",
      "Protocol Overview  The Filecoin protocol is a Decentralized Storage Network construction built on a blockchain and with a native token. Clients spend tokens for storing and retrieving data and miners earn tokens by storing and serving data. The Filecoin DSN handle storage and retrieval requests respectively via two veri\ufb01able markets: the Storage Market and the Retrieval Market. Clients and miners set the prices for the services requested and o\ufb00ered and submit their orders to the markets. The markets are operated by the Filecoin network which employs Proof-of-Spacetime and Proof-of- Replication to guarantee that miners have correctly stored the data they committed to store. Finally, miners can participate in the creations of new blocks for the underlining blockchain. The in\ufb02uence of a miner over the next block is proportional to the amount of their storage currently in use in the network.",
      "A sketch of the Filecoin protocol, using nomenclature de\ufb01ned later within the paper, is shown in Figure 1 accompanied with an illustration in Figure 2. Paper organization The remainder of this paper is organized as follows. We present our de\ufb01nition of and requirements for a theoretical DSNscheme in Section 2. In Section 3 we motivate, de\ufb01ne, and present our Proof-of-Replication and Proof-of-Spacetime protocols, used within Filecoin to cryptographically verify that data is continuously stored in accordance with deals made. Section 4 describes the concrete instantiation of the Filecoin DSN, describing data structures, protocols, and the interactions between participants. Section 5 de\ufb01nes and de- scribes the concept of Veri\ufb01able Markets, as well as their implementations, the Storage Market and Retrieval Market.",
      "Section 6 motivates and describes the use of the Proof-of-Spacetime protocol for demonstrating and evaluating a miners contribution to the network, which is necessary to extend the blockchain and assign the block reward. Section 7 provides a brief description of Smart Contracts within the Filecoin We conclude with a discussion of future work in Section 8. Filecoin Protocol Sketch Network at each epoch t in the ledger L: 1. for each new block: (a) check if the block is in the valid format (b) check if all transactions are valid (c) check if all orders are valid (d) check if all proofs are valid (e) check if all pledges are valid (f) discard block, if any of the above fails 2. for each new order O introduced in t (a) add O to the Storage Markets orderbook. (b) if O is a bid: lock O.funds (c) if O is an ask: lock O.space (d) if O is a deal: run Put.AssignOrders 3.",
      "for each O in the Storage Markets orderbook: (a) check if O has expired (or canceled):  remove O from the orderbook  return unspent O.funds  free O.space from AllocTable (b) if O is a deal, check if the expected proofs exist by running Manage.RepairOrders:  if one missing, penalize the Ms pledge collateral  if proofs are missing for more than fault epochs, cancel order and re-introduce it to the market  if the piece cannot be retrieved and re- constructed from the network, cancel or- der and re-fund the client Client at any time: 1. submit new storage orders via Put.AddOrders (a) \ufb01nd matching orders via Put.MatchOrders (b) send \ufb01le to the matched miner M 2. submit new retrieval orders via Get.AddOrders (a) \ufb01nd matching orders via Get.MatchOrders (b) create a payment channel with M on receiving Odeal from Storage Miners M 1. sign Odeal 2. submit the signed Odeal to the blockchain via Put.AddOrders on receiving (pi) from Retrieval Miners M: 1.",
      "verify that (pi) is valid and it was requested 2. send a micropayment to M Storage Mine at any time: 1. renew expired pledges via Manage.PledgeSector 2. pledge new storage via Manage.PledgeSector 3. submit a new ask order via Put.AddOrder at each epoch t: 1. for each Oask in the orderbook: (a) \ufb01nd matched orders via Put.MatchOrders (b) start a new deal by contacting the matching client 2. for each sector pledged: (a) generate proof storage Manage.ProveSector (b) if time to post the proof (every proof epochs), submit it to the blockchain on receiving piece p from client C: 1. check if the piece is of the size speci\ufb01ed in the order Obid 2. create Odeal and sign it and send it to C 3. store the piece in a sector 4. if the sector is full, run Manage.SealSector Retrieval Mine at any time: 1. gossip ask orders to the network 2. listen to bid orders from the network on retrieval request from C: 1. start payment channel with C 2. split data in multiple parts 3.",
      "only send parts if payments are received Figure 1: Sketch of the Filecoin Protocol. Orders gossiped off-chain Incremental micropayments Order Matching Settlement Storage Market Filecoin Blockchain Retrieval Market Transactions Orderbook Allocation Table (Off Chain) (On Chain) Order Lock storage Transfer filecoin Piece of data Signed by Send Query Claim micropayments deal deal , Miner Client response challenge payment deal , deal deal Data sent in parts Figure 2: Illustration of the Filecoin Protocol, showing an overview of the Client-Miner interactions. The Storage and Retrieval Markets shown above and below the blockchain, respectively, with time advancing from the Order Matching phase on the left to the Settlement phase on the right. Note that before micropayments can be made for retrieval, the client must lock the funds for the microtransaction. De\ufb01nition of a Decentralized Storage Network We introduce the notion of a Decentralized Storage Network (DSN) scheme.",
      "DSNs aggregate storage o\ufb00ered by multiple independent storage providers and self-coordinate to provide data storage and data retrieval to clients. Coordination is decentralized and does not require trusted parties: the secure operation of theses systems is achieved through protocols that coordinate and verify operations carried out by individual parties. DSNs can employ di\ufb00erent strategies for coordination, including Byzantine Agreement, gossip protocols, or CRDTs, depending on the requirements of the system. Later, in Section 4, we provide a construction for the Filecoin DSN. De\ufb01nition 2.1. A DSN scheme \u03a0 is a tuple of protocols run by storage providers and clients: (Put, Get, Manage)  Put(data) key: Clients execute the Put protocol to store data under a unique identi\ufb01er key. Get(key) data: Clients execute the Get protocol to retrieve data that is currently stored using key.",
      "Manage(): The network of participants coordinates via the Manage protocol to: control the available storage, audit the service o\ufb00ered by providers and repair possible faults. The Manage protocol is run by storage providers often in conjunction with clients or a network of auditors1. A DSN scheme \u03a0 must guarantee data integrity and retrievability as well as tolerate management and storage faults de\ufb01ned in the following sections. Fault tolerance 2.1.1 Management faults We de\ufb01ne management faults to be byzantine faults caused by participants in the Manage protocol. A DSN scheme relies on the fault tolerance of its underlining Manage protocol. Violations on the faults tolerance assumptions for management faults can compromise liveness and safety of the system. For example, consider a DSN scheme \u03a0, where the Manage protocol requires Byzantine Agreement (BA) to audit storage providers.",
      "In such protocol, the network receives proofs of storage from storage providers and runs BA to agree on the validity of these proofs. If the BA tolerates up to f faults out of n total nodes, then our DSN can tolerate f  n2 faulty nodes. On violations of these assumptions, audits can be compromised. 2.1.2 Storage faults We de\ufb01ne storage faults to be byzantine faults that prevent clients from retrieving the data: i.e. Storage Miners lose their pieces, Retrieval Miners stop serving pieces. A successful Put execution is (f, m)-tolerant if it results in its input data being stored in m independent storage providers (out of n total) and it can tolerate up to f byzantine providers. The parameters f and m depend on protocol implementation; protocol designers can \ufb01x f and m or leave the choice to the user, extending Put(data) into Put(data, f, m). A Get execution on stored data is successful if there are fewer than f faulty storage providers.",
      "For example, consider a simple scheme, where the Put protocol is designed such that each storage provider stores all of the data. In this scheme m  n and f  m 1. Is it always f  m 1? No, some schemes can be designed using erasure coding, where each storage providers store a special portion of the data, such that x out of m storage providers are required to retrieve the data; in this case f  m x. Properties We describe the two required properties for a DSN scheme and then present additional properties required by the Filecoin DSN. 1In the case where the Manage protocol relies on a blockchain, we consider the miners as auditors, since they verify and coordinate storage providers 2.2.1 Data Integrity This property requires that no bounded adversary A can convince clients to accept altered or falsi\ufb01ed data at the end of a Get execution. De\ufb01nition 2.2.",
      "A DSN scheme \u03a0 provides data integrity if: for any successful Put execution for some data d under key k, there is no computationally-bounded adversary A that can convince a client to accept d, for d  d at the end of a Get execution for identi\ufb01er k. 2.2.2 Retrievability This property captures the requirement that, given our fault-tolerance assumptions of \u03a0, if some data has been successfully stored in \u03a0 and storage providers continue to follow the protocol, then clients can eventually retrieve the data. De\ufb01nition 2.3. A DSN scheme \u03a0 provides retrievability if: for any successful Put execution for data under key, there exists a successful Get execution for key for which a client retrieves data.2. 2.2.3 Other Properties DSNs can provide other properties speci\ufb01c to their application. We de\ufb01ne three key properties required by the Filecoin DSN: public veri\ufb01ability, auditability, and incentive-compatibility. De\ufb01nition 2.4.",
      "A DSN scheme \u03a0 is publicly veri\ufb01able if: for each successful Put, the network of storage providers can generate a proof that the data is currently being stored. The Proof-of-Storage must convince any e\ufb03cient veri\ufb01er, which only knows key and does not have access to data. De\ufb01nition 2.5. A DSN scheme \u03a0 is auditable, if it generates a veri\ufb01able trace of operation that can be checked in the future to con\ufb01rm storage was indeed stored for the right duration of time. De\ufb01nition 2.6. A DSN scheme \u03a0 is incentive-compatible, if: storage providers are rewarded for successfully o\ufb00ering storage and retrieval service, or penalized for misbehaving, such that the storage providers dominant strategy is to store data. 2This de\ufb01nition does not guarantee every Get to succeed: if every Get eventually returns data, then the scheme is fair.",
      "Proof-of-Replication and Proof-of-Spacetime In the Filecoin protocol, storage providers must convince their clients that they stored the data they were paid to store; in practice, storage providers will generate Proofs-of-Storage (PoS) that the blockchain network (or the clients themselves) veri\ufb01es. In this section we motivate, present and outline implementations for the Proof-of-Replication (PoRep) and Proof-of-Spacetime (PoSt) schemes used in Filecoin. Motivation Proofs-of-Storage (PoS) schemes such as Provable Data Possession (PDP) 2 and Proof-of-Retrievability (PoR) 3, 4 schemes allow a user (i.e. the veri\ufb01er V) who outsources data D to a server (i.e. the prover P) to repeatedly check if the server is still storing D. The user can verify the integrity of the data outsourced to a server in a very e\ufb03cient way, more e\ufb03ciently than downloading the data.",
      "The server generates probabilistic proofs of possession by sampling a random set of blocks and transmits a small constant amount of data in a challengeresponse protocol with the user. PDP and PoR schemes only guarantee that a prover had possession of some data at the time of the chal- lengeresponse. In Filecoin, we need stronger guarantees to prevent three types of attacks that malicious miners could exploit to get rewarded for storage they are not providing: Sybil attack, outsourcing attacks, generation attacks. Sybil Attacks: Malicious miners could pretend to store (and get paid for) more copies than the ones physically stored by creating multiple Sybil identities, but storing the data only once. Outsourcing Attacks: Malicious miners could commit to store more data than the amount they can physically store, relying on quickly fetching data from other storage providers.",
      "Generation Attacks: Malicious miners could claim to be storing a large amount of data which they are instead e\ufb03ciently generating on-demand using a small program. If the program is smaller than the purportedly stored data, this in\ufb02ates the malicious miners likelihood of winning a block reward in Filecoin, which is proportional to the miners storage currently in use. Proof-of-Replication Proof-of-Replication (PoRep) is a novel Proof-of-Storage which allows a server (i.e. the prover P) to convince a user (i.e. the veri\ufb01er V) that some data D has been replicated to its own uniquely dedicated physical storage. Our scheme is an interactive protocol, where the prover P: (a) commits to store n distinct replicas (physically independent copies) of some data D, and then (b) convinces the veri\ufb01er V, that P is indeed storing each of the replicas via a challengeresponse protocol.",
      "To the best of our knowledge, PoRep improves on PoR and PDP schemes, preventing Sybil Attacks, Outsourcing Attacks, and Generation Attacks. Note. For a formal de\ufb01nition, a description of its properties, and an in-depth study of Proof-of-Replication, we refer the reader to 5. De\ufb01nition 3.1. (Proof-of-Replication) A PoRep scheme enables an e\ufb03cient prover P to convince a veri\ufb01er V that P is storing a replica R, a physical independent copy of some data D, unique to P. A PoRep protocol is characterized by a tuple of polynomial-time algorithms: (Setup, Prove, Verify)  PoRep.Setup(1\u03bb, D) R, SP, SV, where SP and SV are scheme-speci\ufb01c setup variables for P and V, \u03bb is a security parameter. PoRep.Setup is used to generate a replica R, and give P and V the necessary information to run PoRep.Prove and PoRep.Verify. Some schemes may require the prover or interaction with a third party to compute PoRep.Setup.",
      "PoRep.Prove(SP, R, c) \u03c0c, where c is a random challenge issued by a veri\ufb01er V, and \u03c0c is a proof that a prover has access to R a speci\ufb01c replica of D. PoRep.Prove is run by P to produce a \u03c0c for V. PoRep.Verify(SV, c, \u03c0c) 0, 1, which checks whether a proof is correct. PoRep.Verify is run by V and convinces V whether P has been storing R. Proof-of-Spacetime Proof-of-Storage schemes allow a user to check if a storage provider is storing the outsourced data at the time of the challenge. How can we use PoS schemes to prove that some data was being stored throughout a period of time? A natural answer to this question is to require the user to repeatedly (e.g. every minute) send challenges to the storage provider. However, the communication complexity required in each interaction can be the bottleneck in systems such as Filecoin, where storage providers are required to submit their proofs to the blockchain network.",
      "To address this question, we introduce a new proof, Proof-of-Spacetime, where a veri\ufb01er can check if a prover is storing herhis outsourced data for a range of time. The intuition is to require the prover to (1) generate sequential Proofs-of-Storage (in our case Proof-of-Replication), as a way to determine time (2) recursively compose the executions to generate a short proof. De\ufb01nition 3.2. (Proof-of-Spacetime) A PoSt scheme enables an e\ufb03cient prover P to convince a veri\ufb01er V that P is storing some data D for some time t. A PoSt is characterized by a tuple of polynomial-time algorithms: (Setup, Prove, Verify)  PoSt.Setup(1\u03bb, D) SP, SV, where SP and SV are scheme-speci\ufb01c setup variables for P and V, \u03bb is a security parameter. PoSt.Setup is used to give P and V the necessary information to run PoSt.Prove and PoSt.Verify. Some schemes may require the prover or interaction with a third party to compute PoSt.Setup.",
      "PoSt.Prove(SP, D, c, t) \u03c0c, where c is a random challenge issued by a veri\ufb01er V, and \u03c0c is a proof that a prover has access to D for some time t. PoSt.Prove is run by P to produce a \u03c0c for V. PoSt.Verify(SV, c, t, \u03c0c) 0, 1, which checks whether a proof is correct. PoSt.Verify is run by V and convinces V whether P has been storing D for some time t. Practical PoRep and PoSt We are interested in practical PoRep and PoSt constructions that can be deployed in existing systems and do not rely on trusted parties or hardware. We give a construction for PoRep (see Seal-based Proof-of-Replication in 5) that requires a very slow sequential computation Seal to be performed during Setup to generate a replica. The protocol sketches for PoRep and PoSt are presented in Figure 4 and the underlying mechanism of the proving step in PoSt is illustrated in Figure 3. 3.4.1 Cryptographic building blocks Collision-resistant hashing. We use a collision resistant hash function CRH : 0, 10, 1O(\u03bb).",
      "We also use a collision resistant hash function MerkleCRH, which divides a string in multiple parts, construct a binary tree and recursively apply CRH and outputs the root. zk-SNARKs. Our practical implementations of PoRep and PoSt rely on zero-knowledge Succinct Non- interactive ARguments of Knowledge (zk-SNARKs) 6, 7, 8. Because zk-SNARKs are succinct, proofs are very short and easy to verify. More formally, let L be an NP language and C be a decision circuit for L. A trusted party conducts a one-time setup phase that results in two public keys: a proving key pk and a veri\ufb01cation key vk. The proving key pk enables any (untrusted) prover to generate a proof \u03c0 attesting that x L for an instance x of her choice. The non-interactive proof \u03c0 is both zero-knowledge and proof-of- knowledge. Anyone can use the veri\ufb01cation key vk to verify the proof \u03c0; in particular zk-SNARK proofs are publicly veri\ufb01able: anyone can verify \u03c0, without interacting with the prover that generated \u03c0.",
      "The proof \u03c0 has constant size and can be veri\ufb01ed in time that is linear in x. A zk-SNARK for circuit satis\ufb01ability is a triple of polynomial-time algorithms (KeyGen, Prove, Verify)  KeyGen(1\u03bb, C) (pk, vk). On input security parameter \u03bb and a circuit C, KeyGen probabilistically samples pk and vk. Both keys are published as public parameters and can be used to proveverify membership in LC. Prove(pk, x, w) \u03c0. On input pk and input x and witness for the NP-statement w, the prover Prove outputs a non-interactive proof \u03c0 for the statement x LC. Verify(vk, x, \u03c0) 0, 1. On input vk, an input x, and a proof \u03c0, the veri\ufb01er Verify outputs 1 if x LC. We refer the interested reader to 6, 7, 8 for formal presentation and implementation of zk-SNARK systems. Generally these systems require the KeyGen operation to be run by a trusted party; novel work on Scalable Computational Integrity and Privacy (SCIP) systems 9 shows a promising direction to avoid this initial step, hence the above trust assumption.",
      "3.4.2 Seal operation The role of the Seal operation is to (1) force replicas to be physically independent copies by requiring provers to store a pseudo-random permutation of D unique to their public key, such that committing to store n replicas results in dedicating disk space for n independent replicas (hence n times the storage size of a replica) and (2) to force the generation of the replica during PoRep.Setup to take substantially longer than the time expected for responding to a challenge. For a more formal de\ufb01nition of the Seal operation see 5. The above operation can be realized with Seal\u03c4 AES256, and \u03c4 such that Seal\u03c4 AES256 takes 10-100x longer than the honest challenge-prove-verify sequence. Note that it is important to choose \u03c4 such that running Seal\u03c4 is distinguishably more expensive than running Prove with random access to R.",
      "3.4.3 Practical PoRep construction This section describes the construction of the PoRep protocol and includes a simpli\ufb01ed protocol sketch in Figure 4; implementation and optimization details are omitted. Creating a Replica. The Setup algorithm generates a replica via the Seal operation and a proof that it was correctly generated. The prover generates the replica and sends the outputs (excluding R) to the veri\ufb01er. Setup  inputs:  prover key pair (pkP, skP)  prover SEAL key pkSEAL  data D  outputs: replica R, Merkle root rt of R, proof \u03c0SEAL Proving Storage. The Prove algorithm generates a proof of storage for the replica. The prover receives a random challenge, c, from the veri\ufb01er, which determines a speci\ufb01c leaf Rc in the Merkle tree of R with root rt; the prover generates a proof of knowledge about Rc and its Merkle path leading up to rt. Prove  inputs:  prover Proof-of-Storage key pkPOS  replica R  random challenge c  outputs: a proof \u03c0POS Verifying the Proofs.",
      "The Verify algorithm checks the validity of the proofs of storage given the Merkle root of the replica and the hash of the original data. Proofs are publicly veri\ufb01able: nodes in the distributed system maintaining the ledger and clients interested in particular data can verify these proofs. Verify  inputs:  prover public key, pkP  veri\ufb01er SEAL and POS keys vkSEAL, vkPOS  hash of data D, hD  Merkle root of replica R, rt  random challenge, c  tuple of proofs, (\u03c0SEAL, \u03c0POS)  outputs: bit b, equals 1 if proofs are valid 3.4.4 Practical PoSt construction This section describes the construction of the PoSt protocol and includes a simpli\ufb01ed protocol sketch in Fig- ure 4; implementation and optimization details are omitted. The Setup and Verify algorithm are equivalent to the PoRep construction, hence we describe here only Prove. Proving space and time. The Prove algorithm generates a Proof-of-Spacetime for the replica.",
      "The prover receives a random challenge from the veri\ufb01er and generate Proofs-of-Replication in sequence, using the output of a proof as an input of the other for a speci\ufb01ed amount of iterations t (see Figure 3). Prove  inputs:  prover PoSt key pkPOST  replica R  random challenge c  time parameter t  outputs: a proof \u03c0POST Parameter Function pkSEAL Data flow Hash Output POST Merkle Tree                Generate proof Repeat times Generate new challenge Challenge POST at Loop counter  Figure 3: Illustration of the underlying mechanism of PoSt.Prove showing the iterative proof to demonstrate storage over time. Usage in Filecoin The Filecoin protocol employs Proof-of-Spacetime to audit the storage o\ufb00ered by miners. To use PoSt in Filecoin, we modify our scheme to be non-interactive since there is no designated veri\ufb01er, and we want any member of the network to be able to verify. Since our veri\ufb01er runs in the public-coin model, we can extract randomness from the blockchain to issue challenges.",
      "Filecoin PoRep protocol Setup  inputs:  prover key pair (pkP, skP)  prover SEAL key pkSEAL  data D  outputs: replica R, Merkle root rt of R, proof \u03c0SEAL 1) Compute hD : CRH(D) 2) Compute R : Seal\u03c4(D, skP) 3) Compute rt : MerkleCRH(R) 4) Set x : (pkP, hD, rt) 5) Set w : (skP, D) 6) Compute \u03c0SEAL : SCIP.Prove(pkSEAL, x , w ) 7) Output R, rt, \u03c0SEAL Prove  inputs:  prover Proof-of-Storage key pkPOS  replica R  random challenge c  outputs: a proof \u03c0POS 1) Compute rt : MerkleCRH(R) 2) Compute path : Merkle path from rt to leaf Rc 3) Set x : (rt, c) 4) Set w : (path, Rc) 5) Compute \u03c0POS : SCIP.Prove(pkPOS, x , w ) 6) Output \u03c0POS Verify  inputs:  prover public key, pkP  veri\ufb01er SEAL and POS keys vkSEAL, vkPOS  hash of data D, hD  Merkle root of replica R, rt  random challenge, c  tuple of proofs, (\u03c0SEAL, \u03c0POS)  outputs: bit b, equals 1 if proofs are valid 1) Set x1 : (pkP, hD, rt) 2) Compute b1 : SCIP.Verify(vkSEAL, x1 , \u03c0SEAL) 3) Set x2 : (rt, c) 4) Compute b2 : SCIP.Verify(vkPOS, x2 , \u03c0POS) 5) Output b1 b2 Filecoin PoSt protocol Setup  inputs:  prover key pair (pkP, skP)  prover POST key pair pkPOST  some data D  outputs: replica R, Merkle root rt of R, proof \u03c0SEAL 1) Compute R, rt, \u03c0SEAL : PoRep.Setup(pkP, skP, pkSEAL, D) 2) Output R, rt, \u03c0SEAL Prove  inputs:  prover PoSt key pkPOST  replica R  random challenge c  time parameter t  outputs: a proof \u03c0POST 1) Set \u03c0POST :  2) Compute rt : MerkleCRH(R) 3) For i  0...t: a) Set c : CRH(\u03c0POSTci) b) Compute \u03c0POS : PoRep.Prove(pkPOS, R, c) c) Set x : (rt, c, i) d) Set w : (\u03c0POS, \u03c0POST) e) Compute \u03c0POST : SCIP.Prove(pkPOST, x , w ) 4) Output \u03c0POST Verify  inputs:  prover public key pkP  veri\ufb01er SEAL and POST keys vkSEAL, vkPOST  hash of some data hD  Merkle root of some replica rt  random challenge c  time parameter t  tuple of proofs (\u03c0SEAL, \u03c0POST)  outputs: bit b, equals 1 if proofs are valid 1) Set x1 : (pkP, hD, rt) 2) Compute b1 : SCIP.Verify(vkSEAL, x1 , \u03c0SEAL) 3) Set x2 : (rt, c, t) 4) Compute b2 : SCIP.Verify(vkPOST, x2 , \u03c0POST) 5) Output b1 b2 Figure 4: Proof-of-Replication and Proof-of-Spacetime protocol sketches.",
      "Here CRH denotes a collision- resistant hash, x is the NP-statement to be proven, and w is the witness. Filecoin: a DSN Construction The Filecoin DSN is a decentralized storage network that is auditable, publicly veri\ufb01able and designed on incentives. Clients pay a network of miners for data storage and retrieval; miners o\ufb00er disk space and bandwidth in exchange of payments. Miners receive their payments only if the network can audit that their service was correctly provided. In this section, we present the Filecoin DSN construction, based on the DSN de\ufb01nition and Proof-of- Spacetime. Setting 4.1.1 Participants Any user can participate as a Client, a Storage Miner, andor a Retrieval Miner. Clients pay to store data and to retrieve data in the DSN, via Put and Get requests. Storage Miners provide data storage to the network. Storage Miners participate in Filecoin by o\ufb00ering their disk space and serving Put requests.",
      "To become Storage Miners, users must pledge their storage by depositing collateral proportional to it. Storage Miners respond to Put requests by committing to store the clients data for a speci\ufb01ed time. Storage Miners generate Proofs-of-Spacetime and submit them to the blockchain to prove to the Network that they are storing the data through time. In case of invalid or missing proofs, Storage Miners are penalized and loose part of their collateral. Storage Miners are also eligible to mine new blocks, and in doing so they hence receive the mining reward for creating a block and transaction fees for the transactions included in the block. Retrieval Miners provide data retrieval to the Network. Retrieval Miners participate in Filecoin by serving data that users request via Get. Unlike Storage Miners, they are not required to pledge, commit to store data, or provide proofs of storage. It is natural for Storage Miners to also participate as Retrieval Miners.",
      "Retrieval Miners can obtain pieces directly from clients, or from the Retrieval Market. 4.1.2 The Network, N We personify all the users that run Filecoin full nodes as one single abstract entity: The Network. The Network acts as an intermediary that runs the Manage protocol; informally, at every new block in the Filecoin blockchain, full nodes manage the available storage, validate pledges, audit the storage proofs, and repair possible faults. 4.1.3 The Ledger Our protocol is applied on top of a ledger-based currency; for generality we refer to this as the Ledger, L. At any given time t (referred to as epoch), all users have access to Lt, the ledger at epoch t, which is a sequence of transactions. The ledger is append-only3. The Filecoin DSN protocol can be implemented on any ledger that allows for the veri\ufb01cation of Filecoins proofs; we show how we can construct a ledger based on useful work in Section 6.",
      "4.1.4 The Markets Demand and supply of storage meet at the two Filecoin Markets: Storage Market and Retrieval Market. The markets are two decentralized exchanges and are explained in detail in Section 5. In brief, clients and miners set the prices for the services they request or provide by submitting orders to the respective markets. The exchanges provide a way for clients and miners to see matching o\ufb00ers and initiate deals. By running the Manage protocol, the Network guarantees that miners are rewarded and clients are charged if the service requested has been successfully provided. 3t  t implies that Lt is a pre\ufb01x of L Data Structures Pieces. A piece is some part of data that a client is storing in the DSN. For example, data can be deliber- ately divided into many pieces and each piece can be stored by a di\ufb00erent set of Storage Miners. Sectors. A sector is some disk space that a Storage Miner provides to the network.",
      "Miners store pieces from clients in their sectors and earn tokens for their services. In order to store pieces, Storage Miners must pledge their sectors to the network. AllocationTable. The AllocTable is a data structure that keeps track of pieces and their assigned sectors. The AllocTable is updated at every block in the ledger and its Merkle root is stored in the latest block. In practice, the table is used to keep the state of the DSN, allowing for quick look-ups during proof veri\ufb01cation. For more details, see Figure 5. Orders. An order is a statement of intent to request or o\ufb00er a service. Clients submit bid orders to the markets to request a service (resp. Storage Market for storing data and Retrieval Market for retrieving data) and Miners submit ask orders to o\ufb00er a service. The order data structures are shown in Figure 10. The Market Protocols are detailed in Section 5. Orderbook. Orderbooks are sets of orders.",
      "See the Storage Market orderbook in Section 5.2.2 and Retrieval Market orderbook in Section 5.3.2 for details. Pledge. A pledge is a commitment to o\ufb00er storage (speci\ufb01cally a sector) to the network. Storage Miners must submit their pledge to the ledger in order to start accepting orders in the Storage Market. A pledge consists of the size of the pledged sector and the collateral deposited by the Storage Miner (see Figure 5 for more details). Data Structures Pledge pledge : size, collMi  size, the size of the sector being pledged. coll, the collateral speci\ufb01c to this pledge that Mi deposits. Orderbook OrderBook: (O1..On)  Oi, currently valid deal, ask, bid orders. Allocation allocTable: M1 (allocEntry..allocEntry), M2.. allocEntry: (sid, orders, last, missing)  sid, sector id  Oi, currently valid deal, ask, bid orders.",
      "orders, set of orders Odeal..Odeal  last, last proof of storage in the ledger L  missing, counter for missing proofs Figure 5: Data Structures in a DSN scheme Protocol In this Section, we give an overview of the Filecoin DSN by describing the operations performed by the clients, the Network and the miners. We present the methods of the Get and the Put protocol in Figure 7 and the Manage protocol in Figure 8. An example protocol execution is shown in Figure 6. The overall Filecoin Protocol is presented in Figure 1. 4.3.1 Client Cycle We give a brief overview of the client cycle; an in-depth explanation of the following protocols is given in Section 5. 1. Put: Client stores data in Filecoin. Clients can store their data by paying Storage Miners in Filecoin tokens. The Put protocol is described in detail in Section 5.2. A client initiates the Put protocol by submitting a bid order to the Storage Market orderbook (by submitting their order to the blockchain).",
      "When a matching ask order from miners is found, the client sends the piece to the miner. Both parties sign a deal order and submit it to the Storage Market orderbook. Clients should be able to decide the amount of physical replicas of their pieces either by submitting multiple orders (or specifying a replication factor in the order). Higher redundancy results in a higher tolerance of storage faults. 2. Get: Client retrieves data from Filecoin. Clients can retrieve any data stored in the DSN by paying Retrieval Miners in Filecoin tokens. The Get protocol is described in detail in Section 5.3. A client initiates the Get protocol by submitting a bid order to the Retrieval Market orderbook (by gossiping their order to the network). When a matching ask order from miners is found, the client receives the piece from the miner. When received, both parties sign a deal order and submit it to the blockchain to con\ufb01rm that the exchange succeeded.",
      "4.3.2 Mining Cycle (for Storage Miners) We give an informal overview of the mining cycle. 1. Pledge: Storage Miners pledge to provide storage to the Network. Storage Miners pledge their storage to the network by depositing collateral via a pledge transaction in the blockchain, via Manage.PledgeSector. The collateral is deposited for the time intended to provide the service, and it is returned if the miner generates proofs of storage for the data they commit to store. If some proofs of storage fail, a proportional amount of collateral is lost. Once the pledge transaction appears in the blockchain, miners can o\ufb00er their storage in the Storage Market: they set their price and add an ask order to the markets orderbook. Manage.PledgeSector  inputs:  current allocation table allocTable  pledge request pledge  outputs: allocTable 2. Receive Orders: Storage Miners get storage requests from the Storage Market.",
      "Once the pledge transaction appears in the blockchain (hence in the AllocTable), miners can o\ufb00er their storage in the Storage Market: they set their price and add an ask order to the markets orderbook via Put.AddOrders. Put.AddOrders  inputs: list of orders O1..On  outputs: bit b, equals 1 if successful Check if their orders are matched with a corresponding bid order from a client, via Put.MatchOrders. Put.MatchOrders  inputs:  the current Storage Market OrderBook  query order to match Oq  outputs: matching orders O1..On Once orders are matched, clients send their data to the Storage Miners. When receiving the piece, miners run Put.ReceivePiece. When the data is received, both the miner and the client sign a deal order and submit it to the blockchain. Put.ReceivePiece  inputs:  signing key for Mj. current orderbook OrderBook  ask order Oask  bid order Obid  piece p  outputs: deal order Odeal signed by Ci and Mj 3. Seal: Storage Miners prepare the pieces for future proofs.",
      "Storage Miners storage is divided in sectors, each sector contains pieces assigned to the miner. The Network keeps track of each Storage Miners sector via the allocation table. When a Storage Miner sector is \ufb01lled, the sector is sealed. Sealing is a slow, sequential operation that transforms the data in a sector into a replica, a unique physical copy of the data that is associated to the public key of the Storage Miner. Sealing is a necessary operation during the Proof-of-Replication as described in Section 3.4. Manage.SealSector  inputs:  miner publicprivate key pair M  sector index j  allocation table allocTable  outputs: a proof \u03c0SEAL, a root hash rt 4. Prove: Storage Miners prove they are storing the committed pieces. When Storage Miners are assigned data, they must repeatedly generate proofs of replication to guar- antee they are storing the data (for more details, see Section 3). Proofs are posted on the blockchain and the Network veri\ufb01es them.",
      "Manage.ProveSector  inputs:  miner publicprivate key pair M  sector index j  challenge c  outputs: a proof \u03c0POS 4.3.3 Mining Cycle (for Retrieval Miners) We give an informal overview of the mining cycle for Retrieval Miners. 1. Receive Orders: Retrieval Miners get data requests from the Retrieval Market. Retrieval Miners announce their pieces by gossiping their ask orders to the network: they set their price and add an ask order to the markets orderbook. Get.AddOrders  inputs: list of orders O1..On  outputs: none Then, Retrieval Miners check if their orders are matched with a corresponding bid order from a client. Get.MatchOrders  inputs:  the current Retrieval Market OrderBook  query order to match Oq  outputs: matching orders O1..On 2. Send: Retrieval Miners send pieces to the client. Once orders are matched, Retrieval Miners send the piece to the client (see Section 5.3 for details).",
      "When the piece is received, both the miner and the client sign a deal order and submit it to the blockchain. Put.SendPiece  inputs:  an ask order Oask  a bid order Obid  a piece p  outputs: a deal order Odeal signed by Mi 4.3.4 Network Cycle We give an informal overview of the operations run by the network. 1. Assign: The Network assigns clients pieces to Storage Miners sectors. Clients initiate the Put protocol by submitting a bid order in the Storage Market4. When ask and bid orders match, the involved parties jointly commit to the exchange and submit a deal order in the market. At this point, the Network assigns the data to the miner and makes a note of it in the allocation table. Manage.AssignOrders  inputs:  deal orders O1 deal..On deal  allocation table allocTable  outputs: updated allocation table allocTable 2. Repair: The Network \ufb01nds faults and attempt to repair them. All the storage allocations are public to every participant in the network.",
      "At every block, the Network checks if the required proofs for each assignment are present, checks that they are valid, and acts accordingly:  if any proof is missing or invalid, the network penalizes the Storage Miners by taking part of their collateral,  if a large amount of proofs are missing or invalid (de\ufb01ned by a system parameter fault), the network considers the Storage Miner faulty, settles the order as failed and reintroduces a new order for the same piece into the the market,  if every Storage Miner storing this piece is faulty, then the piece is lost and the client gets refunded. 4Storage orders are submitted via the blockchain, see Section 5.",
      "Manage.RepairOrders  inputs:  current time t  current ledger L  table of storage allocations allocTable  outputs: orders to repair O1 deal..On deal, updated allocation table allocTable Client Network Miner AddOrders(..,Obid) AddOrders(..,Oask) MatchOrders(..) SendPiece(..,Obid, p) ReceivePiece(..,Oask) AddOrders(Odeal) AddOrders(..,Odeal) AddOrder(..,Obid) AddOrder(..,Oask) MatchOrders(..) ReceivePiece(..,Obid) SendPiece(..,Oask, p) AddOrders(..,Odeal) AddOrders(..,Odeal) PledgeSector() Manage AssignOrders(..,Odeal) SealSector(..) ProveSector(..) RepairOrders(..) Figure 6: Example execution of the Filecoin DSN, grouped by party and sorted chronologically by row Guarantees and Requirements The following are the intuitions on how the Filecoin DSN achieves integrity, retrievability, public veri\ufb01ability and incentive-compatibility. Achieving Integrity: Pieces are named after their cryptographic hash.",
      "After a Put request, clients only need to store this hash to retrieve the data via Get and to verify the integrity of the content received. Achieving Retrievability: In a Put request, clients specify the replication factor and the type of erasure coding desired, specifying in this way the storage to be (f, m)-tolerant. The assumption is that given m Storage Miners storing the data, a maximum of f faults are tolerated. By storing data in more than one Storage Miner, a client can increase the chances of recovery, in case Storage Miners go o\ufb04ine or disappear. Achieving Public Veri\ufb01ability and Auditability: Storage Miners are required to submit their proofs of storage (\u03c0SEAL, \u03c0POST) to the blockchain. Any user in the network can verify the validity of these proofs, without having access to the outsourced data. Since the proofs are stored on the blockchain, they are a trace of operation that can be audited at any time.",
      "Achieving Incentive Compatibility: Informally, miners are rewarded for the storage they are providing. When miners commit to store some data, then they are required to generate proofs. Miners that skip proofs are penalized (by losing part of their collateral) and not rewarded for their storage. Achieving Con\ufb01dentiality: Clients that desire for their data to be stored privately, must encrypt their data before submitting them to the network.",
      "Put Protocol Market AddOrders  inputs: list of orders O1..On  outputs: bit b, equals 1 if successful 1) Set txorder : (O1, .., On) 2) Submit txorder to L 3) Wait for txorder to be included in L 4) Output 1 on success, 0 otherwise MatchOrders  inputs:  the current Storage Market OrderBook  query order to match Oq  outputs: matching orders O1..On 1) Match each Oi in OrderBook such that: a) If Oq is an ask order: i) Check if Oi is bid order ii) Check Oi.price Oq.price iii) Check Oi.size Oq.space b) If Oq is a bid order: i) Check if Oi is ask order ii) Check Oi.price Oq.price iii) Check Oi.space Oq.size 2) Output matched orders O1...On Exchange SendPiece  inputs:  an ask order Oask  a bid order Obid  a piece p  outputs: a deal order Odeal signed by Mi 1) Get identity of Mi from Oask signature 2) Send (Oask,Obid,p) to Mi 3) Receive Odeal signed by Mi 4) Check if Odeal is valid according to De\ufb01nition 5.2 5) Output Odeal ReceivePiece  inputs:  signing key for Mj.",
      "current orderbook OrderBook  ask order Oask  bid order Obid  piece p  outputs: deal order Odeal signed by Ci and Mj 1) Check if Obid is valid: a) Check if Obid is in OrderBook b) Check if Obid is not referenced by other active Odeal c) Check if Obid.size is equal to p d) Check if O is signed by Mi 2) Store p locally 3) Set Odeal: Oask, Odeal, H(p) Mi 4) Get identity of Cj from Obid 5) Send Odeal to Cj 6) Output Odeal Get Protocol Market AddOrders  inputs: list of orders O1..On  outputs: none 1) Gossip O1..On to the network MatchOrders  inputs:  the current Retrieval Market OrderBook  query order to match Oq  outputs: matching orders O1..On 1) Match each Oi in OrderBook such that: a) Check Oi.piece is equal to Oq.piece b) If Oq is an ask order: i) Check if Oi is bid order ii) Check Oi.price Oq.price c) If Oq is a bid order: i) Check if Oi is ask order ii) Check Oi.price Oq.price 2) Output matched orders O1...On Exchange SendPiece  inputs:  an ask order Oask  a bid order Obid  a piece p  outputs: a deal order Odeal signed by Ci 1) Create Odeal: a) Set Odeal.ask : Oask b) Set Odeal.bid : Odeal 2) Get identity of Ci from Obid signature 3) Setup a micropayment channel with Ci 4) For each block of data pj of p: a) Set \u03c0j to be a merkle path from H(p) to pj b) Send (Odeal,pj, \u03c0j) to Ci c) Receive Odeal, j Ci 5) Output Odeal ReceivePiece  inputs:  a clients key Cj  an ask order Oask  a bid order Obid  merkle tree hash of p in the orders hp  outputs: a piece p 1) Create Odeal: a) Set Odeal.ask : Oask b) Set Odeal.bid : Obid 2) Get identity of Mi from Oask signature 3) Set up a micropayment channel with Mi (or re-using an existing one) 4) When receiving (Odeal, pj, \u03c0j) from Mi: a) Check if Odeal is valid and matches Oask and Obid b) Check if \u03c0j is a valid merkle-path with root hash c) Send Odeal, j Ci 5) Output p Figure 7: Description of the Put and Get Protocols in the Filecoin DSN Manage Protocol Network AssignOrders  inputs:  deal orders O1 deal..On deal  allocation table allocTable  outputs: updated allocation table allocTable 1) Copy allocTable in allocTable 2) For each order Oi deal: a) Check if Oi deal is valid according to De\ufb01nition 5.2 b) Get Mj from Oi deal signature c) Add details from Oi deal to allocTable 3) Output allocTable RepairOrders  inputs:  current time t  current ledger L  table of storage allocations allocTable  outputs: orders to repair O1 deal..On deal, updated alloca- tion table allocTable 1) For each allocEntry in allocTable: a) If t  allocEntry.last  proof: skip b) Update allocEntry.last t c) Check if \u03c0 is in Ltproof:t and PoSt.Verify(\u03c0) d) On success: update allocEntry.missing 0 e) On failure: i) update allocEntry.missing ii) penalize collateral from Mis pledge f) If allocEntry.missing  fault then set all the orders from the current sector as failed orders 2) Output failed orders O1 deal..On deal and allocTable.",
      "Miner PledgeSector  inputs:  current allocation table allocTable  pledge request pledge  outputs: allocTable 1) Copy allocTable to allocTable 2) Set txpledge : (pledge) 3) Submit txpledge to L 4) Wait for txpledge to be included in L 5) Add new sector of size pledge.size in allocTable 6) Output allocTable SealSector  inputs:  miner publicprivate key pair M  sector index j  allocation table allocTable  outputs: a proof \u03c0SEAL, a root hash rt 1) Find all the pieces p1..pn in sector Sj in the allocTable 2) Set D : p1p2..pn 3) Compute (R, rt, \u03c0SEAL) : PoSt.Setup(M, pkSEAL, D) 4) Output \u03c0SEAL, rt ProveSector  inputs:  miner publicprivate key pair M  sector index j  challenge c  outputs: a proof \u03c0POS 1) Find R for sector j 2) Compute \u03c0POST : PoSt.Prove(pkPOST, R, c, proof) 3) Output \u03c0POST Figure 8: Description of the Manage Protocol in the Filecoin DSN Filecoin Storage and Retrieval Markets Filecoin has two markets: the Storage Market and the Retrieval Market.",
      "The two markets have the same structure but di\ufb00erent design. The Storage Market allows Clients to pay Storage Miners to store data. The Retrieval Market allows Clients to retrieve data by paying Retrieval Miners to deliver the data. In both cases, clients and miners can set their o\ufb00er and demand prices or accept current o\ufb00ers. The exchanges are run by the Network - a personi\ufb01cation of the network of full nodes in Filecoin. The network guarantees that miners are rewarded by the clients when providing the service. Veri\ufb01able Markets Exchange Markets are protocols that facilitate exchange of a speci\ufb01c good or service. They do this by en- abling buyers and sellers to conduct transactions. For our purposes, we require exchanges to be veri\ufb01able: a decentralized network of participants must be able to verify the exchange between buyers and sellers.",
      "We present the notion of Veri\ufb01able Markets, where no single entity governs an exchange, transactions are transparent, and anybody can participate pseudonymously. Veri\ufb01able Market protocols operate the exchange of goodsservices in a decentralized fashion: consistency of the orderbooks, orders settlements and correct execution of services are independently veri\ufb01ed via the participants - miners and full nodes in the case of Filecoin. We simplify veri\ufb01able markets to have the following construction: De\ufb01nition 5.1. A veri\ufb01able Market is a protocol with two phases: order matching and settlement. Orders are statements of intent to buy or sell a security, good or service and the orderbook is the list of all the available orders. Veri\ufb01able Market Protocol Order matching: 1. Participants add buy orders and sell orders to the orderbook. 2.",
      "When two orders match, involved parties jointly create a deal order that commits the two parties to the exchange, and propagate it to the network by adding it to the orderbook. Settlement: 3. The network ensures that the transfer of goods or services has been executed correctly, by requiring sellers to generate cryptographic proofs for their exchangeservice. 4. On success, the network processes the payments and clears the orders from the orderbook. Figure 9: Generic protocol for Veri\ufb01able Markets Storage Market The Storage Market is a veri\ufb01able market which allows clients (i.e. buyers) to request storage for their data and Storage Miners (i.e. sellers) to o\ufb00er their storage.",
      "5.2.1 Requirements We design the Storage Market protocol accordingly to the following requirements:  In-chain orderbook: It is important that: (1) Storage Miners orders are public, so that the lowest price is always known to the network and clients can make informed decision on their orders, (2) client orders must be always submitted to the orderbook, even when they accept the lowest price, in this way the market can react to the new o\ufb00er. Hence, we require orders to be added in clear to the Filecoin blockchain in order to be added to the orderbook. Participants committing their resources: We require both parties to commit to their resources as a way to avoid disservice: to avoid Storage Miners not providing the service and to avoid clients not having available funds. In order to participate to the Storage Market, Storage Miners must pledge, depositing a collateral proportional to their amount of storage in DSN (see Section 4.3.3 for more details).",
      "In this way, the Network can penalize Storage Miners that do not provide proofs of storage for the pieces they committed to store. Similarly, clients must deposit the funds speci\ufb01ed in the order, guaranteeing in this way commitment and availability of funds during settlement. Self-organization to handle faults: Orders are only settled if Storage Miners have repeatedly proved that they have stored the pieces for the duration of the agreed-upon time period. The Network must be able to verify the existence and the correctness of these proofs and act according to the rules outlined in the Repair portion of Subsection 4.3.4. 5.2.2 Datastructures Put Orders. There are three types of orders: bid orders, ask orders and deal orders. Storage Miners create ask orders to add storage, clients create bid orders to request storage, when both parties agree on a price, they jointly create a deal order.",
      "The data structures of the orders are shown in detail in Figure 10, and the parameters of the orders are explicitly de\ufb01ned. Put Orderbook. The Orderbook in the Storage Market is the set of currently valid and open ask, bid and deal orders. Users can interact with the orderbook via the methods de\ufb01ned in the Put protocol: AddOrders, MatchOrders as described in Figure 7. The orderbook is public and every honest user has the same view of the orderbook. At every epoch, new orders are added to the orderbook if new order transactions (txorder) appear in new blockchain blocks; orders are removed if they are cancelled, expired or settled. Orders are added in blockchain blocks, hence in the orderbook, if they are valid: De\ufb01nition 5.2. We de\ufb01ne the validity of bid, ask, deal orders: (Valid bid order): A bid order from client Ci, Obid: size, funds, price, time, coll, codingCi is valid if:  Ci has at least the amount of funds available in their account.",
      "time is not set in the past  The order must guarantee at least a minimum amount5 of epochs of storage. (Valid ask order): An ask order from Storage Miner Mi, Oask: space, priceMi is valid if:  Mi has pledged to be a miner and the pledge will not expire before time epochs. space must be less than Mis available storage: Mi pledged storage minus the storage committed in the orderbook (in ask and deal orders). (Valid deal order): A deal order Odeal: ask, bid, tsCi,Mj is valid if  ask references an order Oask such that: it is in the Storage Market OrderBook, no other deal orders in the Storage Market OrderBook mention it, it is signed by Ci. bid references an order Obid such that: it is in the Storage Market OrderBook, no other deal orders in the Storage Market OrderBook mention it, it is signed by Mj. ts is not set in the future or too far in the past. Remark.",
      "If a malicious client receives a signed deal from a Storage Miner, but never adds it to the orderbook, then the Storage Miner cannot re-use the storage committed in the deal. The \ufb01eld ts prevents this attack because, after ts, the order becomes invalid and cannot be submitted in the orderbook. 5This will be a parameter of the system.",
      "Storage Market Orders bid order Obid: size, funds, price, time, coll, codingCi  size, the size of the piece to be stored  funds, the total amount that client Ci is deposit-  time, the maximum epoch time for which the \ufb01le should be storeda  price, the spacetime price in Filecoinb  coll, the collateral speci\ufb01c to this piece that the miner is required to deposit  coding, the erasure coding scheme for this piece ask order Oask: space, price Mi  space, amount of space Storage Miner Mi is pro- viding in the order  price, the spacetime price in Filecoin deal order Odeal: ask, bid, ts, hash Ci,Mj  ask, a cryptographic reference to Oask from Ci  order, a cryptographic reference to Obid from Mi  ts, timestamp epoch in which the order has been signed by Mi  hash cryptographic hash of the piece that Mj will store aIf not speci\ufb01ed, the piece will be stored until expira- tion of funds. bIf not speci\ufb01ed, when a Storage Miner is faulty, the network can re-introduce the order at the current best price.",
      "Retrieval Market Orders bid order Obid: piece, price Ci  piece, the index of the piece requesteda  price, the price at which Ci is paying for one retrieval ask order Oask: piece, price Mi  piece, the index of the piece requested  price, the price at which Mj is serving the piece for deal order Odeal: ask, order Ci,Mj  ask, a cryptographic reference to Oask from  order, a cryptographic reference to Oask from Ci aOnly pieces stored in Filecoin can be requested Figure 10: Orders data structures for the Retrieval and Storage Markets 5.2.3 The Storage Market Protocol In brief, the Storage Market protocol is divided in two phases: order matching and settlement:  Order Matching: Clients and Storage Miners submit their orders to the orderbook by submitting a transaction to the blockchain (step 1). When orders are matched, the client sends the piece to the Storage Miner and both parties sign a deal order and submit it to the orderbook (step 2).",
      "Settlement: Storage Miners seal their sectors (step 3a), generate proofs of storage for the sector con- taining the piece and submit them to the blockchain regularly (step 3b); meanwhile, the rest of the network must verify the proofs generated by the miners and repair possible faults (step 3c). The Storage Market protocol is explained in detail in Figure 11. Retrieval Market The Retrieval Market allows clients to request retrieval of a speci\ufb01c piece and Retrieval Miners to serve it. Unlike Storage Miners, Retrieval Miners are not required to store pieces through time or generate proofs of storage. Any user in the network can become a Retrieval Miner by serving pieces in exchange for Filecoin tokens. Retrieval Miners can obtain pieces by receiving them directly from clients, by acquiring them from the Retrieval Market, or by storing them from being a Storage Miner.",
      "5.3.1 Requirements We design the Retrieval Market protocol accordingly to the following requirements:  O\ufb00-chain orderbook: Clients must be able to \ufb01nd Retrieval Miners that are serving the required pieces and directly exchange the pieces, after settling on the pricing. This means that the orderbook cannot be run via the blockchain - since this would be the bottleneck for fast retrieval requests - instead participant will have only partial view of the OrderBook. Hence, we require both parties to gossip their orders. Retrieval without trusted parties: The impossibility results on fair exchange 10 remind us that it is impossible for two parties to perform an exchange without trusted parties. In the Storage Market, the blockchain network acts as a (decentralized) trusted party that veri\ufb01es the storage provided by the Storage Miners. In the Retrieval Market, Retrieval Miners and clients exchange data without the network witnessing the exchange of \ufb01le.",
      "We go around this result by requiring the Retrieval Miner to split their data in multiple parts and for each part sent to the client, they receive a payment. In this way, if the client stops paying, or the miner stops sending data, either party can halt the exchange. Note that for this to work, we must assume that there is always one honest Retrieval Miner. Payments channels: Clients are interested in retrieving the pieces as soon as they submit their payments, Retrieval Miners are interested in only serving the pieces if they are sure of receiving a payment. Validating payments via a public ledger can be the bottleneck of a retrieval request, hence we must rely on e\ufb03cient o\ufb00-chain payments. The Filecoin blockchain must support payment channels which enable rapid, optimistic transactions and use the blockchain only in case of disputes. In this way, Retrieval Miners and Clients can quickly send the small payments required by our protocol.",
      "Future work includes the creation of a network of payment channels as previously seen in 11, 12. 5.3.2 Data Structures Get Orders. There are three types of orders in the Retrieval Market: clients create bid orders Obid, Retrieval Miners create ask orders Oask, and deal orders Odeal, are created jointly when a Storage Miner and a client agree on a deal. The datastructures of the orders is shown in detail on Figure 10. Get Orderbook. The Orderbook in the Retrieval Market is the set of valid and open ask, bid and deal orders. Unlike the Storage Market, every user has a di\ufb00erent view of the orderbook, since the orders are gossiped in the network and each miner and client only keep track of the orders they are interested in. Storage Market Protocol Order Matching 1. Storage Miner Mi and Client Ci add orders to the OrderBook: (a) Mi creates Oask 1, Oask 2, .. and Cj creates Obid 1, Obid 2, ...",
      "(b) Orders are submitted to the blockchain via Put.addOrders(O1, O2, ..) (c) On success, the orders are added to the OrderBook, the funds from Cj are deposited and the space from Mi is reserved. 2. When orders match, involved parties jointly create Odeal and add it to the OrderBook: (a) Mi and Cj independently query the OrderBook via Put.matchOrders(O). (b) If Mi and Cj have matching orders :  Cj sends the piece p to Mi via Put.SendPiece(Obid, Oask, p)  Mi receives the piece p from Cj via Put.ReceivePiece(Obid, Oask, p). Mi signs Odeal and sends it to Cj (c) Cj signs Odeal and adds it to the OrderBook via Put.addOrders(Odeal) Settlement 3. The Network checks if the Storage Miners are correctly storing the pieces: (a) When a Storage Miner \ufb01lls a sector, they seal it (they create a unique replica) via Manage.SealSector and submit the proof \u03c0SEAL and rt to the blockchain.",
      "(b) Storage Miners generate new proofs at every epoch and add them to the Filecoin blockchain every proof epochs via Manage.ProveSectors. (c) The Network runs Manage.RepairOrders at every epoch. If proofs are missing or invalid, the network tries to repair in the following ways:  if any proofs are missing or invalid, it penalizes the Storage Miners by taking part of their collateral,  if a large amount of proofs are missing or invalid for more than fault epochs, it considers the Storage Miner faulty, settles the order as failed and reintroduces a new order for the same piece into the the market,  if every Storage Miner storing this piece is faulty, then the piece is lost and the client gets refunded. 4. When the time of the order is expired or funds run out, if the service was correctly provided, the Network processes the payments, and removes the orders.",
      "Figure 11: Detailed Storage Market protocol 5.3.3 The Retrieval Market Protocol In brief, the Retrieval Market protocol is divided in two phases: order matching and settlement:  Order Matching: Clients and Retrieval Miners submit their orders to the orderbook by gossiping their orders (step 1). When orders are matched, the client and the Retrieval Miners establish a micropayment channel (step 2). Settlement: Retrieval Miners send a small parts of the piece to the client and for each piece the client sends to the miner a signed receipt (step 3). The Retrieval Miner presents the delivery receipts to the blockchain to get their rewards (step 4). The protocol is explained in details in Figure 12. Retrieval Market Protocol Order Matching: 1. Retrieval Miners and Clients add orders to the Get.OrderBook: (a) Retrieval Miners Mi creates ask orders (Oask 1, Oask 2, ..) and Client Cj creates bid orders (Obid 1, Obid 2, ..).",
      "(b) Both Mi and Cj gossip their orders in the Filecoin network via Get.addOrders (c) Since there is no commonly shared orderbook, when users receive orders, they add them to their own orderbooks view. Di\ufb00erently from the Storage Market, these orders are not binding and no resource is committed (e.g. clients dont do any deposit). 2. When orders match, involved parties jointly create Odeal and add it to the Get.OrderBook: (a) Retrieval Miner Mi and Client Cj independently run Get.matchOrders that queries their own current Get.OrderBook view. (b) Both Mi and Cj sign Odeal and add it to their Get.OrderBook via Get.addOrders (as described before) (c) Ci and Mj setup a micropayment channel for Odeal Settlement: 3. Both parties check whether the piece has been delivered: (a) Mi sends the piece p in parts via Get.SendPiece (b) Cj receives the p in parts and for each part, Cj acknowledges delivery by sending a micropayment via Get.ReceivePiece 4.",
      "When the p has been received by Cj, Mj can present the micropayments to the network and retrieve the payment, both parties remove their orders from the orderbooks. Figure 12: Detailed Retrieval Market protocol Useful Work Consensus The Filecoin DSN protocol can be implemented on top of any consensus protocol that allows for veri\ufb01cation of the Filecoins proofs. In this section, we present how we can bootstrap a consensus protocol based on useful work. Instead of wasteful Proof-of-Work computation, the work Filecoin miners do generating Proof- of-Spacetime is what allows them to participate in the consensus. Useful Work. We consider the work done by the miners in a consensus protocol to be useful, if the outcome of the computation is valuable to the network, beyond securing the blockchain.",
      "Motivation While securing the blockchain is of fundamental importance, Proof-of-Work schemes often require solving puzzles whose solutions are not reusable or require a substantial amount wasteful computation to \ufb01nd. Non-reusable Work: Most permissionless blockchains require miners to solve a hard computational puzzle, such as inverting a hash function. Often the solutions to these puzzles are useless and do not have any inherent value beyond securing the network. Can we re-purpose this work for something useful? Attempts to re-use work: There have been several attempts to re-use mining power for useful compu- tation. Some e\ufb00orts require miners to perform a special computation alongside the standard Proof- of-Work. Other e\ufb00orts replace Proof-of-Work with useful problems that are still hard to solve.",
      "For example, Primecoin re-uses miners computational power to \ufb01nd new prime numbers, Ethereum re- quires miners to execute small programs alongside with Proof-of-Work, and Permacoin o\ufb00ers archival services by requiring miners to invert a hash function while proving that some data is being archived. Although most of these attempts do perform useful work, the amount of wasteful work is still a preva- lent factor in these computations. Wasteful Work: Solving hard puzzles can be really expensive in terms of cost of machinery and energy consumed, especially if these puzzles solely rely on computational power. When the mining algorithm is embarrassingly parallel, then the prevalent factor to solve the puzzle is computational power. Can we reduce the amount of wasteful work? Attempts to reduce waste: Ideally, the majority of a networks resources should be spent on useful work. Some e\ufb00orts require miners to use more energy-e\ufb03cient solutions.",
      "For example, Spacemint requires miners to dedicate disk space rather than computation; while more energy e\ufb03cient, theses disks are still wasted, since they are \ufb01lled with random data. Other e\ufb00orts replace hard to solve puzzles with a traditional byzantine agreement based on Proof-of-Stake, where stakeholders vote on the next block proportional to their share of currency in the system. We set out to design a consensus protocol with a useful work based on storing users data. Filecoin Consensus We propose a useful work consensus protocol, where the probability that the network elects a miner to create a new block (we refer to this as the voting power of the miner) is proportional to their storage currently in use in relation to the rest of the network. We design the Filecoin protocol such that miners would rather invest in storage than in computing power to parallelize the mining computation.",
      "Miners o\ufb00er storage and re-use the computation for proof that data is being stored to participate in the consensus. 6.2.1 Modeling Mining Power Power Fault Tolerance. In our technical report 13, we present Power Fault Tolerance, an abstraction that re-frames byzantine faults in terms of participants in\ufb02uence over the outcome of the protocol. Every participant controls some power of which n is the total power in the network, and f is the fraction of power controlled by faulty or adversarial participants. Power in Filecoin. In Filecoin, the power pt i of miner Mi at time t is the sum of the Mis storage assign- ments. The in\ufb02uence It i of Mi is the fraction of Mis power over the total power in the network. In Filecoin, power has the following properties:  Public: The total amount of storage currently in use in the network is public.",
      "By reading the blockchain, anyone can calculate the storage assignments of each miner - hence anyone can calculate the power of each miner and the total amount of power at any point in time. Publicly Veri\ufb01able: For each storage assignment, miners are required to generate Proofs-of-Spacetime, proving that the service is being provided. By reading the blockchain, anyone can verify if the power claimed by a miner is correct. Variable: At any point in time, miners can add new storage in the network by pledging with a new sector and \ufb01lling the sector. In this way, miners can change their amount of power they have through time. 6.2.2 Accounting for Power with Proof-of-Spacetime Every proof blocks6, miners are required to submit Proofs-of-Spacetime to the network, which are only successfully added to the blockchain if the majority of power in the network considers them valid.",
      "At every block, every full node updates the AllocTable, adding new storage assignments, removing expiring ones and marking missing proofs. The power of a miner Mi can be calculated and veri\ufb01ed by summing the entries in the AllocTable, which can be done in two ways:  Full Node Veri\ufb01cation: If a node has the full blockchain log, run the NetworkProtocol from the genesis block to the current block and read the AllocTable for miner Mi. This process veri\ufb01es every Proof-of-Spacetime for the storage currently assigned to Mi. Simple Storage Veri\ufb01cation: Assume a light client has access to a trusted source that broadcasts the latest block. A light client can request from nodes in the network: (1) the current AllocTable entry for miner Mi, (2) a Merkle path that proves that the entry was included in the state tree of the last block, (3) the headers from the genesis block until the current block. In this way, the light client can delegate the veri\ufb01cation of the Proof-of-Spacetime to the network.",
      "The security of the power calculation comes from the security of Proof-of-Spacetime. In this setting, PoSt guarantees that the miner cannot lie about the amount of assigned storage they have. Indeed, they cannot claim to store more than the data they are storing, since this would require spending time fetching and running the slow PoSt.Setup, and they cannot generate proofs faster by parallelizing the computation, since PoSt.Prove is a sequential computation. 6.2.3 Using Power to Achieve Consensus We foresee multiple strategies for implementing the Filecoin consensus by extending existing (and future) Proof-of-Stake consensus protocols, where stake is replaced with assigned storage. While we foresee improve- ments in Proof-of-Stake protocols, we propose a construction based on our previous work called Expected Consensus 14. Our strategy is to elect at every round one (or more) miners, such that the probability of winning an election is proportional to each miners assigned storage.",
      "Expected Consensus. The basic intuition of Expected Consensus EC is to deterministically, unpredictably, and secretly elect a small set of leaders at each epoch. On expectation, the number of elected leaders per 6proof is a system parameter. epoch is 1, but some epochs may have zero or many leaders. Leaders extend the chain by creating a block and propagating it to the network. At each epoch, the chain is extended with one or multiple blocks. In case of a leaderless epoch, an empty block is added to the chain. Although the blocks in chain can be linearly ordered, its data structure is a direct acyclic graph. EC is a probabilistic consensus, where each epoch introduces more certainty over previous blocks, eventually reaching enough certainty that the likelihood of a di\ufb00erent history is su\ufb03ciently small. A block is committed if the majority of the participants add their weight on the chain where the block belongs to, by extending the chain or by signing blocks. Electing Miners.",
      "At every epoch, each miner checks if they are elected leader, this is done similarly to previous protocols: CoA 15, Snow White 16, and Algorand 17. De\ufb01nition 6.1. (EC Election in Filecoin) A miner Mi is a leader at time t if the following condition is met: trand(t) 2L  \u03a3jpt Where rand(t) is a public randomness available that can be extracted from the blockchain at epoch t, pt is the power of Mi. Consider the size of H(m) to be L for any m, H to be a secure cryptographic hash function and mMi to be a message m signed by Mi, such that: mMi : (m), SIGMi H(m) In Figure 13, we describe the protocol between a miner (ProveElect) and a network node (VerifyElect). This election scheme provides three properties: fairness, secrecy and public veri\ufb01ability. Fairness: each participant has only one trial for each election, since signatures are deterministic and t and rand(t) are \ufb01xed.",
      "Assuming H is a secure cryptographic hash function, then H(trand(t)Mi)2L must be a real number uniformly chosen from (0, 1). Hence, the probability for the equation to be true must be pt i\u03a3jpt j, which is equal to the miners portion of power within the network. Because this probability is linear in power, this likelihood is preserved under splitting or pooling power. Note that the random value rand(t) is not known before time t. Secret: an e\ufb03cient adversary that does not own the secret key Mi can compute the signature with negligible probability, given the assumptions of digital signatures. Public Veri\ufb01ability: an elected leader i Lt can convince a e\ufb03cient veri\ufb01er by showing t, rand(t), H(trand(t)i)2L; given the previous point, no e\ufb03cient adversary can generate a proof without having a winning secret key. EC Election Storage Miner at epoch t ProveElect(r, t, Mi) , \u03c0t 1.",
      "Compute H(tri)2L  \u03a3jpt  on success, output \u03c0t i  t, ri  otherwise output  Network node on receiving a block at epoch t VerifyElect(\u03c0i t, t, Mi)  1. Check if \u03c0t i is a valid signature from user Mi on t and r 2. Check if pt i is the power from Mi at time t 3. Test if Mi is elected leader H(\u03c0t i)2L  \u03a3jpt  on success, output   otherwise output  Figure 13: Leader Election in the Expected Consensus protocol Smart Contracts Filecoin provides two basic primitives to the end users: Get and Put. These primitives allow clients to store data and retrieve data from the markets at their preferred price. While the primitives cover the default use cases for Filecoin, we enable for more complex operations to be designed on top of Get and Put by support- ing a deployment of smart contracts. Users can program new \ufb01ne-grained storageretrieval requests that we classify as File Contracts as well as generic Smart Contracts.",
      "We integrate a Contracts system (based on 18) and a Bridge system to bring Filecoin storage in other blockchain, and viceversa, to bring other blockchains functionalities in Filecoin. We expect a plethora of smart contracts to exist in the Filecoin ecosystem and we look forward to a community of smart-contract developers. Contracts in Filecoin Smart Contracts enable users of Filecoin to write stateful programs that can spend tokens, request stor- ageretrieval of data in the markets and validate storage proofs. Users can interact with the smart contracts by sending transactions to the ledger that trigger function calls in the contract. We extend the Smart Con- tract system to support Filecoin speci\ufb01c operations (e.g. market operations, proof veri\ufb01cation). Filecoin supports contracts speci\ufb01c to data storage, as well as more generic smart contracts:  File Contracts: We allow users to program the conditions for which they are o\ufb00ering or providing storage services.",
      "There are several examples worth mentioning: (1) contracting miners: clients can specify in advance the miners o\ufb00ering the service without participating in the market, (2) payment strategies: clients can design di\ufb00erent reward strategies for the miners, for example a contract can pay the miner incresignly higher through time, another contract can set the price of storage informed by a trusted oracle, (3) ticketing services: a contract could allow a miner to deposit tokens and to pay for storageretrieval on behalf of their users, (4) more complex operations: clients can create contracts that allow for data update. Smart Contracts: Users can associate programs to their transactions like in other systems (as in Ethereum 18) which do not directly depend on the use of storage. We foresee applications such as: decentralized naming systems, asset tracking and crowdsale platforms.",
      "Integration with other systems Bridges are tools that aim at connecting di\ufb00erent blockchains; while still work in progress, we plan to support cross chain interaction in order to bring the Filecoin storage in other blockchain-based platforms as well as bringing functionalities from other platforms into Filecoin. Filecoin in other platforms: Other blockchain systems such as Bitcoin 19, Zcash 20 and in particular Ethereum 18 and Tezos, allow developers to write smart contracts; however, these platforms provide very little storage capability and at a very high cost. We plan to provide a bridge to bring storage and retrieval support to these platforms. We note that IPFS is already in use by several smart contracts (and protocol tokens) as a way to reference and distribute content. Adding support to Filecoin would allow these systems to guarantee storage of IPFS content in exchange of Filecoin tokens.",
      "Other platforms in Filecoin: We plan to provide bridges to connect other blockchain services with Filecoin. For example, integration with Zcash would allow support for sending requests for storing data in privacy. Future Work This work presents a clear and cohesive path toward the construction of the Filecoin network; however, we also consider this work to be a starting point for future research on decentralized storage systems. In this section we identify and populate three categories of future work. This includes work that has been completed and merely awaits description and publication, open questions for improving the current protocols, and formalization of the protocol. On-going Work The following topics represent ongoing work. A speci\ufb01cation of the Filecoin state tree in every block. Detailed performance estimates and benchmarks for Filecoin and its components. A full implementable Filecoin protocol speci\ufb01cation.",
      "A sponsored-retrieval ticketing model where any client C1 can sponsor the download of another client C2 by issuing per-piece bearer-spendable tokens. A Hierarchical Consensus protocol where Filecoin subnets can partition and continue processing trans- actions during temporary or permanent partitions. Incremental blockchain snapshotting using SNARKSTARK  Filecoin-in-Ethereum interface contracts and protocols. Blockchain archives and inter-blockchain stamping with Braid. Only post Proofs-of-Spacetime on the blockchain for con\ufb02ict resolution. Formally prove the realizations of the Filecoin DSN and the novel Proofs-of-Storage. Open Questions There are a number of open questions whose answers have the potential to substantially improve the network as a whole, despite the fact that none of them have to be solved before launch. A better primitive for the Proof-of-Replication Seal function, which ideally is O(n) on decode (not O(nm)) and publicly-veri\ufb01able without requiring SNARKSTARK.",
      "A better primitive for the Proof-of-Replication Prove function, which is publicly-veri\ufb01able and trans- parent without SNARKSTARK. A transparent, publicly-veri\ufb01able Proof-of-Retrievability or other Proof-of-Storage. New strategies for retrieval in the Retrieval Market (e.g. based on probabilistic payments, zero knowl- edge contingent payments)  A better secret leader election for the Expected Consensus, which gives exactly one elected leader per epoch. A better trusted setup scheme for SNARKs that allows incremental expansion of public parameters (schemes where a sequence of MPCs can be run, where each additional MPC strictly lowers probability of faults and where the output of each MPC is usable for a system). Proofs and Formal Veri\ufb01cation Because of the clear value of proofs and formal veri\ufb01cation, we plan to prove many properties of the Filecoin network and develop formally veri\ufb01ed protocol speci\ufb01cations in the coming months and years. A few proofs are in progress and more in mind.",
      "But it will be hard, long-term work to prove many properties of Filecoin (such as scaling, o\ufb04ine). Proofs of correctness for Expected Consensus and variants. Proof of correctness for Power Fault Tolerance asynchronous 12 impossibility result side-step. Formulate the Filecoin DSN in the universal composability framework, describing Get, Put and Manage as ideal functionalities and prove our realizations. Formal model and proofs for automatic self-healing guarantees. Formally verify protocol descriptions (e.g. TLA or Verdi). Formally verify implementations (e.g. Verdi). Game theoretical analysis of Filecoins incentives. This work is the cumulative e\ufb00ort of multiple individuals within the Protocol Labs team, and would not have been possible without the help, comments, and review of the collaborators and advisors of Protocol Labs. Juan Benet wrote the original Filecoin whitepaper in 2014, laying the groundwork for this work.",
      "He and Nicola Greco developed the new protocol and wrote this whitepaper in collaboration with the rest of the team, who provided useful contributions, comments, review and conversations. In particular David davi- dad Dalrymple suggested the orderbook paradigm and other ideas, Matt Zumwalt improved the structure of the paper, Evan Miyazono created the illustrations and \ufb01nalized the paper, Jeromy Johnson provided insights while designing the protocol, and Steven Allen contributed insightful questions and clari\ufb01cations. We also thank all of our collaborators and advisor for useful conversations; in particular Andrew Miller and Eli Ben-Sasson. Previous version: QmYcf7X6ygKisoVS7EApqY3gxcKW1MigF57zc1cdXjZWrQ 1 Juan Benet. IPFS - Content Addressed, Versioned, P2P File System. 2014. 2 Giuseppe Ateniese, Randal Burns, Reza Curtmola, Joseph Herring, Lea Kissner, Zachary Peterson, and Dawn Song. Provable data possession at untrusted stores.",
      "In Proceedings of the 14th ACM conference on Computer and communications security, pages 598609. Acm, 2007. 3 Ari Juels and Burton S Kaliski Jr. Pors: Proofs of retrievability for large \ufb01les. In Proceedings of the 14th ACM conference on Computer and communications security, pages 584597. Acm, 2007. 4 Hovav Shacham and Brent Waters. Compact proofs of retrievability. In International Conference on the Theory and Application of Cryptology and Information Security, pages 90107. Springer, 2008. 5 Protocol Labs. Technical Report: Proof-of-Replication. 2017. 6 Rosario Gennaro, Craig Gentry, Bryan Parno, and Mariana Raykova. Quadratic span programs and succinct nizks without pcps. In Annual International Conference on the Theory and Applications of Cryptographic Techniques, pages 626645. Springer, 2013. 7 Nir Bitansky, Alessandro Chiesa, and Yuval Ishai. Succinct non-interactive arguments via linear inter- active proofs. Springer, 2013.",
      "8 Eli Ben-Sasson, Alessandro Chiesa, Daniel Genkin, Eran Tromer, and Madars Virza. Snarks for c: Verifying program executions succinctly and in zero knowledge. In Advances in CryptologyCRYPTO 2013, pages 90108. Springer, 2013. 9 Eli Ben-Sasson, Iddo Bentov, Alessandro Chiesa, Ariel Gabizon, Daniel Genkin, Matan Hamilis, Evgenya Pergament, Michael Riabzev, Mark Silberstein, Eran Tromer, et al. Computational integrity with a public random string from quasi-linear pcps. In Annual International Conference on the Theory and Applications of Cryptographic Techniques, pages 551579. Springer, 2017. 10 Henning Pagnia and Felix C Gartner. On the impossibility of fair exchange without a trusted third party. Technical report, Technical Report TUD-BS-1999-02, Darmstadt University of Technology, Department of Computer Science, Darmstadt, Germany, 1999. 11 Joseph Poon and Thaddeus Dryja. The bitcoin lightning network: Scalable o\ufb00-chain instant payments. 2015.",
      "12 Andrew Miller, Iddo Bentov, Ranjit Kumaresan, and Patrick McCorry. Sprites: Payment channels that go faster than lightning. arXiv preprint arXiv:1702.05812, 2017. 13 Protocol Labs. Technical Report: Power Fault Tolerance. 2017. 14 Protocol Labs. Technical Report: Expected Consensus. 2017. 15 Iddo Bentov, Charles Lee, Alex Mizrahi, and Meni Rosenfeld. Proof of activity: Extending bitcoins proof of work via proof of stake extended abstract y. ACM SIGMETRICS Performance Evaluation Review, 42(3):3437, 2014. 16 Iddo Bentov, Rafael Pass, and Elaine Shi. Snow white: Provably secure proofs of stake. 2016. 17 Silvio Micali. Algorand: The e\ufb03cient and democratic ledger. arXiv preprint arXiv:1607.01341, 2016. 18 Vitalik Buterin. Ethereum https:ethereum.org, April 2014. URL https:ethereum.org. 19 Satoshi Nakamoto. Bitcoin: A peer-to-peer electronic cash system, 2008. 20 Eli Ben Sasson, Alessandro Chiesa, Christina Garman, Matthew Green, Ian Miers, Eran Tromer, and Madars Virza.",
      "Zerocash: Decentralized anonymous payments from bitcoin. In Security and Privacy (SP), 2014 IEEE Symposium on, pages 459474. IEEE, 2014."
    ],
    "word_count": 15711,
    "page_count": 36
  },
  "GRT": {
    "chunks": [
      "arXiv:2006.12275v1 math.LO 18 Jun 2020 ZU064-05-FPR Final 23 June 2020 0:55 The Review of Symbolic Logic Volume 0, Number 0, Month 202X How Much Propositional Logic Su\ufb03ces for Rossers Essential Undecidability Theorem? GUILLERMO BADIA School of Historical and Philosophical Inquiry, University of Queensland PETR CINTULA, PETR H\u00c1JEK, and ANDREW TEDDER The Institute of Computer Science of the Czech Academy of Sciences Abstract. In this paper we explore the following question: how weak can a logic be for Rossers essential undecidability result to be provable for a weak arithmetical theory? It is well known that Robinsons Q is essentially undecidable in intuitionistic logic, and P. H\u00e1jek proved it in the fuzzy logic BL for Grzegorczyks variant of Q which interprets the arithmetic operations as non-total non- functional relations.",
      "We present a proof of essential undecidability in a much weaker substructural logic and for a much weaker arithmetic theory, a version of Robinsons R (with arithmetic operations also interpreted as mere relations). Our result is based on a structural version of the undecidability argument introduced by Kleene and we show that it goes well beyond the scope of the Boolean, intuitionistic, or fuzzy logic. 1. Introduction In Theorem III of Rosser (1936), it was famously established that Peano Arithmetic was essentially undecidable(a notion only properly named later by Tarski (1949)); that is, no consistent extension of it is decidable (see Tarski et al (1953) for the standard reference on this topic). After Rossers essential undecidability theorem, it was natural to ask for weaker theories of arithmetic that would still yield undecidability along similar lines. Robinson (1950) provided the perhaps best known example of such a theory, namely, Robinsons Arithmetic Q.",
      "The most noteworthy other essentially undecidable weakening of Q which will play a special role here is R, also due to Robinson (see Tarski et al (1953), p. 531), which allows for the so-called structural essential undecidability argument (to borrow the terminology from \u0160vejdar (2008)) due originally to Kleene (1950); see Proposition 15.9 and Theorem 15.19 of Monk (1976) for the textbook version of the argument, or below for our rendering. Below are the standard axioms of R and Q:2 Received August 2019 0 This paper is an extension and generalisation of work started by Cintula and H\u00e1jek before the unfortunate death of the latter in 2016. H\u00e1jek is included among the authors in recognition of his work on this topic, and with the blessing of his family, but it should be noted that he was not able to contribute directly to the \ufb01nal version of this paper.",
      "1 See Visser (2014) for a survey of results involving R and Vaught (1962) for the original reference regarding undecidability of this theory. Further noteworthy results on R are given by Jones and Shepherdson (1983). 2 In the axiomatisation of R, is often taken to be a de\ufb01ned predicate, which allows for (R4) to be simpli\ufb01ed to just the left-to-right direction of our biconditional version. Since we shall take as primitive, we include both directions. Note also that we use n to refer to a number, and n to refer to the associated numeral (this will be properly de\ufb01ned in Section 4.).",
      "doi: ZU064-05-FPR Final 23 June 2020 0:55 Badia, Cintula, H\u00e1jek, and Tedder (R1) m  n  m  n (R2) m  n  m  n (R3) m , n for m , n (R4) x n (x  0 x  1    x  n) (R5) x n n x (Q1) S(x) , 0 (Q2) S(x)  S(y) x  y (Q3) x , 0 (y)(x  S(y)) (Q4) x  0  x (Q5) x  S(y)  S(x  y) (Q6) x  0  0 (Q7) x  S(y)  (x  y)  x (Q8) x y (z)x  z  y Given the proliferation of non-classical logical systems in the literature, the question not only of potential weakenings of the arithmetic theory but also of the background propositional logic become salient, and our aim here is to consider how much (or how little) propositional logic su\ufb03ces for something like Kleenes argument. Our strategy will consist in a close inspection of the structural essential undecidability argument in order to generalise it to the non-classical case, with an eye to the question: what logical principles are actually required for the argument to work and in which non-classical settings are they available?",
      "It was already well-known at least since the 1950s (see Kleene (1952)) that the undecid- ability results for Q hold intuitionistically as well as classically. H\u00e1jek (2007) showed that it was also true for a wide variety of fuzzy logics and an arithmetic theory called Q, a variant of Q, introduced by Grzegorczyk (2006) (see also \u0160vejdar (2007)) which results from Q by replacing the addition and multiplication functions by ternary predicates A and We shall show that even against the background of a weaker logic than H\u00e1jek ever considered, we can prove essential undecidability for a cousin (in fact, a weakening against a certain logical background) of Qthat we call R, which is a natural generalization of R.",
      "Rand Qare axiomatised as follows: (R1) A(m, n, x) m  n  x (R2) M(m, n, x) m  n  x (R3) m , n for m , n (R4) x n (x  0 x  1    x  n) (R5) x n n x (R6) x n (x n) (Q0) x  y x , y (Q1) S(x) , 0 (Q2) S(x)  S(y) x  y (Q3) x , 0 (y)(x  S(y)) (Q4) A(x, 0, y) x  y (Q5) A(x, S(y), z) (u)(A(x, y, u) z  S(u)) (Q6) M(x, 0, y) y  0 (Q7a) M(x, S(y), z) (u)(M(x, y, u) A(u, x, z)) (Q7b) M(m, n, u) (A(u, n, x) M(m, n  1, x)) (Q8) x y (z)A(z, x, y) Remark 1.1. A brief comment is in order concerning concerning these axioms. First note that our additional axiom (R6) is an instance of excluded middle. We include (R6) because the weaker of our two logics does not allow its derivation from the others (though see Prop. 6.15., in which we derive it from the other Raxioms in our stronger logic (along ZU064-05-FPR Final 23 June 2020 0:55 How Much Propositional Logic Suffices for Essential Undecidability? with the assumption of excluded middle for equalities)).",
      "Furthermore, it should be noted that adding the assumption of functionality and totality of A, M to Ror Qresults in R, Q respectively, in classical logic. Finally, in Section 6. we comment further on the relation of our axiomatisation Qwith the system studied by H\u00e1jek. In Section 2. we present the necessary basic de\ufb01nitions in an abstract setting and outline the aforementioned structural essential undecidability argument. We shall see that the crucial ingredient of the proof is the existence of formulas separating two disjoint recursively enumerable sets: call such a formula one which strongly separates the sets in question. In Section 3. we present our weak logic and prove the existence of strongly separating formulae for Ragainst the background of this logic in Section 4. (after establishing its completeness with respect to the standard model of arithmetic for \u03a31- formulas) and \ufb01nally in Section 6.",
      "we show that, in a slightly stronger logical setting (presented in Section 5.), a variant of H\u00e1jeks Qstrengthens our R, and thus our results indeed generalize those of H\u00e1jek (2007). 2. The Structural Proof of Essential Undecidability The \ufb01rst ingredients we need are the formulas.",
      "As we do not yet want to bind ourselves to any particular syntax (propositional or \ufb01rst order), let us only assume that we have a countable set of formulas Fm which contains a special subset of formulas that we will suggestively call the \u03a31-formulas and that for each \u03a31-formula \u03d5 there is a special formula (not necessarily a \u03a31-formula) which we call the negation of \u03d5 and suggestively denote \u03d5.3 The second ingredient is that of a logic L which is identi\ufb01ed with a consequence relation L over Fm, i.e., L (Fm)  Fm and for each \u0393 \u03d5 Fm we have:  \u0393 \u03d5 L \u03d5 (Re\ufb02exivity)  If \u0393 L \u03d5 and for each \u03b3 \u0393 we have L \u03b3, then L \u03d5 (Cut) By theory we understand simply a set of formulas; for each theory T we de\ufb01ne the set of its consequences in logic L as CL(T)  \u03d5  T L \u03d5. As the \ufb01nal ingredient we need to de\ufb01ne the notion of essential undecidability of a theory.",
      "As the underlying logic can vary, we have to be a bit more careful and formalistic in our de\ufb01nitions of (un)decidability,extension and consistency now (so that we can be a bit looser going forward).4 When we speak about the decidability of a theory T, we actually speak about the decidability of the set CL(T), i.e., questions of decidability depends on the logic in question (e.g. all theories are trivially decidable in the inconsistent logic Inc  (Fm)Fm). Analogously when we say that a theory T strengthens a theory S we do not speak about 3 Note that \u03d5 need not be a formula per se, it is merely a notation of the negation of \u03d5. The argument below would work perfectly well if all formulas would be \u03a31-formulas; we however in principle assume it can be a proper subset to cover a wider logical setting. See mainly our notion of consistency below.",
      "4 The abstract framework we develop here can be seen as a still less general version of a similar framework developed by Smullyan (1961) for abstract reasoning about undecidability and incompleteness. We could use his framework of representation systems in order to present our results by \ufb01xing for one of our arithmetic theories U, a representation where S is the set of \u03a31-formulae in the language of U, T  CL(U), and R  \u03d5  U L \u03d5. This would put our work into Smullyans context, but we follow our current mode of presentation here because we do not need the additional generality provided by Smullyans approach here. Thanks are due to an anonymous referee for pointing out this connection. ZU064-05-FPR Final 23 June 2020 0:55 Badia, Cintula, H\u00e1jek, and Tedder simple subsethood but about the fact that T proves all axioms of S in L, i.e., S CL(T).",
      "Finally the consistency depends on the logic in question and on the class of \u03a31-formulas: we say that a theory T is \u03a31-consistent in L if for no \u03a31-formula \u03d5 we have T L \u03d5 and T L \u03d5 (note the our notion of consistency implies non-triviality, i.e., that there is a \u03d5 such that T L \u03d5, and in su\ufb03ciently strong logics the converse is true as well; furthermore in a su\ufb03ciently strong arithmetical theory it is equivalent with T L 0  1). Therefore we should speak about \u03a31-consistency, decidability, and strengthening in L. To simplify matters, whenever the logic is known from the context we assume that all subsequent uses of these three notions are parameterized by the logic in question. We also omit the pre\ufb01x \u03a31, when this is clear from the context. With this convention in place, we can give the following de\ufb01nition analogously to the classical case: Definition 2.2. A theory T is essentially undecidable in L if it is consistent and each consistent theory strengthening T is undecidable.",
      "The next proposition shows that our notion of essential undecidability is a particularly robust version of essential undecidability as it is preserved not only in stronger theories but also in stronger logics. The statement of this proposition is made somewhat intricate to accommodate for the possibility of a stronger logic being de\ufb01ned over a bigger set of formulas. Proposition 2.3. Let L be a logic over a set of formulas Fm and L L a logic over a set of formulas Fm Fm such that all \u03a31-formulas of Fm are \u03a31-formulas of Fm. Furthermore, assume that T is an essentially undecidable theory in L. Then any theory S of L which is consistent and strengthens T (seen as a L-theory) is essentially undecidable in L. Proof. It su\ufb03ces to show that any theory U which is consistent in L and strengthens S in L is undecidable in L. De\ufb01ne a set V  \u03c7 Fm  U L \u03c7 and observe V  CL(V): indeed if V L \u03b4 implies V L \u03b4 and for each \u03c7 V we have U L \u03c7 and so due to the (cut) rule of L we have U L \u03b4, i.e., \u03b4 V.",
      "Therefore V is consistent in L (otherwise we would obtain contradiction with the assumption that U is consistent in L) and strengthens T in L (actually T U) and so by essential undecidability of T in L we know that V is undecidable. Since \u03c7 V i\ufb00 \u03c7 CL(U), then CL(U) is undecidable as well. When we establish that a theoryT of a logic L is a Rosser theory, de\ufb01ned below, the proof of the essential undecidability theorem can proceed as in the classical setting. We present the proof in some detail so it is obvious that no additional properties of L are needed. Definition 2.4. (Rosser Theories) We say that a theory T is Rosser in logic L if for each pair of disjoint recursively enumerable sets A, B N there is a recursive series of \u03a31-formulas \u03d5n which strongly separate A from B, that is:  n A implies T L \u03d5n. n B implies T L \u03d5n. Theorem 2.5. (Rosser Theorem) Let L be a logic and T an L-consistent Rosser theory. Then T is essentially undecidable in L. Proof.",
      "First recall that from recursion theory (see e.g. Theorem 6.24 of Monk (1976)) that we know that there are disjoint r.e. sets A, B N such that each X A such that X B   is not recursive. Let \u03d5n be the series of \u03a31-formulas guaranteed strongly separating A and ZU064-05-FPR Final 23 June 2020 0:55 How Much Propositional Logic Suffices for Essential Undecidability? Consider a consistent theory S strengthening T, de\ufb01ne X  n  S \u03d5n, and notice that:  n A implies T L \u03d5n and so, due to the (cut) rule, S \u03d5n, which entails X A. n B implies T L \u03d5n and so S \u03d5n, which entails X B  (otherwise S would not be consistent). Thus X cannot be recursive and as it is clearly recursively reducible to CL(S), S cannot be decidable in L. This structural argument leaves open the question of which logical principles are needed to prove that some theory T is Rosser in L, which is where our work really begins. 3.",
      "A weaker logic The minimal logic in which we prove that Ris Rosser will be de\ufb01ned as a natural \ufb01rst-order extension of a particular propositional non-classical logic. We take propositional logics to be substitution-invariant consequence relations over a set of propositional formulas given by a propositional language L (a set of connectives with arities). It is well-known that each such logic can be presented by means of a Hilbert style proof system consisting of axiom and rule schemata (we use the symbol to separate premises of a rule from its conclusion). Our basic propositional language L0 consists of three binary connectives (implication , (lattice) conjunction and (lattice) disjunction ) and a propositional constant .5 Negation  and equivalence are de\ufb01ned in the following standard way: \u03d5 : \u03d5  \u03d5 \u03c8 : (\u03d5 \u03c8) (\u03c8 \u03d5). As usual, we assume that  has the highest binding power, followed by and , and \ufb01nally and have the lowest.",
      "Our basic propositional logic in the language L0, which we denote as L0, is given by the following Hilbert style proof system:6 (identity) \u03d5 \u03d5 (weakening) \u03d5 \u03c8 \u03d5 (elim) \u03d5 \u03c8 \u03d5 (MP) \u03d5, \u03d5 \u03c8 \u03c8 (elim) \u03d5 \u03c8 \u03c8 (assertion) \u03d5 (\u03d5 \u03c8) \u03c8 (intro) \u03d5 \u03d5 \u03c8 (trans) \u03d5 \u03c8, \u03c8 \u03c7 \u03d5 \u03c7 (intro) \u03c8 \u03d5 \u03c8 (morg) \u03d5 \u03c8 (\u03d5 \u03c8) (intro) \u03c7 \u03d5, \u03c7 \u03c8 \u03c7 \u03d5 \u03c8 (elim) \u03d5 \u03c7, \u03c8 \u03c7 \u03d5 \u03c8 \u03c7 The following theorems and derived rules are easily shown to be derivable in L0:7 (assoc) \u03d5 (\u03c8 \u03c7) (\u03d5 \u03c8) \u03c7 (adj) \u03d5, \u03c8 \u03d5 \u03c8 (dni) \u03d5 \u03d5 5 Note that in our weaker logic (axiomatised below) we do not include the axiom \u03d5, so though we use this suggestive notation, for now is merely a propositional constant (however later in Section 5., we will will work with the logic SLw obeying this additional axiom, in which it will become a genuine falsum constant). 6 Note the we use the same symbol for two closely related yet di\ufb00erent axioms, we can a\ufb00ord this slight abuse of language as in any given formal proof it it will be clear which of them we are using.",
      "We use the same convention for some other upcoming axiomtheoremsrules. 7 Associativity is a simple consequence of (intro), (elim) and (trans); adjunction follows using (weakening) for \u03c8 being any theorem and (intro) together with (MP); and \ufb01nally (dni) is an instance of (assertion). ZU064-05-FPR Final 23 June 2020 0:55 Badia, Cintula, H\u00e1jek, and Tedder Remark 3.6. L0 extends lattice logic by the rules (morg), (weakening), and (assertion). These additional rules are chosen to ful\ufb01ll speci\ufb01c tasks in \ufb02eshing out the structural argument for our version of R. So if L0 looks somewhat arti\ufb01cial, thats because it is.",
      "However, it should be noted that it is a sublogic of many well known systems, most notably classical and intuitionistic logic, as well as H\u00e1jeks BL and (related to the stronger system well present later in Section 5.) the non-distributive, non-associative Lambek calculus with weakening in L0, when this is presented with (assertion) as a rule (related systems to this are discussed, for instance, in Galatos et al (2007)). In fact, it is easy to see that L0 is a proper sublogic of all these logics. Now we are ready to introduce \ufb01rst-order logics. We present only the syntactical aspects, excepting where we consider a very special model (namely, the natural numbers with a classical interpretation of the vocabulary). Let us \ufb01x a propositional logic L expanding L0 (thus in particular, L has a propositional language which contains L0).",
      "Our notion of \ufb01rst-order language is standard, i.e., it is given by a set of function and predicate symbols with a special binary predicate symbol  for equality (we write t , s for (t  s)). Then terms; atomic formulas, and formulas are built up as usual. In addition, the notions of freebounded variable, substitutability, sentence, etc. are de\ufb01ned as usual. The \ufb01rst-order logic QL, for a given predicate language, is axiomatised by the substi- tutional instances of all axiomsrules of L (i.e., formulas resulting by replacing atoms by \ufb01rst-order formulas) and the following additional axiomrule schemata (we assume that t substitutable for x in \u03d5 and x not free in \u03c7): (ins) (x)\u03d5(x) \u03d5(t) (intro) \u03d5(t) (x)\u03d5(x) (intro) \u03c7 \u03c8 \u03c7 (x)\u03c8 (elim) \u03c8 \u03c7 (x) \u03c8 \u03c7 (id) x  x (com) x  y y  x (trans) x  y (y  z x  z) (prin) x  y t(x)  t(y) (prin) x  y (\u03d5(x) \u03d5(y)) We can easily establish the following auxiliary derived rules:8 (aux) \u03d5(t) x  t \u03d5(x) (gen) \u03d5 (x)\u03d5 4.",
      "Main result In order to apply the structural argument presented in Section 2. we need to \ufb01x a logic over a set of formulas, identify the \u03a31-formulas, de\ufb01ne a theory, show that it is consistent and Rosser. We will work with the logic QL0 over the set of formulas for the predicate language of Grzegorczyks arithmetic, i.e., the language with constant 0, unary function symbol S, binary predicate symbols  and , and ternary predicate symbols A and M. We de\ufb01ne the nth numeral n as usual: n  S n. . . S(0). 8 To derive (aux), assuming \u03d5(t), use (assertion) to obtain (\u03d5(t) \u03d5(x)) \u03d5(x) and so (prin) and (trans) complete the proof. ZU064-05-FPR Final 23 June 2020 0:55 How Much Propositional Logic Suffices for Essential Undecidability?",
      "\u03a31-formulas be those of the form \u03d5  (x)\u03c8 for some 0-formula \u03c8, where 0-formulas are those arithmetical formulas where all quanti\ufb01ers are bounded, i.e., are of the form:9 (x y)\u03d5  (x)((x y) \u03d5) (x y)\u03d5  (x)(x y \u03d5) As expected the negation of a \u03a31-formula \u03d5 will be the formula \u03d5. Now we can formally present the theory Rwhich we mentioned in the introduction:10 (R1) A(m, n, x) m  n  x for any n and m (R2) M(m, n, x) m  n  x for any n and m (R3) m , n for m , n (R4) x n (x  0 x  1    x  n) for any n (R5) x n n x for any n (R6) x n (x n) for any n Our goal in this section is to establish essential undecidability of Rin QL0. Thanks to Theorem 2.5. we know that it su\ufb03ces to prove that it is QL0-consistent and Rosser.",
      "By N we denote the set of natural numbers and by N we denote the standard model of the natural numbers in our arithmetical language, i.e., the structure with constant 0 interpreted as 0, S interpreted so that S(k)  k 1, interpreted by the usual order of natural numbers, and A and M as: AN(k, l, m) i\ufb00k  l  m MN(k, l, m) i\ufb00k  l  m. In this model we can interpret all connectives and quanti\ufb01ers of QL0 in a fully classical way (so, for instance, \u03d5 \u03c8 should be interpreted as \u03d5\u03c8 for Boolean ). For a formula \u03d5(x1, . . ., xn) with free variables x1, . . ., xn, we write:  N  \u03d5(k1, . . ., kn) if \u03d5 is satis\ufb01ed in N when variables xi are evaluated as ki  N  \u03d5 if N  \u03d5(k1, . . ., kn) for each k1, . . ., kn N. Note that a numeral nis interpreted in N as the numbern and so we have N  \u03d5(k1, . . ., kn) i\ufb00N  \u03d5(k1, . . ., kn).",
      "It is easy to see that the structure N can be interpreted as a model of Rin QL0; formally speaking we can prove the following (as in the rest of this section we work in the logic QL0 only, we omit it as a subscript of ): Proposition 4.7. (Soundness) For each formula \u03d5, R\u03d5 implies N  \u03d5. For no \u03a31-formula \u03d5 do we have N  \u03d5 and N  \u03d5, and this entails that Ris QL0- consistent. As the next step we establish the converse claim for \u03a31-sentences (known as \u03a31-completeness), i.e., for each \u03a31-sentence \u03d5 we have N  \u03d5 only if R\u03d5. We prove a stronger statement. 9 Let us stress that this de\ufb01nition of bounded quanti\ufb01cation is intended for the classical arithmetical formulas; the bounded quanti\ufb01ers in non-classical logics may be (and often are) de\ufb01ned using implication and strong conjunction; but as we have no need for such quanti\ufb01ers in the paper, no confusion should arise. 10 Note that because of (assoc), we may state (R4) with no \ufb01xed association.",
      "ZU064-05-FPR Final 23 June 2020 0:55 Badia, Cintula, H\u00e1jek, and Tedder Theorem 4.8. ( \u03a31-completeness) For each \u03a31-formula \u03d5(x1, . . ., xn), we have: N  \u03d5(k1, . . ., kn) R\u03d5(k1, . . ., kn). Proof. One direction is a consequenceofProposition 4.7.and the fact that N  \u03d5(k1, . . ., kn) i\ufb00N  \u03d5(k1, . . ., kn). We prove the converse direction \ufb01rst for 0-formulas by induction on the complexity of \u03d5. As we cannot deal with negation directly, the induction step for it will have to take care of the next principal connectivequanti\ufb01er. \u03d5 is atomic First note that all terms are of the form S . . . S(x) for some variable x, so it su\ufb03ces to prove the claim for numerals.",
      "Observe that (1) if N  n  m, then Rn  m by (prin); (2) if N  A(n, m, k) (i.e., n  m  k), then as before Rn  m  k which due to (R1) implies RA(n, m, k); (3) analogously N  M(n, m, k) implies RM(n, m, k) using (R2); and \ufb01nally (4) from N  m n, we know that Rm n thanks to axiom (R4) for x  m (as then m  m is one of the disjoints on the right-hand side disjunction). \u03d5  \u03c8 \u03c7 From the assumption we obtain N  \u03c8 and N  \u03c7. Thus by the induction assumption R\u03c8 and R\u03c7, and so (adj) completes the proof. \u03d5  \u03c8 \u03c7 From the assumption we obtain N  \u03c8 or N  \u03c7. Thus by the induction assumption R\u03c8 or R\u03c7, and so (intro) completes the proof. \u03d5  (y x)\u03c8(y, x1, . . ., xn) From N  (y k)\u03c8(y, k1, . . ., kn) we obtain an m such that N  m k N  \u03c8(m, k1, . . ., kn). Thus by the induction assumption Rm k and R\u03c8(m, k1, . . ., kn) and so (adj) and (intro) complete the proof. \u03d5  (y x)\u03c8(y, x1, . . ., xn) From N  (y k)\u03c8(y, k1, . . ., kn) we know that for each m k we have N  \u03c8(m, k1, . . ., kn).",
      "Thus by the induction assumption we obtain, R\u03c8(m, k1, . . ., kn). Next we use (aux) to obtain, for m k: Ry  m \u03c8(y, k1, . . ., kn). Thus, using (elim), we obtain R(y  0 y  1    y  k) \u03c8(y, k1, . . ., kn) which, using (R4), (intro), and (trans), entails: R(y k) (y k) \u03c8(y, k1, . . ., kn). As clearly using (intro) we also have R(y k) (y k) \u03c8(y, k1, . . ., kn). then by (elim) and axiom (R6), R(y k) \u03c8(y, k1, . . ., kn). and (gen) complete the proof. ZU064-05-FPR Final 23 June 2020 0:55 How Much Propositional Logic Suffices for Essential Undecidability? \u03d5  \u03c8 We have to distinguish the structure of \u03c8: \u03c8 is atomic Analogously to the positive case: (1) If N  n , m, then R(n  m) by (R3); (2) if N  A(n, m, k) (i.e., nm , k), then as before Rn  m , k which due to (R1) and (trans) implies RA(n, m, k); (3) analogously N  M(n, m, k) implies RM(n, m, k) using (R2); and \ufb01nally (4) from N  (m n), implies for each k n we have k , m, thus by (R3) we obtain Rm  k .",
      "Thus by (elim) we obtain Rm  0 m  1    m  n  and so (R4) and and (trans) complete the proof. \u03c8  \u03b1 \u03b2 From N  (\u03b1\u03b2) we obtain N  \u03b1 or N  \u03b2. Thus by the induction assumption we know that R\u03b1 or R\u03b2 and so in both cases (elim) and (trans) completes the proof. \u03c8  \u03b1 \u03b2 From N  (\u03b1 \u03b2) we obtain N  \u03b1 and N  \u03b2. Thus by the induction assumption we know that R\u03b1 and R\u03b2 and so (elim) completes the proof. \u03c8  \u03c7 From N  \u03c7 we obtain N  \u03c7. Thus by the induction assumption we know that R\u03c7 and so (dni) completes the proof. \u03c8  (y x)\u03c7(y, x1, . . ., xn) From N  (y k)\u03c7(y, k1, . . ., kn) we obtain for each m k that: N  \u03c7(m, k1, . . ., kn) . Thus as in the the positive case for we could show that R(y k) \u03c7(y, k1, . . ., kn). Thus by (morg) we obtain Ry k \u03c7(y, k1, . . ., kn)  and so (elim) completes the proof. \u03c8  (y x)\u03c7(y, x1, . . ., xn) From N  (y k)\u03c7(y, k1, . . ., kn) we know that there is an m such that N  (m k) N  \u03c7(m, k1, . . ., kn). Thus by the induction assumption we obtain R(m k)  R\u03c7(m, k1, . .",
      "., kn)  which using (elim) entails R((m k) \u03c7(m, k1, . . ., kn))  and so (ins) and (trans) complete the proof. Finally we deal with \u03a31-formulas. Assume that N  (y)\u03c8 for some 0-formula \u03c8. Then there is some m such that N  \u03c8(m, k1, . . ., kn). Hence, R\u03c8(m, k1, . . ., kn) and so (intro) completes the proof. ZU064-05-FPR Final 23 June 2020 0:55 Badia, Cintula, H\u00e1jek, and Tedder Now we have all the ingredients to prove that Ris Rosser in QL0; we will actually show a bit more: there is a single \u03a31-formula \u03d5 such that \u03d5(0), \u03d5(1), . . . is the series of formulas witnessing that Ris Rosser.11 Lemma 4.9. For each pair of disjoint r.e. sets A, B N, there is a \u03a31-formula \u03d5(x) such that n A implies R\u03d5(n) n B implies R\u03d5(n) Proof. From the recursion theory (see e.g. Lindstr\u00f6m (1997), Fact 1.3(b)) we know that A and B are de\ufb01nable using the classical \u03a31-formulas, i.e., formulas in the language with functions  and  instead of predicates A and M.",
      "Let us show that each classical 0- formula is equivalent to a 0-formula in our language (assuming that N interprets all these symbols). Let us call the terms and formulas of our language simple. An classical atomic formula is almost-simple if it is simple or of the form x  t1 t2, where is either  or , x is a variable and t1 and t2 are simple term. Clearly replacing such a formula by A(t1, t2, z) or M(t1, t2, x) respectively yields an equivalent formula in our language. So it su\ufb03ces to show that any classical 0-formula is equivalent to a classical 0-formula where all atomic formulas are simple or almost-simple (we omit the adjective classical from now on). First, we observe the validity of the following three statements of classical logic for being each  or , terms t, t1, t2 and variables x, x1, x2 not occurring in those terms:  N  t  t1 t2 (x1 t)(x2 t)(t  x1 x2)  N  t1 t2 t (x1 t)(x2 t)(x1 x2 t)  N  t t1 t2 (x1 t)(x2 t)(x1 t1 x2 t2 t  x1 x2).",
      "Next we note that applying any of these equivalencies to any non-almost-simple atomic subformula of a given 0-formula \u03c7 strictly decreases the \ufb01nite multiset of depths of terms occurring in the formula according to the standard multiset well-ordering.12 Therefore exhaustively applying these equivalencies yields the required equivalent 0-formula with only almost-simple atomic formulas. Thus we can assume that there are 0-formulas \u03b1(x, v) and \u03b2(x, v) (in our language) such that  n A i\ufb00N  (v)\u03b1(n,v). n B i\ufb00N  (v)\u03b2(n, v). We de\ufb01ne the 0-formula \u03c8(x): \u03c8(x, v)  (\u03b1(x, v) (u v)\u03b2(x, u)) and show that the \u03a31-formula \u03d5(x)  (v)\u03c8(x, v) has the desired properties. 11 The version of the following lemma concerning extensions of R over classical logic was established by Putnam and Smullyan (1960). 12 A \ufb01nite multiset over a set S is an ordered pair S, f , where f is a function f : S N and x S  f (x)  0 is \ufb01nite.",
      "If is a well-ordering of S, then: S, f m S, g x S f (x)  g(x) y Sy  x and g(y)  f (y) is a well-ordering on the set of all \ufb01nite multisets over S, known as the DershowitzManna ordering, for which see Dershowitz and Manna (1979). ZU064-05-FPR Final 23 June 2020 0:55 How Much Propositional Logic Suffices for Essential Undecidability? The \ufb01rst case (n A) is easy: observe that this entails that n  B and so we have not only N  \u03b1(n, m) for some m but also that N  \u03b2(n, k) for each k and so N  (u m)\u03b2(n, u). Therefore N  \u03d5(n) and so by Theorem 4.8. it follows that R\u03d5(n). The proof of the second case (n B) is not so direct because \u03d5 is not a \u03a31-formula and so we cannot use \u03a31-completeness directly. However, because we know that N  \u03b2(n, m) for some m and that N  \u03b1(n, k) for each k we can use it to obtain R\u03b2(n, m) and R\u03b1(n, k). Using these facts we prove that Rv m \u03b1(n, v) Rm v (u v)\u03b2(n, u).",
      "and after establishing these two claims the proof that R\u03d5(n) easily follows: indeed using them together with (intro), (elim), and (R5) we obtain R\u03b1(n, v) (u v)\u03b2(n, u) and so using (dni) we get R\u03c8(x, v) and so (elim) completes the proof. To prove (1) we start with R\u03b1(n, k) and (aux) to obtain for any k m: Rv  k \u03b1(n, v) Therefore the proof is done thanks to (elim) and (R4). To prove (2) we use the claim R\u03b2(n, m) together with (weakening), (identity), and (intro) to obtain: Rm v m v \u03b2(n, m) and so (intro) completes the proof. All that is left is to apply Theorem 2.5. and Proposition 2.3. to obtain the following results. Theorem 4.10. The theory Ris essentially undecidable in QL0. Corollary 4.11. Let L be a propositional logic expanding L0, L a predicate logic expanding QL, and T a theory strengthening Rin L. If T is L-consistent (i.e. proves \u03d5 and \u03d5 for no \u03a31-formula \u03d5), then it is essentially undecidable in L. 5.",
      "A stronger logic Our stronger propositional logic in the propositional language L0, which we denote as SLwshort for the L0-fragment of the non-associative Lambek calculus with left and right weakeningis given by the following Hilbert style proof system (note here that SLw, unlike L0, is not paraconsistent as we include the axiom (elim)): ZU064-05-FPR Final 23 June 2020 0:55 Badia, Cintula, H\u00e1jek, and Tedder (identity) \u03d5 \u03d5 (elim) (\u03d5 \u03c8) \u03d5 (elim) (\u03d5 \u03c8) \u03c8 (intro) ((\u03d5 \u03c8) (\u03d5 \u03c7)) (\u03d5 \u03c8 \u03c7) (intro) \u03d5 (\u03d5 \u03c8) (intro) \u03c8 (\u03d5 \u03c8) (elim) (\u03d5 \u03c7) (\u03c8 \u03c7) (\u03d5 \u03c8 \u03c7) (weakening) \u03d5 (\u03c8 \u03d5) (elim) (MP) \u03d5, \u03d5 \u03c8 \u03c8 (adj) \u03d5, \u03c8 \u03d5 \u03c8 (tone) \u03d5 \u03c8, \u03c7 \u03b8 (\u03c8 \u03c7) (\u03d5 \u03b8) (assertion) \u03d5 (\u03d5 \u03c8) \u03c8 Let us \ufb01rst observe that SLw indeed extends L0: the latters rules of (weakening),(intro), and (elim) follow from the corresponding axioms of SLw using the rules (MP) and (adj); (trans) follows from (tone) by taking \u03c8  \u03c7 using (identity) and (MP); and the rule (morg) follows from the axiomatic form stated below.",
      "Let us note that the rule (tone) can be equivalently replaced by the following two rules:13 (su\ufb03xing) \u03d5 \u03c8 (\u03c8 \u03c7) (\u03d5 \u03c7) (pre\ufb01xing) \u03d5 \u03c8 (\u03c7 \u03d5) (\u03c7 \u03c8) The following theoremsrules are derivable in L0:14 (cont) \u03d5 \u03c8 \u03c8 \u03d5. (morg) (\u03d5 \u03c8) \u03d5 \u03c8 (morg) \u03d5 \u03c8 (\u03d5 \u03c8) (weakening) \u03d5 (\u03c8 \u03d5 \u03c8) (red) \u03d5 (\u03d5 \u03c7) (exp) \u03d5 \u03c8 \u03c7 \u03d5 (\u03c8 \u03c7) Remark 5.12. SLw can be seen as a fragment of the non-associative Lambek calculus with left and right weakening (in the terminology of Galatos et al (2007) (elim) is left weakening and our (weakening) is their right weakening). It is indeed just a fragment: the language of the full logic SLw also involves fusion (residuated conjunction) and dual implication and it is well known that they are not de\ufb01nable from our connectives. The non- associativity of SLw refers to the residuated conjunction, but this fact can be expressed, using implication and their statement of residuation, as the failure of formula (\u03d5 \u03c8)  ((\u03c8 \u03c7) (\u03d5 \u03c7)). Therefore SLw is strictly weaker than H\u00e1jeks logic BL.",
      "SLw can be seen as the extension of the positive fragment (without distribution) of the basic relevant logic B, studied, for instance, by Routley et al (1982), by the weakening axiom, the assertion rule, and negation de\ufb01ned in terms of . 13 For one direction juts consider suitable instances of (tone) and the axiom (identity); for converse direction \ufb01rst use (pre\ufb01xing) to obtain \u03c7 \u03b8 (\u03c8 \u03c7) (\u03c8 \u03b8) and then (su\ufb03xing) to obtain \u03d5 \u03c8 (\u03c8 \u03b8) (\u03d5 \u03b8) and the rule (trans) completes the proof. 14 The \ufb01rst rule is a direct consequence of (su\ufb03xing); both (morg)s are consequences of (intro), (elim), (intro), (elim), and (cont); the stronger form of (weakening) follows from applying (intro) twice on \u03d5 (\u03c8 \u03c8) and \u03d5 (\u03c8 \u03d5); (red) follows from applying (pre\ufb01xing) on (elim); and \ufb01nally to obtain (exp) apply (pre\ufb01xing) twice on \u03d5 \u03c8 \u03c7 and use (weakening). ZU064-05-FPR Final 23 June 2020 0:55 How Much Propositional Logic Suffices for Essential Undecidability?",
      "Finally we need to prove two important facts about crisp formulae, i.e. formulae \u03c8 where \u03c8 \u03c8 is provable. The \ufb01rst claim can be seen as converse of the derived rule (exp). Proposition 5.13. Assume that formula \u03d5 is crisp in T and T \u03d5 (\u03c8 \u03c7). Then also T \u03d5 \u03c8 \u03c7. If furthermore \u03c8 is also a crisp formula in T, then the formula \u03d5 \u03c8 is crisp in T as well. Proof. We present formal derivations of both claims  (1) (\u03c8 \u03c7) (\u03d5 \u03c8 \u03c7) (elim) and (su\ufb03xing) (\u03d5 \u03c7) (\u03d5 \u03c8 \u03c7) (elim) and (su\ufb03xing) \u03d5 (\u03d5 \u03c8 \u03c7) \u03d5 (\u03c8 \u03c7), (1), and (trans) \u03d5 (\u03d5 \u03c8 \u03c7) (red), (2), and (trans) \u03d5 \u03c8 \u03c7 (3), (4), (elim) and crispness of \u03d5. ZU064-05-FPR Final 23 June 2020 0:55 Badia, Cintula, H\u00e1jek, and Tedder  Let us denote the formula (\u03d5 \u03c8) (\u03d5 \u03c8) as \u03c7 \u03d5 \u03c7 (intro) \u03c8 \u03c7 (intro) \u03d5 \u03c8 \u03c7 (morg), (intro), and (trans) \u03d5 (\u03c8 \u03c7) (3) and (exp) \u03d5 (\u03c8 \u03c7) (1), (weakening), and (trans) \u03c8 \u03c7 (4), (5), (elim), and crispness of \u03d5 (2), (6), (elim), and crispness of \u03c8 6.",
      "Qstrengthens Rin QSLw In this section we prove that the arithmetical theory Qproves all theorems of Ragainst the background of QSLw. As a reminder, Qstands to Q as Rstands to R and is axiomatised as follows: (Q0) x  y x , y (Q1) S(x) , 0 (Q2) S(x)  S(y) x  y (Q3) x , 0 (y)(x  S(y)) (Q4) A(x, 0, y) x  y (Q5) A(x, S(y), z) (u)(A(x, y, u) z  S(u)) (Q6) M(x, 0, y) y  0 (Q7a) M(x, S(y), z) (u)(M(x, y, u) A(u, x, z)) (Q7b) M(m, n, u) (A(u, n, x) M(m, n  1, x)) (Q8) x y (z)A(z, x, y) Remark 6.14. It is noteworthy that our Qdi\ufb00ers slightly from H\u00e1jeks. There are two ways in which this is the case. First, we include the additional axiom (Q0) stating that identities are crisp. H\u00e1jek includes this as an assumption of the \ufb01rst order logic, whereas we build it directly into the theory.",
      "In addition, his system includes only one axiom (Q7)the biconditional version of our (Q7a)and furthermore in his (Q5) and (Q7), the conjunction occurring is the strong conjunction of BL (that of which the conditional is residual). Note that all of our Q axioms are provable from H\u00e1jeks version of the theory in BL. First, (Q7a) and (Q7b) are consequences of his version stated with strong conjunction. In addition, one can prove our (Q5) from his arithmetic theory in BL. First, since \u03d5 (\u03c8 \u03d5 \u03c8) is provable in both our systems, our left-to-right direction of (Q5) is an immediate consequence of his (this is the same reason as that for why our (Q7a) is a consequence of his axiom).",
      "Second, (u)(A(x, y, u) z  S(u)) A(x, S(y), z) is provable in his system given the crispness of identity, as he shows that in \ufb01rst order BL (which is actually strictly stronger then QBL by using the additional axiom of constants domains), whenever \u03d5 is crisp, then any weak conjunction of \u03d5 with some other formula is equivalent to their strong conjunction; see H\u00e1jek (2007), remark 2.1(1). So, since z  S(u) is crisp, the result follows. Hence, our results do indeed generalise H\u00e1jeks, despite our using a variant on his Q. Note that in Q, thanks to (Q0), Prop 5.13., and (exp), we can replace the identity axiom (prin) for formulas by its equivalent formulation: (prin) x  y \u03d5(x) \u03d5(y) ZU064-05-FPR Final 23 June 2020 0:55 How Much Propositional Logic Suffices for Essential Undecidability? Theorem 6.15. Qstrengthens Rin QSLw. Proof. Before we start proving the axioms of Rlet us prove two useful preliminaries: Claim 1 Qx y S(x) S(y)).",
      "Claim 2 For any formula \u03d5(x) such that Q\u03d5(0) and Q\u03d5(S(y)) we have Q\u03d5(x). To prove the \ufb01rst claim it clearly su\ufb03ces to prove A(x, y, z) A(x, S(y), S(z)) and use (intro), (elim) and (Q8) to complete the proof. The proof of the left-to-right implication is easy: clearly A(x, y, z) A(x, y, z) S(z)  S(z). Thus, by (intro) and (trans), A(x, y, z) (u)(A(x, y, u) S(z)  S(u)) and so (Q5) completes the proof. The converse implication is a bit more complex: S(z)  S(u) z  u (Q2) A(x, y, u) S(z)  S(u) A(x, y, u) z  u (1), (intro), (elim) A(x, y, u) S(z)  S(u) A(x, y, z) (2), (prin), and (trans) (u)(A(x, y, u) S(z)  S(u)) A(x, y, z) (3) and (elim) A(x, S(y), S(z)) A(x, y, z) (Q5), (4), (trans) To prove the second claim let us use (aux) for both premises to obtain Qx  0 \u03d5(0) and Qx  S(y) \u03d5(x). Using (elim) and (Q3) we obtain Qx , 0 \u03d5(x) and thus (elim) and (Q0) complete the proof. (R1): First we prove A(m, n, x) x  m  n by metainduction on n. The case n  0 follows from (Q4).",
      "The inductive case: A(m, n, u) u  m  n by IH A(m, n, u) x  S(u) u  m  n x  S(u) (1), (intro), (elim) A(m, n, u) x  S(u) x  m  n  1 (2), (prin), (trans) (u)(A(m, n, u) x  S(u)) x  m  n  1 (3), (elim) A(m, n  1, u) x  m  n  1 (4), (Q5), (trans) To prove the converse direction we show, again by metainduction on n, that A(m, n, m  n) and use (aux) to complete the proof. Again, the case n  0 follows from (Q4) and the inductive case follows immediately from the induction assumption and the fact that A(x, y, z) A(x, S(y), S(z)) which we established in the proof of Claim 1. (R2): The proof is similar; \ufb01rst establish M(m, n, x) x  m  n by metainduction on n. The case n  0 follows from (Q6).",
      "The inductive case: M(m, n, u) u  m  n by IH M(m, n, u) A(u, m, x) u  m  n A(u, m, x) (1), (intro), (elim) M(m, n, u) A(u, m, x) A(m  n, m, x) (2), (prin), (trans) M(m, n, u) A(u, m, x) x  m  (n  1) (3), (R1), (trans) (u)(M(m, n, u) A(u, m, x)) x  m  (n  1) (4), (elim) M(m, n  1, u) x  m  (n  1) (5), (Q7a), (trans) To prove the converse direction we show, again by metainduction on n, that M(m, n, m  n) and then (aux) completes the proof.Again, the case n  0 follows from (Q6). The inductive case: M(m, n, m  n) A(m  n, n, m  (n  1)) (R1) M(m, n, m  n) (A(m  n, n, m  (n  1)) M(m, n  1, m  (n  1))) (Q7b) M(m, n  1, m  (n  1)) (3) and MP twice ZU064-05-FPR Final 23 June 2020 0:55 Badia, Cintula, H\u00e1jek, and Tedder (R3): It su\ufb03ces to establish the case where n  m. Observe that m  n m n  0 by repeated use of (Q2). As m n , 0 we know that m  n S(m n 1)  0 and as S(m n 1)  0 due to (Q1), the claim follows.",
      "(R4): To prove the right-to-left direction observe that for k n we have A(n k, k, n) due to (R1) and so k n using (intro) and (Q8) and thus x  k x n by (aux). Repeated use of elim) then completes the proof of this direction. To prove the converse direction set \u03d5n  x n x  0 x  1    x  n and we prove \u03d5n by metainduction over n. For the base case, x 0 x  0, we employ Claim 2. First note that 0 0 0  0 follows from (weakening). Next, (Q1) gives us that 0 , S(u) and so by (elim) and (cont), (u)(A(z, y, u) 0  S(u)) holds. By (Q5), it follows that A(z, S(y), 0), and thus (elim), (trans), (elim), and (Q8) entail that S(y) 0 S(y)  0. So Claim 2 delivers the desired result. Next, observe that thanks to (intro) and (weakening) we have \u03d5n1(0) and so if we prove \u03d5n1(S(y)) the claim follows using Claim 2. S(y) n  1 y n Claim 1 S(y) n  1 y  0 y  1 . . . y  n IH, (2), and (trans) y  0 y  1 . . . y  n S(y)  0 S(y)  1 . . . S(y)  n  1 repeated use of (prin), (elim), and (intro).",
      "\u03d5n1(S(y)) (2), (3), and (trans) (R5): Let us set \u03d5n  x n n x and we prove \u03d5n by metainduction over n. Clearly from (Q4) and (Q8) we get that 0 x and so by (intro) we obtain both the base case and also \u03d5n1(0). Thus again proving \u03d5n1(S(y)) completes the prof due to Claim 2. y n S(y) n  1 Claim 1 n y n  1 S(y) Claim 1 y n n y S(y) n  1 n  1 S(y) (2), (3), (elim), and (intro) \u03d5n1(S(y)) (3), IH, and (trans) (R6): Thanks to (R4) we know that x n is equivalent to a disjunction of crisp formulas and so it is crisp as a result of Proposition 5.13.. As before, it is obvious that the structure N can be interpreted as a model of Qin QSLw, henceQis consistent in QSLw. Thereforethe previoustheoremand Corollary 4.11., allows us to prove the following theorem, which can indeed be seen as a generalisation of H\u00e1jeks result in \ufb01rst order BL. Corollary 6.16. Qis essentially undecidable in QSLw. 7.",
      "Concluding remarks We have shown that the weak arithmetic theory Ris essentially undecidable against the background of the weak propositional logic L0 extended by minimal \ufb01rst-order axioms. The \ufb01rst upshot of this is that the cost of entry for essential undecidability is very low indeed  one needs only a fairly weak arithmetic theory and a fairly weak logic. Furthermore, we can show that Ris a weaker theory than even the very weak Qin the context of a slightly stronger (but still quite weak) logic. This extends and strengthens H\u00e1jeks result and suggests avenues of further investigation, perhaps using Smullyans representation systems, into the limits ofundecidability in mathematical theories against the background provided by weak logics. ZU064-05-FPR Final 23 June 2020 0:55 How Much Propositional Logic Suffices for Essential Undecidability? 8. Acknowledgments P. Cintula was supported by the project GA17-04630S of the Czech Science Foundation (GA\u010cR) and by RVO 67985807. A.",
      "Tedder was supported by the GA\u010cR project 18-19162Y. Thanks are due to an anonymous referee for helpful comments. This paper was presented at the MelbourneLogic Seminar, the conference Logic Colloquium in Prague, and the conference Services to Logic: 50 Years of the Logicians Liberation League in Mexico City. We are grateful to the audiences in all these venues. Finally, Albert Visser provided some useful comments on an earlier version of this work. BIBLIOGRAPHY L. B\u011bhounek, P. Cintula, and P. H\u00e1jek. Introduction to Mathematical Fuzzy Logic. Handbook of Mathematical Fuzzy Logic, Volume 1. eds. P. Cintula, P. H\u00e1jek, and C. Noguera. College Publications, 2011. N. Dershowitz and Z. Manna, Proving Termination with Multiset Orderings. Communications of the Association for Computing Machinery 22:465476, 1979. N. Galatos, P. Jipsen, T. Kowalski, and H. Ono.",
      "Residuated Lattices: An Algebraic Glimpse at Substructural Logics Volume 151 of Studies in Logic and the Foundations of Mathematics Elsevier, Amsterdam, 2007. A. Grzegorczyk. Computable Relations and the Essential Undecidability of a Very Weak Arithmetic of Natural Numbers. Unpublished Manuscript, 2006. P. H\u00e1jek and P. Pudl\u00e1k. Metamathematics of \ufb01rst-order arithmetic. Springer, 1993. P. H\u00e1jek. Mathematical fuzzy logic and natural numbers. Fundamenta Informaticae, 81:155163, 2007. J.P. Jones and J.C. Shepherdson. Variants of Robinsons Essentially Undecidable Theory RArchive for Mathematical Logik 23:6164, 1983. S.C. Kleene. A symmetric form of G\u00f6dels theorem. Indagationes Mathematicae 12:244 246, 1950. S.C. Kleene. Introduction to Metamathematics. North-Holland, 1952. P. Lindstr\u00f6m. Aspects of Incompleteness. Lecture Notes in Logic, Volume 10 Berlin: Springer-Verlag, 1997. J.D. Monk. Mathematical logic. Graduate Texts in Mathematics, no. 37.",
      "Springer-Verlag, New York, Heidelberg, and Berlin, 1976. H. Putnam and R.M. Smullyan. Exact separation of recursively enumerable sets within theories. Proceedings of the AMS 11(4):574577, 1960. R.M. Robinson. An Essentially Undecidable Axiom System Proceedings of the International Congress of Mathematics 1950, pp. 729730, 1950. R. Routley, R.K. Meyer, V. Plumwood, and R.T. Brady. Relevant Logics and Their Rivals 1. Ridgeview, 1982. B. Rosser. Extensions of some theorems of G\u00f6del and Church Journal of Symbolic Logic. 1:8791, 1936. R.M. Smullyan. Theory of Formal Systems Annals of Mathematics Studies 47. Princeton University Press, 1961. V. \u0160vejdar. An Interpretation of Robinson Arithmetic in its Grzegorczyks Weaker Variant Fundamenta Informaticae, 81(13):347354, 2007. V. \u0160vejdar. Weak theories and essential incompleteness. In M. Peli\u0161 ed., The Logica Yearbook 2007, pp. 213224, Philosophia Praha, 2008. A. Tarski. On essential undecidability Journal of Symbolic Logic. 14:7576, 1949. A.",
      "Tarski, A. Mostowski, and R.M. Robinson. Undecidable Theories. North-Holland, 1953. ZU064-05-FPR Final 23 June 2020 0:55 Badia, Cintula, H\u00e1jek, and Tedder R.L. Vaught. On a theorem of Cobham concerning undecidable theories. In E. Nagel, P. Suppes, and A. Tarski, editors, Logic, Methodology and Philosophy of Science. Proceedings of the 1960 International Congress, pp. 1425. Stanford University Press, Stanford, 1962. A. Visser. Why the theory R is special. In Neil Tennant (Eds.),Foundational Adventures - Essays in honour of Harvey M. Friedman, pp. 724, College Publications, 2014. SCHOOL OF HISTORICAL AND PHILOSOPHICAL INQUIRY UNIVERSITY OF QUEENSLAND ST LUCIA QLD 4072, AUSTRALIA E-mail: guillebadia89gmail.com THE INSTITUTE OF COMPUTER SCIENCE OF THE CZECH ACADEMY OF SCIENCES PRAGUE 8, 182 00, CZECH REPUBLIC E-mail: cintulacs.cas.cz E-mail: ajtedder.atgmail.com"
    ],
    "word_count": 8210,
    "page_count": 18
  },
  "HBAR": {
    "chunks": [
      "Knowledge Center  Hedera Skip to content Solutions Services  Products Token Service Consensus Service Smart Contract Service Asset Tokenization Studio Stablecoin Studio AI Studio Sustainability Studio (Guardian) HashSphere Support Use Cases Asset Tokenization Artificial Intelligence (AI) Sustainability Payments Decentralized Identity Decentralized Finance (DeFi) Consumer Engagement dApp Directory Developers Get Started Start Building Documentation Developer Portal Developer Tooling GitHub Funding Community Hackathons Ambassador Program Builders Program Moderator Program Bug Bounty Network Governance Ecosystem Network Activity Network Status Roadmap Open Source  Hiero Partner Program Resources Blog Case Studies Events Fee Calculator Journey Knowledge Center Public Policy Videos Start Building Contact Solutions Services  Products Token Service Consensus Service Smart Contract Service Asset Tokenization Studio Stablecoin Studio AI Studio Sustainability Studio (Guardian) HashSphere Support Use Cases Asset Tokenization Artificial Intelligence (AI) Sustainability Payments Decentralized Identity Decentralized Finance (DeFi) Consumer Engagement dApp Directory Developers Get Started Start Building Documentation Developer Portal Developer Tooling GitHub Funding Community Hackathons Ambassador Program Builders Program Moderator Program Bug Bounty Network Governance Ecosystem Network Activity Network Status Roadmap Open Source  Hiero Partner Program Resources Blog Case Studies Events Fee Calculator Journey Knowledge Center Public Policy Videos Start Building Contact Knowledge Center learn White papers and reports Hedera: A Public Hashgraph Network  Governing Council Read the original Hedera white paper by Dr.",
      "Leemon Baird and Mance Harmon with updates made in 2020. Read More The Swirlds Hashgraph Consensus Algorithm: Fair, Fast, Byzantine Fault Tolerance The original Swirlds hashgraph consensus algorithm paper describing a fair, fast, leaderless Byzantine fault tolerant protocol for replicated state machines. Learn More Hashgraph Consensus: Detailed Examples Step-by-step visual examples of the Swirlds hashgraph consensus algorithm, showing how transactions flow through the hashgraph and reach consensus order and timestamps. Learn More i-TiRE: Incremental Timed-Release Encryption or How to use Timed-Release Encryption on Blockchains? A construction for timed-release encryption on blockchains that enables sealed-bid auctions, scheduled confidential transactions, and other time-locked data use cases.",
      "Learn More Amortized Threshold Symmetric-key Encryption Improves threshold symmetric-key encryption so large datasets can be encrypted and decrypted efficiently by distributing computation and communication across servers. Learn More Cryptography with Weights: MPC, Encryption and Signatures Introduces cryptographic primitives that weight parties differently (not just one-person-one-vote), enabling MPC, encryption, and signatures that reflect stake, reputation, or other weights. Learn More hinTS: Threshold Signatures with Silent Setup Presents hinTS, a threshold signature scheme with a silent setup phase that improves efficiency and practicality for real-world deployments. Learn More Threshold Signatures in the Multiverse Defines threshold signatures over many overlapping signer sets (multiverse) with flexible weights and robust key management across different contexts.",
      "Learn More SublonK: Sublinear Prover PlonK A new zero-knowledge proof system compatible with PlonK that dramatically reduces prover time to sublinear in circuit size while retaining practical performance. Learn More HiSE: Hierarchical (Threshold) Symmetric-key Encryption Builds a hierarchical threshold symmetric-key encryption system (HiSE) on top of DiSE and ATSE for scalable, decentralized key management. Learn More Split Prover Zero-Knowledge SNARKs Introduces split-prover SNARKs where proving is shared between multiple provers, improving efficiency in settings like online transactions and privacy-preserving payments. Learn More Tokenization on Hedera Overview of tokenization using Hedera Token Service in both public and permissioned environments, including design patterns and use-cases. Learn More Hedera Consensus Service Explains the Hedera Consensus Service, its architecture, and how it provides verifiable ordering for application messages and transactions.",
      "Learn More Understanding Decentralization of Hedera: Decentralization of Consensus Analysis of decentralization in the Hedera network with a focus on consensus, governance, and how control is distributed. Learn More Data Privacy Compliance Using Hedera Consensus Service Shows how to use Hedera Consensus Service in privacy-sensitive applications while meeting data protection and regulatory compliance requirements. Learn More Architecting for Privacy and Data Protection on Hedera Guidance for designing dApps on Hedera that meet privacy and data protection requirements, including architecture patterns and best practices. Learn More The Hashgraph Protocol: Efficient ABFT for High-Throughput Distributed Ledgers Describes the hashgraph protocol as an efficient asynchronous BFT consensus algorithm suitable for high-throughput distributed ledgers.",
      "Learn More Hbar Economics: A deep dive into the dual role of hbars  detailed release schedule Deep dive on the economics of hbars, including supply, allocation, release schedule, and the dual role of hbars in the Hedera ecosystem. Learn More learn Featured content What is Hedera? Hedera is a fully open source public distributed ledger that utilizes the fast, fair, and secure hashgraph consensus. Its network services include Solidity-based smart contracts, as well as native tokenization and consensus services used to build decentralized applications. Learn More What is asynchronous Byzantine Fault Tolerance (ABFT)? Asynchronous Byzantine fault tolerance (ABFT) is a property of a consensus algorithm that allows the nodes on a network to agree on the order and timing of transactions. Learn More What are voting-based consensus algorithms?",
      "Voting-based consensus algorithms achieve consensus on transactions (and sometimes network decisions) by tallying the number of votes cast by nodes on a distributed ledger network. Learn More What is hashgraph consensus? The hashgraph consensus algorithm enables distributed consensus in an innovative, efficient way. Hashgraph is a distributed consensus algorithm and data structure that is fast, fair, and secure. Learn More What is gossip about gossip? Hashgraph utilizes the gossip protocol to send information between network nodes and come to consensus on transactions. Gossip about gossip is the history of how these events are related to each other, through their parent hashes, resulting in a directed acyclic graph (DAG) called a hashgraph. Learn More What are economy-based consensus algorithms? An economy-based consensus algorithm simulates an economy, where economic rationality drives the underlying consensus. Learn More What is the Hedera mirror network?",
      "The Hedera Mirror Network is a parallel network dedicated to propagating the state and transaction history of the Hedera Main Network. Learn More Open Source at Hedera Hedera made history in 2024 by becoming the first Layer 1 public ledger to donate its entire codebase to the Linux Foundation, reinforcing its commitment to transparent, community-driven development. Learn More Proof-of-work, and its flaws, explained The proof-of-work form of distributed ledger technology presents room for improvement in terms of fairness, security, expense and efficiency. Learn More Understanding leader-based consensus algorithms Systems that rely on leader-based consensus algorithms are vulnerable to problems of security and fairness. Learn More What are the strengths of the hashgraph virtual voting algorithm? The hashgraph virtual voting algorithm provides strong math proofs, excellent security and the highest speeds possible. Learn More Proof of Stake (PoS) vs.",
      "Proof of Work (PoW) A consensus algorithm is a method of synchronizing the data across a distributed system. Learn More learn Distributed ledger technology What are distributed ledger technologies (DLTs)? A distributed ledger is a database shared by multiple participants in which each participant maintains and updates a synchronized copy of the data. Distributed ledgers allow members to securely verify, execute, and record their own transactions without relying on an intermediary, such as a bank, broker, or auditor. Learn More What Is Cryptography: An Introduction to Cryptography in DLT Cryptography is the backbone of distributed ledger technologies like blockchain and other consensus-oriented distributed networks. If you are interested in building decentralized applications, it8217;s essential to understand wallet generation and transaction signing processes. Both of which rely heavily on underlying cryptographic protocols. Learn More What is a decentralized application (dApp)?",
      "Decentralized applications (dApps) are built using blockchain or distributed ledger technologies. dApps allow users to transparently complete transactions, verify claims, and collaborate without needing to trust a centralized intermediary. Learn More What are smart contracts? Smart contracts are digital contracts that define the terms of a transaction via computer code. They also verify, execute, and enforce that transaction without the need for a third-party intermediary. Learn More Layer 1 v. Layer 2 If you are learning about web3, cryptocurrency, or blockchain the chances are that you have heard different projects referred to as being a layer 1 or a layer 2 project. Hearing these terms without any prior knowledge or context can be confusing. This post will explain the differences between layer one and layer two networks and provide examples of both.",
      "Learn More Smart Legal Contracts Explained As more people begin to recognize the potential of smart contacts, the uses for them across distributed ledger technologies continue to grow. And as it grows, the legal systems in the United States and other countries have been trying to catch up. Since the term 8220;smart contract8221; implies a legally binding contract, it8217;s natural to wonder whether the term 8220;smart legal contracts8221; is redundant. There are differences, however. It8217;s time to look at the intersection of blockchains and common law. Learn More Smart Contract Advantages Smart contract advantages in areas like speed, transparency and security are helping to accelerate the use of this blockchain tool. Learn More Smart Contract vs. Traditional Contract In the debate of smart contract vs. traditional contract, it appears most likely that both will continue to evolve and be valuable. Learn More What Is the Ethereum Virtual Machine and how does it work?",
      "The EVM is a Turing complete virtual machine that can be accessed globally through a participating network node. The EVMs Turing completeness is measured by the fact it can run any program. Without the EVM, developers wouldnt be able to implement the numerous dApps (decentralized applications) for which Ethereum is known. Learn More How to Assess Smart Contract Platforms With DeFi growing exponentially, more businesses are looking at smart contract platforms as a base for new opportunities. Learn More What Is a Smart Contract Audit? A smart contract audit involves a detailed analysis of the contract8217;s code to identify security issues and incorrect and inefficient coding, and to determine ways to resolve the problems. The audit process is an important part of ensuring the security and reliability of blockchain applications. Learn More 10 Real-World Smart Contract Use Cases Smart contracts are digital programs stored on blockchain networks.",
      "When predetermined terms and conditions are met, these contracts are automatically executed . It8217;s a straightforward concept with huge implications. The number of smart contract use cases grows daily as more people see the tremendous potential of smart contract technology. Learn More How to Create a Smart Contract To give you a high-level overview of whats involved in smart contract development, well provide a breakdown of the basic steps of the process. We wont delve into the actual code, but well provide you with a detailed dive into how to get a smart contract written. Learn More Smart Contract Design Patterns Explained Smart contract design patterns are reliable coding segments used in new contracts because the coding has worked in other programs to do routine functions. Learn More A Guide to Smart Contract Security Smart contract security refers to the principles and practices used by developers, users, and exchanges when creating or interacting with smart contracts.",
      "Learn More How to Call a Smart Contract and Why Understanding how to call a smart contract is a big part of developing this revolutionary type of program and a great way to learn about them. Learn More Smart Contract Applications Smart contract applications keep growing as a range of industries explore the possibilities of using blockchain technology. Learn More What Is an NFT Smart Contract? An NFT smart contract is blockchain computer programming that manages and enhances digital assets, or non-fungible tokens. Learn More Smart Contract Challenges Plenty of smart contract challenges remain, but developers and technological advances are making progress. Learn More Testing Smart Contracts Testing smart contracts is a must. Here8217;s a look at some of the most important tests and popular tools used for this vital function. Learn More Types of Smart Contracts As the foundation types of smart contracts become more useful, their adoption by a wide range of industries keeps multiplying.",
      "Learn More Smart Contracts in Finance The growth of smart contracts in finance continues as new applications are found in tokenization, KYC, and insurance. Learn More Smart Contracts in Construction As developers find new uses for smart contracts in construction, leaders in that industry are using it to get ahead of competitors. Learn More Smart Contracts in Healthcare The use of smart contracts in healthcare fields continues to grow. Here8217;s a look at some specific, new applications. Learn More Directed Acyclic Graphs Learn how directed acyclic graphs offer a scalable, secure, and eco-friendly alternative to blockchain. Learn More Smart Contracts Real Estate Smart contracts real estate is gaining momentum as new ways are developed for investing in property and streamlining traditional functions. Learn More Blockchain Transactions What happens during blockchain transactions? Learn the basics about this essential part of cryptocurrency.",
      "Learn More DAG vs Blockchain Understanding the difference between DAG vs blockchain is essential for keeping up with the evolving distributed ledger landscape. Learn More How Web3 Projects are Using Smart Contracts to Change the Web Web3 projects are driving innovation in the digital economy and capturing the attention of users and venture capital investment. Learn More What Is Data Integrity and How Can It Be Protected? It8217;s easy to define what data integrity is, but it takes rigorous application of the right tools to make sure data stays accurate and safe. Learn More How Decentralized Identity Systems Help Us Protect Our Data Decentralized identity systems let users manage their data and send only what they are comfortable sharing to centralized websites. Learn More Zero-knowledge Proof: Don't Say the Secret Word Zero-knowledge proofs refer to cryptographic mechanisms by which an actor is able to prove knowledge of some fact without disclosing that fact.",
      "Learn More Blockchain and Edge Computing: A Powerful Data Duo Blockchain and edge computing can be used together to improve network efficiency and fuel dynamic, transformative, real-time applications. Learn More How to Take Advantage of Blockchain for IoT Security Blockchain offers a promising solution to the growing security needs of both businesses and consumers. By providing a secure and transparent framework, blockchain can help to shore up many vulnerabilities in IoT systems. Learn More Blockchain Scalability Solutions Blockchain scalability is being improved in a variety of ways to make decentralized networks more efficient and practical for mainstream use. Learn More Blockchain Anonymity Explore blockchain anonymity, from cryptography to privacy coins, and discover how technologies like zero-knowledge proofs enhance privacy.",
      "Learn More Blockchain and Digital Transformation Discover how blockchain drives digital transformation across industries with innovative solutions to improve security, transparency, and efficiency Learn More learn Industry insights What is decentralized finance (DeFi)? Decentralized finance (DeFi) is a way to conduct business using distributed ledger technology for cryptocurrency and alternatives to traditional methods. Learn More What is a cryptocurrency exchange? Cryptocurrencies have erupted into a 1.50 trillion industry, sparking a wave of global disruption with blockchain technologies. Like traditional stock and commodities exchanges, cryptocurrency exchanges are platforms by which users can buy and sell digital assets. Learn More What is a stablecoin? Stablecoins are an attempt to create a cryptocurrency token with a stable pricetheir stability commonly achieved by pegging the token to an asset such as gold or fiat.",
      "By being backed by more traditional investments, the market has greater confidence in their price. Learn More What is a security token offering (STO)? A security token is a unique token issued on a permissioned or permissionless blockchain, representing a stake in an external asset or enterprise. Entities like government and businesses can issue security tokens that serve the same purpose as stocks, bonds, and other equities. Learn More What is a central bank digital currency (CBDC)? CBDC is short for Central Bank Digital Currency  it8217;s an electronic form of central bank money that citizens can use to make digital payments and store value. A CBDC is a digital currency, it8217;s issued by a central bank, and is universally accessible. Learn More Custodial vs non-custodial wallets Learn the differences between custodial and non-custodial cryptocurrency wallets and why every web3 developer needs one. This concept is vital for users of the decentralized web.",
      "Learn More What is asset tokenization? Asset tokenization is the process by which an issuer creates digital tokens on a blockchain or other form of distributed ledger to represent digital or physical assets. Blockchain technology guarantees that once you buy tokens representing an asset, no single authority can erase or change your ownership, making it entirely immutable. Learn More What is a non-fungible token (NFT)? A non-fungible token is a type of cryptographic token unique in its attributes typically programmed into the NFTs issuing smart contract or are part of the initial native configuration of the NFT before issuance. Learn More Understanding the Basics of a Decentralized Autonomous Organization (DAO) A decentralized autonomous organization or DAO is a member-owned group that operates without centralized leadership using blockchain technology.",
      "Learn More The Rise of DeFi Lending and How to Get Involved DeFi lending enables users to select which money market they want to lend to and earn interest from, depending on the markets yield. Learn More The Ins and Outs of Decentralized Exchanges (DEXs) A decentralized exchange (DEX) uses smart contracts to enable cryptocurrency traders to execute trades without an intermediary. Learn More The Importance of DeFi Liquidity in Cryptocurrency DeFi liquidity is the ability for tokens, or cryptocurrency, to be swapped for other tokens. Without it, there is no decentralized finance. Learn More Blockchain Oracle: How Smart Contracts Talk to the World A blockchain oracle conveys information to and from smart contracts to enable them to perform their self-executing functions. Learn More A Beginner's Guide to DeFi Yield Farming DeFi yield farming is becoming one of the most popular ways to earn passive income with cryptocurrency, but learn about the risks before diving in.",
      "Learn More Understanding DeFi Prediction Markets DeFi Prediction markets create additional earning opportunities for crypto traders and act as a predictive tool for organizations. Learn More DeFi Insurance: The Next Generation of Insurance DeFi insurance can refer to blockchain-based replacements of traditional policies or insurance that covers blockchain-related activity. Learn More Deciphering the World of DeFi Derivatives DeFi derivatives bring together the volatile worlds of derivatives trading and cryptocurrency to allow for cutting-edge investment and speculation. Learn More NFT Collectibles FAQs FAQs on NFT collectibles covers the who, what, when, where and how regarding the market in non-fungible tokens. Learn More Carbon Credits: What Are They and Do They Work? Carbon credits offer one way to try to mitigate the effects of climate change.",
      "Here8217;s a look at how they work Learn More How the World Is Using Blockchain for Energy Efficiency The use of blockchain for energy efficiency is growing all the time. Here are some prime examples of how it works. Learn More Carbon Markets: A Guide to Understanding this Global Tool Carbon markets enable companies and individuals to help control greenhouse gas emissions through financial incentives. Learn More Is DeFi Safe? Learn and Use These Security Measures The qualities that make decentralized finance valuable also can increase its risks. Learn how to use it in the safest manner possible. Learn More DeFi Stack: Getting a Grip on the DeFi Ecosystem It8217;s important to know as much as possible about decentralized apps before diving in. Learning about the DeFi stack is a good place to start. Learn More NFT Games: How to Jump In and Start Playing NFT games allow players to have fun in new worlds and new ways, and you can even make real money by playing.",
      "Learn More Defi Risks: Watch Out for These Trouble Spots DeFi risks come from all directions. Here8217;s a look at 5 principal threats to decentralized finance and the people who use it. Learn More Types of Tokenization for Commerce, NLP, and Blockchain Learn about the types of tokenization that relate to security devices, finance, blockchain networks, and natural language processing. Learn More How to Transfer an NFT Securely Understanding how to transfer an NFT will enable you to move them between your wallets and trade them with others. Learn More How to Sell an NFT: A Comprehensive Guide Learn how to sell an NFT with this step-by-step approach that covers both promotional and technical aspects. Learn More How to Mint an NFT Learn how to mint an NFT so you can get started with your first NFT collection. Learn More How to Start a DAO: From Community to Code The NFT DeFi world is evolving as developers find new ways to merge their individual strengths in powerful ways.",
      "Learn More NFT DeFi Applications Are Gaining Power The NFT DeFi world is evolving as developers find new ways to merge their individual strengths in powerful ways. Learn More What is the Metaverse? The Vision Is Gaining Focus We will look at the origins of the metaverse, how it can benefit businesses and individuals, and the hurdles it must overcome. Learn More Institutional Defi: A Guide to the Next Wave in Finance In this guide, well cover the value, design, and future of Institutional DeFi. Well also go over use cases of companies at the forefront of integrating decentralized finance into their mission. Learn More Web3 Gaming: Taking Stock and Looking to the Future Web3 gaming continues to build on the advantages of blockchain, NFTs and decentralization to change the way we view, and interact with, video games. Learn More NFT Rarity and How to Figure Out a Rating Each NFT has its own value. And rarity is part of that value.",
      "Learn how to use online tools to determine rarity or calculate it yourself. Learn More NFT Use Cases: Creative Ideas for a Revolutionary Tool Learn about 11 NFT use cases from the practical (supply chains and digital identity) to the profound (art, climate protection and heritage preservation.) Learn More NFT Royalties: Who Is Winning the Cat-and-Mouse Game? NFT royalties support creators and encourage new ones, but high royalties can dampen collectors8217; enthusiasm and participation. Learn More CBDCs: What They Are and How They Work Many central banks have expressed interest in using a digital form of fiat currency known as a CBDC  central bank digital currency. Learn More NFT Terms: A Crypto Glossary NFT enthusiasts have come up with hundreds of terms, many of which may end up in the national lexicon. This list will help you get ahead of the curve. Learn More TradFi and DeFi: Key Differences Many people see TradFi and DeFi as being at odds, with one eventually prevailing.",
      "However, they can exist side by side and even be synergistic. Learn More What Is Fintech, and What Does the Future Hold? Financial technology has become widely accepted, and many businesses and individuals prefer to handle transactions digitally. Learn More Fintech Regulation: Balancing Innovation and Consumer Protection Fintech regulation sometimes struggles to provide consumer protection, data privacy, and stability without hampering innovation. Learn More Fiat vs Crypto: A Comprehensive Comparison Fiat currencies are legal tender controlled by governments. Cryptocurrencies are digital assets that use blockchain technology. Learn More The Multi-Faceted Role of Blockchain in Finance The role of blockchain in finance continues to evolve and grow. Here8217;s a look at its many use cases and benefits. Learn More Fintech vs DeFi: The Evolution of Financial Services DeFi is a subset of fintech, but as time passes, it8217;s an increasingly large subset.",
      "Find out how decentralized finance interacts with fintech. Learn More Asset Tokenization: Improving Data Protection Discover how tokenization strengthens data protection, offering unmatched security and compliance in digital asset management. Learn More How AI integration is helping to solve tokenization challenges Discover how AI integration can support tokenization by boosting efficiency and security, leading to future innovations in digital asset management. Learn More Boosting Supply Chain Resilience Through Tokenization Discover how tokenization boosts supply chain resilience through transparency and efficiency, addressing logistical challenges with blockchain innovation. Learn More Real Estate Tokenization: Transforming Property Investment Learn how real estate tokenization enhances liquidity, security, and efficiency, reshaping investment accessibility and market trends.",
      "Learn More Audits and standards Hedera Transaction Tool Code review During the month of October 2024, Hedera Hashgraph engaged NCC Group to conduct a security code audit of Hedera Transaction Tool. Code Review was performed on the commit id a08e595924d80558258a2c2c2df8906a80e93ef0 of the project hosted on https:github.comhashgraphhedera-transaction-tool. Download Hedera Token Service Connector Hashgraph engaged Halborn to conduct a security assessment on wHBAR8217;s HTS connector smart contracts beginning on April 24th, 2025 and ending on April 24th, 2025. The security assessment was scoped to the HTS connector smart contracts provided to the Halborn team. Download Wrapped HBAR Smart Contract Project Hashgraph engaged Halborn to conduct a security assessment on their wHBAR smart contracts beginning on January 20th, 2025 and ending on January 21st, 2025. The security assessment was scoped to the wHBAR smart contract (WHBAR.sol) provided to Halborn.",
      "Download Load More Carbon offsets Carbon Offsets for Q3 2025 Hedera network operation emissions in Q2 2025 were 10 metric tonnes of CO2. Hedera purchased offsets for 10 metric tonnes of CO2 to offset network emissions in Q3 2025. According to the EPA8217;s greenhouse gas equivalency calculator, 10 metric tonnes of CO2 emitted by the Hedera network was equivalent to 2.3 gasoline-powered passenger vehicles being driven for one year. Download Carbon Offsets for Q2 2025 Hedera network operation emissions in Q1 2025 were 10 metric tonnes of CO2. Hedera purchased offsets for 10 metric tonnes of CO2 to offset network emissions in Q2 2025. According to the EPA8217;s greenhouse gas equivalency calculator, 10 metric tonnes of CO2 emitted by the Hedera network was equivalent to 2.3 gasoline-powered passenger vehicles being driven for one year. Download Carbon Offsets for Q1 2025 Hedera network operation emissions in Q4 2024 were 9 metric tonnes of CO2.",
      "Hedera purchased offsets for 9 metric tonnes of CO2 to offset network emissions in Q1 2025. According to the EPA8217;s greenhouse gas equivalency calculator, 9 metric tonnes of CO2 emitted by the Hedera network was equivalent to 2.1 gasoline-powered passenger vehicles being driven for one year. Download Load More Ready to get started? Discover why Hedera is the trusted institutional-grade network powering the new digital economy. Start Building Contact hedera highlights Subscribe to get the latest Hedera news, updates, and insights delivered right to your inbox. Unsubscribe anytime. funding governance brand book careers faqs support terms  conditions privacy policy  2018-2026 Hedera Hashgraph, LLC. All trademarks and company names are the property of their respective owners. All rights in Hedera Council member, Deutsche Telekom mark, are protected by Deutsche Telekom AG. All rights reserved. Hedera uses the third party marks with permission."
    ],
    "word_count": 4393,
    "page_count": 1
  },
  "ICP": {
    "chunks": [
      "The Internet Computer for Geeks (v1.4) The DFINITY Team December 18, 2023 Abstract Smart contracts are a new form of software that will revolutionize how software is written, IT systems are maintained, and applications and whole businesses are built. Smart contracts are composable and autonomous pieces of software that run on decen- tralized blockchains, which makes them tamperproof and unstoppable. In this paper, we describe the Internet Computer (IC), which is a radical new design of blockchain that unleashes the full potential of smart contracts, overcoming the limitations of smart contracts on traditional blockchains with respect to speed, storage costs, and compu- tational capacity. This allows smart contracts for the \ufb01rst time to implement fully decentralized applications that are hosted end to end on blockchain. The IC consists of a set of cryptographic protocols that connects independently operated nodes into a collection of blockchains.",
      "These blockchains host and execute canisters, the ICs form of smart contracts. Canisters can store data, perform very general computations on that data, and provide a complete technology stack, serving web pages directly to end users. Computational and storage costs are covered by a reverse-gas model, where canister developers pre-pay costs in cycles that are obtained from ICP, the native token of the IC. ICP tokens are also used for governance: the IC is governed by a decentralized autonomous organization, or DAO, which, among other things, determines changes to the topology of the network and upgrades to the protocol. Introduction Unleashing smart contracts Because of their unique features, smart contracts are the key enabler of Web3, the new approach to the web where applications are fully controlled by their users and run on decentralized blockchain platforms.",
      "Such decentralized applications (dapps) are typically tokenized, meaning tokens are distributed to users as rewards for participating in the dapps. Participation can come in many di\ufb00erent forms, ranging from moderating and providing content to governing a dapp and to creating and maintaining a dapp. Usually, tokens can also be bought on exchanges; indeed, selling tokens is a common way to \ufb01nance dapp development. Finally, tokens are also used as a form of payment for the services or contents a dapp o\ufb00ers. Smart contracts running on todays blockchain platforms, including all the https:dfinity.orgfoundation; contact author: Yvonne-Anne Pignolet, yapdfinity.org. popular ones (such as Ethereum), su\ufb00er from many limitations, such as high transaction and storage costs, slow computational speed, and the inability to serve frontends to users.",
      "As a result, many popular blockchain applications are not fully decentralized but are hybrids where most of the application is hosted on traditional cloud platforms and call out to smart contracts on a blockchain for a small part of their overall functionality. Unfortunately, this renders such applications non-decentralized, and opens them to many of the drawbacks of traditional cloud-hosted applications, such as being at the mercy of cloud providers, and being vulnerable to many single points of failure. The Internet Computer (IC) is a new platform for executing smart contracts. Here, we use the term smart contract in a very broad sense: a general-purpose, tam- perproof computer program whose execution is performed autonomously on a decentralized public network. By general purpose, we mean that the class of smart contract programs is Turing complete (i.e., anything computable can be computed by a smart contract).",
      "By tamperproof, we mean that the instructions of the program are carried out faithfully and that intermediate and \ufb01nal results are accurately stored andor transmitted. By autonomous, we mean that a smart contract is executed automatically by the network, without the need for any action on the part of any individual. By a decentralized public network, we mean a network of computers that is publicly accessible, geographically distributed, and not under the control of a small number of individuals or organizations. In addition, smart contracts  are composable, meaning that they may interact with one another, and  support tokenization, meaning that they may use and trade digital tokens.",
      "Compared to existing smart contract platforms, the IC is designed to:  be more cost e\ufb00ective, in particular, allowing applications to compute and store data at a fraction of the cost of previous platforms;  provide higher throughput and lower latency for processing smart contract transac- tions;  be more scalable, in particular, the IC can process unbounded volumes of smart con- tract data and computation natively because it can grow in capacity by adding more nodes to the network. Another property that smart contracts may have is immutability, which means that, once deployed, the code of a smart contract cannot be changed by a party unilaterally. While this feature is essential in some applications, it is not required in all applications, and can also be problematic if a smart contract has a bug that needs to be \ufb01xed. The IC allows a range of mutability policies for smart contracts, ranging from purely immutable to unilaterally upgradable, with other options in between.",
      "In addition to providing a smart contract platform, the IC is designed to act as a complete technology stack, such that systems and services can be built that run entirely on the IC. In particular, smart contracts on the IC can service HTTP requests created by end users, so that smart contracts can directly serve interactive web experiences. This means that systems and services can be created without relying on corporate cloud hosting services or private servers, thus providing all of the bene\ufb01ts of smart contracts in a true end-to-end fashion. Realizing the vision of Web3. For end-users, accessing IC-based services is largely transparent. Their personal data is more secure than when accessing applications on a public or private cloud, but the experience of interacting with the application is the same.",
      "For the people creating and managing those IC-based services, however, the IC elimi- nates many of the costs, risks, and complexities associated with developing and deploying modern applications and microservices. For example, the IC platform provides an alterna- tive to the consolidation driven by large technology companies that are monopolizing the Internet. In addition, its secure protocol guarantees reliable message delivery, transparent accountability, and resilience without relying on \ufb01rewalls, backup facilities, load balancing services, or failover orchestration. Building the IC is about restoring the Internet to its open, innovative, and creative roots  in other words, to realize the vision of Web3. To focus on a few speci\ufb01c examples, the IC does the following:  Supports interoperability, shared functions, permanent APIs, and ownerless applica- tions, all of which reduce platform risk and encourages innovation and collaboration.",
      "Persists data automatically in memory, which eliminates the need for database servers and storage management, improves computational e\ufb03ciency, and simpli\ufb01es software development. Simpli\ufb01es the technology stack that IT organizations need to integrate and manage, which improves operational e\ufb03ciency High level view of the Internet Computer To a \ufb01rst approximation, the IC is a network of interacting replicated state machines. The notion of a replicated state machine is a fairly standard concept in distributed systems Sch90, but we give a brief introduction here, beginning with the notion of a state machine. A state machine is a particular model of computation. Such a machine maintains a state, which corresponds to main memory or other forms of data storage in an ordinary computer. Such a machine executes in discrete rounds: in each round, it takes an input, applies a state transition function to the input and the current state, obtaining an output and a new state.",
      "The new state becomes the current state in the next round. The state transition function of the IC is a universal function, meaning that some of the inputs and data stored in the state may be arbitrary programs which act on other inputs and data. Thus, such a state machine represents a general (i.e., Turing complete) model of computation. To achieve fault tolerance, the state machine may be replicated. A replicated state machine comprises a subnet of replicas, each of which is running a copy of the same state machine. A subnet should continue to function  and to function correctly  even if some replicas are faulty. It is essential that each replica in a subnet processes the same inputs in the same order. To achieve this, the replicas in a subnet must run a consensus protocol Fis83, which ensures that all replicas in a subnet process inputs in the same order.",
      "Therefore, the internal state of each replica will evolve over time in exactly the same way, and each replica will produce exactly the same sequence of outputs. Note that an input to a replicated state machine on the IC may be an input generated by an external user, or an output generated by another replicated state machine. Similarly, an output of a replicated state machine may be either an output directed to an external user, or an input to another replicated state machine. Fault Models In the distributed systems area of computer science, one typically considers two types of replica failures: crash faults and Byzantine faults. A crash fault occurs when a replica abruptly stops and does not resume. Byzantine faults are failures in which a replica may deviate in an arbitrary way from its prescribed protocol. Moreover, with Byzantine faults, one or possibly several replicas may be directly under the control of a malicious adversary who may coordinate the behavior of these replicas.",
      "Of the two types of faults, Byzantine faults are potentially far more disruptive. Protocols for consensus and for realizing replicated state machines typically make as- sumptions about how many replicas may be faulty and to what degree (crash or Byzan- tine) they may be faulty. In the IC, the assumption is that if a given subnet has n replicas, then less than n3 of these replicas are faulty and these faults may be Byzantine. (Note that the di\ufb00erent subnets in the IC may have di\ufb00erent sizes.) Communication Models Protocols for consensus and for implementing replicated state machines also typically make assumptions about the communication model, which characterizes the ability of an ad- versary to delay the delivery of messages between replicas. At opposite ends of the spectrum, we have the following models:  In the synchronous model, there exists some known \ufb01nite time bound \u03b4, such that for any message sent, it will be delivered in less than time \u03b4.",
      "In the asynchronous model, for any message sent, the adversary can delay its delivery by any \ufb01nite amount of time, so that there is no bound on the time to deliver a message. Since the replicas in an IC-subnet are typically distributed around the globe, the syn- chronous communication model would be highly unrealistic. Indeed, an attacker could compromise the correct behavior of the protocol by delaying honest replicas or the commu- nication between them. Such an attack is generally easier to mount than gaining control over and corrupting an honest replica. In the setting of a globally distributed subnet, the most realistic and robust model is the asynchronous model. Unfortunately, there are no known consensus protocols in this model that are truly practical (more recent asynchronous consensus protocols, as in MXC16, attain reasonable throughput, but not very good latency).",
      "So like most other practical Byzantine fault tolerant systems that do not rely on synchronous communication (e.g., CL99, BKM18, YMR18), the IC opts for a compromise: a partial synchrony communication model DLS88. Such partial synchrony models can be formulated in various ways. The partial synchrony assumption used by the IC says, roughly speaking, that for each subnet, communication among replicas in that subnet is periodically synchronous for short intervals of time; moreover, the synchrony bound \u03b4 does not need to be known in advance. This partial synchrony assumption is only needed to ensure that the consensus protocol makes progress (the so-called liveness property). The partial synchrony assumption is not needed to ensure correct behavior of consensus (the so-called safety property), nor is it needed anywhere else in the IC protocol stack. Under the assumption of partial synchrony and Byzantine faults, it is known that our bound of f  n3 on the number of faults is optimal.",
      "Permission Models The earliest protocols for consensus (e.g., PBFT CL99) were permissioned, in the sense that the replicas comprising a replicated state machine are governed by a centralized or- ganization, which determines which entities provide replicas, the topology of the network, and possibly also implements some kind of centralized public-key infrastructure. Permis- sioned consensus protocols are typically the most e\ufb03cient, and while they do avoid a single point of failure, the centralized governance is undesirable for certain applications, and it is antithetical to the spirit of the burgeoning Web3 era. More recently, we have seen the rise of permissionless consensus protocols, such as Bitcoin Nak08, Ethereum But13, and Algorand GHM17. Such protocols are based on a blockchain and either a proof of work (PoW) (e.g., Bitcoin, Ethereum prior to v2.0) or a proof of stake (PoS) (e.g., Algorand, Ethereum v2.0) .",
      "While such protocols are completely decentralized, they are much less e\ufb03cient than permissioned protocols. We also point out that, as observed in PSS17, PoW-based consensus protocols such as Bitcoin cannot guarantee correctness (i.e., safety) in an asynchronous communication network. The ICs permission model is a hybrid model, obtaining the e\ufb03ciency of a permissioned protocol while o\ufb00ering many of the bene\ufb01ts of a decentralized PoS protocol. This hybrid model is called a DAO-controlled network and (roughly speaking) works as follows: each subnet runs a permissioned consensus protocol, but a decentralized autonomous organization (DAO) determines which entities provide replicas, con\ufb01gures the topology of the network, provides a public-key infrastructure, and controls which version of the protocol is deployed to the replicas.",
      "The ICs DAO is called the network nervous system (NNS), and is based on a PoS, so that all decisions taken by the NNS are made by community members whose voting power is determined by how much of the ICs native governance token they have staked in the NNS (see Section 1.8 for more on this token). Through this PoS-based governance system, new subnets can be created, replicas may be added to or removed from existing subnets, software updates may be deployed, and other modi\ufb01cations to the IC may be e\ufb00ected. The NNS is itself a replicated state machine, which (like any other) runs on a particular subnet whose membership is determined via the same PoS-based governance system. The NNS maintains a database called the registry, which keeps track of the topology of the IC: which replicas belong to which subnets, the public keys of the replicas, and so on (see Section 1.10 for a few more details on the NNS).",
      "Thus, one sees that the ICs DAO-controlled network allows the IC to achieve many of the practical bene\ufb01ts of a permissioned network (in terms of more e\ufb03cient consensus), while maintaining many of the bene\ufb01ts of a decentralized network (with governance controlled by a DAO). The replicas running the IC protocol are hosted on servers in geographically distributed, independently operated data centers. This also bolsters the security and decentralized nature of the IC. Chain-key cryptography The ICs consensus protocol does, in fact, use a blockchain, but it also uses public-key cryptography, speci\ufb01cally, digital signatures: the registry maintained by the NNS is used to bind public keys to replicas and subnets as a whole. This enables a unique and pow- erful collection of technologies that we call chain-key cryptography, which has several components.",
      "1.6.1 Threshold signatures The \ufb01rst component of chain-key cryptography is threshold signatures: this is a well es- tablished cryptographic technique that allows a subnet to have a public signature-veri\ufb01cation key whose corresponding secret signing key is split into shares, which are distributed among all of the replicas in a subnet in such a way that the shares held by the corrupt replicas do not let them forge any signatures, while the shares held by the honest replicas allow the subnet to generate signatures consistent with the policies and protocols of the IC. One critical application of these threshold signatures is that an individual output of one subnet may be veri\ufb01ed by another subnet or external user by simply validating a digital signature with respect to the public signature- veri\ufb01cation key of the (\ufb01rst) subnet.",
      "Note that the public signature-veri\ufb01cation key for a subnet may be obtained from the NNS  this public signature-veri\ufb01cation key remains constant over the lifetime of a subnet (even as the membership of a subnet may change over that lifetime). This stands in contrast to many non-scalable blockchain-based protocols, which require the entire blockchain to be validated in order to validate any single output. As we will see, these threshold signatures have a number of other applications within the IC. One such application is to give each replica in a subnet access to unpredictable pseudorandom bits (derived from such signatures). This is the basis for the Random Beacon used in consensus and the Random Tape used in execution.",
      "In order to securely deploy threshold signatures, the IC uses an innovative distributed key generation (DKG) protocol that constructs a public signature-veri\ufb01cation key and provisions each replica with a share of the corresponding secret signing key, and works within our fault and communication model. 1.6.2 Chain-evolution technology Chain-key cryptography also includes a sophisticated collection of technologies for robustly and securely maintaining a blockchain based replicated state machine over time, which together form what we call chain-evolution technology. Each subnet operates in epochs of many rounds (typically on the order of a few hundreds of rounds).",
      "Using threshold signatures and a number of other techniques, chain-evolution technology implements many essential maintenance activities that are executed periodically with a cadence that is tied to epochs: Garbage collection: At the end of each epoch, all inputs that have been processed, and all consensus-level protocol messages needed to order those inputs, may safely be purged from the memory of each replica. This is essential in keeping the storage requirements for the replicas from growing without bound. This is in contrast to many non-scalable blockchain-based protocols, where the entire blockchain from the genesis block must be stored. Fast forwarding: If a replica in a subnet falls very far behind its peers (because it is down or disconnected from the network for a long time), or a new replica is added to a subnet, it can be fast forwarded to the beginning of the most recent epoch, without having to run the consensus protocol and process all of the inputs up to that point.",
      "This is in contrast to many non-scalable blockchain-based protocols, where the entire blockchain from the genesis block must be processed. Subnet membership changes: The membership of the subnet (as determined by the NNS, see Section 1.5) may change over time. This can only happen at an epoch boundary, and needs to be done with care to ensure consistent and correct behavior. Pro-active resharing of secrets: We mentioned above in Section 1.6.1 how the IC uses chain-key cryptography  speci\ufb01cally, threshold signatures  for output veri\ufb01cation. This is based on secret sharing, which avoids any single point of failure by splitting up a secret (in this case, a secret signing key) into shares that are stored among the replicas. At the beginning of each epoch, these shares are pro-actively reshared.",
      "This achieves two goals:  When the membership of a subnet changes, the resharing will ensure that any new members have an appropriate share of the secret, while any replicas that are no longer members no longer have a share of the secret. If a small number of shares are leaked to an attacker in any one epoch, or even in every epoch, those shares will not do an attacker any good. Protocol upgrades: When the IC protocol itself needs to be upgraded, to \ufb01x bugs or add new features, this can be done automatically using a special protocol at the beginning of an epoch. Execution Models As already mentioned, replicated state machines in the IC can execute arbitrary programs. The basic computational unit in the IC is called a canister, which is roughly the same as the notion of a process, in that it comprises both a program and its state (which changes over time).",
      "Canister programs are encoded in WebAssembly, or Wasm for short, which is a binary instruction format for a stack-based virtual machine. Wasm is an open standard.1 While it was initially designed to enable high-performance applications on web pages, it is actually very well suited to general-purpose computation. The IC provides a run-time environment for executing Wasm programs in a canister, and to communicate with other canisters and external users (via message passing). While, in principle, one can write a canister program in any language that may be compiled to Wasm, a language called Motoko has been designed that is well aligned with the operational se- mantics of the IC. Motoko is a strongly typed, actor-based2 programming language with built-in support for orthogonal persistence3 and asynchronous message passing. Orthogo- nal persistence simply means that all memory maintained by a canister is automatically persisted (i.e., it does not have to be written to a \ufb01le).",
      "Motoko has a number of productiv- ity and safety features, including automatic memory management, generics, type inference, pattern matching, and both arbitrary and \ufb01xed-precision arithmetic. In addition to Motoko, the IC also provides a messaging interface de\ufb01nition language and wire format called Candid, for typed, high-level, and cross-language interoperability. This allows any two canisters, even if written in di\ufb00erent high-level languages, to easily communicate with one another. To fully support canister development in any given programming language, besides a Wasm compiler for that language, certain run-time support must also be provided. At the present time, in addition to Motoko, the IC also fully supports canister development in the Rust programming language. Utility token The IC makes use of a utility token called ICP.",
      "This token is used for the following functions: Staking in the NNS: As already discussed in Section 1.5, ICP tokens may be staked in the NNS to acquire voting rights so as to participate in the DAO that controls the IC network. Users that have ICP tokens staked in the NNS and who participate in 1See https:webassembly.org. 2See https:en.wikipedia.orgwikiActor_model. 3See transparent_persistence. the NNS governance also receive newly minted ICP tokens as a voting reward. The amount of the award is determined by policies established and enforced by the NNS. Conversion to Cycles: ICP is used to pay for the usage of the IC. More speci\ufb01cally, ICP tokens can be converted to cycles (i.e., burned), and these cycles are used to pay for creating canisters (see Section 1.7) and for the resources that canisters use (storage, CPU, and bandwidth). The rate at which ICP is converted to cycles is determined by the NNS.",
      "Payment to Node Providers: ICP tokens are used to pay the node providersthese are the entities that own and operate the computing nodes that host the replicas that make up the IC. At regular intervals (currently monthly), the NNS decides on the number of newly minted tokens that each node provider should receive, and sends the tokens to the node providers account. Payment of tokens is conditioned on providing reliable service to the IC, according to speci\ufb01c policies established and enforced by the NNS. Boundary Nodes Boundary nodes provide the network edge services of the IC. In particular, they o\ufb00er  clearly de\ufb01ned entry points to the IC,  denial of service protection for the IC,  seamless access to the IC from legacy clients (e.g., web browsers).",
      "To facilitate seamless access to the IC from a legacy client, boundary nodes provide func- tionality to translate a standard HTTPS request from a user to an ingress message directed toward a canister on the IC, and then route this ingress message to speci\ufb01c replicas on the subnet where this canister resides. Furthermore, boundary nodes o\ufb00er additional services to improve the user experience: caching, load balancing, rate limiting, and the ability for legacy clients to authenticate responses from the IC. A canister is identi\ufb01ed by a URL on the ic0.app domain. Initially, a legacy client looks up the corresponding DNS record for the URL, obtains an IP address of a boundary node, and then sends an initial HTTPS request to this address. The boundary node returns a javascript-based service worker that will be executed in the legacy client. After this, all interactions between the legacy client and the boundary node will be done via this service worker.",
      "One of the essential tasks carried out by the service worker is to authenticate responses from the IC using chain-key cryptography (see Section 1.6). To do this, the public veri\ufb01ca- tion key for the NNS is hard-coded in the service worker. The boundary node itself is responsible for routing requests to a replica on the subnet on which the speci\ufb01ed canister is hosted. The information needed to perform this routing is obtained by the boundary node from the NNS. The boundary node keeps a list with replicas that provide timely replies and selects a random replica from this list. All communication between legacy clients and boundary nodes and between boundary nodes and replicas is secured via TLS.4 In addition to legacy clients, it is also possible to interact with boundary nodes using IC native clients, which already include the service-worker logic, and do not need to retrieve the service worker program from the boundary node.",
      "Just as for replicas, the deployment and con\ufb01guration of boundary nodes is controlled by the NNS. 1.10 More details of the NNS As already mentioned in Section 1.5, the network nervous system (NNS) is an algorithmic governance system that controls the IC. It is realized by a set of canisters on a special sys- tem subnet. This subnet is like any other subnet, but is con\ufb01gured somewhat di\ufb00erently (as one example, canisters on the system subnet are not charged for the cycles they use). Some of the most relevant NNS canisters are  the registry canister, which stores the con\ufb01guration of the IC, i.e., which repli- cas belong to which subnet, the public keys associated with subnets and individual replicas, and so on. the governance canister, which manages the decision making and voting on how the IC should be evolved, and  the ledger canister, which keeps track of the users ICP utility token accounts and the transactions between them.",
      "1.10.1 Decision making on the NNS Anyone can participate in NNS governance by staking ICP tokens in so-called neurons. Neuron holders can then suggest and vote on proposals, which are suggestions on how the IC should be changed, e.g., how the subnet topology or the protocol should be changed. The neurons voting power for decision making is based on proof of stake. Intuitively, neurons with more staked ICP tokens have more voting power. However, the voting power also depends on some other neuron characteristics, e.g., more voting power is given to neuron holders that are committed to keep their tokens staked for a longer period of time. Each proposal has a determined voting period. A proposal is adopted if at the voting periods end, a simple majority of the total voting power has voted in favor of the proposal and these Yes-votes constitute a given quorum (currently 3) of the total voting power. Otherwise, the proposal is rejected.",
      "In addition, a proposal is adopted or rejected at any point if an absolute majority (more than half of the total voting power) is in favor or against the proposal, respectively. If a proposal is adopted, the governance canister automatically executes the decision. For example, if a proposal suggests changing the network topology and is adopted, the governance canister automatically updates the registry with the new con\ufb01gurations. 4See https:en.wikipedia.orgwikiTransport_Layer_Security. Figure 1: The layers of the Internet Computer Protocol 1.11 Advanced Features The architecture of the IC enables functionality to bring the advantages of tokenized DAO governance to any dapp running on the IC as well as direct interaction with other chains. DAO-controlled canisters. Just like the overall con\ufb01guration of the IC is controlled by the NNS, any canister may also be controlled by its own DAO, called the service nervous system (SNS).",
      "The DAO controlling a canister can control updates to the canister logic, as well as issuing privileged commands to be carried out by the canister. Threshold ECDSA. ECDSA signatures JMV01 are used in cryptocurrencies, such as Bitcoin and Ethereum, as well as in many other applications. While threshold signa- tures are already an essential ingredient in the IC, these are not threshold ECDSA signatures. This feature allows individual canisters to control an ECDSA signing key, which is securely distributed among all of the replicas on the subnet hosting the canister. Bitcoin and Ethereum integration. Building on the new threshold ECDSA feature, this feature allows canisters to interact with the Bitcoin and Ethereum blockchains, including the ability to sign transactions. HTTPs outcalls. This feature allows canisters to send HTTPs requests arbitrary web pages (external to the IC) and process the response after it has been agreed upon by consensus.",
      "Architecture overview As illustrated in Figure 1, the Internet Computer Protocol consists of four layers:  peer-to-peer layer (see Section 4);  consensus layer (see Section 5);  routing layer (see Section 6);  execution layer (see Section 7). Chain-key cryptography impacts several layers, and is discussed in detail in Sections 3 (threshold signatures) and 8 (chain-evolution technology). Peer-to-peer layer The peer-to-peer layers task is to transport protocol messages between the replicas in a subnet. These protocol messages consist of  messages used to implement consensus,  input messages generated by an external user. Basically, the service provided by the peer-to-peer is a best e\ufb00ort broadcast channel: if an honest replica broadcasts a message, then that message will eventually be received by all honest replicas in the subnet. Design goals include the following:  Bounded resources. All algorithms must work with bounded resources (memory, bandwidth, CPU). Prioritization.",
      "Di\ufb00erent messages may be treated with di\ufb00erent priorities, depend- ing on certain attributes (e.g., type, size, round), and these priorities may change over time. E\ufb03ciency. High throughput is more important than low latency. DOSSPAM resilience. Corrupt replicas should not prevent honest replicas from communicating with one another. Consensus layer The job of the consensus layer of the IC is to order inputs so that all replicas in a subnet will process such inputs in the same order. There are many protocols in the literature for this problem. The IC uses a new consensus protocol, which is described here at a high level. Any secure consensus protocol should guarantee two properties, which (roughly stated) are:  safety: all replicas in fact agree on the same ordering of inputs, and  liveness: all replicas should make steady progress. The IC consensus protocol is designed to be  extremely simple, and  robust: performance degrades gracefully when some replicas are malicious.",
      "As discussed above, we assume f  n3 faulty (i.e., Byzantine) replicas. Also, liveness holds under a partial synchrony assumption, while safety is guaranteed, even in a completely asynchronous network. Like a number of consensus protocols, the IC consensus protocol is based on a blockchain. As the protocol progresses, a tree of blocks is grown, starting from a special genesis block that is the root of the tree. Each non-genesis block in the tree contains (among other things) a payload, consisting of a sequence of inputs, and a hash of the blocks parent in the tree. The honest replicas have a consistent view of this tree: while each replica may have a di\ufb00erent, partial view of this tree, all the replicas have a view of the same tree. In addition, as the protocol progresses, there is always a path of \ufb01nalized blocks in this tree.",
      "Again, the honest replicas have a consistent view of this path: while each replica may have a di\ufb00erent, partial view of this path, all the replicas have a view of the same path. The inputs in the payloads of the blocks along this path are the ordered inputs that will be processed by the execution layer of the Internet Computer. The protocol proceeds in rounds. In round h of the protocol, one or more blocks of height h are added to the tree. That is, the blocks added in round h are always at a distance of exactly h from the root. In each round, a pseudo-random process is used to assign each replica a unique rank, which is an integer in the range 0, . . . , n 1. This pseudo-random process is implemented using a Random Beacon (this makes use of threshold signatures, mentioned above in Section 1.6.1 and discussed in more detail in Section 3). The replica of lowest rank is the leader of that round.",
      "When the leader is honest and the network is synchronous, the leader will propose a block, which will be added to the tree; moreover, this will be the only block added to the tree in this round and it will extend the \ufb01nalized path. If the leader is not honest or the network is not synchronous, some other replicas of higher rank may also propose blocks, and also have their blocks added to the tree. In any case, the logic of the protocol gives highest priority to the leaders proposed block and some block or blocks will be added to this tree in this round. Even if the protocol proceeds for a few rounds without extending the \ufb01nalized path, the height of the tree will continue to grow with each round, so that when the \ufb01nalized path is extended in round h, the \ufb01nalized path will be of length h. A consequence of this, even if the latency occasionally increases because of faulty replicas or unexpectedly high network latency, the throughput of the protocol remains essentially constant.",
      "The consensus protocol relies on digital signatures to authenticate messages sent between replicas. To implement the protocol, each replica is associated with a public veri\ufb01cation key for a signature scheme. The association of replicas to public keys is obtained from the registry maintained by the NNS. Message routing As discussed in Section 1.7, basic computational unit in the IC is called a canister. The IC provides a run-time environment for executing programs in a canister, and to communicate with other canisters and external users (via message passing). The consensus layer bundles inputs into payloads, which get placed into blocks, and as blocks are \ufb01nalized, the corresponding payloads are delivered to the message routing layer, then processed by the execution environment, which updates the state of the canisters on the replicated state machine and generates outputs, and these outputs are processed by the message routing layer.",
      "It is useful to distinguish between two types of inputs: ingress messages: these are messages from external users; cross-subnet messages: these are messages from canisters on other subnets. We can also distinguish between two types of outputs: ingress message responses: these are responses to ingress messages (which may be re- trieved by external users); cross-subnet messages: these are messages to canisters on other subnets. Upon receiving a payload from consensus, the inputs in that payload are placed into various input queues. For each canister C running on a subnet, there are several input queues  there is one queue speci\ufb01cally for ingress messages to C, and each other canister C, with whom C communicates, gets its own queue.",
      "(In the case where C is not located on the same subnet as C, these are cross-subnet messages.) In each round, the execution layer will consume some of the inputs in these queues, update the replicated state of the relevant canisters, and place outputs in various output queues. For each canister C running on a subnet, there are several output queues  each other canister C, with whom C communicates, gets its own queue. (In the case where C is not located on the same subnet as C, these are cross-subnet messages.) The message routing layer will take the messages in these output queues and place them into subnet-to-subnet streams to be processed by a crossnet transfer protocol, whose job it is to actually transport these messages to other subnets. In addition to these output queues, there is also an ingress history data structure. Once an ingress message has been processed by a canister, a response to that ingress message will be recorded in this data structure.",
      "At that point, the external user who provided the ingress message will be able to retrieve the corresponding response. (Note that ingress history does not maintain the full history of all ingress messages.) Note that the replicated state comprises the state of the canisters, as well as system state, including the above-mentioned queues and streams, as well as the ingress history data structure. Thus, both the message routing and execution layers are involved in updating and maintaining the replicated state of a subnet. It is essential that all of this state is updated in a completely deterministic fashion, so that all replicas maintain exactly the same state.",
      "Also note that the consensus layer is decoupled from the message routing and execution layers, in the sense that any forks in the consensus blockchain are resolved before their payloads are passed to message routing, and in fact, consensus does not have to keep in lock step with message routing and consensus and is allowed to run a bit ahead. 2.3.1 Per-round certi\ufb01ed state In each round, some of the state of a subnet will be certi\ufb01ed. The per-round certi\ufb01ed state is certi\ufb01ed using chain-key cryptography. Among other things, the certi\ufb01ed state in a given round consists of  cross-subnet messages that were recently added to the subnet-to-subnet streams;  other metadata, including the ingress history data structure. The per-round certi\ufb01ed state is certi\ufb01ed using a threshold signature (see Section 1.6.1). Per-round certi\ufb01ed state is used in several ways in the IC:  Output authentication. Cross-subnet messages and responses to ingress messages are authenticated using per-round certi\ufb01ed state.",
      "Preventing and detecting non-determinism. Consensus guarantees that each replica processes inputs in the same order. Since each replica processes these inputs determin- istically, each replica should obtain the same state. However, the IC is designed with an extra layer of robustness to prevent and detect any (accidental) non-deterministic computation, should it arise. The per-round certi\ufb01ed state is one of the mechanisms used to do this. Coordination with consensus. The per-round certi\ufb01ed state is also used to coordinate the execution and consensus layers, in two di\ufb00erent ways:  If consensus is running ahead of execution (whose progress is determined by the last round whose state is certi\ufb01ed), consensus will be throttled. Inputs to consensus must pass certain validity checks, and these validity checks may depend on certi\ufb01ed state, which all replicas must agree upon.",
      "2.3.2 Query calls vs update calls As we have described it so far, ingress messages must pass through consensus so that they are processed in the same order by all replicas on a subnet. However, an important optimization is available to those ingress messages whose processing does not modify the replicated state of a subnet. These are called query calls  as opposed to other ingress messages, which are called update calls. Query calls are allowed to perform computations which read and possibly update the state of a canister, but any updates to the state of a canister are never committed to the replicated state. As such, a query call may be processed directly by a single replica without passing through consensus, which greatly reduces the latency for obtaining a response from a query call. In general, a response to a query call is not recorded in the ingress history data structure, and therefore cannot be authenticated using the per-round certi\ufb01ed state mechanism as described above.",
      "However, the IC makes it possible for canisters to store data (while processing update calls) in special certi\ufb01ed variables, which can be authenticated by this mechanism; as such, query calls that return as their value a certi\ufb01ed variable can still be authenticated. 2.3.3 External user authentication One of the main di\ufb00erences between an ingress message and a cross-subnet message is the mechanism used for authenticating these messages. While chain-key cryptography is used to authenticate cross-subnet messages, a di\ufb00erent mechanism is used to authenticate ingress messages from external users. There is no central registry for external users. Rather, an external user identi\ufb01es himself to a canister using a user identi\ufb01er (aka principal), which is a hash of a public signature- veri\ufb01cation key. The user holds a corresponding secret signing key, which is used to sign ingress messages. Such a signature, as well as the corresponding public key, is sent along with the ingress message.",
      "The IC automatically authenticates the signature and passes the user identi\ufb01er to the appropriate canister. The canister may then authorize the requested operation, based on the user identi\ufb01er and other parameters to the operation speci\ufb01ed in the ingress message. First-time users generate a key pair and derive their user identi\ufb01er from the public key during their \ufb01rst interaction with the IC. Returning users are authenticated using the secret key that is stored by the user agent. A user may associate several key pairs with a single user identity, using signature delegation. This is useful, as it allows a single user to access the IC from several devices using the same user identity. Execution layer The execution layer processes one input at a time. This input is taken from one of the input queues, and is directed to one canister.",
      "Based on this input and the state of the canister, the execution environment updates the state of the canister, and additionally may add messages to output queues and update the ingress history (possibly with a response to an earlier ingress message). Each subnet has access to a distributed pseudorandom generator (PRG). Pseudo- random bits are derived from a seed that itself is a threshold signature called the Random Tape (see Section 1.6.1 and more detail in Section 3). There is a di\ufb00erent Random Tape for each round of the consensus protocol. The basic properties of the random tape are: 1. Before a block at height h is \ufb01nalized by any honest replica, the Random Tape at height h  1 is guaranteed to be unpredictable. 2. By the time a block at height h  1 is \ufb01nalized by any honest replica, that replica will typically have all the shares it needs to construct the Random Tape at height h  1.",
      "To obtain pseudorandom bits, a subnet must make a request for these bits via a system call from the execution layer in some round, say h. The system will then respond to that request later, using the Random Tape at height h1. By property (1) above, it is guaranteed that the requested pseudorandom bits are unpredictable at the time the request is made. Consensus will actually deliver to message routing both the Random Tape and payload at h  1 at the same time; by property (2) above, this will typically not incur any additional delay. Putting it all together We trace through the typical \ufb02ow to process a user request on the IC. Query call 1. A users query call M to a canister C is sent by the users client to a boundary node (see Section 1.9), and the boundary node sends M to a replica on the subnet that hosts canister C. After receiving M, this replica will compute the response and send it back to the user via the boundary node. Update call 1.",
      "A users request M to a canister C is sent by the users client to a boundary node (see Section 1.9), and the boundary node sends M to a replica on the subnet that hosts canister C. 2. After receiving M, this replica will broadcast M to all other replicas on the subnet, using the peer-to-peer layer (see Section 2.1). 3. Having received M, the leader for the next round of consensus (see Section 2.2) will bundle M with other inputs to form the payload for a block B that the leader proposes. 4. Some time later, block B is \ufb01nalized and the payload is sent to the message routing layer (see Section 2.3) for processing. Note that the peer-to-peer layer is also used by consensus to \ufb01nalize this block. 5. The message routing layer will place M in the input queue of the canister C. 6. Some time later, the execution layer (see Section 2.4) will process M, updating the internal state of the canister C.",
      "In some situations, the canister C will be able to immediately compute a response R to the request M. In this case, R is placed in the ingress history data structure. In other situations, processing the request M may require making a request to another canister. In this example, let us suppose that to process this particular request M, the canister C must make a request M to another canister C that resides on another subnet. This second request M will be placed in the output queue of C, and then the following steps are performed. 7. Some time later, message routing will move M into an appropriate cross-subnet stream, and this will eventually be transported to the subnet hosting C. 8. On the second subnet, the request M will be obtained from the \ufb01rst subnet, and eventually pass through consensus and message routing on the second subnet and then be processed by execution. The execution layer will update the internal state of canister C and generate a response R to the request M.",
      "The response R will go in the output queue of canister C, and eventually be placed in a cross-subnet stream and transported back to the \ufb01rst subnet. 9. Back on the \ufb01rst subnet, the response R will be obtained from the second subnet, and eventually pass through consensus and message routing on the \ufb01rst subnet and then be processed by execution. The execution layer will update the internal state of canister C and generate a response R to the original request message M. This response will be recorded in the ingress history data structure. Regardless of which execution path is taken, the response R to request M will eventually be recorded in the ingress history data structure on the subnet that hosts canister C. To obtain this response, the users client must perform a kind of query call (see Section 2.3.2). As discussed in Section 2.3.1, this response will be authenticated via chain-key cryptography (speci\ufb01cally, using a threshold signature).",
      "The authentication logic itself (i.e., threshold signature veri\ufb01cation) will be performed by the client using the service worker originally obtained by the client from the boundary node. Chain-key crytography I: threshold signatures A critical component of the ICs chain-key cryptography is a threshold signature scheme Des87. The IC uses threshold signatures for a number of purposes. Let n be the number of replicas in a subnet and let f be a bound on the number of corrupt replicas. The Consensus Layer makes use of an (f  1)-out-of-n threshold signature to realize a random beacon (see Section 5.5). The Execution Layer makes use of an (f  1)-out-of-n threshold signature to realize a random tape, which is used to provide unpredictable pseudorandom numbers to canisters (see Section 7.1). The Execution Layer makes use of an (n f)-out-of-n threshold signature to certify the replicated state.",
      "This is used both to authenticate the outputs of a subnet (see Section 6.1) and to implement the fast-forwarding feature of the ICs chain-evolution technology (see Section 8.2). For the \ufb01rst two applications (the random beacon and random tape), it is essential that the threshold signatures are unique, i.e., for a given public key and message, there is only one valid signature. This is required as we use the signature as a seed to a pseudorandom generator, and all replicas who compute such a threshold signature must agree on the same seed. Threshold BLS signatures We implement threshold signatures based on the BLS signature scheme BLS01, which is trivial to adapt to the threshold setting. The ordinary (i.e., non-threshold) BLS signature scheme makes use of two groups, G and G, both of prime order q. We assume that G is generated by g G and G is generated by g G. We also assume a hash function HG that maps its inputs to G (and which is modeled as a random oracle).",
      "The secret signing key is an element x Zq and the public veri\ufb01cation key is V : gx G. In the non-threshold setting, to sign a message m, the signer computes h HG(m)  G and then computes the signature \u03c3 : (h)x G. To verify that such a signature is valid, one must test if logh \u03c3  logg V . To be able to perform this test e\ufb03ciently, the BLS scheme uses the notion of a pairing on the groups G and G, which is an algebraic operation that is available when G and G are elliptic curves of a special type. We shall not be able to go into the details of pairings and elliptic curves here. See BLS01 for more details. BLS signatures have the nice property (mentioned above) that signatures are unique. In the t-out-of-n threshold setting, we have n replicas, any t of which may be used to generate a signature on a message. In somewhat more detail, each replica Pj holds a share xj Zq of the secret signing key x Zq, which is privately held by Pj, while the group element Vj : gxj is publicly available.",
      "The shares (x1, . . . , xn) are a t-out-of-n secret-sharing of x (see Section 3.4). Given a message m, replica Pj can generate a signature share \u03c3j : (h)xj G, where h : HG(m) as before. To verify that such a signature share is valid, one must test if logh \u03c3j  logg Vj. Computing the discrete logarithms is intractable, but this can be checked using a pairing  in fact, this is exactly the same as the validity test for an ordinary BLS signature with public key Vj. This scheme satis\ufb01es the following reconstruction property: Given any collection of t valid signature shares \u03c3j on a message m (contributed by distinct replicas), we can e\ufb03ciently compute a valid BLS signature \u03c3 on m under the public veri\ufb01cation key. In fact, \u03c3 can be computed as where the \u03bbjs can be e\ufb03ciently computed just from the indices of the t con- tributing replicas.",
      "Under reasonable intractability assumptions for G, and modeling HG as a random oracle, this scheme satis\ufb01es the following security property: Assume that at most f replicas may be corrupted by an adversary. Then it is infeasible for the adversary to compute a valid signature on a message unless it obtains signature shares on that message from at least t f honest replicas. Distributed key distribution To implement threshold BLS, we need a way to distribute the shares of the secret signing key to the replicas. One way to do this would be to have a trusted party compute all of these shares directly and distribute them to all the replicas. Unfortunately, this would create a single point of failure. Instead, we use a distributed key generation (DKG) protocol, which allows the replicas to essentially carry out the logic of such a trusted party using a secure distributed protocol. We sketch the high level ideas of the protocol currently implemented.",
      "We refer the reader to Gro21 for more details. The DKG protocol used is essentially non-interactive. It uses two essential ingredients:  a publicly veri\ufb01able secret sharing (PVSS) scheme, and  a consensus protocol. Although any consensus protocol could be used, not surprisingly, the one we use is that in Section 5 (see also Section 8). Assumptions The basic assumptions made are the same as outlined in Section 1:  asynchronous communication, and  f  n3. We only indirectly make use of a partial synchrony assumption (as in Section 5.1) to ensure that the consensus protocol attains liveness. We also assume that for a t-out-of-n threshold signature scheme, we have f  t n f, which (among other things) ensures that (1) the corrupt replicas cannot sign all by them- selves, and (2) the honest replicas can sign all by themselves. We also assume that every replica is associated with some public keys, where each replica also holds the corresponding private key.",
      "One public key is the signing key (the same one as in Section 5.4). Another public key is a public encryption key for a speci\ufb01c public-key encryption scheme needed to implement the PVSS scheme (details follow). PVSS scheme Let G be the group of prime order q generated by g G introduced above. Let s Zq be a secret. Recall that a t-out-of-n Shamir secret-sharing of s is a vector (s1, . . . , sn) Zn where sj : a(j) (j  1, . . . , n). a(x) : a0  a1x      at1xt1 Zqx is a polynomial of degree less than t with a0 : s. The key properties of such a secret sharing are  from any collection of t of the sjs, we can e\ufb03ciently compute (via polynomial inter- polation) the secret s  a0  a(0), and  if a1, . . . , at1 are chosen uniformly and independently over Zq, then any collection of fewer than t of the sjs reveals no information about the secret s.",
      "At a high level, a PVSS scheme allows one replica, Pi, called the dealer, to take such a sharing, and compute an object called a dealing, which contains  a vector of group elements (A0, . . . , At1), where Ak : gak for k  0, . . . , t 1,  a vector of ciphertexts (c1, . . . , cn), where cj is the encryption of sj under Pjs public encryption key,  a non-interactive zero-knowledge proof \u03c0 that each cj does indeed encrypt such a share  more precisely, that each cj decrypts the value sj satisfying gsj  k  ga(j). We note that to establish the overall security of our DKG protocol, the PVSS scheme must provide an appropriate level of chosen ciphertext security. Speci\ufb01cally, the dealer must embed its identity as associated data in the dealing, and the encrypted shares must remain hidden, even under a chosen ciphertext attack wherein an adversary is allowed to decrypt arbitrary dealings which are decrypted under associated data that is distinct from the associated data used to create the dealing.",
      "It is easy to realize a PVSS scheme, if one is not too concerned about e\ufb03ciency. The idea is to use an ElGamal-like encryption scheme to encrypt each sj bit by bit, and then use a standard non-interactive zero-knowledge proof for the relation (2), which would be based on a standard application of the Fiat-Shamir transform (see FS86) to an appropriate Sigma protocol (see CDS94). While this yields a polynomial-time scheme, it is not that practical. However, there are many possible ways to optimize this type of scheme. See Gro21 for the details on the highly optimized PVSS scheme used in the IC. The basic DKG protocol Using the PVSS scheme and a consensus protocol, the basic DKG protocol is very simple. 1. Each replica broadcasts a signed dealing of a random secret to all other replicas. Such a signed dealing includes a dealing, along with the identity of the dealer and a signature on the dealing under the dealers public signing key.",
      "Such a signed dealing is called valid if it has the right syntactic form, and the signature and non-interactive zero knowledge proof are valid. 2. Using consensus the replicas agree on a set S of f  1 valid signed dealings (from distinct dealers). 3. Suppose that the ith dealing in the set S contains the vector of group elements (Ai,0, . . . , Ai,t1) and the vector of ciphertexts (ci,1, . . . , ci,n). Then the public veri\ufb01cation key for the threshold signature scheme is V : Ai,0. Note that the secret signing key is implicitly de\ufb01ned as x : logg V. Pjs share of the secret signing key x is xj : si,j, where si,j is the decryption of ci,j under Pjs secret decryption key. The public veri\ufb01cation key for replica Pj is Vj : i,k  gxj. Note that the shares xj comprise a t-out-of-n Shamir secret-sharing of x. As such, the \u03bbj values appearing in (1) are just Lagrange interpolation coe\ufb03cients. This establishes the reconstruction property stated in Section 3.1.",
      "As for the security property stated in Section 3.1, this can be proved to hold modeling HG as a random oracle, and assuming that the PVSS scheme is secure, and that the groups G and G (with a pairing) satisfy a certain type of one-more Di\ufb03e-Hellman hardness assumption, which can be stated as saying that no e\ufb03cient adversary can win the following game with non-negligible probability: The challenger chooses \u00b51, . . . , \u00b5k Zq and \u03bd1, . . . , \u03bd\u2113Zq at random, and gives g\u00b5ik i1 and (g)\u03bdj\u2113 j1 to the adversary. The adversary makes a sequence of queries to the challenger, each of which is a vector of the form \u03bai,ji,j, to which the challenger responds with (g)\u00b5i\u03bdj \u03bai,j. To end the game, the adversary outputs a vector \u03bbi,ji,j and a group element h G, and wins the game if h  (g)\u00b5i\u03bdj \u03bbi,j and the output vector \u03bbi,ji,j is not a linear combination of the query vectors.",
      "While this type of one-more Di\ufb03e-Hellman assumption is needed in the case where t  f  1, one can get by with a weaker assumption when t  f  1 (the so-called co-CDH assumption, on which the security of the ordinary BLS scheme is based). A resharing protocol The basic DKG protocol can be easily modi\ufb01ed so that instead of creating a sharing of a fresh random secret x, it instead creates a fresh, random sharing of a previously shared secret. Step 1 of the basic protocol is modi\ufb01ed so that each replica broadcasts a signed dealing of its existing share. Step 2 is modi\ufb01ed so that a set of t valid signed dealings is agreed upon. Also, each dealing is veri\ufb01ed to ensure that it is indeed a dealing of the appropriate existing share (this means that the value of Ai,0 in the ith dealing should be equal to the old value of Vi). In Step 3, the computation of the new xj (and Vj) values weight the sum (and product) on i Lagrange interpolation coe\ufb03cients.",
      "Peer-to-peer layer The peer-to-peer layers task is to transport protocol messages between the replicas in a subnet. These protocol messages consist of  messages used to implement consensus, e.g., block proposals, notarizations, etc. (see Section 5);  ingress messages (see Section 6). Basically, the service provided by the peer-to-peer is a best e\ufb00ort broadcast channel: if an honest replica broadcasts a message, then that message will eventually be received by all honest replicas in the subnet. Design goals include the following:  Bounded resources. All algorithms must work with bounded resources (memory, bandwidth, CPU). Prioritization. Di\ufb00erent messages may be treated with di\ufb00erent priorities, depend- ing on certain attributes (e.g., type, size, round), and these priorities may change over time. E\ufb03ciency. High throughput is more important than low latency. DOSSPAM resilience. Corrupt replicas should not prevent honest replicas from communicating with one another.",
      "Observe that in the consensus protocol, some messages, notably block proposals (which can be quite large), will be rebroadcast by all replicas. This is necessary to ensure correct behavior of that protocol. However, if implemented naively, this would be a huge waste of resources. To avoid having all replicas broadcasting the same message, the peer-to-peer layer makes use of an advertise-request-deliver mechanism. Instead of broadcasting a (large) message directly, it will instead broadcast a (small) advertisement for the message: if a replica receives such an advertisement for a message, which it has not already received, and deems the message to be important, it will request that the message is delivered. This strategy decreases bandwidth utilization at the cost of higher latency. For small messages, this trade-o\ufb00is not worthwhile, and it makes more sense to just send the message directly, rather than an advertisement.",
      "For relatively small subnets, a replica that wishes to broadcast a message will send an advertisement to all replicas in the subnet, each of which may then request that the message is delivered. For larger subnets, this advertise-request-deliver mechanism may operate over an overlay network. An overlay network is a connected, undirected graph whose vertices comprise the replicas in a subnet. Two replicas are peers if there is an edge connecting them in this graph, and a replica only communicates with its peers. So when a replica wishes to broadcast a message, it sends an advertisement for that message to its peers. Those peers may request that the message be delivered, and upon receiving the message, if certain conditions are met, those peers will advertise the message to their peers. This is essentially a gossip network. This strategy again decreases bandwidth utilization at the cost of even higher latency.",
      "Consensus Layer The job of the consensus layer of the IC is to order inputs so that all replicas in a subnet will process such inputs in the same order. There are many protocols in the literature for this problem. The IC uses a new consensus protocol, which is described here at a high level. For more details, see the paper CDH21 (in particular, Protocol ICC1 in that paper). Any secure consensus protocol should guarantee two properties, which (roughly stated) are:  safety: all replicas in fact agree on the same ordering of inputs, and  liveness: all replicas should make steady progress. The paper CDH21 proves that the IC consensus protocol satis\ufb01es both of these prop- erties. The IC consensus protocol is designed to be  extremely simple, and  robust: performance degrades gracefully when some replicas are malicious. Assumptions As discussed in the introduction, we assume  a subnet of n replicas, and  at most f  n3 of the replicas are faulty.",
      "Faulty replicas may exhibit arbitrary, malicious (i.e., Byzantine) behavior. We assume that communication is asynchronous, with no a priori bound on the delay of messages sent between replicas. In fact, the scheduling of message delivery may be completely under adversarial control. The IC consensus protocol guarantees safety under this very weak communication assumption. However, to guarantee liveness, we need to assume a form of partial synchrony, which (roughly stated) says that the network will be periodically synchronous for short intervals of time. In such intervals of synchrony, all undelivered messages will be delivered in less than time \u03b4, for some \ufb01xed bound \u03b4. The bound \u03b4 does not have to be known in advance (the protocol is initialized with a reasonable bound, but will dynamically adapt and increase this bound if it is too small).",
      "Regardless of whether we are assuming an asynchronous or a partially synchronous network, we assume that every message sent from one honest replica to another will eventually be delivered. Protocol overview Like a number of consensus protocols, the IC consensus protocol is based on a blockchain. As the protocol progresses, a tree of blocks is grown, starting from a special genesis block that is the root of the tree. Each non-genesis block in the tree contains (among other things) a payload, consisting of a sequence of inputs, and a hash of the blocks parent in the tree. The honest replicas have a consistent view of this tree: while each replica may have a di\ufb00erent, partial view of this tree, all the replicas have a view of the same tree. In addition, as the protocol progresses, there is always a path of \ufb01nalized blocks in this tree.",
      "Again, the honest replicas have a consistent view of this path: while each replica may have a di\ufb00erent, partial view of this path, all the replicas have a view of the same path. The inputs in the payloads of the blocks along this path are the ordered inputs that will be processed by the execution layer of the Internet Computer (see Section 7). The protocol proceeds in rounds. In round h of the protocol, one or more blocks of height h are added to the tree. That is, the blocks added in round h are always at a distance of exactly h from the root. In each round, a pseudo-random process is used to assign each replica a unique rank, which is an integer in the range 0, . . . , n 1. This pseudo-random process is implemented using a random beacon (see Section 5.5 below). The replica of lowest rank is the leader of that round.",
      "When the leader is honest and the network is synchronous, the leader will propose a block, which will be added to the tree; moreover, this will be the only block added to the tree in this round and it will extend the \ufb01nalized path. If the leader is not honest or the network is not synchronous, some other replicas of higher rank may also propose blocks, and also have their blocks added to the tree. In any case, the logic of the protocol gives highest priority to the leaders proposed block and some block or blocks will be added to this tree in this round. Even if the protocol proceeds for a few rounds without extending the \ufb01nalized path, the height of the tree will continue to grow with each round, so that when the \ufb01nalized path is extended in round h, the \ufb01nalized path will be of length h. A consequence of this, even if the latency occasionally increases because of faulty replicas or unexpectedly high network latency, the throughput of the protocol remains essentially constant.",
      "Additional properties An additional property enjoyed by the IC consensus protocol (just like PBFT CL99 and HotStu\ufb00YMR18, and unlike others, such as Tendermint BKM18) is optimistic respon- siveness PS18, which means that when the leader is honest, the protocol may proceed at the pace of the actual network delay, rather than some upper bound on the network delay. We note that the simple design of the IC consensus protocol also ensures that its per- formance degrades quite gracefully when and if Byzantine failures actually do occur. As pointed out in CWA09, much of the recent work on consensus has focused so much on improving the performance in the optimistic case where there are no failures, that the resulting protocols are dangerously fragile, and may become practically unusable when failures do occur. For example, CWA09 show that the throughput of existing implemen- tations of PBFT drops to zero under certain types of (quite simple) Byzantine behavior.",
      "The paper CWA09 advocates for robust consensus, in which peak performance under optimal conditions is partially sacri\ufb01ced in order to ensure reasonable performance when some parties actually are corrupt (but still assuming the network is synchronous). The IC consensus protocols is indeed robust in the sense of CWA09: in any round where the leader is corrupt (which itself happens with probability less than 13), the protocol will e\ufb00ectively allow another party to take over as leader for that round, with very little fuss, to move the protocol forward to the next round in a timely fashion. Public keys To implement the protocol, each replica is associated with a public key for the BLS signature scheme BLS01, and each replica also holds the corresponding secret signing key. association of replicas to public keys is obtained from the registry maintained by the NNS (see Section 1.5). These BLS signatures will be used to authenticate messages sent by replicas.",
      "The protocol also uses the signature aggregation feature of BLS signatures BGLS03, which allows many signatures on the same message to be aggregated into a compact multi- signature. The protocol will use these multi-signatures for notarizations (see Section 5.7) and \ufb01nalizations (see Section 5.8), which are aggregations of nf signatures on messages of a certain form. Random Beacon In addition to BLS signatures and multi-signatures as discussed above, the protocol makes use of a BLS threshold signature scheme to implement the above-mentioned random beacon. The random beacon for height h is a (f  1)-threshold signature on a message unique to height h. In each round of the protocol, each replica broadcasts its share of the beacon for the next round, so that when the next round begins, all replicas should have enough shares to reconstruct the beacon for that round.",
      "As discussed above, the random beacon at height h is used to assign a pseudo-random rank to each replica that will be used in round h of the protocol. Because of the security properties of the threshold signature, an adversary will not be able to predict the ranking of the replicas more than one round in advance, and these rankings will e\ufb00ectively be as good as random. See Section 3 for more on BLS threshold signatures. Block making Each replica may at di\ufb00erent points in time play the role of a block maker. As a block maker in round h, the replica proposes a block B of height h that is to be a child of some block B of height h 1 in the tree of blocks. To do this, the block maker \ufb01rst gathers together a payload consisting of all inputs it knows about (but not including those already included in payloads in blocks in the path through the tree ending at B). The block B consists of  the payload,  the hash of B,  the rank of the block maker,  the height h of the block.",
      "After forming the block B, the block maker forms a block proposal, consisting of  the block B,  the block makers identity, and  the block makers signature on B. A block maker will broadcast its block proposal to all other replicas. Notarization A block is e\ufb00ectively added to the tree of blocks when it becomes notarized. For a block to become notarized, n f distinct replicas must support its notarization. Given a proposed block B at height h, a replica will determine if the proposal is valid, which means that B has the syntactic form described above. In particular, B should contain the hash of a block B of height h that is already in the tree of blocks (i.e., already notarized). In addition, the payload of B must satisfy certain conditions (in particular, all of the inputs in the payload must satisfy various constraints, but these constraints are generally independent of the consensus protocol).",
      "Also, the rank of the block maker (as recorded in the block B) must match the rank assigned in round h by the random beacon to the replica that proposed the block (as recorded in the block proposal) . If the block is valid and certain other constraints hold, the replica will support the notarization of the block by broadcasting a notarization share for B, consisting of  the hash of B,  the height h of B,  the identity of the supporting replica, and  the supporting replicas signature on a message comprising the hash of B and the height h. Any set of n f notarization shares on B may be aggregated together to form a nota- rization for B, consisting of  the hash of B,  the height h of B,  the set of identities of the n f supporting replicas,  an aggregation of the n f signatures on the message comprising the hash of B and the height h.",
      "As soon as a replica obtains a notarized block of height h, it will \ufb01nish round h, and will subsequently not support the notarization of any other blocks at height h. At this point in time, such a replica will also relay this notarization to all other replicas. Note that this replica may have obtained the notarization either by (1) receiving it from another replica, or (2) aggregating n f notarization shares that it has received. The growth invariant states that each honest replica will eventually complete each round and start the next, so that the tree of notarized blocks continues to grow (and this holds only assuming asynchronous eventual delivery, and not partial synchrony). We prove the growth invariant below (see Section 5.11.4). Finalization There may be more than one notarized block at a given height h. However, if a block is \ufb01nalized, then we can be sure that there is no other notarized block at height h. Let us call this the safety invariant.",
      "For a block to become \ufb01nalized, n f distinct replicas must support its \ufb01nalization. Recall that round h ends for a replica when it obtains a notarized block B of height h. At that point in time, such a replica will check if it supported the notarization of any block at height h other than block B (it may or may not have supported the notarization of B itself). If not, the replica will support the \ufb01nalization of B by broadcasting a \ufb01nalization share for B. A \ufb01nalization share has exactly the same format as a notarization share (but is tagged in such a way notarization shares and \ufb01nalization shares cannot be confused with one another). Any set of n f \ufb01nalization shares on B may be aggregated together to form a \ufb01nalization for B, which has exactly the same format as a notarization (but again, is appropriately tagged). Any replica that obtains a \ufb01nalized block will broadcast the \ufb01nalization to all other replicas. We prove the safety invariant below (see Section 5.11.5).",
      "One consequence of the safety invariant is the following. Suppose two blocks B and B are \ufb01nalized, where B has height h, B has height h h. Then the safety invariant implies that the path in the tree of notarized blocks ending at B is a pre\ufb01x of the path ending at B (if not, then there would be two notarized blocks at height h, contradicting the \ufb01nalization invariant). Thus, whenever a replica sees a \ufb01nalized block B, it may view all ancestors of B as being implicitly \ufb01nalized, and because of the safety invariant, the safety property is guaranteed to hold for these (explicitly and implicitly) \ufb01nalized blocks  that is, all replicas agree on the ordering of these \ufb01nalized blocks. Delay functions The protocol makes use of two delay functions, m and n, which control the timing of block making and notarization activity.",
      "Both of these functions map the rank r of the proposing replica to a nonnegative delay amount, and it is assumed that each function Rank 0 Rank 1 Rank 2 Rank 0 Rank 0 Rank 1 Rank 0 Rank 0 Rank 2 Rank 0 Figure 2: An example tree of blocks is monotonely increasing in r, and that m(r) n(r) for all r  0, . . . , n 1. recommended de\ufb01nition of these functions is m(r)  2\u03b4r and n(r)  2\u03b4r  \u03f5, where \u03b4 is an upper bound on the time to deliver messages from one honest replica to another, and \u03f5 0 is a governor to keep the protocol from running too fast. With these de\ufb01nitions, liveness will be ensured in those rounds in which (1) the leader is honest, and (2) messages really are delivered between honest replicas within time \u03b4. Indeed, if (1) and (2) both hold in a given round, then the block proposed by the leader in that round will be \ufb01nalized. Let us call this the liveness invariant. We prove this below (see Section 5.11.6). 5.10 An example Figure 2 illustrates a block tree.",
      "Each block is labeled with its height (30, 31, 32, . . . ) and the rank of its block maker. The \ufb01gure also shows that each block in the tree is notarized, as indicated by the  N symbol. This means that for each notarized block in the tree, at least n f distinct replicas supported its notarization. As one can see, there can be more than one notarized block in the tree at a given height. For example, at height 31, we see there are two notarized blocks, proposed by block makers of rank 1 and 2. The same thing happens at height 34. We can also see that the block at height 36 is also explicitly \ufb01nalized, as indicated by the  F symbol. This means that n f distinct replicas supported this blocks \ufb01nalization, which means that these replicas (or at least, the honest replicas among these) did not support the notarization of any other block. All of the ancestors of this block, which are shaded gray, are considered implicitly \ufb01nalized.",
      "5.11 Putting it all together We now describe in more detail how the protocol works; speci\ufb01cally, we describe more precisely when a replica will propose a block and when a replica will support the notarization of a block. A given replica P will record the time at which it enters a given round h, which happens when it has obtained (1) some notarization for a block of height h 1, and (2) the random beacon for round h. Since the random beacon for round h has been determined, P can determine its own rank rP , as well as the rank rQ of each other replica Q for round h. 5.11.1 Random beacon details As soon as a replica has received the random beacon for round h, or enough shares to contruct the random beacon for round h, it will relay the random beacom for round h to all other replicas. As soon as a replica enters round h, it will generate and broadcast its share of the random beacon at round h  1.",
      "5.11.2 Block making details Replica P will only propose its own block BP provided (1) at least m(rP ) time units have passed since the beginning of the round, and (2) there is no valid lower ranked block currently seen by P. Note that since P is guaranteed to have a notarized block of height h 1 when it enters round h, it can make its proposed block a child of this notarized block (or any other notarized block of height h 1 that it may have). Also note that when P broadcasts its proposal for BP , it must also ensure that it also has relayed the notarization of BP s parent to all replicas. Suppose a replica Q sees a valid block proposal from a replica P of rank rP  rQ such that (1) at least m(rP ) time units have passed since the beginning of the round, and (2) there is no block of rank less than rP currently seen by Q.",
      "Then at this point in time, if it has not already done so, Q will relay this block proposal (along with the notarization of the proposed blocks parent) to all other replicas. 5.11.3 Notarization details Replica P will support the notarization of a valid block BQ proposed by a replica Q of rank rQ provided (1) at least n(rQ) time units have passed since the beginning of the round, and (2) there is no block of rank less than rQ currently seen by P. 5.11.4 Proof of growth invariant The growth invariant states that each honest replica will eventually complete each round and start the next. Assume that all honest replicas have started round h. Let rbe the rank of the lowest ranked honest replica P in round h. Eventually, P will either (1) propose its own block, or (2) relay a valid block proposed by a lower ranked replica.",
      "In either case, some block must eventually be supported by all honest replicas, which means that some block will become notarized and all honest replicas will \ufb01nish round h. All honest replicas will also receive the shares needed to construct the random beacon for round h  1, and so will start round h  1. 5.11.5 Proof of safety invariant The safety invariant states that if a block is \ufb01nalized in a given round, then no other block may be notarized in that round. Here is a proof of the safety invariant: 1. Suppose that the number of corrupt replicas is exactly ff  n3. 2. If a block B is \ufb01nalized, then its \ufb01nalization must have been supported by a set S of at least n f fhonest replicas (by the security property for aggregate signatures). 3. Suppose (by way of contradiction) that another block B  B were notarized. Then its notarization must have been supported by a set S of at least n f fhonest replicas (again, by the security property for aggregate signatures). 4.",
      "The sets S and S are disjoint (by the \ufb01nalization logic). 5. Therefore, n fS S  S  S 2(n f f), which implies n 3f, a contradiction. 5.11.6 Proof of liveness invariant We say that the network is \u03b4-synchronous at time t if all messages that have been sent by honest replicas at or before time t arrive at their destinations before time t  \u03b4. The liveness invariant may be stated as follows. Suppose that n(1) m(0)  2\u03b4. Also suppose that in a given round h, we have  the leader P in round h is honest,  the \ufb01rst honest replica Q to enter round h does so at time t, and  the network is \u03b4-synchronous at times t and t  \u03b4  m(0). Then the block proposed by P in round h will be \ufb01nalized. Here is a proof of the liveness invariant: 1. Under partial synchrony at time t, all honest replicas will enter round h before time t  \u03b4 (the notarization that ended round h 1 for Q as well as the random beacon for round h random will arrive at all honest replicas before this time). 2.",
      "The leader P in round h will propose a block B before time t  \u03b4  m(0), and again by partial synchrony, this block proposal will be delivered to all other replicas before time t  2\u03b4  m(0). 3. Since n(1) m(0)  2\u03b4, the protocol logic guarantees that each honest replica supports the notarization of block B and no other block, and thus B will become notarized and \ufb01nalized. 5.12 Other issues 5.12.1 Growth latency Under a partial synchrony assumption, we can also formulate and prove a quantitative version of the growth invariant. For simplicity, assume that the delay functions are de\ufb01ned as recommended above: m(r)  2\u03b4r and n(r)  2\u03b4r  \u03f5, and further assume that \u03f5 \u03b4. Suppose that at time t, the highest numbered round entered by any honest replica is h. Let rbe the rank of the lowest ranked honest replica P in round h. Finally, suppose that the network is \u03b4-synchronous at all times in the interval t, t  (3r 2)\u03b4. Then all honest replicas will start round h  1 before time t  3(r 1)\u03b4.",
      "5.12.2 Locally adjusted delay functions When a replica does not see any \ufb01nalized blocks for several rounds, it will start increasing its own delay function n for notarization. Replicas need not agree on these locally adjusted notarization delay functions. Also, while replicas do not explicitly adjust the delay function p, we can mathemati- cally model local clock drift by locally adjusting both delay functions. Thus, there are many delay functions, parameterized by replica and round. The critical condition n(1) m(0)  2\u03b4 needed for liveness then becomes max n(1) min m(0)  2\u03b4, where the max and min are taken over all the honest replicas in a given round. Thus, if \ufb01nalization fails for enough rounds, all honest replicas will eventually increase their no- tarization delay until this holds and \ufb01nalization will then resume.",
      "If some honest replicas increase their notarization latency function more than other replicas, there is no penalty in terms of liveness (but there may be in terms of growth latency). 5.12.3 Fairness Another property that is important in consensus protocols is fairness. Rather than give a general de\ufb01nition, we simply observe that the liveness invariant also implies a useful fairness property. Recall that the liveness invariant basically says that in any round where the leader is honest and the network is synchronous, then the block proposed by the leader will be \ufb01nalized. In those rounds where this happens, the fact that the leader is honest ensures that it will include in the payload of its block all of the inputs it knows about (modulo limits on the payload size). So, very roughly speaking, any input that is disseminated to enough replicas will be included in a \ufb01nalized block in a reasonable amount of time with high probability.",
      "Message Routing Layer As discussed in Section 1.7, basic computational unit in the IC is called a canister, which is roughly the same as the notion of a process, in that it comprises both a program and its state. The IC provides a run-time environment for executing programs in a canister, and to communicate with other canisters and external users (via message passing). The consensus layer (see Section 5) bundles inputs into payloads, which get placed into blocks, and as blocks are \ufb01nalized, the corresponding payloads are delivered to the message routing layer, then processed by the execution environment, which updates the state of the canisters on the replicated state machine and generates outputs, and these outputs are processed by the message routing layer. It is useful to distinguish between two types of inputs: ingress messages: these are messages from external users; cross-subnet messages: these are messages from canisters on other subnets.",
      "We can also distinguish between two types of outputs: ingress message responses: these are responses to ingress messages (which may be re- trieved by external users); cross-subnet messages: these are messages to canisters on other subnets. Upon receiving a payload from consensus, the inputs in that payload are placed into various input queues. For each canister C running on a subnet, there are several input queues  there is one queue speci\ufb01cally for ingress messages to C, and each other canister C, with whom C communicates, gets its own queue. (In the case where C is not located on the same subnet as C, these are cross-subnet messages.) As described below in more detail, in each round, the execution layer will consume some of the inputs in these queues, update the replicated state of the relevant canisters, and place outputs in various output queues.",
      "For each canister C running on a subnet, there are several output queues  each other canister C, with whom C communicates, gets its own queue. (In the case where C is not located on the same subnet as C, these are cross-subnet messages.) The message routing layer will take the messages in these output queues and place them into subnet-to-subnet streams to be processed by a crossnet transfer protocol, whose job it is to actually transport these messages to other subnets. In addition to these output queues, there is also an ingress history data structure. Once an ingress message has been processed by a canister, a response to that ingress message will be recorded in this data structure. At that point, the external user who provided the ingress message will be able to retrieve the corresponding response.",
      "(Note that ingress history does not maintain the full history of all ingress messages.) We also should mention that in addition to cross-subnet messages, there are also intra- subnet messages, which are messages from one canister to another on the same subnet. The message routing layer moves such messages directly from output queues to correspond- ing input queues. Figure 3 illustrates the basic functionality of the message routing and execution layers. Note that the replicated state comprises the state of the canisters, as well as system state, including the above-mentioned queues and streams, as well as the ingress history data structure. Thus, both the message routing and execution layers are involved in updating and maintaining the replicated state of a subnet. It is essential that all of this state is updated in a completely deterministic fashion, so that all replicas maintain exactly the same state.",
      "Also note that the consensus layer is decoupled from the message routing and execution layers, in the sense that any forks in the consensus blockchain are resolved before their payloads are passed to message routing, and in fact, consensus does not have to keep in lock step with message routing and consensus and is allowed to run a bit ahead. Per-round certi\ufb01ed state In each round, some of the state of a subnet will be certi\ufb01ed.",
      "The per-round certi\ufb01ed state is certi\ufb01ed using chain-key cryptography (see Section 1.6), speci\ufb01cally, using the Finalized Blocks Message Routing Layer Execution Layer Take inputs from \ufb01nalized blocks and put them into input queues of the respective canisters Take input messages from input queues, execute them and put produced messages in output queues CROSSNET MESSAE ROUTING Take messages from output queues, route them to subnet-to-subnet streams Stream to subnet 2 Stream to subnet 3 Stream to subnet N Canister 3 Canister 1 Canister 2 Figure 3: Message routing and execution layers (n f)-out-of-n threshold signature scheme mentioned in Section 3. In more detail, after each replica generates the per-round certi\ufb01ed state for a given round, it will generate a share of the corresponding threshold signature and broadcast this to all other replicas in its subnet.",
      "Upon collecting nf such shares, each replica can construct the resulting threshold signature, which serves as the certi\ufb01cate for the per-round certi\ufb01ed state for that round. Note that before signing, the per-round certi\ufb01ed state is hashed as a Merkle tree Mer87. The per-round certi\ufb01ed state in a given round consists of 1. cross-subnet messages that were recently added to the subnet-to-subnet streams; 2. other metadata, including the ingress history data structure; 3. the Merkle-tree root hash of the per-round certi\ufb01ed state from the previous round. Note that the per-round certi\ufb01ed state does not include the entire replicated state of a subnet, as this in general will be quite huge and it would be impractical to certify all of this state in every round.5 Figure 4 illustrates how the per-round certi\ufb01ed state may be organized into a tree. The \ufb01rst branch of the tree stores various metadata about each canister (but not the entire replicated state of the canister).",
      "The second branch stores the ingress history data struc- ture. The third branch stores information about the subnet-to-subnet streams, including a window of recently added cross-subnet messages for each stream. The other branches store other types of metadata, not discussed here. This tree structure may then be hashed into a Merkle tree, which has essentially the same size and shape as this tree. Per-round certi\ufb01ed state is used in several ways in the IC: 5But see Section 8.2 SwMxEJ34WetX1aOXYBE8ld0e1GPRi8cK9kPapWTbBuaZJckK5Slv8KLB0W8nO8WMtnvQ1gcDjdmJkXJoIb63nfaG19Y3Nru7RT3t3bPzisHB23TZxqylo0FrH uhsQwRVrW4F6yaERkK1gknt7nfeWLa8Fg92GnCAklGikecEukR0qU2H8QaXq1bw58CrxC1KFAs1B5asjGkqmbJUEGN6vpfYICPacirYrNxPDUsInZAR6zmqiG QmyOYHzC5U4Y4irUrZfFcT2REWnMVIauUxI7NsteLv7n9VIbXQcZV0lqmaKLRVEqsI1xj0ecs2oFVNHCNXc3YrpmGhC8wzKLgReV0q7XMuad1vNm6KOEpwC mdwAT5cQPuoAktoCDhGV7hDWn0gt7Rx6J1DRUzJAH6PMHKRSQowlatexitcanisters ingress history streams . . . . . . . . . . . .",
      "TwJBEJ3DL8Qv1NJmI5hYkTsKtSTaWGLiAQlcyN4ywIa9vcvungm58BtsLDTG1h9k579xgSsUfMkLNZGZemAiujetO4WNza3tneJuaW4PCofHzS0nGqGPosFrH qhFSj4BJ9w43ATqKQRqHAdji5mvtJ1SaxLRTBMIjqSfMgZNVbyq6xfrbLFbfmLkDWiZeTCuRo9stfvUHM0gilYJq3fXcxAQZVYzgbNSL9WYUDahIxaKmEOs gWx87IhVUGZBgrW9KQhfp7IqOR1tMotJ0RNWO96s3F7xuaoY3QcZlkhqUbLlomApiYjLnAy4QmbE1BLKFLe3EjamijJj8ynZELzVl9dJq17zrmruQ73SuM3jKMIZn MleHANDbiHJvjAgMzvMKbI50X5935WLYWnHzmFP7AfwBprKN6wlatexitc2 req1 req2 subnet1 subnet2 . . . . . . . . . . . . . . . . . . Figure 4: Per-round certi\ufb01ed state organized as a tree  Output authentication. Cross-subnet messages and responses to ingress messages are authenticated using per-round certi\ufb01ed state.",
      "Using the Merkle tree structure, an individual output (cross-subnet message or ingress message response) may be authen- ticated to any party by providing a threshold signature on the root of the Merkle tree, along with hash values on (and adjacent to) the path in the Merkle tree from the root to the leaf representing that output. The number of hash values needed to authen- ticate an individual output is therefore proportional to the depth of the Merkle tree, which is typically quite small, even if the size of the Merkle tree is very large. Thus, a single threshold signature can be used to e\ufb03ciently authenticate many individual outputs. Preventing and detecting non-determinism. Consensus guarantees that each replica processes inputs in the same order. Since each replica processes these in- puts deterministically, each replica should obtain the same state.",
      "However, the IC is designed with an extra layer of robustness to prevent and detect any (accidental) non-deterministic computation, should it arise. The per-round certi\ufb01ed state is one of the mechanisms used to do this. Since we use an (n f)-out-of-n threshold signature for certi\ufb01cation, and since f  n3, there can only be one sequence of states that is certi\ufb01ed. To see why state chaining is important, consider the following example. Suppose we have 4 replicas, P1, P2, P3, P4, and one is corrupt, say P4. Each of the replicas P1, P2, P3 start out in the same state. In round 1, because of a non-deterministic computation, P1, P2 compute a mes- sage m1 to send to subnet A, while P3 computes a message m 1 to send to subnet  In round 2, P1, P3 compute a message m2 to send to subnet B, while P2 computes a message m 2 to send to subnet B. In round 3, P2, P3 compute a message m3 to send to subnet C, while P1 computes a message m 3 to send to subnet C.",
      "This is illustrated in the following table: m1 A m2 B 3 C m1 A 2 B m3 C 1 A m2 B m3 C We are assuming that replicas P1, P2, and P3 each individually perform a valid se- quence of computations, but that because of non-determinism, these sequences are not identical. (Even though there is not supposed to be any non-determinism, in this example, we are supposing that there is.) Now suppose we did not chain the states. Because P4 is corrupt and may sign anything, he could create a 3-out-of-4 signature on a round-1 state that says m1 A, and similarly on a round-2 state that says m2 B, and on a round-3 state that says m3 C, even though the corresponding sequence m1 A, m2 B, m3 C may not be compatible with any valid sequence of computations. Worse yet, such an invalid sequence of computations could then lead to inconsistent states on other subnets.",
      "By chaining, we ensure that even if there is some non-determinism, any sequence of certi\ufb01ed states corresponds to some valid sequence of computations that was actually carried out by honest replicas. Coordination with consensus. The per-round certi\ufb01ed state is also used to coor- dinate the execution and consensus layers, in two di\ufb00erent ways:  Consensus throttling. Each replica will keep track of the latest round for which it has a certi\ufb01ed state  this is called the certi\ufb01ed height. It will also keep track of the latest round for which it has a notarized block  this is called the notarized height. If the notarized height is signi\ufb01cantly greater than the certi\ufb01ed height, this is a signal that execution is lagging consensus, and that consensus needs to be throttled. This lagging could be due to non-deterministic computation, or it could just be due to a more benign performance mismatch between the layers.",
      "Consensus is throttled by means of the delay functions discussed in Section 5.9  speci\ufb01cally, each replica will increase the governor value \u03f5 as the gap between notarized height and certi\ufb01ed height grows (this makes use of the notion of locally adjusted delay functions, as in Section 5.12.2). State-speci\ufb01c payload validation. As discussed in Section 5.7, the inputs in a pay- load must pass certain validity checks. In fact, these validity checks may depend to a certain degree on the state. A detail we skipped is that each block includes a round number, with the understanding that these validity checks should be made with respect to the certi\ufb01ed state for that round number. A replica that needs to perform this validation will wait until the state for that round number has been certi\ufb01ed, and then use the certi\ufb01ed state for that round to perform the validation.",
      "This ensures that even with non-deterministic computation, all replicas are performing the same validity tests (as otherwise, consensus could get stuck). Query calls vs update calls As we have described it so far, ingress messages must pass through consensus so that they are processed in the same order by all replicas on a subnet. However, an important optimization is available to those ingress messages whose processing does not modify the replicated state of a subnet. These are called query calls  as opposed to other ingress messages, which are called update calls. Query calls are allowed to perform computations which read and possibly update the state of a canister, but any updates to the state of a canister are never committed to the replicated state. As such, a query call may be processed directly by a single replica without passing through consensus, which greatly reduces the latency for obtaining a response from a query call.",
      "Note that a response to a query call is not recorded in the ingress history data structure. As such, we cannot directly use the per-round certi\ufb01ed state mechanism to authenticate responses to query calls. However, a separate mechanism for authenticating such responses is provided: certi\ufb01ed variables. As a part of the per-round certi\ufb01ed state, each canister on a subnet is allocated a small number of bytes, which is the certi\ufb01ed variable for that canister, whose value may be updated via update calls, and may be authenticated using the per-round certi\ufb01ed state mechanism. Moreover, a canister may use its certi\ufb01ed variable to store a root of a Merkle tree. In this way, a response to a query call to a canister may be authenticated so long the response is a leaf in the Merkle tree rooted at the certi\ufb01ed variable for that canister.",
      "External user authentication One of the main di\ufb00erences between an ingress message and a cross-subnet message is the mechanism used for authenticating these messages. We have already seen above (see Section 6.1) how threshold signatures are used to authenticate cross-subnet messages. The NNS registry (see Section 1.5) holds the public veri\ufb01cation keys for the threshold signatures used to authenticate cross-subnet messages. There is no central registry for external users. Rather, an external user identi\ufb01es himself to a canister using a user identi\ufb01er (aka principal), which is a hash of a public signature- veri\ufb01cation key. The user holds a corresponding secret signing key, which is used to sign ingress messages. Such a signature, as well as the corresponding public key, is sent along with the ingress message. The IC automatically authenticates the signature and passes the user identi\ufb01er to the appropriate canister.",
      "The canister may then authorize the requested operation, based on the user identi\ufb01er and other parameters to the operation speci\ufb01ed in the ingress message. First-time users generate a key pair and derive their user identi\ufb01er from the public key during their \ufb01rst interaction with the IC. Returning users are authenticated using the secret key that is stored by the user agent. A user may associate several key pairs with a single user identity, using signature delegation. This is useful, as it allows a single user to access the IC from several devices using the same user identity. Execution Layer The execution environment processes one input at a time. This input is taken from one of the input queues, and is directed to one canister. Based on this input and the state of the canister, the execution environment updates the state of the canister, and additionally may add messages to output queues and update the ingress history (possibly with a response to an earlier ingress message).",
      "In a given round, the execution environment will process several inputs. A scheduler determines which inputs are executed in a given round, and in which order. Without going into all the details of the scheduler, we highlight some of the goals:  it must be deterministic, i.e., only depend on the given data;  it should distribute workloads fairly among canisters (but optimizing for throughput over latency). the total amount of work done in each round, measured in terms of cycles (see Sec- tion 1.8), should be close to some pre-determined amount. Another task that the execution environment (together with the message router) must deal with are situations where a canister on one subnet is producing cross-subnet messages faster than they can be consumed by a canister on another subnet. For this, a self-regulating mechanism is implemented that throttles the producing canister. There are many other resource management and bookkeeping tasks that are dealt with by the execution environment.",
      "However, all of these tasks must be dealt with determinis- tically. Random tape Each subnet has access to a distributed pseudorandom generator (PRG). As men- tioned in Section 3, pseudorandom bits are derived from a seed that itself is an (f  1)-out- of-n BLS signature, called the Random Tape. There is a di\ufb00erent Random Tape for each round of the consensus protocol. While this BLS signature is similar to that used for the Random Beacon used in consensus (see Section 5.5), the mechanics are somewhat di\ufb00erent. In the consensus protocol, as soon as a block at height h is \ufb01nalized, each honest replica will release its share of Random Tape for height h  1. This has two implications: 1. Before a block at height h is \ufb01nalized by any honest replica, the Random Tape at height h  1 is guaranteed to be unpredictable. 2. By the time a block at height h  1 is \ufb01nalized by any honest replica, that replica will typically have all the shares it needs to construct the Random Tape at height h  1.",
      "To obtain pseudorandom bits, a subnet must make a request for these bits. Such a pseudorandom-bit request will be made as a system call from the execution layer in some round, say h. The system will then respond to that request later, when the Random Tape of height h  1 is available. By property (1) above, it is guaranteed that the requested pseudorandom bits are unpredictable at the time the request is made. By property (2) above, the requested random bits will typically be available at the time the next block is \ufb01nalized. In fact, in the current implementation, at the time a block of height h is \ufb01nalized, the Consensus Layer (see Section 5) will deliver both (the payload of) the block of height h and the Random Tape of height h  1 simultaneously to the message routing layer for processing.",
      "Chain-key cryptography II: chain-evolution technology As mentioned in Section 1.6.2, chain-key cryptography includes a collection of technologies for robustly and securely maintaining a blockchain-based replicated state machine over time, which together form what is called chain-evolution technology. Each subnet operates in epochs of many rounds (typically on the order of a few hundreds of rounds). Chain- evolution technology implements many essential maintenance activities that are executed periodically with a cadence that is tied to epochs: garbage collection, fast forwarding, subnet membership changes, pro-active resharing of secrets, and protocol upgrades. There are two essential ingredients to chain-evolution technology: summary blocks and catch-up packages (CUPs). Summary blocks The \ufb01rst block in each epoch is a summary block. A summary block contains special data that will be used to manage the shares of the various threshold signature schemes (see Section 3).",
      "There are two threshold schemes:  one (f  1)-out-of-n scheme, for which a new signing key is generated every epoch;  one (n f)-out-of-n scheme, for which the signing key is reshared once every epoch. The low-threshold scheme is used for the random beacon and the random tape, while the high-threshold scheme is used to certify the replicated state of the subnet. Recall that the DKG protocol (see Section 3.5) requires that for each signing key, we have a set of dealings, and that each replica can non-interactively obtain its share of the signing key from this set of dealings. Also recall that NNS maintains a registry that, among other things, determines the membership of a subnet (see Section 1.5). The registry (and hence the subnet membership) may change over time. Thus, subnets must agree on which registry version they use at various times for various purposes. This information is also stored in the summary block. The summary block for epoch i contains the following data \ufb01elds.",
      "currentRegistryVersion. This registry version will determine the consensus committee used throughout epoch i  all tasks performed by the consensus layer (block making, notarization, \ufb01nalization) will be performed by this committee. nextRegistryVersion. In each round of consensus, a block maker will include in its proposal the latest registry version it knows about (which must be no earlier than the block the proposed block extends). This ensures that the value nextRegistryVersion in the summary block of epoch i is fairly up to date. The value of currentRegistryVersion in epoch i is set to the value of nextRegistryVer- sion in epoch i 1. currentDealingSets. These are the dealing sets that determine the threshold signing keys that will be used to sign messages in epoch i. As we will see, the threshold signing committee for epoch i (i.e., the replicas that hold the corresponding threshold signing key shares) is the consensus committee for epoch i 1 . nextDealingSets.",
      "This is where dealings that are collected during epoch i 1 are gathered and stored.6 The value of currentDealingSets in epoch i will be set to the value of nextDealingSets in epoch i 1 (which itself consists of dealings collected in epoch i 2). collectDealingParams. This describes the parameters that de\ufb01ne the dealing sets to be collected during epoch i. During epoch i, block makers will include dealings in their proposed blocks that are validated relative to these parameters. The receiving committee for these dealings is based on the nextRegistryVersion value of the summary block of epoch i. For the low-threshold scheme, the dealing committee is the consensus committee for epoch i. For the high-threshold scheme, the shares to be reshared are based on the value of nextDealingSets of epoch i. Therefore, the dealing committee is the receiving committee for epoch i 1, which is also the consensus committee for epoch i .",
      "Also observe that the threshold signing committee for epoch i is the receiving committee in epoch i 2, which is the consensus committee for epoch i 1 . Consensus in epoch i relies on the values currentRegistryVersion and currentDealingSets in epoch i  in particular, the makeup of the consensus committee itself is based on curren- tRegistryVersion and the random beacon used in consensus is based on currentDealingSets. Moreover, just like any other block, there could be more than one summary block notarized at the beginning of epoch i, and that ambiguity needs to be resolved by consensus in epoch i. This seeming circularity is resolved by insisting that a summary block at the beginning of epoch i1 has been \ufb01nalized before epoch i starts, since the relevant values in the newer summary block are copied directly from that older summary block. This is actually an implicit synchrony assumption, but it is quite an academic assumption.",
      "Indeed, because of the consensus throttling discussed in Section 5.12.2 to ensure liveness, and because of 6A detail we have omitted is that if we fail to collect all the required dealings in epoch i 1, then as a fallback, the value of nextDealingSets in epoch i will e\ufb00ectively be set to the value of currentDealingSets in epoch i. If this happens, then the protocol will make use of dealing committees and threshold signing committees from further in the past, as appropriate.",
      "the length of an epoch is quite large, this can essentially never happen in practice: long before consensus could reach the end of epoch i 1 without \ufb01nalizing a summary block for epoch i 1, the notarization delay function would grow to be astronomically large, and so the partial synchrony assumption needed for \ufb01nalization will be satis\ufb01ed (essentially) with certainty (for all practical purposes).7 CUPs Before describing a CUP, we \ufb01rst point out one detail of random beacon: the random beacon for each round depends on the random beacon for the previous round. This is not an essential feature, but it impacts the design of the CUP. A CUP is a special message (not on the blockchain) that has (mostly) everything a replica needs to begin working in a given epoch, without knowing anything about previous epochs. It consists of the following data \ufb01elds:  The root of a Merkle hash tree for the entire replicated state (as opposed to the partial, per-round certi\ufb01ed state as in Section 6.1).",
      "The summary block for the epoch. The random beacon for the \ufb01rst round of the epoch. A signature on the above \ufb01elds under the (n f)-out-of-n threshold signing key for the subnet. To generate a CUP for a given epoch, a replica must wait until the summary block for that epoch is \ufb01nalized and the corresponding per-round state is certi\ufb01ed. As already mentioned, the entire replicated state must be hashed as a Merkle tree  even though a number of techniques are used to accelerate this process, this is still quite expensive, which is why it is only done once per epoch. Since a CUP contains only the root of this Merkle tree, a special state sync subprotocol is used that allows a replica to pull any state that it needs from its peers  again, a number of techniques are used to accelerate this process, but it is still quite expensive.",
      "Since we are using a high-threshold signature for a CUP, we can be sure that there is only one valid CUP in any epoch, and moreover, there will be many peers from which the state may be pulled. Also, since the public key of the threshold signature scheme remains constant over time, the CUP can be validated without knowing the current participants of the subnet. Implementing chain-evolution technology Garbage collection: Because of the information contained in a CUP for a given epoch, it is safe for each replica to purge all inputs that have been processed, and all consensus- level protocol messages needed to order those inputs, prior to that epoch. 7Also note that dealings that are collected in epoch i depend on data in the summary block for epoch i, in particular, the values of nextDealingSets and nextRegistryVersion . As such, these dealings should not be generated and cannot be validated until a summary block for epoch i has been \ufb01nalized.",
      "CUP 101 Replicated states Blocks Random beacons Figure 5: Fast forwarding Fast forwarding: If a replica in a subnet falls very far behind its peers (because it is down or disconnected from the network for a long time), or a new replica is added to a subnet, it can be fast forwarded to the beginning of the most recent epoch, without having to run the consensus protocol and process all of the inputs up to that point. Such a replica may do so by obtaining the most recent CUP. Using the summary block and random beacon contained in the CUP, along with protocol messages from other replicas (which have not yet been purged), this replica may run the consensus protocol forward from the beginning of the corresponding epoch. The replica will also use the state sync subprotocol to obtain the replicated state corresponding to the beginning of the epoch, so that it may also process the inputs generated by consensus. Figure 5 illustrates fast forwarding.",
      "Here, we assume that a replica that needs to catch up has a CUP at the beginning of an epoch, which starts (say) at height 101. The CUP contains the root of the Merkle tree for the replicated state at height 101, the summary block at height 101 (shown in green), and the random beacon at height 101. This replica will use the state sync subprotocol to obtain from its peers the full replicated state at height 101, using the root of the Merkle tree in the CUP to validate this state. Having obtained this state, the replica can then participate in the protocol, obtaining from its peers blocks (and other messages associated with consensus) at heights 102, 103, and so on, and updating its copy of the replicated state. If its peers have already \ufb01nalized blocks at greater heights, this replica will process those \ufb01nalized blocks as quickly as it can obtain them (and their notarizations and \ufb01nalizations) from its peers (and as quickly as the execution layer will allow).",
      "Subnet membership changes: We have already discussed how summary blocks are used to encode which version of the registry is in force in a given epoch, and how that is used to determine the subnet membership, and more speci\ufb01cally, the membership committees for various tasks. Note that even after a replica is removed from a subnet, it should (if possible) participate in its assigned committee duties for one additional epoch. Pro-active resharing of secrets: We have already discussed how summary blocks are used to generate and reshare signing keys. If necessary, the required summary block may be obtained from a CUP. Protocol upgrades: CUPs are also used to implement protocol upgrades. Protocol up- grades are initiated by the NNS (see Section 1.5).",
      "The basic idea, without going into all the details, is this:  when it is time to install a new version of the protocol, the summary block at the beginning of an epoch will indicate this;  the replicas running the old version of the protocol will continue running con- sensus long enough to \ufb01nalize the summary block and to create a corresponding CUP; however, they will create only empty blocks and not pass along any pay- loads to message routing and execution;  the new version of the protocol will be installed, and the replicas running the new version of the protocol will resume running the full protocol from the above CUP. BGLS03 D. Boneh, C. Gentry, B. Lynn, and H. Shacham. Aggregate and Veri\ufb01ably Encrypted Signatures from Bilinear Maps. In E.",
      "Biham, editor, Advances in Cryptology - EUROCRYPT 2003, International Conference on the Theory and Applications of Cryptographic Techniques, Warsaw, Poland, May 4-8, 2003, Proceedings, volume 2656 of Lecture Notes in Computer Science, pages 416 432. Springer, 2003. BKM18 E. Buchman, J. Kwon, and Z. Milosevic. The latest gossip on BFT consensus, 2018. arXiv:1807.04938, http:arxiv.orgabs1807.04938. BLS01 D. Boneh, B. Lynn, and H. Shacham. Short Signatures from the Weil Pairing. In C. Boyd, editor, Advances in Cryptology - ASIACRYPT 2001, 7th Interna- tional Conference on the Theory and Application of Cryptology and Information Security, Gold Coast, Australia, December 9-13, 2001, Proceedings, volume 2248 of Lecture Notes in Computer Science, pages 514532. Springer, 2001. But13 V. Buterin. Ethereum whitepaper, 2013. whitepaper. CDH21 J. Camenisch, M. Drijvers, T. Hanke, Y.-A. Pignolet, V. Shoup, D. Williams. Internet Computer Consensus. Cryptology ePrint Archive, Report 2021632, 2021.",
      "https:ia.cr2021632. CDS94 R. Cramer, I. Damgard, and B. Schoenmakers. Proofs of Partial Knowledge and Simpli\ufb01ed Design of Witness Hiding Protocols. In Advances in Cryptology - CRYPTO 94, 14th Annual International Cryptology Conference, Santa Bar- bara, California, USA, August 21-25, 1994, Proceedings, volume 839 of Lecture Notes in Computer Science, pages 174187. Springer, 1994. CL99 M. Castro and B. Liskov. Practical Byzantine Fault Tolerance. In M. I. Seltzer and P. J. Leach, editors, Proceedings of the Third USENIX Symposium on Op- erating Systems Design and Implementation (OSDI), New Orleans, Louisiana, USA, February 22-25, 1999, pages 173186. USENIX Association, 1999. CWA09 A. Clement, E. L. Wong, L. Alvisi, M. Dahlin, and M. Marchetti. Making Byzantine Fault Tolerant Systems Tolerate Byzantine Faults. In J. Rexford and E. G.",
      "Sirer, editors, Proceedings of the 6th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2009, April 22-24, 2009, Boston, MA, USA, pages 153168. USENIX Association, 2009. http:www.usenix. orgeventsnsdi09techfull_papersclementclement.pdf. Des87 Y. Desmedt. Society and Group Oriented Cryptography: A New Concept. In C. Pomerance, editor, Advances in Cryptology - CRYPTO 87, A Conference on the Theory and Applications of Cryptographic Techniques, Santa Barbara, California, USA, August 16-20, 1987, Proceedings, volume 293 of Lecture Notes in Computer Science, pages 120127. Springer, 1987. DLS88 C. Dwork, N. A. Lynch, and L. J. Stockmeyer. Consensus in the presence of partial synchrony. J. ACM, 35(2):288323, 1988. Fis83 M. J. Fischer. The Consensus Problem in Unreliable Distributed Systems (A Brief Survey).",
      "In Fundamentals of Computation Theory, Proceedings of the 1983 International FCT-Conference, Borgholm, Sweden, August 21-27, 1983, volume 158 of Lecture Notes in Computer Science, pages 127140. Springer, 1983. FS86 A. Fiat and A. Shamir. How to Prove Yourself: Practical Solutions to Identi\ufb01ca- tion and Signature Problems. In Advances in Cryptology - CRYPTO 86, Santa Barbara, California, USA, 1986, Proceedings, volume 263 of Lecture Notes in Computer Science, pages 186194. Springer, 1986. GHM17 Y. Gilad, R. Hemo, S. Micali, G. Vlachos, and N. Zeldovich. Algorand: Scaling Byzantine Agreements for Cryptocurrencies. Cryptology ePrint Archive, Report 2017454, 2017. https:eprint.iacr.org2017454. Gro21 J. Groth. Non-interactive distributed key generation and key resharing. Cryp- tology ePrint Archive, Report 2021339, 2021. https:ia.cr2021339. JMV01 D. Johnson, A. Menezes, and S. A. Vanstone. The Elliptic Curve Digital Sig- nature Algorithm (ECDSA). Int. J. Inf. Sec., 1(1):3663, 2001. Mer87 R.",
      "C. Merkle. A Digital Signature Based on a Conventional Encryption Func- tion. In Advances in Cryptology - CRYPTO 87, A Conference on the Theory and Applications of Cryptographic Techniques, Santa Barbara, California, USA, August 16-20, 1987, Proceedings, volume 293 of Lecture Notes in Computer Sci- ence, pages 369378. Springer, 1987. MXC16 A. Miller, Y. Xia, K. Croman, E. Shi, and D. Song. The Honey Badger of BFT Protocols. In E. R. Weippl, S. Katzenbeisser, C. Kruegel, A. C. My- ers, and S. Halevi, editors, Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, Vienna, Austria, October 24-28, 2016, pages 3142. ACM, 2016. Nak08 S. Nakamoto. Bitcoin: A peer-to-peer electronic cash system, 2008. https: bitcoin.orgbitcoin.pdf. PS18 R. Pass and E. Shi. Thunderella: Blockchains with Optimistic Instant Con- \ufb01rmation. In J. B. Nielsen and V.",
      "Rijmen, editors, Advances in Cryptology - EUROCRYPT 2018 - 37th Annual International Conference on the Theory and Applications of Cryptographic Techniques, Tel Aviv, Israel, April 29 - May 3, 2018 Proceedings, Part II, volume 10821 of Lecture Notes in Computer Science, pages 333. Springer, 2018. PSS17 R. Pass, L. Seeman, and A. Shelat. Analysis of the Blockchain Protocol in Asynchronous Networks. In Advances in Cryptology - EUROCRYPT 2017 - 36th Annual International Conference on the Theory and Applications of Cryp- tographic Techniques, Paris, France, April 30 - May 4, 2017, Proceedings, Part II, volume 10211 of Lecture Notes in Computer Science, pages 643673, 2017. Sch90 F. B. Schneider. Implementing Fault-Tolerant Services Using the State Machine Approach: A Tutorial. ACM Comput. Surv., 22(4):299319, 1990. YMR18 M. Yin, D. Malkhi, M. K. Reiter, G. G. Gueta, and I. Abraham. HotStu\ufb00: BFT Consensus in the Lens of Blockchain, 2018. arXiv:1803.05069, http:arxiv. orgabs1803.05069."
    ],
    "word_count": 19695,
    "page_count": 45
  },
  "LDO": {
    "chunks": [
      "Abstract. Lido DAO is a community that builds liquid staking service for Ethereum. Lido allows users to earn staking rewards without locking assets or maintaining staking infrastructure. Staking with Lido is primed to start along with Phase 0 of Ethereum 2.0. Ethereum is soon to be the biggest staking economy in the space. However, staking on the first stages of Ethereum 2.0 comes with a high market risk related to frozen staked assets until transfers will be available in Ethereum 2.0 (Phase 1.5 or Phase 2), which is expected to happen next year at the earliest. Until that time, no one will be able to withdraw staked ether, and, for example, sell them on an exchange. Lido liquid staking protocol is an Ethereum 2.0 liquid staking protocol solving these drawbacks. Users can deposit their ether in Lido smart contracts and receive stETH -- a tokenized version of staked ether -- in return. The DAO-controlled smart contracts then stake tokens with DAO-picked node operators.",
      "Users' deposited funds are controlled by the DAO, node operators never have direct access to the users' assets. Unlike staked ether, the stETH token is free from the limitations associated with a lack of liquidity and can be transferred at any time. The stETH token balance will be calculated based on the total amount of staked ether, plus rewards and minus any slashing penalties. Lido is a much more flexible solution than self-staking since it avoids freezing assets and maintaining a validator node. In addition, it allows staking users to earn rewards on as small a deposit as they want without restriction on the number of ether deposited. At the start, the system applies a 10 fee (this can be changed by the DAO) on staking rewards that are split between node operators, the DAO, and a slashing insurance fund.",
      "This fee level should make Lido staking more profitable than what is offered with most available exchange staking, but, unlike them, Lidos amount of staked ether is fully auditable and does not rely on a single partys private key management. Despite the strict limitations of the beacon chain, the first stage of Ethereum 2.0, we propose a decentralized approach for liquid staking. Lido: Ethereum Liquid Staking, 2020 Lido: Ethereum Liquid Staking Ethereum 2.0 is undergoing heavy research and development and is going to bring innovation, including the transition to a proof-of-stake based consensus algorithm. The process of staking involves locking up an amount of ether in a wallet to participate in the blockchain consensus in return for rewards. A lot of users are showing interest in staking, which will allow them to generate income. However, the transition to Ethereum 2.0 is planned to occur gradually.",
      "Staking will be available from the very beginning (deposits are already enabled and the network itself will launch, most likely, in December), but the coins that the user deposits cannot be withdrawn until transfers are enabled. Full support for withdrawal mechanics will not appear until Phase 2 or Phase 1.5, which is scheduled to roll out over the next few years. Ethereum 2.0 launch will involve 4 stages (release dates provided by ConsenSys): Ethereum 2.0 staking summary Phase 0: the main beacon chain without shards will be implemented - chain validators create blocks according to the PoS algorithm. Transactions are not supported and, in particular, there are no fund transfers, i.e. the chain consists only of service data. Release date: 1 December 2020. Phase 1: 64 shards will be added. There is still no transaction support (i.e., all shards contain only service data). Release date: 2021. Phase 1.5: the current Ethereum network becomes one of the shards. Release date: 2021.",
      "Phase 2: ether accounts, transactions, transfers, and withdrawals will be added. There are no clearly defined specifications yet. Release date: 2021 or later. Disclaimer: The information in this document was actual as of the date of writing (October 2020). To keep up to date with the latest information, please check out the resources: Documentation, Help center, FAQ Taking into account all the details and risks, ether staking becomes less attractive in the early stages. The main issue is locking the staked deposit for a long time without the ability to withdraw. Exchange staking is a custodial alternative to self-staking, as users can earn rewards with the ability to withdraw their coins at any time. Exchanges can instantly return coins on demand from the liquidity pool without waiting until the end of the unbonding period. Exchanges apply a fee on profits from a staking deposit.",
      "Exchange staking for Ethereum 2.0 is additionally complicated by a lack of ability to withdraw staked coins in the first stages. Therefore, an exchange could safely stake only a share of deposits, up to 60 of the deposited ether in our estimations, to ensure liquidity to allow users to withdraw their staked funds. As a result, the Ethereum 2.0 exchange liquid staking interest rate will be significantly less than self-staking. There is a risk of loss or loss of profits, which occurs if the validator is slashed for misbehaving. This can happen, for example, due to a bug in the validator's node code or due to connectivity issues. Lido aims to allow users to stake ether without losing the ability to trade or otherwise use their tokens. Lido will be a decentralized infrastructure for issuing a liquid token that is safer than exchange staking and has incredible flexibility compared to self-staking.",
      "The primary goals of Lido are: The Lido DAO is a Decentralized Autonomous Organization, which builds liquid staking protocol for Ethereum. In the case of liquid staking, the competitors are well-known providers like centralized exchanges and other decentralized protocols like RocketPool. The DAO is the logical compromise between full centralization and decentralization, which allows the deployment of competitive products without full centralization and custody on the exchanges. We do not believe that it is possible to make a liquid staking protocol that is completely trustless. For the first phases of Ethereum 2.0, it is not possible at all.",
      "A DAO is an optimal structure for launching Lido because: Lido is highly dependent on the design and restrictions of the beacon chain; Ethereum 2.0 staking protocol may change and therefore Lido should be upgradable; An insurance provider must be selected and terms for slashing insurance must be negotiated; DAO governance is better than one person or a developer's team for making decisions about changes in Lido; and a DAO will be able to cover the costs of developing and upgrading the protocol from the DAO token treasury. The DAO will accumulate service fees from Lido, which can be funneled into the insurance and development funds, distributed by the DAO.",
      "Goals Why DAO To allow users to earn staking rewards without fully locking their ether; To make it possible to earn rewards on as small a deposit as users want without restriction on deposits different than 32 ether; To reduce the risks of losing a staked deposit due to software failures or malicious third-parties; To provide the stETH token as a building block for other applications and protocols (e.g., as collateral in lending or other trading DeFi solutions); To provide an alternative to exchange staking, self-staking, and other semi-custodial and decentralized protocols. Lido: Ethereum Liquid Staking, 2020 To validate the beacon chain, a user needs to deposit 32 ether, specify a validating public key, and a withdrawal address where their assets and rewards will stay frozen until transfers are enabled. Until then, the only two activities performed on the beacon chain are validating and to stop validating. To participate in the network consensus, a validator must maintain high uptime.",
      "Launch Lido: Propose and update Lidos parameters; Approve incentives for parties that contribute towards DAOs goals (e.g., stETH liquidity providers); Propose and update Lidos implementation for incoming Ethereum 2.0 features using DAO treasury funds; Assign oracles to deliver rewardslashing rate feed to help establish stETH token balances; Scout and qualify new node operators and penalize the existing ones slashed by Ethereum 2.0s rules; Manage the Lido DAOs insurance and development funds; Manage unbonding and withdrawals once available in Ethereum 2.0; and Respond to emergencies. Deploy protocol smart contracts; Set fees and other protocol parameters; Select the threshold signature scheme participants among reputable individuals or organizations willing to provide the service; Facilitate the multi-party computation ceremony to create the threshold signature account for staking rewards; Assign initial DAO-vetted node operators.",
      "Lido is implemented in a trust-minimized way as a set of Ethereum 1.0 smart contracts. Lido allows users to earn staking rewards on their ether holdings, without locking capital or maintaining the validator's node. Lido consists of several parts: stETH Token; Deposits and stETH token minting; Node operator registry; Beacon chain oracles and stETH token balance update; and Withdrawals (disabled until Ethereum 2.0 transfers are available). To stake ether with Lido, the user sends ether to the smart contract and gets stETH tokens in return. stETH tokens represent a tokenized staking deposit. stETH tokens can be held, traded, or sold. The balance of stETH is based on the total amount of staked ether plus total staking rewards and minus slashing applied on validators. All deposits into Lido are delineated by 32 ETH and assigned to node operators who validates using these deposits. Node operators never have direct access to the users' ether.",
      "Funds are deposited to the Lido protocol smart contract and then are locked into the Ethereum proof-of-stake deposit contract. The threshold signature account controlled by the Lido DAO is specified as a staking withdrawal address. Staked ether will be withdrawable only when transfers and smart contracts will be implemented on Ethereum 2.0 (expected in Phase 2). Unlike similar systems, Lido does not require node operators to deposit equal collateral of staking positions. Instead, Lido DAO-chosen node operators should have a track record with assets staking, which will be supplemented with slashing insurance. This approach will allow the system to be more capital-efficient. Lido is managed by the Lido DAO. The DAO members govern Lido to ensure its efficiency and stability. Besides technical development, the Lido DAOs mandate is to promote Lido and recruit new users, node operators, and validators with educational content, promotional campaigns, and affiliate marketing.",
      "The Lido DAO should do the following: System Architecture Lido: Ethereum Liquid Staking, 2020 User validation key stETH validation key withdraw (once available) Ethereum 1.0 Beacon Chain Lido Staking Contracts Node operators ETH1 account Validators node Lido DAO threshold signature withdrawal account Ethereum Deposit Contract Figure 1: Stake Deposit Flow Figure 2: Staking profit distribution The stETH token balance is based on the amount of ether deposited in Lido with associated total rewards and slashing penalties. Since the beacon chain is a separate network, Lido smart contracts cannot get direct access to its data. Communication between the Ethereum 1.0 part of the system and the beacon network is performed by the Lido DAO appointed oracles. They monitor node operators beacon chain accounts and submit corresponding data to Lidos Ethereum 1.0 smart contracts. On every update submitted by oracle, the system recalculates the stETH token ratio.",
      "If the overall staking rewards are greater than the slashing penalties, the system registers a profit. In this case, the stETH token balances will increase and Lido would apply a 10 fee. The fee is applied by minting stETH tokens corresponding to 10 of Lido's profit. The minted stETH tokens are distributed between the node operators and the DAOs treasury account. Node operators part of the fee is distributed proportionally to the corresponding active validation keys on the beacon chain. Slashing penalties negatively impact stETH token balances. To compensate for this negative impact, part of the Lido fee is transferred to the slashing insurance provider who protects against reasonably-sized slashing events. The Lido DAO governance must intervene in case of massive slashings. Withdrawals will be available once transfers are implemented in Ethereum 2.0 (scheduled as Phase 2). Once Ethereum 2.0 transfers are rolled out, the Lido DAO would upgrade Lido to implement the feature.",
      "Before that point, rewards restaking is not available either. All validators accounts 10 of net rewards used to mint stETH as a fee Beacon Chain collects data about rewardsslashing submits total ETH balance 50 of fee 50 of fee is split between node operators according their share in the total staked value Ethereum 1.0 All Lidos validators Oracle Lido Staking Contracts 90 of rewards accounts for stETH balance increase Node operators accounts Lido treasury Lido: Ethereum Liquid Staking, 2020 Lido has two tokens: liquid stETH token  a tokenized version of staked ethereum  and LDO  a token granting governance rights in the Lido DAO. The stETH token is a tokenized version of staked ether. When a user sends ether into the Lido liquid staking smart contract, the user receives the corresponding amount of stETH tokens. The stETH token represents Lido users deposits and the corresponding staking rewards and slashing penalties.",
      "The stETH token is a liquid alternative for the staked ether: it could be transferred, traded, or used in DeFi applications. Lido makes the stETH token balance track a balance of corresponding balance of beacon chain ether. A users balance of stETH tokens corresponds 1 to 1 to an amount of ether a user could receive if withdrawals were enabled and instant. Ethereum 2.0 transfers and smart contracts are scheduled at Phase 2. Once these features are deployed, the Lido DAO will upgrade Lido to allow the users to burn stETH tokens in exchange for ether. While the fact that a stETH balance tracks the corresponding amount of beacon chain such that ether should be the main driver of the stETHETH exchange rate, several other factors are affecting the market prices. There is a market risk that the stETH token supply will outweigh the market demand.",
      "While the goal of the Lido is to provide liquidity for ether staked on the beacon chain, the same liquidity makes it possible to sell the token on exchanges. Before Phase 2 deployment, it is the only way to take profit from the stETH token. However, stETH tokens also can be used in various decentralized financial products. For instance, stETH could be used as collateral. The higher the rate of stETH adoption in different DeFi applications, the more demand for it there would be. The security of Lido must be the Lido DAOs highest priority beginning at the time of its deployment. Users should investigate risks involved with Lido before engaging with it. There is an inherent risk that Lido could contain vulnerabilities or bugs causing, among other things, the complete failure of Lido andor its parts. Lido on Ethereum is built on top of experimental technology under active development. There is no guarantee that the beacon chain network would be error-free or have a minimum uptime.",
      "Failures in Ethereum 2.0 might lead to validators slashing and result in a significant drop in the balance and price of the stETH token. The beacon chains staking rewards are the source of the value increase of the stETH token. In case the beacon chain network does not reach its target adoption, the value of the staked beacon chain ether and the staking rewards could be significantly lower than Ethereum 1.0 ether. Beacon chain adoption risk Lido DAO governs a set of liquid staking protocols with Lido on Ethereum among them. The Lido DAO decides on Lidos key parameters (e.g., fees) and executes Lido upgrades. The Lido DAO members govern Lido to ensure its efficiency and stability. To have a vote in the Lido DAO, one must hold its governance token, LDO. LDO voting weight is proportional to the amount of LDO a voter stakes in the voting contract. The more LDO locked in a users voting contract, the greater the decision-making power the voter gets.",
      "The exact mechanism of LDO voting can be upgraded just like the other DAO applications. Tokenomics Risks stETH token Smart contract security Beacon chain technical risk LDO token Lido: Ethereum Liquid Staking, 2020 All Lido DAO staked ether is held on distributedly managed accounts backed by a BLS based m-of-n threshold scheme. The threshold scheme is more secure than a single key controlled by the custody. However, there is still a non-zero probability of failure. If at least (n-m1) signatories lose their key shares, get hacked, or go rogue, funds might become locked. If m or more key shares are compromised, funds can be stolen (after transfers are unlocked). Beacon chain validators are at risk of receiving staking penalties (for going offline, for example) and slashing (for double signing). In the worst case, when a lot of validators misconduct simultaneously, up to 100 of the stake can be slashed.",
      "To mitigate this risk, the stake is distributed to a plethora of professional and reputable node operators with heterogeneous setups. Additional mitigation comes in form of insurance that is continuously paid for from Lido fees. Besides the risk associated with validators' slashing and a stETH token balance drop, there is a chance that the exchange price of stETH will be less than fair price for a while. In the beginning, there is no withdrawal feature in Lido. As a result, arbitrage and risk-free market-making are impossible. DAO threshold key management risk Slashing risk stETH price risk We are in the middle of a big Ethereum transition from the proof of work to the proof of stake consensus model. Security of the network depends on the amount of the total staked ether and the level of validators' decentralization  how many the network would have and how big they would be.",
      "As withdrawals are not available on the beacon chain, there is a risk that some users would not be able to afford self-staking. Because of that, users would either use some sort of exchange staking or pass on the staking altogether. Limitations of the beacon chain affect the liquidity of staked capital for exchanges. We expect that most exchanges would not be able to offer rewards comparable to self-staking. The other thing to look out for is the high risk of network centralization. Exchanges are historically among the biggest ether holders, and they could become even bigger due to exchange staking. Liquid staking provides a viable alternative to both self and exchange staking. Lido provides a balance of risk, reward, and \u0441onvenience. It allows users to trade staked ether without a negative impact on the Ethereum network's decentralized nature. Lido is useful for both small and large ether holders. Small wallets could use staking without having to stake big chunks of their funds.",
      "Larger entities would be able to hedge their funds against ether volatility and use staking without having to maintain staking infrastructure. Conclusion Lido: Ethereum Liquid Staking, 2020"
    ],
    "word_count": 3113,
    "page_count": 6
  },
  "LINK": {
    "chunks": [
      "ChainLink A Decentralized Oracle Network Steve Ellis, Ari Juels, and Sergey Nazarov 4 September 2017 (v1.0) Abstract Smart contracts are poised to revolutionize many industries by replacing the need for both traditional legal agreements and centrally automated digital agreements. Both performance veri\ufb01cation and execution rely on manual actions from one of the contracting parties, or an automated system that programmat- ically retrieves and updates relevant changes. Unfortunately, because of their underlying consensus protocols, the blockchains on which smart contracts run cannot support native communication with external systems. Today, the solution to this problem is to introduce a new functionality, called an oracle, that provides connectivity to the outside world. Existing oracles are centralized services. Any smart contract using such services has a single point of failure, making it no more secure than a traditional, centrally run digital agreement.",
      "In this paper we present ChainLink, a decentralized oracle network. We de- scribe the on-chain components that ChainLink provides for contracts to gain external connectivity, and the software powering the nodes of the network. We present both a simple on-chain contract data aggregation system, and a more e\ufb03cient o\ufb00-chain consensus mechanism. We also describe supporting reputation and security monitoring services for ChainLink that help users make informed provider selections and achieve robust service even under aggressively adver- sarial conditions. Finally, we characterize the properties of an ideal oracle as guidance for our security strategy, and lay out possible future improvements, including richly featured oracle programming, data-source infrastructure modi- \ufb01cations, and con\ufb01dential smart-contract execution. Contents Introduction Architectural Overview On-Chain Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . O\ufb00-Chain Architecture . . . . . . . . . . . . . . . .",
      ". . . . . . . . . . Oracle Security ChainLink Decentralization Approach Distributing sources . . . . . . . . . . . . . . . . . . . . . . . . . . . . Distributing oracles . . . . . . . . . . . . . . . . . . . . . . . . . . . . ChainLink Security Services Validation System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Reputation System . . . . . . . . . . . . . . . . . . . . . . . . . . . . Certi\ufb01cation Service . . . . . . . . . . . . . . . . . . . . . . . . . . . Contract-Upgrade Service . . . . . . . . . . . . . . . . . . . . . . . . LINK token usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Long-Term Technical Strategy Con\ufb01dentiality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Infrastructure changes . . . . . . . . . . . . . . . . . . . . . . . . . . O\ufb00-chain computation . . . . . . . . . . . . . . . . . . . . . . . . . . Existing Oracle Solutions Conclusion A O\ufb00-Chain Aggregation A.1 OCA protocol . . . . . . . . . . . . . . . . . . . . . . .",
      ". . . . . . . . A.2 Proof sketches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B SGX Trust Assumptions Introduction Smart contracts are applications that execute on decentralized infrastructure, such as a blockchain. They are tamperproof, in the sense that no party (even their cre- ator) can alter their code or interfere with their execution. Historically, contracts embodied in code have run in a centralized manner that leaves them subject to al- teration, termination, and even deletion by a privileged party. In contrast, smart contracts execution guarantees, which bind all parties to an agreement as written, create a new and powerful type of trust relationship that does not rely on trust in any one party.",
      "Because they are self-verifying and self-executing (i.e., tamperproof as explained above), smart contracts thus o\ufb00er a superior vehicle for realizing and administering digital agreements. The powerful new trust model that smart contracts embody, though, introduces a new technical challenge: connectivity. The vast majority of interesting271 smart contract applications rely on data about the real world that comes from key resources, speci\ufb01cally data feeds and APIs, that are external to the blockchain. Because of the mechanics of the consensus mechanisms underpinning blockchains, a blockchain cannot directly fetch such critical data. We propose a solution to the smart contract connectivity problem in the form of ChainLink, a secure oracle network. What di\ufb00erentiates ChainLink from other oracle solutions is its ability to operate as a fully decentralized network.",
      "This decentralized approach limits the trust in any single party, enabling the tamperproof quality valued in smart contracts to be extended to the end-to-end operation between smart contracts and the APIs they rely on. Making smart contracts externally aware, meaning capable of interacting with o\ufb00-chain resources, is necessary if they are going to replace the digital agreements in use today. Today, the lions share of traditional contractual agreements that have been digi- tally automated use external data to prove contractual performance, and require data outputs to be pushed to external systems. When smart contracts replace these older contractual mechanisms, they will require high-assurance versions of the same types of data inputs and outputs.",
      "Examples of potential next-generation smart contracts and their data requirements include:  Securities smart contracts such as bonds, interest rate derivatives, and many others will require access to APIs reporting market prices and market reference data, e.g. interest rates. 1The main use of smart contracts in Ethereum today is management of tokens, which are a common functionality in most smart contract networks. We believe that the current focus on tokens to the exclusion of many other possible applications is due to a lack of adequate oracle services, a situation ChainLink speci\ufb01cally aims to remedy. Insurance smart contracts will need data feeds about IoT data related to the insurable event in question, e.g.: was the warehouses magnetic door locked at the time of breach, was the companys \ufb01rewall online, or did the \ufb02ight you had insurance for arrive on time.",
      "Trade \ufb01nance smart contracts will need GPS data about shipments, data from supply chain ERP systems, and customs data about the goods being shipped in order to con\ufb01rm ful\ufb01llment of contractual obligations. Another problem common to these examples is the inability for smart contracts to output data into o\ufb00-chain systems. Such output often takes the form of a payment message routed to traditional centralized infrastructure in which users already have accounts, e.g., for bank payments, PayPal, and other payment networks. ChainLinks ability to securely push data to APIs and various legacy systems on behalf of a smart contract permits the creation of externally-aware tamperproof contracts. Whitepaper roadmap In this whitepaper, we review the ChainLink architecture (Section 2). We then explain how we de\ufb01ne security for oracles (Section 3).",
      "We describe the ChainLink approach to decentralization  distribution of oracles and data sources (Section 4), and follow with a discussion of the four security services proposed by ChainLink, as well as the role played by LINK tokens (Section 5). We then describe a proposed long-term development strategy, which includes better con\ufb01dentiality protections, the use of trusted hardware, infrastructure changes, and general oracle programmability (Section 6). We brie\ufb02y review alternative oracle designs (Section 7), and conclude with a short discussion of the design principles and philosophy guiding ChainLink development (Section 8). Architectural Overview ChainLinks core functional objective is to bridge two environments: on-chain and o\ufb00- chain. We describe the architecture of each ChainLink component below. ChainLink will initially be built on Ethereum 16, 35, but we intend for it to support all leading smart contract networks for both o\ufb00-chain and cross-chain interactions.",
      "In both its on and o\ufb00-chain versions, ChainLink has been designed with modularity in mind. Every piece of the ChainLink system is upgradable, so that di\ufb00erent components can be replaced as better techniques and competing implementations arise. On-Chain Architecture As an oracle service, ChainLink nodes return replies to data requests or queries made by or on behalf of a user contract, which we refer to as requesting contracts and denote by USER-SC. ChainLinks on-chain interface to requesting contracts is itself an on-chain contract that we denote by CHAINLINK-SC. Behind CHAINLINK-SC, ChainLink has an on-chain component consisting of three main contracts: a reputation contract, an order-matching contract, and an aggregating contract. The reputation contract keeps track of oracle-service-provider performance metrics. The order-matching smart contract takes a proposed service level agreement, logs the SLA parameters, and collects bids from oracle providers.",
      "It then selects bids using the reputation contract and \ufb01nalizes the oracle SLA. The aggregating contract collects the oracle providers responses and calculates the \ufb01nal collective result of the ChainLink query. It also feeds oracle provider metrics back into the reputation contract. ChainLink contracts are designed in a modular manner, allowing for them to be con\ufb01gured or replaced by users as needed. The on-chain work \ufb02ow has three steps: 1) oracle selection, 2) data reporting, 3) result aggregation. Oracle Selection An oracle services purchaser speci\ufb01es requirements that make up a service level agreement (SLA) proposal. The SLA proposal includes details such as query parameters and the number of oracles needed by the purchaser. Additionally, the purchaser speci\ufb01es the reputation and aggregating contracts to be used for the rest of the agreement.",
      "Using the reputation maintained on-chain, along with a more robust set of data gathered from logs of past contracts, purchasers can manually sort, \ufb01lter, and select oracles via o\ufb00-chain listing services. Our intention is for ChainLink to maintain one such listing service, collecting all ChainLink-related logs and verifying the binaries of listed oracle contracts. We further detail the listing service and reputation systems in Section 5. The data used to generate listings will be pulled from the blockchain, allowing for alternative oracle-listing services to be built. Purchasers will submit SLA proposals to oracles o\ufb00-chain, and come to agreement before \ufb01nalizing the SLA on-chain. Manual matching is not possible for all situations. For example, a contract may need to request oracle services dynamically in response to its load. Automated solu- tions solve this problem and enhance usability.",
      "For these reasons, automated oracle matching is also being proposed by ChainLink through the use of order-matching contracts. Once the purchaser has speci\ufb01ed their SLA proposal, instead of contacting the ora- cles directly, they will submit the SLA to an order-matching contract. The submission of the proposal to the order-matching contract triggers a log that oracle providers can monitor and \ufb01lter based on their capabilities and service objectives. ChainLink nodes then choose whether to bid on the proposal or not, with the contract only accepting bids from nodes that meet the SLAs requirements. When an oracle service provider bids on a contract, they commit to it, speci\ufb01cally by attaching the penalty amount that would be lost due to their misbehavior, as de\ufb01ned in the SLA. Bids are accepted for the entirety of the bidding window. Once the SLA has received enough quali\ufb01ed bids and the bidding window has ended, the requested number of oracles is selected from the pool of bids.",
      "Penalty payments that were o\ufb00ered during the bidding process are returned to oracles who were not selected, and a \ufb01nalized SLA record is created. When the \ufb01nalized SLA is recorded it triggers a log notifying the selected oracles. The oracles then perform the assignment detailed by the SLA. Data Reporting Once the new oracle record has been created, the o\ufb00-chain oracles execute the agreement and report back on-chain. For more detail about o\ufb00-chain interactions, see Sections 2.2 and 4. Result Aggregation Once the oracles have revealed their results to the oracle con- tract, their results will be fed to the aggregating contract. The aggregating contract tallies the collective results and calculates a weighted answer. The validity of each oracle response is then reported to the reputation contract. Finally, the weighted answer is returned to the speci\ufb01ed contract function in USER-SC.",
      "Detecting outlying or incorrect values is a problem that is speci\ufb01c to each type of data feed and application. For instance, detecting and rejecting outlying answers before averaging may be necessary for numeric data but not boolean. For this reason, there will not be a speci\ufb01c aggregating contract, but a con\ufb01gurable contract address which is speci\ufb01ed by the purchaser. ChainLink will include a standard set of ag- gregating contracts, but customized contracts may also be speci\ufb01ed, provided they conform to the standard calculation interface. O\ufb00-Chain Architecture O\ufb00-chain, ChainLink initially consists of a network of oracle nodes connected to the Ethereum network, and we intend for it to support all leading smart contract net- works. These nodes independently harvest responses to o\ufb00-chain requests. As we explain below, their individual responses are aggregated via one of several possible consensus mechanisms into a global response that is returned to a requesting con- tract USER-SC.",
      "The ChainLink nodes are powered by the standard open source core implementation which handles standard blockchain interactions, scheduling, and con- necting with common external resources. Node operators may choose to add software extensions, known as external adapters, that allow the operators to o\ufb00er additional specialized o\ufb00-chain services. ChainLink nodes have already been deployed along- side both public blockchains and private networks in enterprise settings; enabling the nodes to run in a decentralized manner is the motivation for the ChainLink network. ChainLink Core. The core node software is responsible for interfacing with the blockchain, scheduling, and balancing work across its various external services. Work done by ChainLink nodes is formatted as assignments. Each assignment is a set of smaller job specifcations, known as subtasks, which are processed as a pipeline.",
      "Each subtask has a speci\ufb01c operation it performs, before passing its result onto the next subtask, and ultimately reaching a \ufb01nal result. ChainLinks node software comes with a few subtasks built in, including HTTP requests, JSON parsing, and conversion to various blockchain formats. External Adapters. Beyond the built-in subtask types, custom subtasks can be de\ufb01ned by creating adapters. Adapters are external services with a minimal REST API. By modeling adapters in a service-oriented manner, programs in any program- ming language can be easily implemented simply by adding a small intermediate API in front of the program. Similarly, interacting with complicated multi-step APIs can be simpli\ufb01ed to individual subtasks with parameters. Subtask Schemas. We anticipate that many adapters will be open sourced, so that services can be audited and run by various community members.",
      "With many di\ufb00erent types of adapters being developed by many di\ufb00erent developers, ensuring compatibility between adapters is essential. ChainLink currently operates with a schema system based on JSON Schema 36, to specify what inputs each adapter needs and how they should be formatted. Sim- ilarly, adapters specify an output schema to describe the format of each subtasks output. Oracle Security In order to explain ChainLinks security architecture, we must \ufb01rst explain why se- curity is importantand what it means. Why must oracles be secure?",
      "Returning to our simple examples in Section 1, if a smart contract security gets a false data feed, it may payout the incorrect party, if smart contract insurance data feeds can be tampered with by the insured party Figure 1: ChainLink work\ufb02ow: 1) USER-SC makes an on-chain request; 2) CHAINLINK-SC logs an event for the oracles; 3) ChainLink core picks up the event and routes the assignment to an adapter; 4) ChainLink adapter performs a request to an external API; 5) ChainLink adapter processes the response and passes it back to the core; 6) ChainLink core reports the data to CHAINLINK-SC; 7) CHAINLINK-SC aggregates responses and passes them back as a single response to USER-SC. there may be insurance fraud, and if GPS data given to a trade \ufb01nance contract can be modi\ufb01ed after it leaves the data provider, payment can be released for goods that havent arrived.",
      "More generally, a well-functioning blockchain, with its ledger or bulletin-board abstraction, o\ufb00ers very strong security properties. Users rely on the blockchain as a functionality that correctly validates transactions and prevents data from being altered. They treat it in e\ufb00ect like a trusted third party (a concept we discuss at length below). A supporting oracle service must o\ufb00er a level of security commensurate with that of the blockchain it supports. An oracle too must therefore serve users as an e\ufb00ective trusted third party, providing correct and timely responses with very high probability. The security of any system is only as strong as its weakest link, so a highly trustworthy oracle is required to preserve the trustworthiness of a well- engineered blockchain. De\ufb01ning oracle security: An ideal view. In order to reason about oracle se- curity, we must \ufb01rst de\ufb01ne it. An instructive, principled way to reason about oracle security stems from the following thought experiment.",
      "Imagine that a trusted third party (TTP)an ideal entity or functionality that always carries out instructions faithfully to the letterwere tasked with running an oracle. Well denote this oracle by ORACLE (using all caps in general to denote an entity fully trusted by users), and suppose that the TTP obtains data from a perfectly trustworthy data source Src. Given this magical service ORACLE, what instructions would we ask it to carry out? To achieve the property of integrity, also referred to as the authenticity prop- erty 24, we would simply ask that ORACLE perform the following steps: Figure 2: Behavior of an ideal oracle ORACLE is de\ufb01ned by steps: 1) Accept request; 2) Obtain data; 3) Return data. Additionally, to protect the con\ufb01dentiality of a request, upon decrypting it, ORACLE never uses or reveals the data it contains, except to query Src. 1.",
      "Accept request: Ingest from a smart contract USER-SC a request Req  (Src, \u03c4, q) that speci\ufb01es a target data source Src, a time or range of times \u03c4, and a query 2. Obtain data: Send query q to Src at time \u03c4; 3. Return data: On receiving answer a, return a to the smart contract. These simple instructions, correctly carried out, de\ufb01ne a strong, meaningful, but simple notion of security. Intuitively, they dictate that ORACLE acts as a trustworthy bridge between Src and USER-SC.2 For example, if Src is https:www.FountOfKnowledge.com, \u03c4 is 4 p.m., and q  price for ticker INTC, the integrity of ORACLE guarantees that it will provide USER-SC with exactly the price of INTC as queried at 4 p.m. at Con\ufb01dentiality is another desirable property for oracles. As USER-SC sends Req to ORACLE in the clear on the blockchain, Req is public. There are many situations in which Req is sensitive and its publication could be harmful.",
      "If USER-SC is a \ufb02ight insurance contract, for example, and sends ORACLE a query Req regarding a particular users \ufb02ight (q  Ether Air Flight 338), the result would be that a users \ufb02ight plans are revealed to the whole world. If USER-SC is a contract for 2Of course, many details are omitted here. ORACLE should communicate with both USER-SC and source Src over secure, i.e., tamperproof, channels. (If Src is a web server, TLS is required. To communicate with USER-SC, ORACLE must be sure to scrape the right blockchain and digitally sign A appropriately.) \ufb01nancial trading, Req could leak information about a users trades and portfolio. There are many other examples, of course. To protect the con\ufb01dentiality of Req, we can require that data in Req be encrypted under a (public key) belonging to ORACLE. Continuing to leverage the TTP nature of ORACLE, we could then simply give ORACLE the information-\ufb02ow constraint: Upon decrypting Req, never reveal or use data in Req except to query Src.",
      "There are other important oracle properties, such as availability, the last of the classical CIA (Con\ufb01dentiality-Integrity-Availability) triad. A truly ideal service OR- ACLE, of course, would never go down. Availability also encompasses more subtle properties such as censorship resistance: An honest ORACLE will not single out par- ticular smart contracts and deny their requests. The concept of a trusted third party is similar to the notion of an ideal function- ality 7 used to prove the security of cryptographic protocols in certain models. We can also model a blockchain in similar terms, conceptualizing it in terms of a TTP that maintains an ideal bulletin board. Its instructions are to accept transactions, validate them, serialize them, and maintain them permanently on the bulletin board, an append-only data structure. Why the ideal oracle (ORACLE) is hard to achieve. There is, of course, no perfectly trustworthy data source Src.",
      "Data may be benignly or maliciously corrupted due to faulty web sites, cheating service providers, or honest mistakes. If Src isnt trustworthy, then even if ORACLE does operate exactly like a TTP as in- structed above, it still doesnt completely meet the notion of security we want. Given a faulty source Src, the integrity property de\ufb01ned above no longer means that an oracles answer a is correct. If the true price of Intel is 40 and https:www.FountOfKnowledge.com misreports it as 50, for example, then ORACLE will send the incorrect value a  50 to USER-SC. This problem is unavoidable when using a single source Src. ORACLE simply has no way to know whether the answers Src provides to its queries are correct. A bigger issue, of course, is the fact that our TTP for ORACLE is just an abstrac- tion. No service provider is unconditionally trustworthy. Even the best-intentioned may be buggy or hacked.",
      "So there is no way to for a user or smart contract to have absolute assurance that a service ORACLE will carry out its instructions faithfully. ChainLink reasons about its security protocols in terms of this ideal functionality ORACLE. Our goal in ChainLink is to achieve a real world system with properties as close as possible to those of ORACLE under realistic trust assumptions. We now explain how. For simplicity in what follows, we now denote by CHAINLINK-SC the complete set of ChainLink contracts, i.e., its full on-chain functionality (not just its interface to requesting contracts). We thereby abstract away the multiple individual contracts actually used in the system architecture. ChainLink Decentralization Approach We propose three basic complementary approaches to ensuring against faulty nodes: (1) Distribution of data sources; (2) Distribution of oracles; and (3) Use of trusted hardware. We discuss the \ufb01rst two approaches, which involve decentralization, in this section.",
      "We discuss our long-term strategy for trusted hardware, a di\ufb00erent and complementary approach, in Section 6. Distributing sources A simple way to deal with a faulty single source Src is to obtain data from multiple sources, i.e., distribute the data source. A trustworthy ORACLE can query a collection of sources Src1, Src2, . . . , Srck, obtain responses a1, a2, . . . , ak, and aggregate them into a single answer A  agg(a1, a2, . . . , ak). ORACLE might do this in any of a number of ways. One, for example, is majority voting. If a majority of sources return the identical value a, the function agg returns a; otherwise it returns an error. In this case, provided that a majority ( k2) sources are functioning correctly, ORACLE will always return a correct value A. Many alternative functions agg can ensure robustness against erroneous data or handle \ufb02uctuations in data values over time (e.g, stock prices).",
      "For example, agg might discard outliers (e.g., the largest and smallest values ai) and output the mean of the remaining ones. Of course, faults may be correlated across data sources in a way that weakens the assurances provided by aggregation. If site Src1  EchoEcho.com obtains its data from Src2  TheHorsesMouth.com, an error at Src2 will always imply an error at Src1. More subtle correlations between data sources can also occur. Chainlink also proposes to pursue research into mapping and reporting the independence of data sources in an easily digestible way so that oracles and users can avoid undesired correlations. Distributing oracles Just as sources can be distributed, our ideal service ORACLE itself can be approxi- mated as a distributed system. This is to say that instead of a single monolithic oracle node O, we can instead have a collection of n di\ufb00erent oracle nodes O1, O2, . . . , On.",
      "Each oracle Oi contacts its own distinct set of data sources which may or may not Figure 3: Requests are distributed across both oracles and data sources. This \ufb01gure shows an example of such two-level distribution. overlap with those of other oracles. Oi aggregates responses from its data sources and outputs its own distinct answer Ai to a query Req. Some of these oracles may be faulty. So clearly the set of all oracles answers A1, A2, . . . , An will need to be aggregated in a trustworthy way into a single, author- itative value A. But given the possibility of faulty oracles, where and how will this aggregation happen in ChainLink? Initial solution: In-contract aggregation. Our initial proposed solution in ChainLink will be a simple one called in-contract aggregation. CHAINLINK-SC which, again, denotes the on-chain part of ChainLinkwill itself aggregate oracle responses.",
      "(Alternatively, CHAINLINK-SC may call another aggregation contract, but for conceptual simplicity we assume that the two components form a single contract.) In other words, CHAINLINK-SC will compute A  Agg(A1, A2, . . . , An) for some func- tion Agg (similar to agg, as described above), and send the result A to USER-SC. This approach is practical for small n, and has several distinct bene\ufb01ts:  Conceptual simplicity: Despite the fact that the oracle is distributed, a single entity, CHAINLINK-SC, performs aggregation by executing Agg. Trustworthiness: As CHAINLINK-SCs code can be publicly inspected, its correct behavior can be veri\ufb01ed. (CHAINLINK-SC will be a relatively small, simple piece of code.) Additionally, CHAINLINK-SCs execution is fully visible on- chain. Thus users, i.e., creators of USER-SC, can achieve a high degree of trust in CHAINLINK-SC. Flexibility: CHAINLINK-SC can implement most desired aggregation functions Aggthe majority function, averaging, etc.",
      "Simple as it is, this approach presents a novel and interesting technical challenge, namely the problem of freeloading. A cheating oracle Oz can observe the response Ai of another oracle Oi and copy it. In this way, oracle Oz avoids the expense of querying data sources, which may charge per-query fees. Freeloading weakens security by undermining the diversity of data source queries and also disincentivizes oracles from responding quickly: Responding slowly and freeloading is a cheaper strategy. We suggest a well known solution to this problem, namely the use of a commit  reveal scheme. In a \ufb01rst round, oracles send CHAINLINK-SC cryptographic commit- ments to their responses. After CHAINLINK-SC has received a quorum of responses, it initiates a second round in which oracles reveal their responses. Algorithm 1 shows a simple sequential protocol that guarantees availability given 3f  1 nodes. It uses a commit  reveal scheme to prevent freeloading.",
      "Oracle responses are decommitted, and thus exposed to a potential freeloader only after all commitments have been made, thereby excluding the freeloader from copying other oracles responses. On-chain protocols can leverage block times to support synchronous protocol de- signs. In ChainLink, however, oracle nodes obtain data from sources that may have highly variable response times, and decommitment times by nodes can vary due to, e.g., use of di\ufb00erent gas prices in Ethereum. To ensure the fastest possible protocol responsiveness, therefore, Alg. 1 is designed as an asynchronous protocol. Here, Commitr(A) denotes a commitment of value A with witness r, while SID denotes the set of valid session ids. The protocol assumes authenticated channels among all players. It is easy to see that Alg. 1 will terminate successfully. Given 3f  1 nodes in total, at most f are faulty, so at least 2f  1 will send commitments in Step 4.",
      "Of those commitments, at most f come from faulty nodes, so at least f  1 come from honest nodes. All such commitments will eventually be decommitted. Additionally, it is easy to see that A will be correct in Alg.1. Of the f  1 decommitments on the single value A, at least one has to come from an honest node. In-contract aggregation via Alg. 1 will be the main approach supported by Chain- Link in the short term. The proposed initial implementation will involve a more so- phisticated, concurrent variant of the algorithm. Our longer-term proposal is re\ufb02ected in the rather more complicated protocol OCA (O\ufb00-Chain Aggregation) speci\ufb01ed in Algorithms 2 and 3 in Appendix A. OCA is an o\ufb00-chain aggregation protocol that Algorithm 1 InChainAgg(Oin i1) (code for CHAINLINK-SC) 1: Wait until Req is received from USER-SC. 2: sid  SID 3: Broadcast (request, sid). 4: Wait until set C of 2f 1 messages (commit, ci  Commitri(Ai), sid) from distinct Oi are received. 5: Broadcast (committed, sid).",
      "6: Wait until set D of f  1 distinct valid decommitments (decommit, (ri, Ai), sid) are received where, for some A, all Ai  A. 7: Send (Answer, A, sid) to USER-SC. minimizes on-chain transaction costs. That protocol also includes payment to oracle nodes and ensures against payments to freeloaders. Medium-term strategy: O\ufb00-chain aggregation. In-contract aggregation has a key disadvantage: Cost. It incurs the cost of transmitting and processing on- chain O(n) oracle messages (commits and reveals for A1, A2, . . . , An). In permissioned blockchains, this overhead may be acceptable. In permissionless blockchains with on- chain transaction fees such as Ethereum, if n is large, the costs can be prohibitive. A more cost-e\ufb00ective approach is to aggregate oracle responses o\ufb00-chain and transmit a single message to CHAINLINK-SC A. We propose deployment of this approach, called o\ufb00-chain aggregation, in the medium-to-long term.",
      "The problem of achieving a consensus value A in the face of potentially faulty nodes is much like the problem of consensus that underpins blockchains themselves. Given a predetermined set of oracles, one might consider using a classical Byzantine Fault Tol- erant (BFT) consensus algorithm to compute A. Classical BFT protocols, however, aim to ensure that at the end of a protocol invocation, all honest nodes store the same value, e.g., in a blockchain, that all nodes store the same fresh block. In our oracle setting, the goal is slightly di\ufb00erent. We want to ensure that CHAINLINK-SC (and then USER-SC) obtains aggregate answer A  Agg(A1, A2, . . . , An) without partici- pating in the consensus protocol and without needing to receive answers from multiple oracles. The problem of freeloading, moreover, still needs to be addressed. The ChainLink system proposes the use of a simple protocol involving thresh- old signatures.",
      "Such signatures can be realized using any of a number of signature schemes, but are especially simple to implement using Schnorr signatures 4. In this approach, oracles have a collective public key pk and a corresponding private key sk that is shared among O1, O2, . . . , On in a (t, n)-threshold manner 3. Such a sharing means that every node Oi has a distinct private  public keypair (ski, pki). Oi can Figure 4: SigskA can be achieved by any n21 of the oracles. generate a partial signature \u03c3i  SigskiAi that can be veri\ufb01ed with respect to pki. The key feature of this setup is that partial signatures on the same value A can be aggregated across any set of t oracles to yield a single valid collective signature \u03a3  SigskA on an answer A. No set of t 1 oracles, however, can produce a valid signature on any value. The single signature \u03a3 thus implicitly embodies the partial signatures of at least t oracles.",
      "Threshold signatures can be realized na\u00efvely by letting \u03a3 consist explicitly of a set of t valid, independent signatures from individual nodes. Threshold signatures have similar security properties to this na\u00efve approach. But they provide a signi\ufb01cant on- chain performance improvement: They reduce the size and cost of verifying \u03a3 by a factor of t. With this setup, it would seem that oracles can just generate and broadcast partial signatures until t such partial signatures enable the computation of \u03a3. Again, though, the problem of freeloading arises. We must therefore ensure that oracles genuinely obtain data from their designated sources, rather than cheating and copying Ai from another oracle. Our solution involves a \ufb01nancial mechanism: An entity PROVIDER (realizable as a smart contract) rewards only oracles that have sourced original data for their partial signatures. In a distributed setting, determining which oracles qualify for payment turns out to be tricky.",
      "Oracles may intercommunicate o\ufb00-chain and we no longer have a sin- gle authoritative entity (CHAINLINK-SC) receiving responses and are therefore un- able to identify eligible payees directly among participating oracles. Consequently, PROVIDER must obtain evidence of misbehavior from the oracles themselves, some of which may be untrustworthy. We propose the use of consensus-like mechanisms in our solution for ChainLink to ensure that PROVIDER does not pay freeloading oracles. The o\ufb00-chain aggregation system we propose for ChainLink, with accompanying security proof sketches, may be found in Appendix A. It makes use of a distributed protocol based on threshold signatures that provides resistance to freeloading by f  n3 oracles. We believe resistance to freeloading is an interesting new technical problem.",
      "ChainLink Security Services Thanks to the protocols we have just described in the previous section, ChainLink proposes to ensure availability and correctness in the face of up to f faulty oracles. Additionally, trusted hardware, as discussed in Section 6, is being actively considered as a secure approach toward protecting against corrupted oracles providing incorrect responses. Trusted hardware, however, may not provide de\ufb01nitive protection for three reasons. First, it will not be deployed in initial versions of the ChainLink network. Second, some users may not trust trusted hardware (see Appendix B for a discussion). Finally, trusted hardware cannot protect against node downtime, only against node misbehavior. Users will therefore wish to to ensure that they can choose the most reliable oracles and minimize the probability of USER-SC relying on  f faulty oracles.",
      "To this end, we propose the use of four key security services: a Validation System, a Reputation System, a Certi\ufb01cation Service, and a Contract-Upgrade Service. All of these services may initially be run by one company or group interested in launching the ChainLink network, but are designed to operate strictly accordingly to ChainLinks philosophy of decentralized design. ChainLinks proposed security services cannot block oracle node participation or alter oracle responses. The \ufb01rst three services only provide ratings or guidance to users, while the Contract-Upgrade Service is entirely optional for users. Additionally, these services are designed to support independent providers, whose participation should be encouraged so that users will eventually have multiple security services among which to choose. Validation System The ChainLink Validation System monitors on-chain oracle behavior, providing an objective performance metric that can guide user selection of oracles.",
      "It will seek to monitor oracles for:  Availability: The Validation System should record failures by an oracle to re- spond in a timely way to queries. It will compile ongoing uptime statistics. Correctness: The Validation System should record apparent erroneous responses by an oracle as measured by deviations from responses provided by peers.3 In our initial, on-chain aggregation system in ChainLink, such monitoring is straightforward, as all oracle activity is visible to CHAINLINK-SC. Recall, however, that in the o\ufb00-chain aggregation system envisaged for ChainLink, its the oracles themselves that perform aggregation. Consequently, CHAINLINK-SC does not have direct visibility into oracle responses and cannot itself monitor avail- ability and correctness. Fortunately, oracles digitally sign their responses, and thus, as a side e\ufb00ect, gen- erate non-repudiable evidence of their answers.",
      "Our proposed approach will therefore be to realize the validation service as a smart contract that would reward oracles for submitting evidence of deviating responses. In other words, oracles would be incentivized to report apparently erroneous behavior. Availability is somewhat trickier to monitor, as oracles of course dont sign their failures to respond. Instead, a proposed protocol enhancement would require oracles to digitally sign attestations to the set of responses they have received from other oracles. The validation contract would then accept (and again reward) submission of sets of attestations that demonstrate consistent non-responsiveness by an underper- forming oracle to its peers. In both the on-chain and o\ufb00-chain cases, availability and correctness statistics for oracles will be visible on-chain.",
      "Users  developers will thus be able to view them in real time through an appropriate front end, such as a Dapp in Ethereum or an equivalent application for a permissioned blockchain. Reputation System The Reputation System proposed for ChainLink would record and publish user ratings of oracle providers and nodes, o\ufb00ering a means for users to evaluate oracle performance holistically. Validator System reports are likely to be a major factor in determining oracle reputations and placing these reputations on a \ufb01rm footing of trust. Factors beyond on-chain history, though, can provide essential information about oracle node 3Deviation must be de\ufb01ned in a data-speci\ufb01c manner. For simple boolean responsesfor ex- ample, whether a \ufb02ight arrived on timedeviation simply means a response opposite that of the majority. For, say, the temperature of a city, which may vary legitimately across sensors and sources, deviation may mean signi\ufb01cant numerical deviation.",
      "Of course, for various reasons, e.g., broken sen- sors, even a well-functioning oracle may deviate from the majority answer some fraction of the time. security pro\ufb01les. These may include users familiarity with oracles brands, operating entities, and architectures. We envision the ChainLink Reputation System to include a basic on-chain component where users ratings would be available for other smart contracts to reference. Additionally, reputation metrics should be easily accessible o\ufb00-chain where larger amounts of data can be e\ufb03ciently processed and more \ufb02exibly weighted. For a given oracle operator, the Reputation System is initially proposed as sup- porting the following metrics, both at the granularity of speci\ufb01c assignment types (see Section 2), and also in general for all types supported by a node:  Total number of assigned requests: The total number of past requests that an oracle has agreed to, both ful\ufb01lled and unful\ufb01lled.",
      "Total number of completed requests: The total number of past requests that an oracle has ful\ufb01lled. This can be averaged over number of requests assigned to calculate completion rate. Total number of accepted requests: The total number of requests that have been deemed acceptable by calculating contracts when compared with peer responses. This can be averaged over total assigned or total completed requests to get insight into accuracy rates. Average time to respond: While it may be necessary to give oracle responses time for con\ufb01rmation, the timeliness of their responses will be helpful in determin- ing future timeliness. Average response time is calculated based on completed requests. Amount of penalty payments: If penalty payments were locked in to assure a node operators performance, the result would be a \ufb01nancial metric of an oracle providers commitment not to engage in an exit scam attack, where the provider takes users money and doesnt provide services.",
      "This metric would involve both a temporal and a \ufb01nancial dimension. High-reputation services are strongly incentivized in any market to behave cor- rectly and ensure high availability and performance. Negative user feedback will pose a signi\ufb01cant risk to brand value, as do the penalties associated with misbehavior. Consequently, we anticipate a virtuous circle in which well-functioning oracles de- velop good reputations and good reputations give rise to incentives for continued high performance. Certi\ufb01cation Service While our Validation and Reputation Systems are intended to address a broad range of faulty behaviors by oracles and is proposed as a way to ensure system integrity in the vast majority of cases, ChainLink may also include an additional mechanism called a Certi\ufb01cation Service. Its goal is to prevent andor remediate rare but catastrophic events, speci\ufb01cally en bloc cheating in the form of Sybil and mirroring attacks, which we now explain. Sybil and mirroring attacks.",
      "Both our simple and in-contract aggregation proto- cols seek to prevent freeloading in the sense of dishonest nodes copying honest nodes answers. But neither protects against Sybil attacks 9. Such attacks involve an ad- versary that controls multiple, ostensibly independent oracles. This adversary can attempt to dominate the oracle pool, causing more than f oracles to participate in the aggregation protocol and provide false data at strategic times, e.g., in order to in\ufb02uence large transactions in high-value contracts. Quorums of cheating oracles can also arise not just under the control of a single adversary, but also through collusion among multiple adversaries. Attacks or faults involving  f oracles are especially pernicious in that they are undetectable from on-chain behavior alone.",
      "Additionally, to reduce operational costs, a Sybil attacker can adopt a behavior called mirroring, in which it causes oracles to send individual responses based on data obtained from a single data-source query. In other words, misbehaving oracles may share data o\ufb00-chain but pretend to source data independently. Mirroring bene\ufb01ts an adversary whether or not it chooses to send false data. It poses a much less serious security threat than data falsi\ufb01cation, but does slightly degrade security in that it eliminates the error correction resulting from diversi\ufb01ed queries against a given source Src. For example, if https:www.datasource.com emits erroneous data due to, say, a sporadically triggered bug, multiple queriers may still obtain a correct majority result. Sybil attacks resulting in false data, mirroring, and collusion in general may be eliminated by the use of trusted hardware in our long-term strategy (see Section 6). Certi\ufb01cation Service design.",
      "The ChainLink Certi\ufb01cation Service would seek to provide general integrity and availability assurance, detecting and helping prevent mirroring and colluding oracle quorums in the short-to-medium term. The Certi\ufb01ca- tion Service would issue endorsements of high-quality oracle providers. We emphasize again, as noted above, that the service will only rate providers for the bene\ufb01t of users. It is not meant to dictate oracle node participation or non-participation in the system. The Certi\ufb01cation Service supports endorsements based on several features of or- acle deployment and behavior. It would monitor the Validation System statistics on oracles and perform post-hoc spot-checking of on-chain answersparticularly for high-value transactionscomparing them with answers obtained directly from rep- utable data sources.",
      "With su\ufb03cient demand for an oracle providers data, we expect there to be enough economic incentive to justify o\ufb00-chain audits of oracle providers, con\ufb01rming compliance with relevant security standards, such as relevant controls in the Cloud Security Alliance (CSA) Cloud Controls Matrix 26, as well as providing useful security information that they conduct proper audits of oracles source and bytecode for their smart contracts. In addition to the reputation metrics, automated on-chain and automated o\ufb00- chain systems for fraud detection, the Certi\ufb01cation Service is planned as a means to identify Sybil attacks and other malfeasance that automated on-chain systems cannot. For example, if all nodes agree that the moon is made of green cheese, they can cause USER-SC to ingest this false fact. MOON COMPONENTS  GREEN CHEESE will be recorded on the blockchain, however, and visible in a post-hoc review.",
      "Contract-Upgrade Service As recent smart contract hacks have shown, coding bulletproof smart contracts is an extremely challenging exercise 1, 20, 22. And even if a smart contract has been correctly programmed, environmental changes or bugs can still result in vulnerabili- ties, e.g., 2. For this reason, we propose a Contract-Upgrade Service. We emphasize that use of this service is entirely optional and in control of users. In the short term, if vulnerabilities are discovered, the Contract-Upgrade Service would simply make a new set of supporting oracle contracts available in ChainLink. Newly created requesting smart contracts will then be able to migrate to the new set of oracle contracts. Unfortunately, though, existing ones would be stuck with the old, potentially vulnerable set.",
      "In the longer term, therefore, CHAINLINK-SC would support a \ufb02ag (MIGFLAG) in oracle calls from requesting contracts indicating whether or not a call should be forwarded to a new CHAINLINK-SC should one become available. by default (i.e., if the \ufb02ag is missing) to false, MIGFLAG would enable requesting contracts to bene\ufb01t from automatic forwarding and thus migration to the new version of CHAINLINK-SC. In order to activate forwarding, a user will cause her requesting contract to issue ChainLink requests with MIGFLAG  true. (Users can engineer their smart contracts so that they change this \ufb02ag upon receiving an instruction to do so on-chain from an authorized contract administrator.) Migration of users to new oracle contracts functions as a kind of escape hatch, something long advocated for by blockchain researchers (see, e.g., 23) as a mechanism to \ufb01x bugs and remediate hacks without resorting to such cumbersome approaches as whitehat hacking 1 or hard forks.",
      "Migration to the updated contracts will be visible on the blockchain, and available to audit for users to review before upgrading. We recognize nonetheless that some users will not feel comfortable with any one group controlling an escape hatch in the form of migration  forwarding. Forced migration could empower the migrating contracts controller, or a hacker who com- promises relevant credentials, to undertake malicious activity, such as changing oracle responses. It is for this reason that requesting contracts have full control of the for- warding feature and can thus opt out of escape-hatch activation. Additionally, in accordance with ChainLinks focus on decentralization, we expect that providers will be able to support multiple versions of CHAINLINK-SC developed by the community.",
      "LINK token usage The ChainLink network utilizes the LINK token to pay ChainLink Node operators for the retrieval of data from o\ufb00-chain data feeds, formatting of data into blockchain readable formats, o\ufb00-chain computation, and uptime guarantees they provide as op- erators. In order for a smart contract on networks like Ethereum to use a ChainLink node, they will need to pay their chosen ChainLink Node Operator using LINK tokens, with prices being set by the node operator based on demand for the o\ufb00-chain resource their ChainLink provides, and the supply of other similar resources. The LINK to- ken is an ERC20 token, with the additional ERC223 transfer and call functionality of transfer(address,uint256,bytes), allowing tokens to be received and processed by contracts within a single transaction.",
      "Long-Term Technical Strategy The long-term technical strategy for ChainLink proposed in this whitepaper includes three key directions: Oracle con\ufb01dentiality, infrastructure changes, and o\ufb00-chain com- putation. Con\ufb01dentiality A distributed oracle network aims to o\ufb00er a high degree of protection against faulty oracles. In most deployment scenarios, it seeks to attain a correct response in the face of f Byzantine faults (for f  n2 in our simple aggregation protocol). Trusted hardware can o\ufb00er much more and is proposed as a better approach to securing the ChainLink network. Trusted hardware is the keystone of the Town Crier (TC) oracle 24, which is currently operating on the Ethereum mainnet 33 and whose creators partnered with SmartContract in the TC launch. Certain forms of trusted hardware, most notably Intels recent Software Guard eXtensions (SGX) set of instruction-set architecture extensions 1215, 18, seek to provide a powerful adjunct to distributed forms of trust.",
      "Brie\ufb02y, SGX permits an application to be executed in an environment called an enclave that claims two critical security properties. First, enclaves protect the integrity of the application, meaning its data, code, and control \ufb02ow, against subversion by other processes. Second, an enclave protects the con\ufb01dentiality of an application, meaning that its data, code, and execution state are opaque to other processes. SGX seeks to protect enclaved applications even against a malicious operating system, and thus against even the administrator of the host on which an application is running. While alternative forms of trusted hardware, such as ARM TrustZone, have been in existence for some time, SGX provides an additional key feature lacking in these technologies. It enables a platform to generate an attestation to the execution of a particular application (identi\ufb01ed by a build of its hash state).",
      "This attestation can be veri\ufb01ed remotely and allows a speci\ufb01c application instance to be bound to a public key and thus to establish authenticated and con\ufb01dential channels with other parties. Running an oracle in an enclave and distributing attestations can provide very strong assurance that the oracle is executing a particular application, speci\ufb01cally one created or endorsed by developers in the ChainLink ecosystem. Additionally, an oracle running in an enclave that can connect to a data source via HTTPS can provide a strong assurance that the data it retrieves has not been tampered with. (See 24, 33 for details.) These properties go a long way toward protecting against oracle misbehavior in the sense of data corruption, Sybil attacks, etc. A still greater opportunity, however, lies in the ability of trusted hardware to provide strong con\ufb01dentiality. The need for con\ufb01dentiality is in general one of the main hurdles to blockchain deployment.",
      "Con\ufb01dentiality-preserving oracles can be instrumental in solving the problem. Why distributed oracles dont ensure con\ufb01dentiality. Con\ufb01dentiality is fun- damentally hard to achieve in any oracle system. If an oracle has a blockchain front end such as a smart contract, then any queries to the oracle will be publicly vis- ible. Queries can be encrypted on-chain and decrypted by the oracle service, but then the oracle service itself will see them. Even heavyweight tools such as secure multiparty computation, which permits computation over encrypted data, cant solve this problem given existing infrastructure. (See, e.g.,11 for an application-oriented perspective.) At some point a server needs to send a query to a target data source server. Thus it must see the query, irrespective of whatever con\ufb01dentiality the query previously enjoyed. It will also see the response to the query. Con\ufb01dentality-preserving oracles via SGX.",
      "An oracle using SGX can ingest and process data within an enclave, in essence acting like a TTP trusted for integrity and con\ufb01dentiality. To begin with, such an oracle can decrypt queries within its enclave. It can then process them without exposing them to any other process (or any human being). The enclave can also process data from sources con\ufb01dentially and can securely manage sensitive information such as user credentials, a powerful capability, as we illustrate below. The Town Crier system supports con\ufb01dential \ufb02ight data queries. Flight infor- mation can be passed to a TC smart contract front end encrypted under the public key of the TC service. TC decrypts the query and then contacts a data source (e.g., \ufb02ightaware.com) over HTTPS. It returns to the querying smart contract a simple yesno answer to the question Has this \ufb02ight been delayed? and exposes no other information on-chain. An even more interesting TC capability is its support for trading on the Steam gaming platform.",
      "TC can securely ingest user credentials (passwords) to check that game ownership has been transferred from a buyer to a seller. It can thereby create a secure marketplace that would be otherwise unachievable, with high assurance fair swaps of cryptocurrency for digital goods. (A simple distributed oracle, in contrast, could not securely manage users passwords on their behalf.) TC can also perform trusted o\ufb00-chain aggregation of data from multiple sources, as well as trusted computation over data from multiple sources (e.g., averaging) and interactive querying of data sources (e.g., searching the database of one source in response to the answer of another). Trusted hardware o\ufb00ers an exciting new approach to the scalable usage of blockchains 24, 29, in which large portions of blockchain infrastructure, including smart contracts, execute in enclaves.",
      "Such an architecture would combine transparency bene\ufb01ts of blockchains with the con\ufb01dentiality properties of o\ufb00-chain execution and trusted hardware. While similar ideas have been suggested using other techniques, such as zk-SNARKs 21, trusted hardware is far more practical (and less complicated). Our current research agenda includes this expansive vision, with oracles as a catalyzing service. We brie\ufb02y discuss the issue of Intel as a root of trust in Appendix B. De\ufb01ning security given SGX. It is possible, given the use of trusted hardware, to de\ufb01ne oracle correctness more formally, starting with the formalism for Intel SGX proposed in 32. This formalism allows SGX to be treated as a global Universally Composable (UC) 6 functionality Fsgx(\u03a3sgx)progencl, R. Here, and in what follows, \u03a3 denotes a signature scheme with signing and veri\ufb01cation functions \u03a3.Sign and \u03a3.Verify. An instance of Fsgx(\u03a3sgx)progencl, R is parameterized by a group signature scheme \u03a3sgx.",
      "Argument progencl denotes the program running in an enclave, i.e., environment protected by the hardware. R denotes the untrusted code running on an SGX host, i.e., the software that calls the application running in the enclave. Figure 5 (taken from 24) shows the operation of the functionality Fsgx. Upon ini- tialization, it runs outp : progencl.Initialize(), generating and attestation to the code of progencl and outp. An attestation \u03c3att is a digitally signed statement by the platform that progencl is running in an enclave and has yielded output outp. In typical usage, progencl.Initalize() outputs an instance-speci\ufb01c public key that can be used to create a secure channel to the application instance. Upon a resume call with (id, params), Fsgx continues execution and outputs the result of progencl.Resume(id, params), where id denotes a session identi\ufb01er and params denotes parameters input to progencl.",
      "Fsgxprogencl, R: abstraction for SGX Hardcoded: sksgx (private key for \u03a3sgx) Assume: progencl has entry points Initialize and Resume Initialize: On receive (init) from R: Let outp : progencl.Initalize() \u03c3att : \u03a3sgx.Sign(sksgx, (progencl, outp)) Output (outp, \u03c3att) Resume: On receive (resume, id, params) from R: Let outp : progencl.Resume(id, params) Output outp Figure 5: Formal abstraction for SGX execution capturing a subset of SGX features.. Given the formalism of Figure 5, it is possible to precisely de\ufb01ne the integrity of an oracle. De\ufb01nition 1 does so in a slight generalization of the de\ufb01nition given in 24 that we call Authenticity of Oracle. De\ufb01nition 1 (Authenticity of Oracle).",
      "We say that an oracle O running program progencl using Fsgx and outputting instance key pkO satis\ufb01es Authenticity of Oracle if, for any polynomial-time adversary A that can interact arbitrarily with Fsgx, A cannot cause an honest veri\ufb01er to accept (pkO, \u03c3att, params : (url, , T), data, \u03c3) where data is not the contents of url with the public key at time T (progencl.Resume(id, params) in our model). More formally, for any probabilistic polynomial-time adversary A, (pkO, \u03c3att, id, params, data, \u03c3) AFsgx(1\u03bb) : \u03a3sgx.Verify(pksgx, \u03c3att, (progencl, pkO))  1 \u03a3.Verify(pkO, \u03c3, (id, params, data))  1 data  progencl.Resume(id, params) negl(\u03bb), for security parameter \u03bb. Infrastructure changes Many of the challenges in constructing secure oracles arise from the fact that existing data sources dont digitally sign the data they serve. If they did, then oracles would not need to be trusted to refrain from tampering with data.",
      "HTTPS, the protocol for secure web communications, does not enable data signing. It does, though, have an underlying public-key infrastructure (PKI) that requires servers to possess certi\ufb01cates that could in principle support data signing. This observation is the basis of TLS-N, a TLS extension, that allows HTTPS servers to sign portions of their sessions with clients. The selective nature of the signing provides other nice features, such as the ability for clients to exclude from signed transcripts and thus protect the con\ufb01dentiality of credentials (e.g., passwords) they use to connect to servers. We believe infrastructure changes such as TLS-N are promising approaches to supporting oracle security. They will probably need to be used in concert with other technologies such as SGX, however, because of the following limitations: 1. Infrastructure modi\ufb01cations: Unfortunately, until and unless TLS-N becomes a standard, data sources must expressly deploy it for clients to bene\ufb01t.",
      "Few data sources are likely to in the near future. 2. Aggregation and computation: TLS-N cannot support aggregation or other forms of trusted computation over data from data sources, so some trusted mechanism will still be required to accomplish these tasks. 3. Cost: Veri\ufb01cation of TLS-N-signed data incurs relatively high on-chain costs compared with simple signature veri\ufb01cation. 4. Con\ufb01dentiality: TLS-N cannot support out-of-band con\ufb01dential management of user credentials or queries, but instead requires users to query a data source themselves for this purpose. For example, con\ufb01dential \ufb02ight information cannot be stored in a smart contract for later con\ufb01dential automated query of a website. O\ufb00-chain computation Some intriguing uses of oracles, such as the use of credential-dependent APIs, require that an oracle do considerably more than just transmit data. It may need to manage credentials, log into accounts to scrape data, and so forth.",
      "Indeed, given truly trust- worthy and con\ufb01dential oracles, something that SGX-backed systems \u00e0 la Town Crier and techniques such as zero-knowledge proofs 21 can help achieve, the boundary between oracles and smart contracts may become \ufb02uid. ChainLink already supports a regex-based language for queries that enables users to \ufb02exibly specify the processing of o\ufb00-chain data. Our long-term strategy, however, seeks to create a world where oracles are a key o\ufb00-chain computation resource used by most smart contracts. We believe this will be enabled by building towards a model of fully general, private o\ufb00-chain computation within oracles whose results are consumed by smart contracts. If this can be achieved with strong security, as we believe it can, pushing costly and sensitive computation logic into oracles will result in better con\ufb01dentiality, lower contract execution costs, and more \ufb02exible architectures.",
      "Existing Oracle Solutions ChainLink is designed to \ufb01ll a pervasive need for new oracle technology in smart contract systems. Unfortunately today there is a very limited supply of highly secure and \ufb02exible oracle systems. We believe this lack of trustworthy oracles is a major impediment to the evolution of smart contracts. The most commonly used option for oracle services today are centralized oracle providers. This approach is problematic as it creates a centralized point of control, and thus does not meet the high standards of tamper resistance that trustless smart contracts require. Some such systems, e.g. 31, attempt to remedy this problem by relying on notarization, to prove correct behavior. This use of notarization services is worrisome in view of documented problems with these services 37, and the fact that their attestations cannot be feasibly veri\ufb01ed on-chain, resulting in a (potentially recursive) need for further veri\ufb01cation.",
      "Another approach to delivering trustworthy oracle data is to rely on manual human input of unstructured data. These manual-input oracle are commonly proposed for use in prediction-markets 17, 25, 28. By creating appropriate \ufb01nancial stakes and assuming economically rational players with limited \ufb01nancial incentives for cheating, such oracles provide a high assurance of correct crowd-sourced answers. This approach is decentralized and \ufb02exible. Since manual-input oracles obtain their responses from human beings, they can respond to questions for which structured data is hard to \ufb01nd, or di\ufb03cult to extract in a reliable way, e.g., requires natural language processing of news events. Unfortunately, though, because human cognition is costly and slow, manual-input oracles are resource-intensive, not real-time, and can handle only a limited set of questions at any given time.",
      "We believe that ChainLink could also be very useful for quickly and automatically resolving prediction-market contracts that can be resolved by structured data. A \ufb01nal approach is to change the form of data at the source. If a data source digi- tally signed the data it provided, then the relaying server wouldnt need to be trusted. USER-SC could simply check the signatures on data it receives. An excellent, general approach of this kind is provided by TLS-N, as discussed above. Unfortunately, as already mentioned, TLS-N requires changes to existing infrastructure. Conclusion We have introduced ChainLink, a decentralized oracle network for smart contracts to securely interact with resources external to the blockchain. We have outlined the ChainLink architecture, describing both on and o\ufb00-chain components. After de\ufb01ning security in the context of oracles, we described ChainLinks multilayered approach to decentralization.",
      "We proposed a novel protocol with new features such as protection against freeloading (with additional protocols and security-proof sketches in the paper appendix). We also laid out a roadmap for how ChainLink can harness technological and infrastructural advances, such as trusted hardware and digital signing of data by sources. Finally, having examined existing oracle solutions and their shortcomings, we have exposed the need today for a system such as ChainLink. Design Principles. As we continue our work on ChainLink, we will seek to prior- itize the following core values:  Decentralization for secure and open systems. Decentralization is not only the foundation of the tamperproof properties of blockchains, but the basis of their permissionless nature. By continuing to build decentralized systems, we aim to further enable permissionless development within the ecosystem.",
      "We believe that decentralization is a crucial component for a globally thriving ecosystem with long-term sustainability. Modularity for simple, \ufb02exible system design. We appreciate the philosophy of building small tools which do one thing well. Simple components can be easily reasoned about and thus securely combined into larger systems. We believe that modularity not only enables upgradable systems, but facilitates decentral- ization. Wherever key pieces of ChainLink depend on or are managed by too few parties, we will seek to design an ecosystem which allows for competing implementations to be used. Open source for secure, extensible systems. ChainLink is made possible by standing on the shoulders of many open source projects. We value the commu- nity and will continue to contribute by developing ChainLink in an open source manner. We plan to engage continually with developers, academics, and secu- rity experts for peer review.",
      "We encourage testing, audits, and formal proofs of security, all with the aim of creating a platform whose robustness and security can support future innovations. With these principles in mind, we look forward to extending the reach and impact of blockchains and smart contracts by making oracles a secure cornerstone of the ecosystem. Parity. The Multi-sig hack: A postmortem. https:blog.ethcore.iothe-multi- sig-hack-a-postmortem. 20 July 2017. Gun Sirer. Cross-Chain Replay Attacks. Hacking, Distributed blog. 17 July 2016. Adi Shamir. How to share a secret. In: Communications of the ACM 22.11 (1979), pp. 612613. Claus-Peter Schnorr. E\ufb03cient signature generation by smart cards. In: Journal of cryptology 4.3 (1991), pp. 161174. Rosario Gennaro, Stanislaw Jarecki, Hugo Krawczyk, et al. Secure distributed key generation for discrete-log based cryptosystems. In: Eurocrypt. Vol. 99. Springer. 1999, pp. 295310. R. Canetti.",
      "Universally Composable Security: A New Paradigm for Crypto- graphic Protocols. In: FOCS. 2001. Ran Canetti. Universally composable security: A new paradigm for crypto- graphic protocols. In: Foundations of Computer Science, 2001. Proceedings. 42nd IEEE Symposium on. IEEE. 2001, pp. 136145. Douglas R Stinson and Reto Strobl. Provably secure distributed Schnorr signa- tures and a (t, n) threshold scheme for implicit certi\ufb01cates. In: ACISP. Vol. 1. Springer. 2001, pp. 417434. John R Douceur. The sybil attack. In: International Workshop on Peer-to- Peer Systems. Springer. 2002, pp. 251260. 10 Aniket Kate and Ian Goldberg. Distributed key generation for the internet. In: Distributed Computing Systems, 2009. ICDCS09. 29th IEEE International Conference on. IEEE. 2009, pp. 119128. 11 Claudio Orlandi. Is multiparty computation any good in practice? In: Acous- tics, Speech and Signal Processing (ICASSP), 2011 IEEE International Con- ference on. IEEE. 2011, pp. 58485851.",
      "12 Ittai Anati, Shay Gueron, Simon Johnson, et al. Innovative technology for CPU based attestation and sealing. In: Proceedings of the 2nd International Workshop on Hardware and Architectural Support for Security and Privacy. Vol. 13. 2013. url: https :   software . intel . com  en - us  articles  innovative-technology-for-cpu-based-attestation-and-sealing (vis- ited on 05232016). 13 Matthew Hoekstra, Reshma Lal, Pradeep Pappachan, et al. Using Innovative Instructions to Create Trustworthy Software Solutions. In: Proceedings of the 2Nd International Workshop on Hardware and Architectural Support for Se- curity and Privacy. HASP 13. Tel-Aviv, Israel: ACM, 2013, 11:111:1. isbn: 978-1-4503-2118-1. doi: 10.11452487726.2488370. url: http:doi.acm. org10.11452487726.2488370. 14 Frank McKeen, Ilya Alexandrovich, Alex Berenzon, et al. Innovative instruc- tions and software model for isolated execution.",
      "In: Proceedings of the 2nd International Workshop on Hardware and Architectural Support for Security and Privacy. 2013, p. 10. url: http:css.csail.mit.edu6.8582015 readingsintel-sgx.pdf (visited on 05232016). 15 Intel. Intel Software Guard Extensions Programming Reference. 2014. (Visited on 05232016). 16 Gavin Wood. Ethereum: A secure decentralised generalised transaction ledger. In: Ethereum Project Yellow Paper (2014). 17 Jack Peterson and Joseph Krug. Augur: a decentralized, open-source platform for prediction markets. In: arXiv preprint arXiv:1501.01042 (2015). 18 Victor Costan and Srinivas Devadas. Intel SGX Explained. In: Cryptology ePrint Archive (2016). url: https :   eprint . iacr . org  2016  086 . pdf (visited on 05242016). 19 Victor Costan, Ilia A Lebedev, and Srinivas Devadas. Sanctum: Minimal Hard- ware Extensions for Strong Software Isolation. In: USENIX Security Sympo- sium. 2016, pp. 857874. 20 Kevin Delmolino, Mitchell Arnett, Ahmed Kosba, et al.",
      "Step by step towards creating a safe smart contract: Lessons and insights from a cryptocurrency lab. In: International Conference on Financial Cryptography and Data Security. Springer. 2016, pp. 7994. 21 Ahmed Kosba, Andrew Miller, Elaine Shi, et al. Hawk: The blockchain model of cryptography and privacy-preserving smart contracts. In: SP16. IEEE. 2016. 22 Loi Luu, Duc-Hiep Chu, Hrishi Olickel, et al. Making smart contracts smarter. In: Proceedings of the 2016 ACM SIGSAC Conference on Computer and Com- munications Security. ACM. 2016, pp. 254269. 23 Bill Marino and Ari Juels. Setting standards for altering and undoing smart contracts. In: International Symposium on Rules and Rule Markup Languages for the Semantic Web. Springer. 2016, pp. 151166. 24 Fan Zhang, Ethan Cecchetti, Kyle Croman, et al. Town Crier: An authenti- cated data feed for smart contracts. In: Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. ACM. 2016, pp. 270 282.",
      "25 Augur project page. https:augur.net. 2017. 26 CSA Cloud Controls Matrix. URL: https:cloudsecurityalliance.orggroupcloud- controls-matrix. 2017. 27 Mark Flood and Oliver Goodenough. Contract as Automaton: The Computa- tional Representation of Financial Agreements. https:www.financialresearch. govworking-papersfilesOFRwp-2015-04_Contract-as-Automaton- The-Computational-Representation-of-Financial-Agreements.pdf. Of- \ufb01ce of Financial Research, 2017. 28 Gnosis project page. https:gnosis.pm. 2017. 29 Hyperledger Sawtooth. https:intelledger.github.iointroduction.html. 2017. 30 Abhiram Kothapalli, Andrew Miller, and Nikita Borisov. SmartCast: An In- centive Compatible Consensus Protocol Using Smart Contracts. In: Financial Cryptography and Data Security (FC). 2017. 31 Oraclize project page. http:www.oraclize.it. 2017. 32 Rafael Pass, Elaine Shi, and Florian Tramer. Formal abstractions for attested execution secure processors. In: Eurocrypt. Springer. 2017, pp. 260289.",
      "33 Town Crier Ethereum service. http:www.town-crier.org. 2017. 34 Florian Tramer, Fan Zhang, Huang Lin, et al. Sealed-glass proofs: Using trans- parent enclaves to prove and sell knowledge. In: Security and Privacy (Eu- roSamp;P), 2017 IEEE European Symposium on. IEEE. 2017, pp. 1934. 35 Vitalik Buterin et al. Ethereum white paper. https:github.comethereum wikiwikiWhite-Paper. 36 JSON Schema. http:json-schema.org. 37 Hubert Ritzdorf, Karl W\u00fcst, Arthur Gervais, et al. TLS-N: Non-repudiation over TLS Enabling Ubiquitous Content Signing for Disintermediation. IACR ePrint report 2017578. URL: https:eprint.iacr.org2017578. Disclosures  Ari Juels is a faculty member at the Jacobs Institute at Cornell Tech. He co-authored this work in his separate capacity as an advisor to SmartContract ChainLink Ltd., in which he has a \ufb01nancial interest. This whitepaper is being provided by SmartContract ChainLink, Ltd. (SCCL), a British Virgin Islands corporation, in support of the ChainLink Platform.",
      "Secure Asset Exchange, Inc. (SAE) dba SmartContract.com, is providing administrative, technical and other support to SCCL, including support of SCCLs LINK Token sale, and is receiving compensation from SCCL. The technological, social and business structures utilizing blockchain technology are continually developing and will evolve for the foreseeable future. Accordingly, the plans, strategies and implementation de- tails described in this whitepaper will likely evolve as well and, accordingly, may never be adopted. SCCL and SAE reserve all rights to develop or pursue additional or al- ternative plans, strategies or implementation details associated with the ChainLink Platform. LINK Tokens are being sold by SCCL pursuant to the Terms and Conditions of the token sale terms available at https:link.smartcontract.comterms. For complete details, review the terms. LINK Tokens are not securities, investments or currency, and are not sold or marketed as such.",
      "Also: participating in the sale involves signi\ufb01- cant technological and systemic risks; the sale is not open to individuals who reside in or are citizens of the United States or Canada. The sale period, duration, pricing, and other provisions may change, as stated in the token sale terms. The LINK To- ken sale involves known and unknown risks, uncertainties, and other factors that may cause the actual functionality, utility, or levels of use of LINK Tokens to be materially di\ufb00erent from any projected future results, use, functionality or utility expressed or implied by SCCL in the terms. O\ufb00-Chain Aggregation To ensure both valid, signed answers and to prevent freeloading, our o\ufb00-chain aggre- gation protocol, discussed in Section 4.2, will rely on a simple distributed protocol based on threshold signatures 8. The bene\ufb01t of this approach is that for a given query, a single signature can be generated o\ufb00-chain by a collection of n oracle nodes.",
      "As a result, only a single authenticated message needs to be handled on-chain, instead of O(n) messages from distinct oracle nodes. This approach greatly reduces costs by comparison with those incurred by Algorithm 1. The idea can be further extended, as in 30, to aggregate the answers to multiple queries within a single threshold signature, an idea we dont explore here but may consider in our architecture. Suppose that f  n3 oracles are faulty and t  f  1. Faulty nodes may try to perform freeloading andor any of a range of other dishonest behaviors, such as signing invalid answers. Our complete protocol for o\ufb00-chain aggregation consists of a pair of algorithms OCA  (DistOracle, RewardOracles) for computing a signature SigskA on value A  Agg(A1, A2, . . . , An), for the majority function Agg. A simpli\ufb01ed, single-round version of these protocols is shown respectively in Algorithms 2 and 3.",
      "The \ufb01rst is executed by the participating oracles, while the second is executed by an entity PROVIDER that may, as mentioned above, take the form of a smart contract. Before presenting OCA, we give some basic background on Schnorr signatures and the threshold scheme for computing them given in 8. Schnorr signatures. The Schnorr signature scheme makes use of a group G of prime order p with generator g for which the discrete log problem is presumed to be hard. A users key pair takes the form (sk, pk)  (x, y  gx) for x  Z p , where p  Zp 0. Without loss of generality, we denote group operations multiplicatively. Schnorr signatures may be computed over elliptic curve groups, and indeed typically are in modern crypto implementations. Figure 6 shows the Schnorr signature scheme. Threshold Schnorr signature scheme. We make use of the threshold signature scheme of 8. This scheme generates a global private  public keypair (sk, pk).",
      "It enables threshold generation of a full signature Sigskm for a desired message m. In the initial key generation protocol, each player is assigned a key share xi  ski. This setup is a one-time operation and can be done using a distributed key generation protocol, e.g., 5 or, for an asynchronous setting, 10. To generate a signature, players (oracles in our setting) \ufb01rst perform a distributed key generation protocol, as for key-share generation at setup. The output of this protocol is a global ephemeral secret key e. Each player (Oi) receives a (secret) share ei of e. Schnorr signature: Signer input (m, sk  x); Veri\ufb01er input pk  y Signer Veri\ufb01er r  Zx e gr c  H(m  e) s  cx  e (e,s),m  c  H(m  e) gs ? ryc Figure 6: Schnorr signature scheme. A partial signature of Oi given ephemeral key e takes the form \u03c3(e)  cxi  ei, where c  H(m  e), as in the full signature. For each player Oi there also exists a function validi(\u03c3i; (pk, e)) that veri\ufb01es its partial signature.",
      "We refer to a partial signature as valid for Oi if it veri\ufb01es correctly under validi. We have somewhat modi\ufb01ed notation and greatly condensed the scheme for our exposition here. We refer the reader to 8 for details. OCA protocol We now present the DistOracle and RewardOracles algorithms that compose OCA. These are speci\ufb01ed below in Algorithms 2 and 3. Using condensed notation (rather than including witnesses explicitly, as in Algorithm 1) we let Commit denote a com- mitment function. Note that all players can see the messages received by CHAINLINK-SC, as it is on- chain. Let \u03a3be the \ufb01rst valid signature \u03a3 sent to CHAINLINK-SC. In Alg. 3, we let PSdenote a set of decommitments received by PROVIDER whose partial signatures yield \u03a3. (PScould come from the oracle that sent \u03a3, but need not.",
      "Any oracle with a partial signature in PSis incentivized to send PS.) Algorithm 2 DistOracle(f, n, i, ski  xi, pk, Src) (code for Oi) Jointly generate ephemeral key: 1: Execute distributed key-generation protocol and receive (ei, e). Obtain data: 2: Obtain Ai from Src. Generate partial signature: 3: Compute \u03c3(e) ( cxi  ei, for c  H(m  e), where m  Ai). Commit round: 4: Broadcast commitment commi  Commit(\u03c3(e) i , Ai); i 5: Wait until a set Ci of n f valid commitments from distinct oracles is received. 6: Send Ci to PROVIDER. Prepare round: 7: Broadcast prepared. 8: Wait until n f distinct prepared messages are received. Reveal  decommit round: 9: Broadcast decommitment of (\u03c3(e) i , Ai) for commi. 10: Wait until a set PS of t valid decommitments are received. Full signature computation: 11: if no valid \u03a3 has yet been received by CHAINLINK-SC then Aggregate partial signatures in PS into \u03a3  SigskA. Send \u03a3 to CHAINLINK-SC. Send PS to PROVIDER.",
      "15: end if Algorithm 3 RewardOracles (code for PROVIDER) 1: Wait until set C of n f commitment sets (Ci) from distinct oracles and PSare received. 2: for every oracle Oj do if \u03c3j PSand  2f sets in C include commitments to \u03c3j from Oj then Send reward to Oj. end if 6: end for Proof sketches We o\ufb00er proof sketches of the key properties of OCA. We show that assuming at most f faulty nodes, the protocol always generates a valid signature on a correct answer, and never rewards freeloading oracle nodes. Claim 2. The protocol OCA will never reward a freeloading node. Proof. (Sketch) Suppose that Oz is freeloading. Then it can broadcast a valid com- mitment only after time \u03c4, the time at which the \ufb01rst honest Oi decommits in Step 9 of Alg. 2. Oi has received n f prepared messages of which at least n 2f come from honest nodes. Let Oj denote one of these at least n 2f honest nodes.",
      "Oj will no longer accept commitments after sending a prepared message, so the set Cj of any such honest node Oj will no longer change after time \u03c4, and thus Cj will ex- clude a commitment with a correct partial signature \u03c3z from Oz. Therefore, at most n (n 2f)  2f sets in C in Alg. 3 will include Oz. Thus Oz will not receive a reward. Unfortunately, OCA cannot ensure that non-freeloading nodes are paid. A cheat- ing adversary can, after receiving a decommitment, rush honest nodes in Step 13 of Alg. 2 by generating its own partial signatures and including only one honest nodes partial signature in the generation of \u03a3. That honest nodes commitment may not have been included among the n f collected by any node. It could have come afterward. Claim 3. OCA will always result in a valid signature \u03a3  SigskA eventually being sent to CHAINLINK-SC. Proof. (Sketch) There are nf honest nodes, and f  n3, so there are  2f f 1 honest nodes, and thus at least t honest nodes. So Step 1 of Alg.",
      "2 will complete successfully. Similarly, since there are nf honest nodes, every oracle will eventually complete Step 7 of Alg. 2, sending a prepared message. Honest nodes will eventually then receive at least n f prepared messages and will decommit, permitting Step 13 to be completed by some honest node. Claim 4. Any valid signature SigskA received by CHAINLINK-SC in oca will be on a valid value A. Proof. (Sketch) It is easy to see that a valid signature SigskA includes a correct value A. As t partial signatures are needed to compute SigskA, and at most f  t nodes are faulty, at least one partial signature on A was provided by an honest node and thus it must be correct. Discussion OCA introduces a few design challenges that we brie\ufb02y discuss here. Payment for honest nodes. Unfortunately, while it penalizes freeloaders with non-payment, OCA is not able to guarantee conversely that honest nodes are paid.",
      "Indeed, even in the benign case where no nodes are faulty, unlucky message ordering can result in honest nodes that have contributed partial signatures to \u03a3 not receiving payment. This problem could be addressed in part by making Alg. 2 synchronous. Speci\ufb01- cally, Wait steps could require that nodes wait a period of time such that receipt of messages from honest nodes is guaranteed. In this case, all nodes with partial signatures incorporated into \u03a3 would be guaranteed payment. Side e\ufb00ects, however, would be slower execution and the challenge of setting correctly. The problem of designing an asynchronous protocol with strong payment guaran- tees is an open research problem that we are currently exploring. Redundant messages. OCA aims to minimize on-chain communication and ide- ally involves just one on-chain message, namely a signed response \u03a3, from the set of participating oracles.",
      "In practice, however, because a signature \u03a3 will not post imme- diately to the blockchain, multiple oracles could independently send signed responses to the blockchain. The best way to limit such redundant messages is for oracles to monitor not just the blockchain, but the mempool, i.e., pending messages. Key management. Of course, as is often the case, key management is a major challenge in protocols of this kind. The distribution of shares of sk can be performed in a distributed manner 5 and updates can be made to accommodate new nodes and remove departing nodes, as well as to provide proactive security, i.e., ongoing resilience against compromise of nodes keys. Additionally, nodes can be organized into distinct cliques to bound the size of n. We propose that ChainLink employ these techniques to ensure a \ufb02exible, responsive, and secure distributed oracle.",
      "SGX Trust Assumptions In Intels role of providing stronger assurance of correctness, SGX enhances but does not supplant other integrity protections in ChainLink. In other words, use of SGX makes the system strictly stronger. Trusting SGX for con\ufb01dentiality does require trust in Intel, but this trust is cir- cumscribed. Assuming that Intel does not have a backdoor in its CPUs enabling leakage of enclave data, it does not have a means of inspecting enclave state. (Such a backdoor is possible, but which would require the presence of physical evidence on every users machine and pose a serious reputational risk.) Intel or an adversary that corrupts Intels manufacturing processes could in prin- ciple falsify attestation keys (platform EPID keys). Such an adversary could generate EPID keys that are not embedded in SGX-enabled servers, but instead permit attes- tations to be generated in a non-SGX platform.",
      "In e\ufb00ect, this adversary could create bogus servers that generate valid-looking SGX attestations, but provide no protec- tions for enclaved code. Should there be more than f such nodes, of course, the adversary could corrupt oracle responses. More problematically, such nodes would expose sensitive data handled by oracle nodes to the adversary. The ability to fal- sify EPID keys, however, does not imply an ability to corrupt existing, valid SGX instances. It is also important to recognize that of course today, whether or not we like it, trust in Intel is inescapable. The CPU in the machine on which you are reading this paper bears witness to this factor, if not, the CPU in the server from which you downloaded this paper. Of course, it would be preferable to make use of trusted hardware from multiple vendors, and it is to be hoped that others will create equivalent capabilities.",
      "New, open architectures for trusted hardware, and ways to weaken the trust assumptions required of such hardware, are active areas of research, e.g., 19, 34. The ability to diversify across vendors or architectures per se would not ensure data con\ufb01dentiality, however. We are also interested in research into techniques for con\ufb01dentiality assurance in distributed networks through the use of cover tra\ufb03c."
    ],
    "word_count": 13490,
    "page_count": 38
  },
  "MANA": {
    "chunks": [
      "White paper Decentraland is a virtual reality platform powered by the Ethereum blockchain. Users can create, experience, and monetize content and applications. Land in Decentraland is permanently owned by the community, giving them full control over their creations. Users claim ownership of virtual land on a blockchain-based ledger of parcels. Landowners control what content is published to their portion of land, which is identified by a set of cartesian coordinates (x,y). Contents can range from static 3D scenes to interactive systems such as games. Land is a non-fungible, transferrable, scarce digital asset stored in an Ethereum smart contract. It can be acquired by spending an ERC20 token called MANA. MANA can also be used to make in-world purchases of digital goods and services. People are spending increasingly more time in virtual worlds, for both leisure and work1. This occurs predominantly in 2D interfaces such as the web and mobile phones.",
      "But a traversable 3D world adds an immersive component as well as adjacency to other content, enabling physical clusters of communities. Unlike other virtual worlds and social networks, Decentraland is not controlled by a centralized organization. There is no single agent with the power to modify the rules of the software, contents of land, economics of the currency, or prevent others from accessing the world. This document lays out the philosophical underpinnings, technical foundations, and economic mechanisms of Decentraland. Abstract Esteban Ordano estebandecentraland.org Ariel Meilich aridecentraland.org Yemel Jardi yemeldecentraland.org Manuel Araoz manueldecentraland.org A blockchain-based virtual world Decentraland Decentraland 114 1 The State of Mobile, Flurry Analytics Blog.",
      "https:flurrymobile.tumblr.compost155761509355 We would like to thank the following reviewers, whose contributions and feedback made this document possible: Jake Brukhman (Founder, CoinFund) Luis Cuende (Project Lead, Aragon) Simon de la Rouviere (ConsenSys) Diego Doval (Founder of n3xt, previously CTO, Ning) Michael Bosworth (Google) Jesse Walden (MediachainSpotify) Chris Burniske (ex ARK Invest) Guillermo Rauch (CEO, Zeit) Joe Urgo (Co-founder, District0x) David Wachsman (Wachsman PR) Jon Choi (Dropbox) Allen Hsu Decentraland 214 1 Introduction 1.1 Rationale 1.2 History 1.3 A Traversable World 1.4 Foundations for an In-world Economy 1.5 Use Cases 2 Architecture 2.1 Consensus Layer 2.2 Content Distribution Layer 2.3 Real-Time Layer 2.4 Payment Channels 2.5 Identity System 3 Economy 3.1 LAND and the MANA Token 3.2 Fostering the Network 4 Challenge 5 Summary Decentraland 314 Decentraland provides an infrastructure to support a shared virtual world, also known as a metaverse2.",
      "It consists of a decentralized ledger for land ownership, a protocol for describing the content of each land parcel, and a peer-to-peer network for user interactions. 01 Introduction The development of large proprietary platforms, such as Facebook, has allowed hundreds of millions of users to gather, interact, share content, and play games. Their network e\ufb00ects helped cultivate vast online communities and gaming companies. These platforms, controlled by centralized organizations, manage the networks rules and content flow, while extracting significant revenue from the communities and content creators who drive tra\ufb00ic to the platforms. Decentraland aims to establish a network that allows its content creators to own and capture the full value of their contributions. The current team began working on Decentraland in 2015. At the time, the adoption of cryptoassets was still in its infancy, as much of the blockchain-based infrastructure necessary for a consumer-oriented platform was lacking.",
      "Since then, the rate of consumer adoption and infrastructure creation has exploded. For instance, by July 2017, Coinbase alone reached 8.4 million user accounts, with half of these added over the past 12 months4. This growth has given rise to a pool of users large enough to fuel the decentralized commerce that will take place in a virtual world like Decentraland. While blockchain infrastructure, spearheaded by Ethereum, is now more widely available, the lack of an e\ufb00icient method to quickly process micropayments constrains the throughput of network transactions. The maturation of cryptocurrencies as a global, instant, and low cost payment method is still evolving. Payment transactions will need to occur o\ufb00-chain to achieve short- to medium-term scalability in blockchain payment networks. Solutions such as Bitcoin's Lightning Network5 or Ethereum's state channels6 are on the verge of enabling a fast, global payment system with low fees. 1.1 Rationale 01. Introduction 414 2 The Metaverse.",
      "Wikipedia. https:en.wikipedia.orgwikiMetaverse 3 http:www.kccllc.netthqdocument1213398130702000000000003 4 https:www.coinbase.comabout 5 Lightning Apps and the Emerging Developer Ecosystem on LND. Lightning Networks Blog. 6 Sprites: Payment Channels that Go Faster than Lightning. A. Miller, I. Bentov, R. Kumaresan, P. McCorry, 2017. Figure 1: Stone Age. A visual representation of the blockchain state. Users could mine, transfer, and change the color of pixels they own. More centralized solutions can work today7, although at the expense of operability with other systems, privacy, and standardization. Decentraland is built on the premise that low cost, direct payments between content creators and users will radically change internet commerce. Decentraland began as a proof of concept for allocating ownership of digital real estate to users on a blockchain.",
      "This digital real estate was initially implemented as a pixel on an infinite 2D grid, where each pixel contained metadata identifying the owner and describing the pixel's color. The experiment was entitled Decentralands Stone Age. In late 2016, the team started developing the Bronze Age, a 3D virtual world divided into land parcels. The owner of each parcel was able to associate it with a hash reference to a file, using a modified Bitcoin blockchain. From this reference, users exploring the virtual world could use a Distributed Hash Table (DHT) and BitTorrent to download the file containing the parcels content, which defines the models and textures to be displayed at that location. We hosted the first world viewer at decentraland.orgworld. Any enthusiast can run a node, download and verify the blockchain, and explore the world by following more advanced instructions8. 1.2 History 01. Introduction 514 Figure 2: Bronze Age.",
      "Structures created by the community around the Genesis parcel, located at coordinates (0,0) 7 Near-zero fee transactions with hub-and-spoke micropayments. Peter Todd, bitcoin-development mailing list, December 2014. https:www.mail-archive.combitcoin-developmentlists.sourceforge.netmsg06576.html 8 https:github.comdecentralandbronzeage-noderun-a-node 1.3 A Traversable World The adjacency of land makes Decentraland parcels unique from web domains. New land parcels must be contiguous to existing ones. This adjacency allows for spatial discovery of new content and the creation of districts devoted to a special topic or theme. While each web domain can have an unlimited number of hyperlinks to other content, parcels in Decentraland have a fixed amount of adjacencies. Additionally, the content of adjacent parcels can be seen from a distance. For content creators, the establishment of districts provides access to targeted tra\ufb00ic; for end users, it enables discovery of themed experiences.",
      "Users can travel through neighborhoods and interact with applications that they stumble upon. This discovery by adjacency is at odds with having infinite land: in that scenario, users would have a hard time finding relevant content by traveling through it. With scarce land, developers can acquire users by purchasing land in high-tra\ufb00ic areas. This will allow secondary markets to develop around land ownership and rentals, as is already happening on district0x.io.9 9 https:blog.district0x.iodecentraland-districts-40b9ada0431b The next version of Decentraland, the Iron Age, will create a social experience with an economy driven by the existing layers of land ownership and content distribution. In the Iron Age, developers will be able to create applications on top of Decentraland, distribute them to other users, and monetize them.",
      "The Iron Age will implement peer-to-peer communications, a scripting system to enable interactive content, and a system of fast cryptocurrency payments for in-world transactions. A communication layer is essential for social experiences, providing positioning, postures, voice chat, and more; Decentraland achieves this with a P2P network. The scripting system is the tool that landowners will use to describe the behavior and interactions of 3D objects, sound, and applications running on land parcels. Finally, a payment system with low fees is key to developing an economy in the quick environment of a virtual world. 01. Introduction 614 Decentralands value proposition to application developers is that they can fully capitalize on the economic interactions between their applications and users. To allow those economic interactions, the platform must allow three things to be traded: currency, goods, and services.",
      "Decentraland will integrate a core system that allows global, instant, and cost-e\ufb00ective payments between any two users on the internet. Cryptocurrencies allow for trustless payment channels to be established between parties, with low-trust hub-and-spoke systems already possible. 1.4 Foundations for an In-world Economy 10 Rare Pepe. Fred Wilson. https:avc.com201705rare-pepe 01. Introduction 714 For services to be provided on Decentraland, we are developing a scripting system that enables developers to program the interactions between users and applications. This scripting system runs exclusively on the client side but allows for di\ufb00erent data flow models: from mere local e\ufb00ects and traditional client-server architectures, to P2P interactions based on state channels.",
      "Developers programming in it will benefit from the availability of fast, cheap micropayments, provably fair games, decentralized storage, and other features enabled by the advent of cryptographic techniques using blockchain-based smart contracts. To foster the exchange of virtual goods, economic incentives must be in place to ensure the continued creation and distribution of avatars, items, and scripts. Because static content can be arbitrarily copied, the user experience should empower social agreements that recognize original creations. By implementing an identity system to establish authorship, users will be able to track and verify an authors consent through cryptographic signatures. These experiments are already happening, as in the case of Rare Pepes10. Applications The Decentraland scripting language will allow the development of applications, games, gambling, and dynamic 3D scenes.",
      "This scripting language will be designed to handle a wide range of capabilities, including creating objects, loading textures, handling physics, encoding user interactions, sounds, payments, and external calls, among others. Content Curation Users in Decentraland will gather around neighborhoods of shared interest. Being located near high-tra\ufb00ic hubs will drive users to the landowners content. Advertising Brands may advertise using billboards near, or in, high-tra\ufb00ic land parcels to promote their products, services, and events. Some neighborhoods may become virtual versions of Times Square in New York City. Additionally, brands may position products and create shared experiences to engage with their audience. Digital Collectibles We expect users to publish, distribute, and collect rare digital assets issued on the blockchain by their creators.",
      "Just as it occurs today in other virtual worlds or through online forums, these digital assets will be traded inside this world through the scripting system and be backed by the aforementioned naming system. 1.5 Use Cases 02. Architecture 814 Social Groups that currently gather in online forums, chat groups, or even other centralized multiplayer games could port their communities into Decentraland. O\ufb00line communities could also find in Decentraland a space to gather. Other use cases There are no technical specifications to what could be built in Decentraland. Therefore, other use cases could emerge, such as training and professional development, education, therapy, 3D design, and virtual tourism, among others. The Decentraland protocol is comprised of three layers: 02 Architecture 1) Consensus layer: Track land ownership and its content. 2) Land content layer: Download assets using a decentralized distribution system.",
      "3) Real-time layer: Enable users world viewers to connect to each other. Land ownership is established at the consensus layer, where land content is referenced through a hash of the files content. From this reference the content can be downloaded from BitTorrent or IPFS. The downloaded file contains a description of objects, textures, sounds, and other elements needed to render the scene. It also contains the URL of a rendezvous server to coordinate connections between P2P users that are exploring the tile simultaneously. Figure 3 shows a diagram of the steps the Decentraland clients execute to provide the experience of a shared virtual world in a decentralized way. Figure 3: The Decentraland protocol for simultaneous users in a decentralized virtual world. 11 https:ipfs.io 12 https:filecoin.io 02. Architecture 914 Decentraland will use an Ethereum smart contract to maintain a ledger of ownership for land parcels in the virtual world.",
      "We call these non-fungible digital assets LAND: each LAND has unique (x, y) coordinates, an owner, and a reference to the content description file, which encodes what the landowner wants to serve there. Decentraland clients will connect to the Ethereum network to fetch updates to the state of the LAND smart contract. LAND is bought by burning MANA, a fungible ERC20 token of fixed supply. This token serves as a proxy for the cost of claiming a new parcel. The LAND contract uses a burn function to destroy MANA and create a new entry in the LAND registry. New parcels need to be adjacent to a non-empty parcel. 2.1 Consensus Layer Decentraland uses a decentralized storage system to distribute the content needed to render the world. For each parcel that needs to be rendered, a reference to a file with the description of the parcels content is retrieved from the smart contract.",
      "The current solution uses the battle-tested BitTorrent and Kademlia DHT networks by storing a magnet link for each parcel. However, the Inter-Planetary File System (IPFS)11 provides a compelling alternative as its technology matures. This decentralized distribution system allows Decentraland to work without the need of any centralized server infrastructure. This allows the world to exist as long as it has users distributing content, shifting the cost of running the system to the same actors that benefit from it. It also provides Decentraland with strong censorship-resistance, eliminating the power of a central authority to change the rules or prevent users from participating. However, hosting these files and the bandwidth required to serve this content has significant costs. Currently, users of the Decentraland P2P network are seeding the content without compensation and out of goodwill.",
      "However, in the future, this infrastructure cost can be covered by the use of protocols like Filecoin12. Until this technology becomes available, automated micropayments can be used to pay for quality of service. The proceeds of Decentralands continuous sale of MANA can cover these costs over the long run (see Section 3.2). 2.2 Content Distribution Layer Additionally, two other systems are key for Decentralands economy to develop: Payment Channel Infrastructure for fast payments with low fees. Identity System that allows users to establish ownership over original creations. 02. Architecture 1014 13 https:tools.ietf.orghtmlrfc5389 14 https:webrtc.org The description of a parcel will contain a list of di\ufb00erent files required to render it, a list of services hosted by the landowner, and an entry point to orchestrate the placement of objects and their behavior.",
      "This document must declare: Content files References to, or blobs with, 3D meshes, as well as textures, audio files, and other relevant content required to render the parcel. These are specified so that the client knows what contents the renderization will need, without any instructions on how to place them. Scripting entry point The scripting system controls how the content is placed in the parcel, as well as its behavior. This enables applications and animations to take place within the parcel. It also coordinates behaviors such as the positioning and movement of objects, the timing and frequency of sounds played, the possible interactions with users, among other features. P2P interactions This allows the client to connect to a server that bootstraps user-to-user connections, coordinates positions and postures, and enables voice chat and messaging.",
      "Clients will communicate with each other by establishing peer-to-peer connections with the help of servers hosted by landowners or third parties. Without a centralized server, peer-to-peer connections are needed to provide social interactions between users, as well as applications that the landowner wants to run inside the parcel. To coordinate the bootstrap of peer-to-peer connections, landowners will have to provide rendezvous servers or understand that users will not be able to see each other in their parcel. The maintenance of these servers can be incentivized the same way as content servers. When lightweight protocols like STUN13 can cover the functionality required from the server, the costs would be fairly low. But for more advanced features, such as a voice chat between multiple concurrent users or network traversal services, micropayments can be used to cover the operating costs.",
      "The social experience of users in Decentraland will include avatars, the positioning of other users, voice chat, messaging, and interaction with the virtual environment. The di\ufb00erent protocols used to coordinate these features can work on top of existing P2P solutions like Federated VoIP or WebRTC.14 2.3 Real-Time Layer 02. Architecture 1114 General purpose, public, and distributed HTLC networks like Lightning may be at least one year away from materializing, but low-trust hub-and-spoke payment channel networks allow for fast and low-cost transactions that can be implemented today. Payment channels are key for Decentraland for two reasons: In-world purchases Incentivizing quality of service of content and P2P servers Today, platforms mitigate the risk inherent in credit card payments: users trust the platform, rather than the application, with their payment details. With payment channels, they could make direct purchases to the developer with no risk of identity theft.",
      "Some parts of Decentralands infrastructure can be paid for with micropayments. These costs include hosting content, serving it, and running P2P protocols like spatial audio processing for multiple users. The marginal cost of running applications on Decentraland for developers, given a market of incentivized servers to provide the infrastructure, approaches its real cost as this becomes essentially commoditized. However, in order to have no barrier to entry for incoming developers, Decentraland will subsidize these services with the proceeds of selling MANA tokens. 2.4 Payment Channels Decentralands ownership of land is one kind of identity system, where credentials are the coordinates of ones land. Economic incentives are also necessary to ensure that the creators of avatars, items, and scripts continue building and distributing them. Since content can be arbitrarily copied, we must rely in social agreements to enforce retribution to the creator.",
      "Social agreements can make digital scarcity possible. In centralized systems, this scarcity is defended by the company that creates the platform. For Bitcoin and other proof-of-work blockchains, scarcity is enabled by a computational puzzle and the fact that mining blocks requires an onerous economic sacrifice. Decentraland can use decentralized identity systems to create a layer of ownership over in-world items. This system must allow users to easily verify the consent of an author by linking public keys and signatures with human-readable names. 2.5 Identity System 03. Economy 1214 15 https:www.uport.me 16 The Ethereum Name System is one of the most successful projects in this area: https:ens.domains 17 http:www.mediachain.io 18 Curation Clubs: Tokenizing  Incentivizing Public Funding With Curation Markets. Simon de la Rouviere. 2017.",
      "19 http:avc.com201705rare-pepe 3.1 LAND and the MANA Token In Section 1.1, we make a case for how the increasing adoption of cryptocurrencies creates the necessary conditions for the emergence of a distributed platform for a virtual world. Below, we introduce the utility of LAND and the MANA token, how their strategic allocation can help bootstrap the utility of the network, and outline how the issuance of MANA will be conducted. With the launch of the Iron Age, we are introducing two digital assets: LAND, the non-fungible parcels in which the virtual world is divided; and MANA, an ERC-20 token that is burned to claim LAND, as well as to make in-world purchases of goods and services. The utility of LAND is based on its adjacency to other attention hubs, its ability to host applications, and also as an identity mechanism. Developers and other content creators will demand LAND so that they can build on top of it and reach their target audience.",
      "Although every unclaimed LAND can be purchased at the same exchange rate (1000 MANA  1 LAND), LAND parcels are distinguishable from each other, potentially trading at di\ufb00erent prices on a secondary market due to di\ufb00erences in adjacencies and tra\ufb00ic. On the other hand, MANA serves as a proxy to asses the price of a new parcel of LAND. Also, MANA used to buy goods and services in the virtual world creates utility value for the token. 03 Economy Projects like uPort15 or the Ethereum Name Service16 can be used to do this. Social reputation is also needed to facilitate contributions to the author. The ability to incentivize content creation on decentralized economies is evolving quickly, with multiple projects working in the space directly or indirectly. Potential solutions include Mediachain17, Curation Markets18, Rare Pepes19, and more. 04. Challenges 1314 3.2 Fostering the Network Land ownership is acquired through an ERC-20 token called MANA.",
      "This token will serve to bring value into the network, and to acquire a new parcel of land. MANA can also be used to purchase in-world goods and services. In order to jumpstart the network, developers and content creators will be rewarded to set up shop in Decentraland. The Foundation will hold contests to create art, games, applications, and experiences, with prizes contingent on meeting a set of milestones. At the same time, new users will be assigned allowances, allowing them to participate in the economy immediately. These financial incentives will help bootstrap the utility value of the network until it independently attracts users and developers. 04 Challenges Decentralized Content Distribution Content distribution through a P2P network has two main issues. The first involves download speed: retrieving a file from a DHT or distributed peer-to-peer storage system has traditionally been too slow.",
      "Specially, in a graphical application like Decentraland, users will be adverse to using a system that does not load the experience quickly. The second issue involves availability: ensuring that content is su\ufb00iciently distributed around the network without loss. IPFS and the upcoming FileCoin protocol are addressing these issues and were looking forward to when they become production ready. Scripting Scripting will be the most important element used to create valuable experiences for users in Decentraland. Its APIs will need to be secure enough for clients to hold private keys and authorize micropayments frequently. Ease-of-use is also critical to penetrate a broad audience of developers. Content Curation The issue of filtering content for mature audiences (like pornography, violence, or gambling) is di\ufb00icult to solve within a decentralized network.",
      "We expect a market to emerge here: with a reputation-based approach, users could select one or more providers of whitelistsblacklists that track the type of content being served on each parcel. 05. Summary 1414 05 Summary Decentraland is a distributed platform for a shared virtual world that enables developers to build and monetize applications on top of it. The scarcity of land, on top of which applications can be built, creates hubs that capture user attention, which drives revenue to content creators. MANA tokens will be used to purchase land, goods, and services in-world. MANA tokens will also be used to incentivize content creation and user adoption, therefore bootstrapping the first decentralized virtual world."
    ],
    "word_count": 3784,
    "page_count": 15
  },
  "MKR": {
    "chunks": [
      "The Dai Stablecoin System Whitepaper By the Maker Team December 2017 Overview of the Dai Stablecoin System Collateralized Debt Position Smart Contracts The CDP interaction process Single-Collateral Dai vs Multi-Collateral Dai Pooled Ether (Temporary mechanism for Single-Collateral Dai) Price Stability Mechanisms Target Price Target Rate Feedback Mechanism Sensitivity Parameter Global Settlement Global Settlement: Step by Step Risk Management of The Maker Platform Risk Parameters MKR Token Governance MKR and Multi-Collateral Dai Automatic Liquidations of Risky CDPs Liquidity Providing Contract (Temporary mechanism for Single-Collateral Dai) Debt and Collateral Auctions (Multi-Collateral Dai) Key External Actors Keepers Oracles Global Settlers Examples Addressable Market Risks and their Mitigation Malicious hacking attack against the smart contract infrastructure Black swan event in one or more collateral assets Competition and the importance of ease-of-use Pricing errors, irrationality and unforeseen events Failure of centralized infrastructure Conclusion Glossary of Terms Links Overview of the Dai Stablecoin System Popular digital assets such as Bitcoin (BTC) and Ether (ETH) are too volatile to be used as everyday currency.",
      "The value of a bitcoin often experiences large fluctuations, rising or falling by as much as 25 in a single day and occasionally rising over 300 in a month. . The Dai Stablecoin is a collateral-backed cryptocurrency whose value is stable relative to the US Dollar. We believe that stable digital assets like Dai Stablecoin are essential to realizing the full potential of blockchain technology. Maker is a smart contract platform on Ethereum that backs and stabilizes the value of Dai through a dynamic system of Collateralized Debt Positions (CDPs), autonomous feedback mechanisms, and appropriately incentivized external actors. Maker enables anyone to leverage their Ethereum assets to generate Dai on the Maker Platform. Once generated, Dai can be used in the same manner as any other cryptocurrency: it can be freely sent to others, used as payments for goods and services, or held as long term savings.",
      "Importantly, the generation of Dai also creates the components needed for a robust decentralized margin trading platform. Collateralized Debt Position Smart Contracts Anyone who has collateral assets can leverage them to generate Dai on the Maker Platform through Makers unique smart contracts known as Collateralized Debt Positions. CDPs hold collateral assets deposited by a user and permit this user to generate Dai, but generating also accrues debt. This debt effectively locks the deposited collateral assets inside the CDP until it is later covered by paying back an equivalent amount of Dai, at which point the owner can again withdraw their collateral . Active CDPs are always collateralized in excess, meaning that the value of the collateral is higher than the value of the debt.",
      "1  David Ernst Hard Problems in Cryptocurrency 2 https:github.commakerdao The CDP interaction process Step 1: Creating the CDP and depositing collateral The CDP user first sends a transaction to Maker to create the CDP, and then sends another transaction to fund it with the amount and type of collateral that will be used to generate Dai. At this point the CDP is considered collateralized. Step 2: Generating Dai from the collateralized CDP The CDP user then sends a transaction to retrieve the amount of Dai they want from the CDP, and in return the CDP accrues an equivalent amount of debt, locking them out of access to the collateral until the outstanding debt is paid. Step 3: Paying down the debt and Stability Fee When the user wants to retrieve their collateral, they have to pay down the debt in the CDP, plus the Stability fee that continuously accrue on the debt over time. The Stability Fee can only be paid in MKR.",
      "Once the user sends the requisite Dai and MKR to the CDP, paying down the debt and Stability Fee, the CDP becomes debt free. Step 4: Withdrawing collateral and closing the CDP With the Debt and Stability Fee paid down, the CDP user can freely retrieve all or some of their collateral back to their wallet by sending a transaction to Maker. Single-Collateral Dai vs Multi-Collateral Dai Dai will initially launch with support for only one type of collateral, Pooled Ether. In the next 6-12 months we plan to upgrade Single-Collateral Dai to Multi-Collateral Dai. The primary difference is that it will support any number of CDP types. 3  Mechanics that are temporarily in place in the system during the Single-Collateral phase are marked in this white paper Pooled Ether (Temporary mechanism for Single-Collateral Dai) At first, Pooled Ether (PETH) will be the only collateral type accepted on Maker.",
      "Users who wish to open a CDP and generate Dai during the first phase of the Maker Platform need to first obtain PETH. This is done instantly and easily on the blockchain by depositing ETH into a special smart contract that pools the ETH from all users, and gives them corresponding PETH in return. If there is a sudden market crash in ETH, and a CDP ends up containing more debt than the value of its collateral, the Maker Platform automatically dilutes the PETH to recapitalize the system. This means that the proportional claim of each PETH goes down. After the Maker Platform is upgraded to support multiple collateral types, PETH will be removed and replaced by ETH alongside the other new collateral types.",
      "Price Stability Mechanisms Target Price The Dai Target Price has two primary functions on the Maker Platform: 1) It is used to calculate the collateral-to-debt ratio of a CDP, and 2) It is used to determine the value of collateral assets Dai holders receive in the case of a global settlement. The Target Price is initially denominated in USD and starts at 1, translating to a 1:1 USD soft peg. Target Rate Feedback Mechanism In the event of severe market instability, the Target Rate Feedback Mechanism (TRFM) can be engaged. Engaging the TRFM breaks the fixed peg of Dai, but maintains the same denomination. The TRFM is the automatic mechanism by which the Dai Stablecoin System adjusts the Target Rate in order to cause market forces to maintain stability of the Dai market price around the Target Price.",
      "The Target Rate determines the change of the Target Price over time, so it can act either as an incentive to hold Dai (if the Target Rate is positive) or an incentive to borrow Dai (If the Target Rate is negative). When the TRFM is not engaged the target rate is fixed at 0, so the target price doesnt change over time and Dai is pegged. When the TRFM is engaged, both the Target Rate and the Target Price change dynamically to balance the supply and demand of Dai by automatically adjusting user incentives for generating and holding Dai. The feedback mechanism pushes the market price of Dai towards the variable Target Price, dampening its volatility and providing real-time liquidity during demand shocks. With the TRFM engaged, when the market price of Dai is below the Target Price, the Target Rate increases. This causes the Target Price to increase at a higher rate, causing generation of Dai with CDPs to become more expensive.",
      "At the same time, the increased Target Rate causes the capital gains from holding Dai to increase, leading to a corresponding increase in demand for Dai. This combination of reduced supply and increased demand causes the Dai market price to increase, pushing it back up towards the Target Price. The same mechanism works in reverse if the Dai market price is higher than the Target Price: the Target Rate decreases, leading to an increased demand for generating Dai and a decreased demand for holding it. This causes the Dai market price to decrease, pushing it down towards the Target Price. This mechanism is a negative feedback loop: Deviation away from the Target Price in one direction increases the force in the opposite direction. Sensitivity Parameter The TRFMs Sensitivity Parameter is a parameter that determines the magnitude of Target Rate change in response to Dai targetmarket price deviation. This tunes the rate of feedback to the scale of the system.",
      "MKR voters can set the Sensitivity Parameter but when the TRFM is engaged the Target Price and the Target Rate are determined by market dynamics, and not directly controlled by MKR voters. The Sensitivity Parameter is also what is used to engage or disengage the TRFM. If the Sensitivity Parameter and the Target Rate are both zero, Dai is pegged to the current Target Price. Global Settlement Global settlement is a process that can be used as a last resort to cryptographically guarantee the Target Price to holders of Dai. It shuts down and gracefully unwinds the Maker Platform while ensuring that all users, both Dai holders and CDP users, receive the net value of assets they are entitled to. The process is fully decentralized, and MKR voters govern access to it to ensure that it is only used in case of serious emergencies. Examples of serious emergencies are long term market irrationality, hacking or security breaches, and system upgrades.",
      "Global Settlement: Step by Step Step 1: Global Settlement is activated If enough actors who have been designated as global settlers by Maker Governance believe that the system is subject to a serious attack, or if a global settlement is scheduled as part of a technical upgrade, they can active the Global Settlement function. This stops CDP creation and manipulation, and freezes the Price Feed at a fixed value that is then used to process proportional claims for all users. Step 2: Global Settlement claims are processed After Global Settlement has been activated, a period of time is needed to allow keepers to process the proportional claims of all Dai and CDP holders based on the fixed feed value. After this processing is done, all Dai holders and CDP holders will be able to claim a fixed amount of ETH with their Dai and CDPs.",
      "Step 3: Dai and CDP holders claim the collateral with their Dai and CDPs Each Dai and CDP holder can call a claim function on the Maker Platform to exchange their Dai and CDPs directly for a fixed amount of ETH that corresponds to the calculated value of their assets, based on the target price of Dai. E.g. If the Dai Target Price is 1 U.S. Dollar, The ETHUSD Price is 200 and a user holds 1000 Dai when Global Settlement is activated, after the processing period they will be able to claim exactly 5 ETH from the Maker Platform. There is no time limit for when the final claim can be made. Risk Management of The Maker Platform The MKR token allows holders to vote to perform the following Risk Management actions: Add new CDP type: Create a new CDP type with a unique set of Risk Parameters. A CDP type can either be a new type of collateral, or a new set of Risk Parameters for an existing collateral type.",
      "Modify existing CDP types: Change the Risk Parameters of one or more existing CDP types that were already added Modify Sensitivity Parameter: Change the sensitivity of the Target Rate Feedback Mechanism Modify Target Rate: Governance can change the Target Rate. In practice modifying the Target Rate will only be done in one specific circumstance: When MKR voters want to peg the price of Dai to its current Target Price. It will always be done in conjunction with modifying the Sensitivity Parameter. By setting both Sensitivity Parameter and Target Rate to 0, the TRFM becomes disabled and the Target Price of Dai becomes pegged to its current value. Choose the set of trusted oracles: The Maker Platform derives its internal prices for collateral and the market price of Dai from a decentralized oracle infrastructure, consisting of a wide set of individual oracle nodes. MKR voters control how many nodes are in the set of trusted oracles, and who those nodes are.",
      "Up to half of the oracles can be compromised or malfunction without causing a disruption to the continued safe operation of the system Modify Price Feed Sensitivity: Change the rules that determine the largest change that the price feeds can affect on the internal price values in the system. Choose the set of global settlers: Global settlement is a crucial mechanic that allows the Maker Platform to survive attacks against the oracles or the governance process. The governance process chooses a set of global settlers and determines how many settlers are needed to activate global settlement. Risk Parameters Collateralized Debt Positions have multiple Risk Parameters that enforce how they can be used. Each CDP type has its own unique set of Risk Parameters, and these parameters are determined based on the risk profile of the collateral used by the CDP type. These parameters are directly controlled by MKR holders through voting, with one MKR giving its holder one vote.",
      "The key Risk Parameters for CDPs are: Debt Ceiling: The Debt Ceiling is the maximum amount of debt that can be created by a single type of CDP. Once enough debt has been created by a CDP of any given type, it becomes impossible to create more unless existing CDPs are closed. The debt ceiling is used to ensure sufficient diversification of the collateral portfolio. Liquidation Ratio: The Liquidation Ratio is the collateral-to-debt ratio at which a CDP becomes vulnerable to Liquidation. A low Liquidation Ratio means MKR voters expect low price volatility of the collateral, while a high Liquidation Ratio means high volatility is expected. Stability Fee: The Stability Fee is a fee paid by every CDP. It is an annual percentage yield that is calculated on top of the existing debt of the CDP and has to be paid by the CDP user. The Stability Fee is denominated in Dai, but can only be paid using the MKR token.",
      "The amount of MKR that has to be paid is calculated based on a Price Feed of the MKR market price. When paid, the MKR is burned, permanently removing it from the supply. Penalty Ratio: The Penalty Ratio is used to determined the maximum amount of Dai raised from a Liquidation Auction that is used to buy up and remove MKR from the supply, with excess collateral getting returned to the CDP user who owned the CDP prior to its liquidation. The Penalty Ratio is used to cover the inefficiency of the liquidation mechanism. During the phase of Single-Collateral Dai, the Liquidation Penalty goes to buy and burn of PETH, benefitting the PETH to ETH ratio. MKR Token Governance In addition to payment of the Stability Fee on active CDPs, the MKR token plays an important role in the governance of the Maker Platform. Governance is done at the system level through election of an Active Proposal by MKR voters.",
      "The Active Proposal is the smart contract that has been empowered by MKR voting to gain root access to modify the internal governance variables of the Maker Platform. Proposals can be in two forms: Single Action Proposal Contracts SAPC, and Delegating Proposal Contracts DPC. Single Action Proposal Contracts are proposals that can only be executed once after gaining root access, and after execution immediately applies its changes to the internal governance variables of the Maker Platform. After the one-time execution, the SAPC deletes itself and cannot be re-used. This type of proposal is what will be used during the first phases of the system, as it is not very complicated to use, but is less flexible. Delegating Proposal Contracts are proposals that continuously utilize their root access through second layer governance logic that is codified inside the DPC.",
      "The second layer governance logic can be relatively simple, such as defining a protocol for holding a weekly vote on updated risk parameters. It can also implement more advanced logic, such as restrictions on the magnitude of governance actions within defined time periods, or even delegating some or all of its permissions further to one or more third layer DPCs with or without restrictions. Any Ethereum account can deploy valid proposal smart contracts. MKR voters can then use their MKR tokens to cast approval votes for one or more proposals that they want to elect as the Active Proposal. The smart contract that has the highest total number of approval votes from MKR voters is elected as the Active Proposal. MKR and Multi-Collateral Dai After the upgrade to Multi-Collateral Dai, MKR will take on a more significant role in the Dai Stablecoin System by replacing PETH as the the recapitalization resource.",
      "When CDPs become undercollateralized due to market crashes, the MKR supply is automatically diluted and sold off in order to raise enough funds to recapitalize the system. Automatic Liquidations of risky CDPs To ensure there is always enough collateral in the system to cover the value of all outstanding Debt (according to the Target Price), a CDP can be liquidated if it is deemed to be too risky. The Maker Platform determines when to liquidate a CDP by comparing the Liquidation Ratio with the current collateral-to-debt ratio of the CDP. Each CDP type has its own unique Liquidation Ratio that is controlled by MKR voters and established based on the risk profile of the particular collateral asset of that CDP type. Liquidation occurs when a CDP hits its Liquidation Ratio. The Maker Platform will automatically buy the collateral of the CDP and subsequently sell it off. There is a temporary mechanism in place for Single-Collateral Dai called a Liquidity Providing Contract.",
      "For Multi-Collateral Dai an auction mechanism will be used. Liquidity Providing Contract (Temporary mechanism for Single-Collateral Dai) During Single-Collateral Dai, the mechanism for liquidation is a Liquidity Providing Contract: a smart contract that trades directly with ethereum users and keepers according to the price feed of the system. When a CDP is liquidated, it is immediately acquired by the system. The CDP owner receives the value of the leftover collateral minus the debt, Stability Fee and Liquidation Penalty. The PETH collateral is set for sale in the Liquidity Providing Contract, and keepers can atomically purchase the PETH by paying Dai. All Dai paid this way are immediately removed from the Dai supply, until an amount equal to the CDP debt has been removed. If any Dai is paid in excess of the debt shortfall, the excess Dai is used to purchase PETH from the market and burn it, which positively changes the ETH to PETH ratio.",
      "This results in a net value gain for PETH holders. If the PETH selloff initially does not raise enough Dai to cover the entire debt shortfall, more PETH is continuously created and sold off. New PETH created this way negatively changes the ETH to PETH ratio, causing PETH holders to lose value. Debt and Collateral Auctions (Multi-Collateral Dai) During a liquidation, the Maker platform buys the collateral of a CDP and subsequently sells it in an automatic auction. This auction mechanism enables the system to settle CDPs even when price information is unavailable. In order to take over the collateral of the CDP so that it can be sold, the system first needs to raise enough Dai to cover the CDPs debt. This is called a Debt Auction, and works by diluting the supply of the MKR token and selling it to bidders in an auction format.",
      "In parallel, the collateral of the CDP is sold in a Collateral Auction where all proceeds (also denominated in Dai) up to the CDP debt amount plus a Liquidation Penalty (A Risk Parameter determined by MKR voting) is used to buy MKR and remove it from the supply. This directly counteracts the MKR dilution that happened during the Debt Auction. If enough Dai is bid to fully cover the CDP debt plus the Liquidation Penalty, the Collateral Auction switches to a reverse auction mechanism and tries to sell as little collateral as possible--any leftover collateral is returned to the original owner of the CDP. Key External Actors In addition to its smart contract infrastructure, the Maker Platform relies on certain external actors to maintain operations. Keepers are external actors who take advantage of the economic incentives presented by the Maker platform. Oracles and Global Settlers are external actors with special permissions in the system assigned to them by MKR voters.",
      "Keepers A keeper is an independent (usually automated) actor that is incentivized by profit opportunities to contribute to decentralized systems. In the context of the Dai Stablecoin System, keepers participate in the Debt Auctions and Collateral Auctions when CDPs are liquidated. Keepers also trade Dai around the Target Price. Keepers sell Dai when the market price is higher than the Target Price and buy Dai when the market price is below the Target Price to profit from the expected long-term convergence towards the Target Price. Oracles The Maker Platform requires real time information about the market price of the assets used as collateral in CDPs in order to know when to trigger liquidations. The Maker Platform also needs information about the market price of Dai and its deviation from the Target Price in order to adjust the Target Rate when the TRFM is engaged. MKR voters choose a set of trusted oracles to feed this information to the Maker Platform through Ethereum transactions.",
      "To protect the system from an attacker who gains control of a majority of the oracles, and from other forms of collusion, there is a global variable that determines the maximum change to the value of the price feed permitted by the system. This variable is known as the Price Feed Sensitivity Parameter. As an example of how the Price Feed Sensitivity Parameter works, if the Price Feed Sensitivity Parameter is defined as 5 in 15 minutes, the price feeds cannot change more than 5 within one 15 minute period, and changing 15 would take 45 minutes. This restriction ensures there is enough time to trigger a global settlement in the event that an attacker gains control over a majority of the oracles. Global Settlers Global Settlers are external actors similar to price feed oracles and are the last line of defense for the Dai Stablecoin System in the event of an attack. The set of global settlers, selected by MKR voters, have the authority to trigger global settlement.",
      "Aside from this authority, these actors do not have any additional special access or control within the system. Examples The Dai Stablecoin System can be used by anyone without any restrictions or sign-up process. Example 1: Bob needs a loan, so he decides  to generate 100 Dai. He locks an amount of ETH worth significantly more than 100 Dai into a CDP and uses it to generate 100 Dai. The 100 Dai is instantly sent directly to his Ethereum account. Assuming that the Stability Fee is 1 per year, Bob will need 101 Dai to cover the CDP if he decides to retrieve his ETH one year later. One of the primary use cases of CDPs is margin trading by CDP users. Example 2: Bob wishes to go margin long on the ETHDai pair, so he generates 100 USD worth of Dai by posting 150 USD worth of ETH to a CDP. He then buys another 100 USD worth of ETH with his newly generated Dai, giving him a net 1.66x ETHUSD exposure. Hes free to do whatever he wants with the 100 USD worth of ETH he obtained by selling the Dai.",
      "The original ETH collateral (150 USD worth) remains locked in the CDP until the debt plus the Stability Fee is covered. Although CDPs are not fungible with each other, the ownership of a CDP is transferable. This allows CDPs to be used in smart contracts that perform more complex methods of Dai generation (for example, involving more than one actor). Example 3: Alice and Bob collaborate using an Ethereum OTC contract to issue 100 USD worth of Dai backed by ETH. Alice contributes 50 USD worth of ETH, while Bob contributes 100 USD worth. The OTC contract takes the funds and creates a CDP, thus generating 100 USD worth of Dai. The newly generated Dai are automatically sent to Bob. From Bob's point of view, he is buying 100 USD worth of Dai by paying the equivalent value in ETH. The contract then transfers ownership of the CDP to Alice. She ends up with 100 USD worth of debt (denominated in Dai) and 150 USD worth of collateral (denominated in ETH).",
      "Since she started with only 50 USD worth of ETH, she is now 3x leveraged long ETHUSD. Liquidations ensure that in the event of a price crash of the collateral backing a CDP type, the system will automatically be able to close CDPs that become too risky. This ensures that the outstanding Dai supply remains fully collateralized. Example 4: Let's assume that there is an Ether CDP type with a Liquidation Ratio of 145, a Penalty Ratio of 105, and we have an Ether CDP with a collateral-to-debt ratio of 150 . The Ether price now crashes 10 against the Target Price, causing the collateral-to-debt ratio of the CDP to fall to 135. As it falls below the Liquidation Ratio, traders can trigger its Liquidation and begin bidding with Dai for buying MKR in the debt auction. Simultaneously, traders can begin bidding with Dai for buying the 135 Dai worth of collateral in the collateral auction.",
      "Once there is at least 105 Dai being bid on the Ether collateral, traders reverse bid to take the least amount of collateral for 105 Dai. Any remaining collateral is returned to the CDP owner. Addressable Market As mentioned in the introduction, a cryptocurrency with price stability is a basic requirement for the majority of decentralized applications. As such, the potential market for Dai is at least as large as that of the entire blockchain industry. The following is a short, non-exhaustive list of some of the immediate markets (in both the blockchain and the wider industry) for the Dai Stablecoin System in its capacity as a cryptocurrency with price stability and its use case as a decentralized margin trading platform: Prediction Markets  Gambling Applications: When making an unrelated prediction, it is obvious not to want to increase ones risk by placing the bet using a volatile cryptocurrency.",
      "Long term bets become especially infeasible if the user has to also gamble on the future price of the volatile asset used to place the bet. Instead, a cryptocurrency with price stability like Dai will be the natural choice for prediction market and gambling users. Financial Markets; Hedging, Derivatives, Leverage: CDPs will allow for permissionless leveraged trading. Dai will also be useful as stable and reliable collateral in custom derivative smart contracts, such as options or CFDs. Merchant receipts, Cross-border transactions and remittances: Foreign exchange volatility mitigation and a lack of intermediaries means the transaction costs of international trade can be significantly reduced by using Dai. Transparent accounting systems: Charities, NGOs and Governments will all see increases in efficiency and lower levels of corruption by utilizing Dai.",
      "Risks and their Mitigation There are many potential risks facing the successful development, deployment, and operation of the Maker Platform. It is vital that the Maker community takes all necessary steps to mitigate these risks. The following is a list spells out some of the risks identified and the accompanying plan for risk mitigation: Malicious hacking attack against the smart contract infrastructure The greatest risk to the system during its early stages is the risk of a malicious programmer finding an exploit in the deployed smart contracts, and using it to break or steal from the system before the vulnerability can be fixed. In a worst case scenario, all decentralized digital assets that are held as collateral in The Maker Platform, such as Ether (ETH) or Augur Reputation (REP), could be stolen without any chance of recovery.",
      "The part of the collateral portfolio that is not decentralized, such as Digix Gold IOUs, would not be stolen in such an event as they can be frozen and controlled through a centralized backdoor. Mitigation: Smart contract security and best security practices have been the absolute highest priority of the Dai development effort since its inception. The codebase has already undergone three independent security audits by some of the best security researchers in the blockchain industry. In the very long term, the risk of getting hacked can theoretically be almost completely mitigated through formal verification of the code. This means mathematically proving that the code does exactly what it is intended to do.",
      "While complete formal verification is a very long term goal, significant work towards it has already been completed, including a full reference implementation of the Dai Stablecoin System in the functional programming language Haskell, which serves as a stepping stone towards more sophisticated formalizations that are currently under active research and development Black swan event in one or more collateral assets Another high impact risk is a potential Black Swan event on collateral used for the Dai. This could either happen in the early stages of Dai Stablecoin System, before MKR is robust enough to support inflationary dilutions, or after the Dai Stablecoin System supports a diverse portfolio of collateral. Mitigation: CDP collateral will be limited to ETH in the early stages, with the debt ceiling initially limited and growing gradually over time.",
      "Competition and the importance of ease-of-use As mentioned previously, there is a large amount of money and brainpower working on cryptocurrency with price stability. By virtue of having true decentralization, the Dai Stablecoin System is by far the most complex model being contemplated in the blockchain industry. A perceived risk is a movement among cryptocurrency users where the ideals of decentralization are exchanged for the simplicity and marketing of centralized digital assets. Mitigation: We expect that Dai will be very easy to use for a regular cryptocurrency user. Dai will be a standard Ethereum token adhering to the ERC-20 standard and will be readily available with high liquidity across the ecosystem. Dai has been designed in such a way that the average user need not understand the underlying mechanics of the system in order to use it.",
      "The complexities of the Dai Stablecoin System will need to be understood primarily by Keepers and capital investment companies that use the Dai Stablecoin System for margin trading. These types of users have enough resources to onboard themselves as long as there is abundant and clear documentation of every aspect of the system's mechanics. The Maker community will ensure that this is the case. Pricing errors, irrationality and unforeseen events A number of unforeseen events could potentially occur, such as a problem with the price feed from the Oracles, or irrational market dynamics that cause variation in the value of Dai for an extended period of time. If confidence is lost in the system, the TRFM adjustments or even MKR dilution could reach extreme levels while still not bringing enough liquidity and stability to the market.",
      "Mitigation: The Maker community will need to incentivize a sufficiently large capital pool to act as Keepers of the market in order to maximize rationality and market efficiency and allow the Dai supply to grow at a steady pace without major market shocks. Failure of centralized infrastructure The Maker Team plays a major role in the development and governance of the Maker Platform in its early days: budgeting for expenses, hiring new developers, seeking partnerships and institutional users, and interfacing with regulators and other key external stakeholders. Should the Maker Team fail in some capacity  for legal reasons, or due to internal problems with management  the future of Maker could be at risk without a proper backup plan. Mitigation: The Maker community exists partly to act as the decentralized counterparty to the Maker Team.",
      "It is a loose collective of independent actors who are all aligned by holding the MKR token, giving them a strong incentive to see the Maker Platform succeed. During the early phases of MKR distribution, great care was taken to ensure that the most important core developers received a significant MKR stake. In the event that the Maker Team is no longer effectively able to lead the development of the Maker Platform, individual MKR holders will be incentivized to fund developers (or simply carry out development themselves) in an effort to protect their investment. Conclusion The Dai Stablecoin System was designed to solve the crucial problem of stable exchange of value in the Ethereum ecosystem and the wider blockchain economy. We believe that the mechanism through which Dai is created, transacted, and retired, along with the direct Risk Management role of MKR holders, will allow for self-interested Keepers to maintain the price stability of Dai over time in an efficient manner.",
      "The founders of the Maker community have established a prudent governance roadmap that is appropriate for the needs of agile development in the short term, but also coherent with the ideals of decentralization over time. The development roadmap is aggressive and focused on widespread adoption of Dai in a responsible fashion. Glossary of Terms Collateralized Debt Position (CDP): A smart contract whose users receive an asset (Dai), which effectively operates as a debt instrument with an interest rate. The CDP user has posted collateral in excess of the value of the loan in order to guarantee their debt position. Dai: The cryptocurrency with price stability that is the asset of exchange in the Dai Stablecoin System. It is a standard Ethereum token adhering to the ERC20 standard. Debt Auction: The reverse auction selling MKR for Dai to cover Emergency Debt when a CDP becomes undercollateralized. Collateral Auction: The auction selling collateral from a CDP undergoing liquidation.",
      "It is designed to prioritize covering the debt owed by the CDP, and secondarily to give the CDP owner the best possible price for their excess collateral refund. The Dai Foundation: A decentralized team of smart contract developers committed to the development and successful launch of the Maker Platform. Keepers: Independent economic actors that trade Dai, CDPs andor MKR; create Dai or close CDPs; and seek arbitrage on The Dai Stablecoin System. As a result, Keepers help maintain Dai market rationality and price stability. MKR: The ERC20 token used by MKR voters for voting. It also serves as a backstop in the case of insolvent CDPs. MKR Voters: MKR holders who actively manage the risk of the Dai Stablecoin System by voting on Risk Parameters. Maker: The name of the Decentralized Autonomous Organization that is made up of the Maker Platform technical infrastructure, and the community of MKR voters.",
      "Oracles: Ethereum accounts (either contracts or users) selected to provide price feeds into various components of Maker Platform. Risk Parameters: The variables that determine (among other things) when the Maker Platform automatically judges a CDP to be Risky, allowing Keepers to liquidate it. Sensitivity Parameter: The variable that determines how aggressively the Dai Stablecoin System automatically changes the Target Rate in response to Dai market price deviations. Target Rate Feedback Mechanism (TRFM): The automatic mechanism by which the Dai Stablecoin System adjusts the Target Rate in order to cause market forces to maintain stability of the Dai market price around the Target Price.",
      "Links Chat: https:chat.makerdao.com  Primary platform of community interaction Forum: https:forum.makerdao.com  For debate and proposals Subreddit: https:reddit.comrmakerdao  Best place to get latest news and links GitHub: https:github.commakerdao  Repository of the public Maker code TeamSpeak: https:ts.makerdao.com  For governance meeting conference calls SoundCloud: https:soundcloud.commakerdao  Governance meeting recordings Oasis: https:oasisdex.com  MKR and Dai decentralized exchange Sai: https:sai.makerdao.com  Experimental stablecoin"
    ],
    "word_count": 5905,
    "page_count": 21
  },
  "NEAR": {
    "chunks": [
      "Randomization matters How to defend against strong adversarial attacks Rafael Pinot  1 2 Raphael Ettedgui  1 Geovani Rizk 1 Yann Chevaleyre 1 Jamal Atif 1 Abstract Is there a classi\ufb01er that ensures optimal robust- ness against all adversarial attacks? This paper tackles this question by adopting a game-theoretic point of view. We present the adversarial attacks and defenses problem as an in\ufb01nite zero-sum game where classical results (e.g. Nash or Sion theorems) do not apply. We demonstrate the non- existence of a Nash equilibrium in our game when the classi\ufb01er and the Adversary are both determin- istic, hence giving a negative answer to the above question in the deterministic regime. Nonethe- less, the question remains open in the randomized regime. We tackle this problem by showing that any deterministic classi\ufb01er can be outperformed by a randomized one.",
      "This gives arguments for using randomization, and leads us to a simple method for building randomized classi\ufb01ers that are robust to state-or-the-art adversarial attacks. Empirical results validate our theoretical analysis, and show that our defense method considerably outperforms Adversarial Training against strong adaptive attacks, by achieving 0.55 accuracy un- der adaptive PGD-attack on CIFAR10, compared to 0.42 for Adversarial training. 1. Introduction Adversarial example attacks recently became a major con- cern in the machine learning community. An adversarial attack refers to a small, imperceptible change of an input that is maliciously designed to fool a machine learning al- gorithm. Since the seminal work of (Biggio et al., 2013) and (Szegedy et al., 2014) it became increasingly important Equal contribution 1Universit\u00e9 Paris-Dauphine, PSL Re- search University, CNRS, LAMSADE, Paris, France 2Institut LIST, CEA, Universit\u00e9 Paris-Saclay, France.",
      "Correspondence to: Rafael Pinot rafael.pinotdauphine.fr, Raphael Ettedgui raphael.ettedguidauphine.eu. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). to understand the very nature of this phenomenon (Fawzi et al., 2016; 2018; Bubeck et al., 2019; Ilyas et al., 2019; Gourdeau et al., 2019). Furthermore, a large body of work has been published on designing attacks (Goodfellow et al., 2015; Papernot et al., 2016a; Madry et al., 2018; Carlini  Wagner, 2017; Athalye et al., 2018) and defenses (Goodfel- low et al., 2015; Papernot et al., 2016b; Madry et al., 2018; Cohen et al., 2019). Besides, in real-life scenarios such as for an autonomous car, errors can be very costly. It is not enough to just defend against new attacks as they are published. We would need an algorithm that behaves optimally against every single attack. However, it remains unknown whether such a defense exists.",
      "This leads to the following questions, for which we provide principled and theoretically-grounded answers. Q1: Is there a deterministic classi\ufb01er that ensures optimal robustness against any adversarial attack? A1: To answer this question, in Section 3.1, we cast the adversarial examples problem as an in\ufb01nite zero-sum game between a Defender (the classi\ufb01er) and an Adversary that produces adversarial examples. Then we demonstrate, in Section 4, the non-existence of a Nash equilibrium in the deterministic setting of this game. This entails that no deter- ministic classi\ufb01er can claim to be more robust than all other classi\ufb01ers against any possible adversarial attack. Another consequence of our analysis is that there is no free lunch for transferable attacks: an attack that works on all classi\ufb01ers will never be optimal against any of them. Q2: Would randomized defense strategies be a suitable alternative to defend against strong adversarial attacks?",
      "A2: We tackle this problem both theoretically and empiri- cally. In Section 5, we demonstrate that for any deterministic defense there exists a mixture of classi\ufb01ers that offers better worst-case theoretical guarantees. Building upon this, we devise a method that generates a robust randomized clas- si\ufb01er with a one step boosting method. We evaluate this method, in Section 6, against strong adaptive attacks on CI- FAR10 and CIFAR100 datasets. It outperforms Adversarial Training against both \u2113-PGD (Madry et al., 2018), and \u21132- CW (Carlini  Wagner, 2017) attacks. More precisely, on CIFAR10, our algorithm achieves 0.55 (resp. 0.53) accuracy arXiv:2002.11565v5 cs.LG 6 Jan 2021 Randomization matters under attack against these attacks, which is an improvement of 0.13 (resp. 0.18) over Adversarial Training. 2. Related Work Many works have studied adversarial examples, in several different settings.",
      "We discuss hereafter the different frame- works that we believe to be related to our work, and discuss the aspects on which our contribution differs from them. Distributionally robust optimization. The work in (Sinha et al., 2018) addresses the problem of adversarial exam- ples through the lens of distributionally robust optimization. They study a min-max problem where the Adversary ma- nipulates the test distribution while being constrained in a Wasserstein distance ball (they impose a global constraint on distributions for the Adversary, while we study a local, pointwise constraint, leading to different attack policies). A similar analysis was presented in (Lee  Raginsky, 2018) in a more general setting that does not focus on adversarial ex- amples. Even though our work studies a close problem, our reasoning is very different. We adopt a game theoretic stand- point, which allows us to investigate randomized defenses and endow them with strong theoretical evidences. Game Theory.",
      "Some works have tackled the problem of adversarial examples as a two player game. For exam- ple (Br\u00fcckner  Scheffer, 2011) views adversarial example attacks and defenses as a Stackelberg game. More recently, (Rota Bul\u00f2 et al., 2017) and (Perdomo  Singer, 2019) investigated zero-sum games. They consider restricted ver- sions of the game where classical theorems apply, such as when the players only have a \ufb01nite set of possible strate- gies. We study a more general setting. Finally, (Dhillon et al., 2018) motivates the use of noise injection as a defense mechanism by game theoretic arguments but only present empirical results. Randomization. Following the work of (Dhillon et al., 2018) and (Xie et al., 2018), several recent works stud- ied noise injection as a defense mechanism.",
      "In particular, (Lecuyer et al., 2018), followed by (Cohen et al., 2019; Li et al., 2019; Pinot et al., 2019; Wang et al., 2019) demon- strated that noise injection can, in some cases, give provable defense against adversarial attacks. The analysis and de- fense method we propose in this paper are not based on noise injection. However, a link could be made between these works and the mixture we propose, by noting that a classi\ufb01er in which noise is being injected can be seen as an in\ufb01nite mixture of perturbed classi\ufb01ers. Optimal transport. Our work considers a distributionnal setting, in which the Adversary manipulating the dataset is formalized by a push-forward measure. This kind of setting is close to optimal transport settings recently developed by (Bhagoji et al., 2019) and (Pydi  Jog, 2019). Speci\ufb01cally, these works investigate classi\ufb01er-agnostic lower bounds on the risk for binary classi\ufb01cation under attack, with some hypothesis on the data distribution.",
      "The main differences are that we focus on studying equilibria and not deriving bounds. Moreover, these works do not study the in\ufb02uence of randomization. Finally they express the optimal risk of the Defender in terms of transportation costs between two distributions, whereas we explicitly study the Adversarys behaviour as a transport from one distribution to another. Even though they do not treat the problem from the same prism, we believe that these works are profoundly related and complementary to ours. Ensemble of classi\ufb01ers. Some works have been done to improve the robustness of a model by constructing ensem- ble of classi\ufb01ers (Abbasi  Gagn\u00e9, 2017; Xu et al., 2017; Verma  Swami, 2019; Pang et al., 2019; Sen et al., 2020). However all the defense methods proposed in those papers subsequently proved to be ineffective against adaptive at- tacks introduced in (He et al., 2017; Tramer et al., 2020).",
      "The main difference with our method is that it is not an ensemble method since it uses sampling instead of voting to aggregate the classi\ufb01ers output. Hence in terms of volatil- ity, in voting methods, whenever a majority agrees on an opinion, all others votes will be ignored, whereas here each classi\ufb01er always contributes according to its probability weights, which do not depend on the others. 3. A Game Theoretic point of view. 3.1. Initial problem statement Notations. For any set Z with \u03c3-algebra \u03c3 (Z), if there is no ambiguity on the considered \u03c3-algebra, we denote P (Z) the set of all probability measures over (Z, \u03c3 (Z)), and FZ the set of all measurable functions from (Z, \u03c3 (Z)) to (Z, \u03c3 (Z)). For \u00b5 P (Z) and \u03c6 FZ, the pushforward measure of \u00b5 by \u03c6 is the measure \u03c6\u00b5 such that \u03c6\u00b5(B)  \u00b5(\u03c6-1(B)) for any B \u03c3(Z). Binary classi\ufb01cation task. Let X Rd and Y  -1, 1. We consider a distribution D P (X  Y) that we assume to be of support X  Y.",
      "The Defender is looking for a hy- pothesis (classi\ufb01er) h in a class of functions H, minimizing the risk of h w.r.t. D: R(h) :  (X,Y )D 1 h(X)  Y  Y \u03bd X\u00b5Y 1 h(X)  Y  Where H : h : x 7sgn g(x)  g : X R continuous, \u03bd P (Y) is the probability measure that de\ufb01nes the law of the random variable Y , and for any y Y, \u00b5y P (X) is the conditional law of X(Y  y). Randomization matters Adversarial example attack (point-wise). Given a clas- si\ufb01er h : X Y and a data sample (x, y) D, the Adversary seeks a perturbation \u03c4 X that is visually im- perceptible, but modi\ufb01es x enough to change its class, i.e. h(x  \u03c4)  y. Such a perturbation is called an adversarial example attack. In practice, it is hard to evaluate the set of visually imperceptible modi\ufb01cations of an image. However, a suf\ufb01cient condition to ensure that the attack is undetectable is to constrain the perturbation \u03c4 to have a small norm, be it for the \u2113or the \u21132 norm.",
      "Hence, one should always ensure that \u03c4\u03f5, or \u03c42 \u03f52, depending on the norm used to measure visual imperceptibility. The choice of the threshold depends on the application at hand. For example, on CIFAR datasets, typical values for \u03f5and \u03f52 are respectively, 0.031 and 0.40.60.8. In the remaining of this work, we will de\ufb01ne our constraint using an \u21132 norm, but all our results are valid for an \u2113based constraint. Adversarial example attack (distributional). The Adver- sary chooses, for every x X, a perturbation that depends on its true label y. This amounts to construct, for each label y Y, a measurable function \u03c6y such that \u03c6y(x) is the per- turbation associated with the labeled example (x, y). This function naturally induces a probability distribution over adversarial examples, which is simply the push-forward measure \u03c6y\u00b5y. The goal of the Adversary is thus to \ufb01nd \u03c6  (\u03c6-1, \u03c61) (FX\u03f52)2 that maximizes the adversarial risk Radv(h, \u03c6) de\ufb01ned as follows: Radv(h, \u03c6) : Y \u03bd X\u03c6Y \u00b5Y 1 h(X)  Y  .",
      "(2) Where for any \u03f52 (0, 1), FX\u03f52 is the set of functions that imperceptibly modi\ufb01es a distribution: FX\u03f52 : \u03c8 FX  essup \u03c8(x) x2 \u03f52 Adversarial defense, a two-player zero-sum game. With the setting de\ufb01ned above, the adversarial examples problem can be seen as a two-player zero-sum game, where the Defender tries to \ufb01nd the best possible hypothesis h, while a strong Adversary is manipulating the dataset distribution: \u03c6(FX\u03f52) 2 Radv(h, \u03c6). This means that the Defender tries to design the classi\ufb01er with the best performance under attack, whereas the Adver- sary will each time design the optimal attack on this speci\ufb01c classi\ufb01er. In the game theoretical terminology, the choice of a classi\ufb01er h (resp. an attack \u03c6) for the Defender (resp. the Adversary) is called a strategy. It is crucial to note that the sup-inf and inf-sup problems do not necessarily coincide. In this paper, we mainly focus on the Defenders point of view which corresponds to the inf-sup problem.",
      "We will be interested in understanding the behaviour of players in this game, i.e. the best responses they have to a given strategy, and whether some equilibria may arise. This motivates the following de\ufb01nitions. De\ufb01nition 1 (Best Response). Let h H, and \u03c6  FX\u03f52 2. A best response from the Defender to \u03c6 is a clas- si\ufb01er hH such that Radv(h, \u03c6)  min hH Radv(h, \u03c6). Similarly, a best response from the Adversary to h is an attack \u03c6 FX\u03f52 2 such that Radv(h, \u03c6)  \u03c6(FX\u03f52) 2 Radv(h, \u03c6). In the remaining, we denote BR(h) the set of all best re- sponses of the Adversary to a classi\ufb01er h. Similarly BR(\u03c6) denotes the set of best responses to an attack \u03c6. De\ufb01nition 2 (Pure Nash Equilibrium). In the zero-sum game (Eq. 3), a Pure Nash Equilibrium is a couple of strate- gies (h, \u03c6) H  FX\u03f52 2 such that BR(\u03c6), and, BR(h). When it exists, a Pure Nash Equilibrium is a state of the game in which no player has any incentive to modify its strategy.",
      "In our setting, this simultaneously means that no attack could better fool the current classi\ufb01er, and that the classi\ufb01er is optimal for the current attack. Remark. All the de\ufb01nitions in this section assume a de- terministic regime, i.e. that neither the Defender nor the Adversary use randomization, hence the notion of Pure Nash Equilibrium in the game theory terminology. The randomized regime will be studied in Section 5. 3.2. Trivial solution and Regularized Adversary Trivial Nash equilibrium. Our current de\ufb01nition of the problem implies that the Adversary has perfect information on the dataset distribution and the classi\ufb01er. It also has unlimited computational power and no constraint on the attack except on the size of the perturbation. Going back to the example of the autonomous car, this would mean that the Adversary can modify every single image that the camera may receive during any trip, which is highly unrealistic.",
      "The Adversary has no downside to attacking, even when the attack is unnecessary, e.g. if the attack cannot work or if the point is already misclassi\ufb01ed. This type of behavior for the Adversary can lead to the existence of a pathological (and trivial) Nash Equilibrium as demonstrated in Figure 1 for the uni-dimensional set- ting with Gaussian distributions. The unbounded Adversary moves every point toward the decision boundary (each time maximizing the perturbation budget), and the Defender can- not do anything to mitigate the damage. In this case the Randomization matters decision boundary for the Optimal Bayes Classi\ufb01er remains unchanged, even though both curves have been moved to- ward the center, hence a trivial equilibrium. In the remaining of this work, we show that such an equilibrium does not exist as soon as there is a small restraint on the Adversarys strength, i.e. as soon as it is not perfectly indifferent to produce unnecessary perturbations. Regularized Adversary.",
      "To mitigate the Adversary strength, we introduce a penalization term: \u03c6(FX\u03f52) 2 Radv(h, \u03c6) \u03bb \u2126(\u03c6) adv(h, \u03c6) The penalty function \u2126represents the limitations on the Ad- versarys budget, be it because of computational resources or to avoid being detected. \u03bb (0, 1) is some regularization weight. In this paper, we study two types of penalties: the mass penalty \u2126mass, and the norm penalty \u2126norm. From a computer-security point of view, the \ufb01rst limitation that comes to mind is to limit the number of queries the Adversary can send to the classi\ufb01er. In our distributional setting, this boils down to penalizing the mass of points that the function \u03c6 moves. Hence we de\ufb01ne the mass penalty as: \u2126mass(\u03c6) : Y \u03bd X\u00b5Y 1 X  \u03c6Y (X) The mass penalty discourages the Adversary from attacking too many points by penalizing the overall mass of trans- ported points.",
      "The second limitation we consider penalizes the expected norm under \u03c6: \u2126norm(\u03c6) : Y \u03bd X\u00b5Y X \u03c6Y (X)2 This regularization is very common in both the optimization and adversarial example communities. In particular, it is used by Carlini  Wagner (Carlini  Wagner, 2017) to com- pute the eponymous attack1. In the following, we denote BR\u2126mass (resp. BR\u2126norm) the best responses for the Adver- sary w.r.t the mass (resp. norm) penalty. Section 4 shows that whatever penalty the Adversary has, no Pure Nash Equi- librium exists. We characterize the best responses for each player, and show that they can never satisfy De\ufb01nition 2. 4. Deterministic regime Notations. we denote x X  h(x)  1, and Nh : x X  h(x)  -1 re- spectively the set of positive and negative outputs of h. We 1\u2126norm is not limited to \u21132 norm. The results we present hold as long as the norm used to compare X and \u03c6Y (X) comes from a scalar product on X.",
      "also denote the set of attackable points from the positive outputs Ph(\u03b4) : x Ph  z Nh and z x2 \u03b4, and Nh(\u03b4) likewise. Adversarys best response. Let us \ufb01rst present the best responses of the Adversary under respectively the mass penalty and the norm penalty. Both best responses share a fundamental behavior: the optimal attack will only change points that are close enough to the decision boundary. This means that, when the Adversary has no chance of making the classi\ufb01er change its decision about a given point, it will not attack it. However, for the norm penalty all attacked points are projected on the decision boundary, whereas with the mass penalty the attack moves the points across the border. Lemma 1. Let h H and \u03c6 BR\u2126mass(h). Then the following assertion holds: \u03c61(x) (Ph) if x Ph(\u03f52) \u03c61(x)  x otherwise. Where (Ph), the complement of Ph in X. \u03c6-1 is character- ized symmetrically. Lemma 2. Let h H and \u03c6 BR\u2126norm(h). Then the following assertion holds: \u03c61(x)  \u03c0(x) if x Ph(\u03f52) otherwise.",
      "Where \u03c0 is the orthogonal projection on (Ph). \u03c6-1 is char- acterized symmetrically. These best responses are illustrated in Figure 1 with two uni- dimensional Gaussian distributions. For the mass penalty, \u00b51 is set to 0 in Ph(\u03f52), and this mass is transported into Nh(\u03f52). The symmetric holds for \u00b5-1. After attack, we now have \u00b51 (Ph(\u03f52))  0, so a small value of \u00b5-1 in Ph(\u03f52) suf\ufb01ces to make it dominant, and that zone will now be classi\ufb01ed -1 by the Optimal Bayes Classi\ufb01er. For the norm penalty, the part of \u00b51 that was in Ph(\u03f52) is transported on a Dirac distribution at the decision boundary. Similarly to the mass penalty, the best response now predicts -1 for the zone Ph(\u03f52). Remark. In practice, it might be computationally hard to generate the exact best response for the norm penalty, i.e. the projection on the decision boundary. That will happen for example if this boundary is very complex (e.g. highly non-smooth), or when X is in a high dimensional space.",
      "To keep the attack tractable, the Adversary will have to compute an approximated best response by allowing the projection to reach the point within a small ball around the boundary. This means that the best responses of the norm penalty and the mass penalty problems will often match. Defenders best response. At a \ufb01rst glance, one would suspect that the best response for the Defender ought to be Randomization matters Figure 1. Representation of the \u00b5-1 (blue dotted line) and \u00b51 (red plain line) distributions, without attack (left) and with three different attacks: no penalty (second drawing), with mass penalty (third) and with norm penalty (fourth). On all \ufb01gures blue area on the left of the axis is Ph(\u03f52) and red area on the right is Nh(\u03f52). the Optimal Bayes Classi\ufb01er for the transported distribution. However, it is only well de\ufb01ned if the conditional distribu- tions admit a probability density function. This might not always hold here for the transported distribution.",
      "Neverthe- less, we show that there is a property, shared by the Optimal Bayes Classi\ufb01er when de\ufb01ned, that always holds for the Defenders best response. Lemma 3. Let us consider \u03c6  FX\u03f52 2. If we take h  BR(\u03c6), then for y  1 (resp. y  -1), and for any B Ph (resp. B Nh) one has P(Y  yX B) P(Y  yX B) with Y \u03bd and for all y Y, X(Y  y) \u03c6y\u00b5y. In particular, when \u03c61\u00b51 and \u03c6-1\u00b5-1 admit proba- bility density functions, Lemma 3 simply means that h is the Optimal Bayes Classi\ufb01er for the distribution (\u03bd, \u03c61\u00b51, \u03c6-1\u00b5-1)2. We can now state our main theo- rem, as well as two of its important consequences. Theorem 1 (Non-existence of a pure Nash equilibrium). In the zero-sum game (Eq. 4) with \u03bb (0, 1) and penalty \u2126\u2126mass, \u2126norm, there is no Pure Nash Equilibrium. Consequence 1. (No free lunch for transferable attacks) To understand this statement, remark that, thanks to weak duality, the following inequality always holds: \u03c6(FX\u03f52) 2 inf hH R\u2126 adv(h, \u03c6) inf \u03c6(FX\u03f52) 2 R\u2126 adv(h, \u03c6).",
      "On the left side problem (sup-inf), the Adversary looks for the best strategy \u03c6 against any unknown classi\ufb01er. This is tightly related to the notion of transferable attacks (see e.g. (Tram\u00e8r et al., 2017)), which refers to attacks success- ful against a wide range of classi\ufb01ers. On the right side (our) problem (inf-sup), the Defender tries to \ufb01nd the best classi\ufb01er under any possible attack, whereas the Adversary plays in second and speci\ufb01cally attacks this classi\ufb01er. As a 2We prove this result in the supplementary material. consequence of Theorem 1, the inequality is always strict: \u03c6(FX\u03f52) 2 inf hH R\u2126 adv(h, \u03c6)  inf \u03c6(FX\u03f52) 2 R\u2126 adv(h, \u03c6). This means that both problems are not equivalent. In par- ticular, an attack designed to succeed against any classi\ufb01er (i.e. a transferable attack) will not be as good as an attack tailored for a given classi\ufb01er. Hence she has to trade-off between effectiveness and transferability of the attack. Consequence 2.",
      "(No deterministic defense may be proof against every attack) Let us consider the state-of-the-art defense which is Adversarial Training (Goodfellow et al., 2015; Madry et al., 2018). The idea is to compute an ef\ufb01- cient attack \u03c6, and train the classi\ufb01er on created adversarial examples, in order to move the decision boundary and make the classi\ufb01er more robust to new perturbations by \u03c6. To be fully ef\ufb01cient, this method requires that \u03c6 remains an optimal attack on h even after training. Our theorem shows that it is never the case: after training our classi\ufb01er h to become (h) robust against \u03c6, there will always be a different optimal attack \u03c6 that is ef\ufb01cient against h. Hence Adversarial Training will never achieve a perfect defense. 5. Randomization matters As we showed that there is no Pure Nash Equilibrium, no deterministic classi\ufb01er may be proof against every attack. We would therefore need to allow for a wider class of strate- gies.",
      "A natural extension of the game would thus be to allow randomization for both players, who would now choose a distribution over pure strategies, leading to this game: \u03b7P(H) (FX\u03f52) 2 E adv(h, \u03c6) Without making further assumptions on this game (e.g. com- pactness), we cannot apply known results from game theory (e.g. Sion theorem) to prove the existence of an equilibrium. These assumptions would however make the problem loose much generality, and do not hold here. Randomization matters Randomization matters. Even without knowing if an equi- librium exists in the randomized setting, we can prove that randomization matters. More precisely we show that any deterministic classi\ufb01er can be outperformed by a random- ized one in terms of the worst case adversarial risk. To do so we simplify Equation 7 in two ways: 1. We do not consider the Adversary to be randomized, i.e. we restrict the search space of the Adversary to (FX )2 instead of P (FX )2 .",
      "This condition corresponds to the current state-of-the-art in the domain: to the best of our knowledge, no ef\ufb01cient randomized adversarial example attack has been designed (and so is used) yet. 2. We only consider a subclass of randomized classi\ufb01ers, called mixtures, which are discrete probability mea- sures on a \ufb01nite set of classi\ufb01ers. We show that this kind of randomization is enough to strictly outperform any deterministic classi\ufb01er. We will discuss later the use of more general randomization (such as noise in- jection) for the Defender. Let us now de\ufb01ne a mixture of classi\ufb01ers. De\ufb01nition 3 (Mixture of classi\ufb01er). Let n N, h  (h1, ..., hn) Hn , and q P (1, ..., n). A mixed classi- \ufb01er of h by q is a mapping mq h from X to P (Y) such that for all x X, mq h(x) is the discrete probability distribution that is de\ufb01ned for all y Y as follows: h(x)(y) : E iq 1 hi(x)  y . We call such a mixture a mixed strategy of the Defender.",
      "Given some x X, this amounts to picking a classi\ufb01er hi from h at random following the distribution q, and use it to output the predicted class for x, i.e. hi(x). Note that a mixed strategy for the Defender is a non deterministic algorithm, since it depends on the sampling one makes on q. Hence, even if the attacks are de\ufb01ned in the same way as before, the Adversary now needs to maximize a new objective function which is the expectation of the adversarial risk under the distribution mq h. It writes as follows: Y \u03bd X\u03c6Y \u00b5Y \u02c6Y mq h(X) \u02c6Y  Y oi \u03bb \u2126(\u03c6) . We also write R\u2126 adv to mean the left part of Equation (8), when it is clear from context that the Defender uses a mixed classi\ufb01er. Using this new set of strategies for the Defender, we can study whether mixed classi\ufb01ers outperform deter- ministic ones, and how to ef\ufb01ciently design them. Mixed strategy. We demonstrate that the ef\ufb01ciency of any deterministic defense can be improved using a simple mixed strategy.",
      "This method presents similarities with the notions of \ufb01ctitious play (Brown, 1951) in game theory, and boost- ing in machine learning (Freund  Schapire, 1995). Given a deterministic classi\ufb01er h1, we combine it (via randomiza- tion) with the best response h2 to its optimal attack. The rational behind this idea is that, by construction, ef\ufb01- cient attacks on one of these two classi\ufb01ers will not work on the other. Mixing h1 with h2 has two opposite consequences on the adversarial risk. On one hand, where we only had to defend against attack on h1, we are now also vulnerable to attacks on h2, so the total set of possible attacks is now bigger. On the other hand, each attack will only work part of the time, depending on the probability distribution q. If we can calibrate the weights so that attacks on important zones have a low probability of succeeding, then the average risk under attack on the mixture will be low. Toy example where a mixture outperforms AT.",
      "To better understand how randomization can work, let us look at a sim- ple toy example. Figure 2 illustrates a binary classi\ufb01cation setting between two set of points. Attacking the Optimal Bayes Classi\ufb01er (bold straight line) consists in moving all the points that lie between the dotted lines to the opposite side of the decision boundary (Figure 2, left). The general tactic to defend against an attack is to change the classi\ufb01ers output for points that are too close to the boundary. This can be done all the time, as in Adversarial Training (where we move the decision boundary to incorporate adversarial examples), or part of the time as in a randomized algorithm (so that the attack only works with a given probability). When we use Adversarial Training for the star points (Fig- ure 2, middle), we change the output on the blue zone, so that 2 of the star (squared) points cannot be successfully attacked anymore. But in exchange, the dilation of the new boundary can now be attacked.",
      "For Adversarial Training to work, we need the number of new potential attacks (i.e. the points that are circled, 2 crosses in the dilation and 2 stars that are close to the new boundary) to be smaller than the number of attacks we prevent (the squared points, 2 blue ones that an attack would send in the blue zone, and 3 red points that are far from the new decision boundary). Here we prevent 5 attacks at the cost of 4 new ones, so the Adversarial Training improves the total score from 8 to 7. Similarly, we observe what happens for the randomized de- fense (Figure 2, right). We mix the Optimal Bayes Classi\ufb01er with the best response to attacking all the points. We get a classi\ufb01er that is determinsitic outside the gray area, and random inside it3.",
      "If the \ufb01rst classi\ufb01er has a weight \u03b1  0.5, 6 of the old attacks now succeed only with probability 0.5 3The grey area should actually be bigger since the best response to the attack would also change the decision on the upper part between the OBC and the doted line. We focus on what happens on the star points for simplicity. Randomization matters Figure 2. Illustration of adversarial examples (only on class 1 for more readability) crossing the decision boundary (left), adversarially trained classi\ufb01er for the class 1 (middle), and a randomized classi\ufb01er that defends class 1. Stars are natural examples for class 1, and crosses are natural examples for class -1. The straight line is the optimal Bayes classi\ufb01er, and dashed lines delimit the points close enough to the boundary to be attacked resp. for class 1 and -1. We focus the drawing on the star points. Crosses can be treated symmetrically.",
      "(crosses between the dotted lines), whereas 3 new attacks are created (stars outside of the gray area) that succeed with probability 0.5 also. At the end, the average rate of suc- cessful attacks is 6.5, where adversarial training previously achieved 7. More formally, Theorem 2 shows that whatever penalty we consider, a deterministic classi\ufb01er can always be outper- formed by a randomized algorithm. We now can state our second main result: randomization matters. Theorem 2. (Randomization matters) Let us consider h1  H, \u03bb (0, 1), \u2126 \u2126mass, \u03c6 BR\u2126(h1) and h2 BR(\u03c6). Then for any \u03b1 (max(\u03bb, 1 \u03bb), 1) and for any \u03c6  BR\u2126(mq h) one has adv(mq h, \u03c6)  R\u2126 adv(h1, \u03c6). Where h  (h1, h2), q  (\u03b1, 1\u03b1), and mq h is the mixture of h by q. A similar result holds when \u2126 \u2126norm (see supplementary materials). Remark Note that depending on the initial hypothesis h1 and the conditional distributions \u00b51 and \u00b5-1, the gap be- tween R\u2126 adv(mq h, \u03c6)and R\u2126 adv(h1, \u03c6) could vary.",
      "Hence, with additional conditions on h1, \u00b51 and \u00b5-1, we could make the gap appear more explicitly. We keep the formulation general to emphasize that for any deterministic classi\ufb01er, there exists a randomized one that outperforms it in terms of worst-case adversarial score. Based on Theorem 2 we devise a new procedure called Boosted Adversarial Training (BAT) to construct a robust mixture of two classi\ufb01ers. It is based on three core princi- ples: Adversarial Training, Boosting and Randomization. 6. Experiments: How to build the mixture Simple mixture procedure (BAT). Given a dataset D and a weight parameter \u03b1 0, 1, we construct h1 the \ufb01rst classi\ufb01er of the mixture using Adversarial Training4 on D. Then, we train the second classi\ufb01er h2 on a data set D that contains adversarial examples against h1 created from examples of D. At the end we return the mixture constructed with those two classi\ufb01ers where the \ufb01rst one has a weight of 1 \u03b1 and the second one a weight of \u03b1.",
      "The parameter \u03b1 is found by conducting a grid-search. In Table 1 we present results for \u03b1  0.2 under strong state-of-the-art attacks. The procedure is summarized in Algorithm 25 Algorithm 1 Boosted Adversarial Training Input : D the training data set and \u03b1 the weight parameter. Create and adversarially train h1 on D Generate the adversarial data set D against h1. Create and naturally train h2 on D q (1 \u03b1, \u03b1) h (h1, h2) return mq Comparison to \ufb01ctitious play. Contrary to classical algo- rithms such as Fictitious play that also generates mixtures of classi\ufb01ers, and whose theoretical guarantees rely on the existence of a Mixed Nash Equilibrium, the performance of our method is ensured by Theorem 2 to be at least as good as the classi\ufb01er it uses as a basis. Moreover, the implemen- tation of Fictitious Play would be impractical on the high 4We use \u2113-PGD with 20 iterations and \u03f5 0.031 to train the \ufb01rst classi\ufb01er and to build D.",
      "5More algorithmic and implementation details can be found in the supplementary materials. Randomization matters Dataset Method Natural Adaptive-l-PGD Adaptive-\u21132-CW Accuracy \u03f5 0.031 \u03f52  0.4 \u03f52  0.6 \u03f52  0.8 CIFAR10 Natural 0.88 0.00 0.00 0.00 0.00 AT (Madry et al., 2018) 0.83 0.42 0.60 0.47 0.35 Ours 0.80 0.55 0.60 0.57 0.53 CIFAR100 Natural 0.62 0.00 0.00 0.00 0.00 AT (Madry et al., 2018) 0.58 0.26 0.38 0.29 0.22 Ours 0.56 0.40 0.45 0.41 0.38 Table 1. Evaluation on CIFAR10 and CIFAR100 without data augmentation. Accuracy under attack of a single adversarially trained classi\ufb01er (AT) and the mixture formed with our method (Ours). The evaluation is made with Adaptive-\u2113-PGD and Adaptive-\u21132-CW attacks both computed with 100 iterations. For Adaptive-\u2113-PGD we use an epsilon equal to 8255 (0.031), a step size equal to 2255 (0.008) and we allow random initialization.",
      "For Adaptive-\u21132-CW we use a learning rate equal to 0.01, 9 binary search steps, the initial constant to 0.001, we allow the abortion when it has already converged and we give the results for the different values of rejection threshold \u03f52 0.4, 0.6, 0.8. As for EOT, we dont need to estimate the expected accuracy of the mixture through Monte Carlo sampling since we have the exact weight of each classi\ufb01er of the mixture. Thus we give the exact expected accuracy. dimensional datasets we consider, due to its computational costs. Evaluating against strong adversarial attacks. When evaluating a defense against adversarial examples, it is cru- cial to test the robustness of the method against the best possible attack. Accordingly, the defense method should be evaluated against attacks that were speci\ufb01cally tailored to it (a.k.a. adaptive attacks).",
      "In particular, when evaluating randomized algorithms, one should use Expectation over Transformation (EOT) to avoid gradient masking as pointed out by (Athalye et al., 2018) and (Carlini et al., 2019). More recently, (Tramer et al., 2020) emphasized that one should also make sure that EOT is computed properly6. Previ- ous works such as (Dhillon et al., 2018) and (Pinot et al., 2019) estimate the EOT through a Monte Carlo sampling which can introduce a bias in the attack if the sample size is too small. Since we assume perfect information for the Adversary, it knows the exact distribution of the mixture. Hence it can directly compute the expectation without using a sampling method, which avoid any bias. Table 1 eval- uates our method against strong adaptive attacks namely Adaptive-\u2113-PGD and Adaptive-\u21132-CW. Hard constraint parameter. The typical value of \u03f5 in the hard constraint depends on the norm we consider in the problem setting.",
      "In this paper, we use an \u21132 norm, however, the constraint parameter for \u2113-PGD attack was initially set to be an \u2113constraint. In order to compare attacks of similar strength, we choose different threshold (\u03f52 or \u03f5) values which result in balls of equivalent volumes. For CIFAR10 an CIFAR100 datasets (Krizhevsky  Hinton, 2009), which are 3  32  32 dimensional spaces, this 6In order for the attack to succeed, it it more ef\ufb01cient to com- pute the expected transformation of the logits instead of taking the expectation over the loss. More details on this in the supplementary materials. gives \u03f5 0.03 and \u03f52  0.8 (we also give results for \u03f52 equal to 0.6 and 0.4 as this values are sometimes used in the literature). Since Adaptive-\u21132-CW attack creates an unbounded perturbation on the examples, we implemented the constraint from Equation 6 by checking at test time whether the \u21132-norm of the perturbation exceeds a certain threshold \u03f52 0.4, 0.6, 0.8.",
      "If it does, the adversarial example is disregarded, and we keep the natural example instead. Experimental results. In Table 1 we compare the accuracy, on CIFAR10 and CIFAR100, of our method and classical Adversarial Training under attack with Adaptive-\u2113-PGD and Adaptive-\u21132-CW, both run for 100 iterations. We used 5 times more iterations for the evaluation as we used during training, and carefully check for convergence. the rational behind this is that, for a classi\ufb01er to be fully robust, its loss of accuracy should be controlled when the attacks are stronger than the ones it was trained on. For both attacks, both datasets and all thresholds (i.e. the budget for a pertur- bation), the accuracy under attack of our mixture is higher than the single classi\ufb01er with Adversarial Training. Our defense is especially more robust than Adversarial Training when the threshold is high. Extension to more than two classi\ufb01ers.",
      "In this paper we focus our experiments on a mixture of two classi\ufb01ers to present a proof of concept of Theorem 2. Nevertheless, a mixture of more than two classi\ufb01ers can be constructed by adding at each step t a new classi\ufb01er trained naturally on the dataset D that contains adversarial examples against the mixture at step t 1. Since D has to be constructed from a mixture, one would have to use an adaptive attack as Adaptive-\u2113-PGD. We refer the reader to the supple- mentary material for this extended version of the algorithm and for all the implementation details related to our ex- periments (architecture of models, optimization settings, hyper-parameters, etc.). Randomization matters 7. Discussion  Conclusion Finally, is there a classi\ufb01er that ensures optimal robustness against all adversarial attacks? We gave a negative answer to this question in the deterministic regime, but part of the question remains open when considering randomized algorithms.",
      "We demonstrated that randomized defenses are more ef\ufb01cient than deterministic ones, and devised a simple method to implement them. Game theoretical point of view. There remains to study whether an Equilibrium exists in the Randomized regime. This question is appealing from a theoretical point of view, and requires to investigate the space of randomized Adver- saries P((FX )2). The characterization of this space is not straightforward, and would require strong results in the the- ory of optimal transport. A possible research direction is to quotient the space (FX )2 so as to simplify the search in P((FX )2) and the characterization of the Adversarys best responses. The study of this equilibrium is tightly related to that of the value of the game, which would be interesting for obtaining min-max bounds on the accuracy under attack, as well as certi\ufb01cates of robustness for a set of classi\ufb01ers. Advocating for more provable defenses.",
      "Although the experimental results show that our mixture of classi\ufb01ers outperforms Adversarial Training, our algorithm does not provide guarantees in terms of certi\ufb01ed accuracy. As the literature on adversarial attacks and defenses demonstrated, better attacks always exist. This is why, more theoretical works need to be done to prove the robustness of a mixture created from this particular algorithm. More generally, our work advocates for the study of mixtures as a provable de- fense against adversarial attacks. One could, for example, build upon the connection between mixtures and noise injec- tion to investigate a broader range of randomized strategies for the Defender, and devise certi\ufb01cates accordingly. Improving Boosted Adversarial Training. From an algo- rithmic point of view, BAT can be improved in several ways. For instance, the weights can be learned while choosing the new classi\ufb01er for the mixture.",
      "This could lead to an improved accuracy under attack, but would lack some the- oretical justi\ufb01cations that still need to be set up. Finally, tighter connections with standard boosting algorithms could be established to improve the analysis of BAT. We thank anonymous reviewers, whose comments helped us improve the paper signi\ufb01cantly. We also thank Rida Laraki and Guillaume Carlier for fruitful discussions on game theory as well as Alexandre Araujo for proof reading our experiments. This work was granted access to the HPC resources of IDRIS under the allocation 2020-101141 made by GENCI. Abbasi, M. and Gagn\u00e9, C. Robustness to adversarial exam- ples through an ensemble of specialists. arXiv preprint arXiv:1702.06856, 2017. Athalye, A., Carlini, N., and Wagner, D. Obfuscated gra- dients give a false sense of security: Circumventing de- fenses to adversarial examples.",
      "In Proceedings of the 35th International Conference on Machine Learning, vol- ume 80 of Proceedings of Machine Learning Research, pp. 274283, Stockholmsm\u00e4ssan, Stockholm Sweden, 1015 Jul 2018. PMLR. Bhagoji, A. N., Cullina, D., and Mittal, P. Lower bounds on adversarial robustness from optimal transport. In Ad- vances in Neural Information Processing Systems 32, pp. 74967508. Curran Associates, Inc., 2019. Biggio, B., Corona, I., Maiorca, D., Nelson, B., \u0160rndic, N., Laskov, P., Giacinto, G., and Roli, F. Evasion attacks against machine learning at test time. In Joint European conference on machine learning and knowledge discovery in databases, pp. 387402. Springer, 2013. Brown, G. W. Iterative solution of games by \ufb01ctitious play. Activity analysis of production and allocation, 13(1):374 376, 1951. Br\u00fcckner, M. and Scheffer, T. Stackelberg games for adversarial prediction problems.",
      "In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 11, pp. 547555, New York, NY, USA, 2011. Association for Computing Machinery. ISBN 9781450308137. doi: 10.11452020408.2020495. Bubeck, S., Lee, Y. T., Price, E., and Razenshteyn, I. Ad- versarial examples from computational constraints. In Proceedings of the 36th International Conference on Ma- chine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 831840, Long Beach, California, USA, 0915 Jun 2019. PMLR. Carlini, N. and Wagner, D. Towards evaluating the robust- ness of neural networks. In 2017 IEEE Symposium on Security and Privacy (SP), pp. 3957. IEEE, 2017. Carlini, N., Athalye, A., Papernot, N., Brendel, W., Rauber, J., Tsipras, D., Goodfellow, I., and Madry, A. On evaluating adversarial robustness. arXiv preprint arXiv:1902.06705, 2019. Cohen, J. M., Rosenfeld, E., and Kolter, J. Z. Certi\ufb01ed adversarial robustness via randomized smoothing.",
      "CoRR, abs1902.02918, 2019. Randomization matters Dhillon, G. S., Azizzadenesheli, K., Bernstein, J. D., Kos- sai\ufb01, J., Khanna, A., Lipton, Z. C., and Anandkumar, A. Stochastic activation pruning for robust adversarial defense. In International Conference on Learning Repre- sentations, 2018. Fawzi, A., Moosavi-Dezfooli, S.-M., and Frossard, P. Ro- bustness of classi\ufb01ers: from adversarial to random noise. In Advances in Neural Information Processing Systems 29, pp. 16321640. Curran Associates, Inc., 2016. Fawzi, A., Fawzi, H., and Fawzi, O. Adversarial vulner- ability for any classi\ufb01er. In Advances in Neural Infor- mation Processing Systems 31, pp. 11861195. Curran Associates, Inc., 2018. Freund, Y. and Schapire, R. E. A Decision The- oretic Generalization On-Line Learning an Application to Boosting. In Vit\u00e1nyi, P. M. B. (ed.), Second European Conference Compu- tational Learning Theory (EuroCOLT-95), 2337, 1995. URL citeseer.nj.nec.com freund95decisiontheoretic.html.",
      "Goodfellow, I., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples. In International Conference on Learning Representations, 2015. Gourdeau, P., Kanade, V., Kwiatkowska, M., and Worrell, J. On the hardness of robust classi\ufb01cation. In Advances in Neural Information Processing Systems 32, pp. 7444 7453. Curran Associates, Inc., 2019. He, W., Wei, J., Chen, X., Carlini, N., and Song, D. Adver- sarial example defense: Ensembles of weak defenses are not strong. In 11th USENIX Workshop on Offensive Technologies (WOOT 17), 2017. Ilyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran, B., and Madry, A. Adversarial examples are not bugs, they are features. In Advances in Neural Information Processing Systems 32, pp. 125136. Curran Associates, Inc., 2019. Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009.",
      "Lecuyer, M., Atlidakis, V., Geambasu, R., Hsu, D., and Jana, S. Certi\ufb01ed robustness to adversarial examples with differential privacy. In 2019 IEEE Symposium on Security and Privacy (SP), pp. 727743, 2018. Lee, J. and Raginsky, M. Minimax statistical learning with wasserstein distances. In Advances in Neural Information Processing Systems 31, pp. 26872696. Curran Asso- ciates, Inc., 2018. Li, B., Chen, C., Wang, W., and Carin, L. Certi\ufb01ed adversar- ial robustness with additive noise. In Advances in Neural Information Processing Systems 32, pp. 94599469. Cur- ran Associates, Inc., 2019. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models resistant to adversarial attacks. In International Conference on Learn- ing Representations, 2018. Pang, T., Xu, K., Du, C., Chen, N., and Zhu, J. Improving adversarial robustness via promoting ensemble diversity. arXiv preprint arXiv:1901.08846, 2019. Papernot, N., McDaniel, P., Jha, S., Fredrikson, M., Celik, Z.",
      "B., and Swami, A. The limitations of deep learning in adversarial settings. In Security and Privacy (EuroSP), 2016 IEEE European Symposium on, pp. 372387. IEEE, 2016a. Papernot, N., McDaniel, P., Wu, X., Jha, S., and Swami, A. Distillation as a defense to adversarial perturbations against deep neural networks. In 2016 IEEE Symposium on Security and Privacy (SP), pp. 582597. IEEE, 2016b. Perdomo, J. C. and Singer, Y. Robust attacks against multi- ple classi\ufb01ers. CoRR, abs1906.02816, 2019. Pinot, R., Meunier, L., Araujo, A., Kashima, H., Yger, F., Gouy-Pailler, C., and Atif, J. Theoretical evidence for adversarial robustness through randomization. Advances in Neural Information Processing Systems 32 (NeurIPS). 2019. Pydi, M. S. and Jog, V. Adversarial risk via optimal transport and optimal couplings, 2019. Rota Bul\u00f2, S., Biggio, B., Pillai, I., Pelillo, M., and Roli, F. Randomized prediction games for adversarial machine learning.",
      "IEEE Transactions on Neural Networks and Learning Systems, 28(11):24662478, Nov 2017. Sen, S., Ravindran, B., and Raghunathan, A. Empir: En- sembles of mixed precision deep networks for increased robustness against adversarial attacks. arXiv preprint arXiv:2004.10162, 2020. Sinha, A., Namkoong, H., and Duchi, J. Certi\ufb01able distribu- tional robustness with principled adversarial training. In International Conference on Learning Representations, 2018. Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., and Fergus, R. Intriguing proper- ties of neural networks. In International Conference on Learning Representations, 2014. Randomization matters Tramer, F., Carlini, N., Brendel, W., and Madry, A. On adaptive attacks to adversarial example defenses. arXiv preprint arXiv:2002.08347, 2020. Tram\u00e8r, F., Papernot, N., Goodfellow, I., Boneh, D., and McDaniel, P. The space of transferable adversarial ex- amples. arXiv, 2017. URL https:arxiv.org abs1704.03453. Verma, G.",
      "and Swami, A. Error correcting output codes im- prove probability estimation and adversarial robustness of deep neural networks. In Advances in Neural Information Processing Systems, pp. 86438653, 2019. Wang, B., Shi, Z., and Osher, S. Resnets ensemble via the feynman-kac formalism to improve natural and robust accuracies. In Advances in Neural Information Process- ing Systems 32, pp. 16551665. Curran Associates, Inc., 2019. Xie, C., Wang, J., Zhang, Z., Ren, Z., and Yuille, A. Mit- igating adversarial effects through randomization. In International Conference on Learning Representations, 2018. Xu, W., Evans, D., and Qi, Y. Feature squeezing mitigates and detects carliniwagner adversarial examples. arXiv preprint arXiv:1705.10686, 2017. Zagoruyko, S. and Komodakis, N. Wide residual networks. In Proceedings of the British Machine Vision Conference (BMVC), pp. 87.187.12. BMVA Press, September 2016. ISBN 1-901725-59-6. doi: 10.5244C.30.87. Supplementary Material 1.",
      "Omitted proofs and Additional results Notations. Let us suppose that (X,.) is a normed vector space. B.(x, \u03f5)  z X  x z\u03f5 is the closed ball of center x and radius \u03f5 for the norm .. Note that H : h : x 7sgn g(x)  g : X R continuous, with sgn the function that outputs 1 if g(x)  0, 1 if g(x)  0, and 0 otherwise. Hence for any (x, y) D, and h H one has 1h(x)  y  1g(x)y 0. Finally, we denote \u03bd1 and \u03bd-1 respectively the probabilities of class 1 and -1. Introducing remarks. Let us \ufb01rst note that in the paper, the penalties are de\ufb01ned with an \u21132 norm. However, Lemma 1 and 2 hold as long as X is an Hilbert space with dot product  and associated norm . . . . We \ufb01rst demonstrate Lemma 2 with these general notations. Then we present the proof of Lemma 1 that follows the same schema. Note that, for Lemma 1, we do not even need the norm to be Hilbertian, since the core argument rely on separation property of the norm, i.e. on the property x y 0 x  y. Lemma 2. Let h H and \u03c6 BR\u2126norm(h).",
      "Then the following assertion holds: \u03c61(x)   \u03c0(x) if x Ph(\u03f52) otherwise. Where \u03c0 is the orthogonal projection on (Ph). \u03c6-1 is characterized symmetrically. Proof. Let us \ufb01rst simplify the worst case adversarial risk for h. Recall that h  sgn(g) with g continuous. From the de\ufb01nition of adversarial risk we have: \u03c6(FX\u03f52) 2 R\u2126norm adv (h, \u03c6) \u03c6(FX )2 y1 X\u00b5y 1 h (\u03c6y(X))  y \u03bbX \u03c6y(X)1 X \u03c6y(X) \u03f52 (10) \u03c6(FX )2 y1 X\u00b5y 1 g (\u03c6y(X)) y 0 \u03bbX \u03c6y(X)1 X \u03c6y(X) \u03f52 (11) y1 \u03c6yFX X\u00b5y 1 g (\u03c6y(X)) y 0 \u03bbX \u03c6y(X)1 X \u03c6y(X) \u03f52 (12) Finding \u03c61 and \u03c61 are two independent optimization problems, hence, we focus on characterizing \u03c61 (i.e. y  1). \u03c61FX X\u00b51 1 g (\u03c61(X)) 0 \u03bbX \u03c61(X)1 X \u03c61(X) \u03f52 (13) X\u00b51 essup zB.(X,\u03f52) 1(g(z) 0) \u03bbX z (14) essup zB.(x,\u03f52) 1 g(z) 0 \u03bbx zd\u00b51(x). (15) Let us now consider (Hj)jJ a partition of X, we can write. \u03c61FX X\u00b51 1 g (\u03c61(X)) 0 \u03bbX \u03c61(X)1 X \u03c61(X) \u03f52 (16) Randomization matters essup zB.(x,\u03f52) 1 g(z) 0 \u03bbx zd\u00b51(x) (17) In particular, we consider here H0  P  h, H1  Ph  Ph(\u03f52), and H2  Ph(\u03f52).",
      "For x H0  P  Taking z  x we get 1 g(z) 0 \u03bbx z 1. Since for any z X we have 1 g(z) 0  \u03bbx z1, this strategy is optimal. Furthermore, for any other optimal strategy z, we would have x z 0, hence z  x, and an optimal attack will never move the points of H0  P  For x H1  Ph  Ph(\u03f52). We have B.(x, \u03f52) Ph by de\ufb01nition of Ph(\u03f52). Hence, for any z B.(x, \u03f52), one gets g(z)  0. Then 1 g(z) 0 \u03bbx z0. The only optimal z will thus be z  x, giving value 0. Let us now consider x H2  Ph(\u03f52) which is the interesting case where an attack is possible. We know that B.(x, \u03f52) P  h  , and for any z in this intersection, 1(g(z) 0)  1. Hence : essup zB.(x,\u03f52) 1 g(z) 0 \u03bbx z max(1 \u03bb essinf zB.(x,\u03f52)P  x z, 0) (18)  max(1 \u03bb\u03c0B.(x,\u03f52)P  h (x), 0) (19) Where \u03c0B.(x,\u03f52)P  h is the projection on the closure of B.(x, \u03f52) P  h. Note that \u03c0B.(x,\u03f52)P  h exists: g is continuous, so B.(x, \u03f52) P  h is a closed set, bounded, and thus compact, since we are in \ufb01nite dimension.",
      "The projection is however not guaranteed to be unique since we have no evidence on the convexity of the set. Finally, let us remark that, since \u03bb (0, 1), and \u03f52 1, one has 1 \u03bb\u03c0B.(x,\u03f52)P  h (x) 0 for any x H2. Hence, on Ph(\u03f52), the optimal attack projects all the points on the decision boundary. For simplicity, and since there is no ambiguity, we write the projection \u03c0. Finally. Since H0 H1 H2  X, Lemma 2 holds. Furthermore, the score for this optimal attack is: \u03c6(FX\u03f52) 2 R\u2126norm adv (h, \u03c6) (20) y1 essup zB.(x,\u03f52) 1 g(z)y 0 \u03bbx zd\u00b5y(x) (21) Since the value is 0 on Ph  Ph(\u03f52) (resp. on Nh  Nh(\u03f52) ) for \u03c61 (resp. \u03c6-1), one gets: Ph(\u03f52) 1 \u03bbx \u03c0(x) d\u00b51(x)  1d\u00b51(x)  \u03bd-1 Nh(\u03f52) 1 \u03bbx \u03c0(x) d\u00b5-1(x)  1d\u00b5-1(x) (22) Ph(\u03f52) 1 \u03bbx \u03c0(x) d\u00b51(x)  \u00b51(P   \u03bd-1 Nh(\u03f52) 1 \u03bbx \u03c0(x) d\u00b5-1(x)  \u00b5-1(N  (23)  R(h)  \u03bd1 Ph(\u03f52) 1 \u03bbx \u03c0(x) d\u00b51(x)  \u03bd-1 Nh(\u03f52) 1 \u03bbx \u03c0(x) d\u00b5-1(x) (24) (16) holds since R(h)  P(h(X)  Y )P(g(X)Y 0)  \u03bd1\u00b51(P  h)  \u03bd-1\u00b5-1(N  h).",
      "This provides an interesting decomposition of the adversarial risk into the risk without attack and the loss on the attack zone. Randomization matters Lemma 1. Let h H and \u03c6 BR\u2126mass(h). Then the following assertion holds: \u03c61(x) (Ph) if x Ph(\u03f52) \u03c61(x)  x otherwise. Where (Ph), the complement of Ph in X. \u03c6-1 is characterized symmetrically. Proof. Following the same proof schema as before the adversarial risk writes as follows: \u03c6(FX\u03f52) 2 R\u2126mass adv (h, \u03c6) (25) \u03c6(FX )2 y1 X\u00b5y 1 h (\u03c6y(X))  y \u03bb1 X  \u03c6y(X) 1 X \u03c6y(X) \u03f52 (26) \u03c6(FX )2 y1 X\u00b5y 1 g (\u03c6y(X)) y 0 \u03bb1 X  \u03c6y(X) 1 X \u03c6y(X) \u03f52 (27) y1 \u03c6yFX X\u00b5y 1 g (\u03c6y(X)) y 0 \u03bb1 X  \u03c6y(X) 1 X \u03c6y(X) \u03f52 (28) Finding \u03c61 and \u03c61 are two independent optimization problem, hence we focus on characterizing \u03c61 (i.e. y  1). \u03c61FX X\u00b51 1 g (\u03c61(X)) 0 \u03bb1 X  \u03c61(X) 1 X \u03c61(X) \u03f52 (29) X\u00b51 essup zB.(X,\u03f52) 1 g(z) 0 \u03bb1 X  z (30) essup zB.(x,\u03f52) 1 g(z) 0 \u03bb1 x  z d\u00b51(x). (31) Let us now consider (Hj)jJ a partition of X, we can write.",
      "\u03c61FX X\u00b51 1 g (\u03c61(X)) 0 \u03bb1 X  \u03c61(X) 1 X \u03c61(X) \u03f52 (32) essup zB.(x,\u03f52) 1 g(z) 0 \u03bb1 x  z d\u00b51(x) (33) In particular, we can take H0  P  h, H1  Ph  Ph(\u03f52), and H2  Ph(\u03f52). For x H0  P  h or x H1  Ph  Ph(\u03f52). With the same reasoning as before, any optimal attack will choose \u03c61(x)  x. Let x H2  Ph(\u03f52). We know that B.(x, \u03f52)P  h  , and for any z in this intersection, one has g(z) 0 and z  x. Hence essup zB.(x,\u03f52) 1 g(z) 0 \u03bb1 z  x  max(1 \u03bb, 0). Since \u03bb (0, 1) one has 1 g(z) 0 \u03bb1 z  x  1 \u03bb for any z B.(x, \u03f52) P  h. Then any function that given a x X outputs \u03c61(x) B.(x, \u03f52) P  h is optimal on Finally. Since H0 H1 H2  X, Lemma 1 holds. Lemma 3. Let us consider \u03c6  FX\u03f52 2. If we take h BR(\u03c6), then for y  1 (resp. y  -1), and for any B Ph (resp. B Nh) one has P(Y  yX B) P(Y  yX B) with Y \u03bd and for all y Y, X(Y  y) \u03c6y\u00b5y. Randomization matters Proof. We reason ad absurdum. Let us consider y  1, the proof for y  1 is symmetrical. Let us suppose that there exists C Ph such that \u03bd-1\u03c6-1\u00b5-1(C)  \u03bd1\u03c61\u00b51(C).",
      "We can then construct h1 as follows: h1(x)  h(x) if x C otherwise. Since h and h1 are identical outside C, the difference between the adversarial risks of h and h1 writes as follows: R\u2126mass adv (h, \u03c6) R\u2126mass adv (h1, \u03c6) (34) y1 1 h(x)  y 1 h1(x)  y d(\u03c6y\u00b5y)(x) (35) \u03bd11 h(x)  1 \u03c61\u00b5-1(C) \u03bd11 h1(x)  1 \u03c61\u00b51(C) (36) \u03bd1\u03c61\u00b5-1(C) \u03bd1\u03c61\u00b51(C) (37) Since by hypothesis \u03bd1\u03c61\u00b5-1(C)  \u03bd1\u03c61\u00b51(C) the difference between the adversarial risks of h and h1 is strictly positive. This means that h1 gives strictly better adversarial risk than the best response h. Since, by de\ufb01nition h is supposed to be optimal, this leads to a contradiction. Hence Lemma 3 holds. Additional Result. Let us assume that there is a probability measure \u03b6 that dominates both \u03c61\u00b51 and \u03c6-1\u00b5-1. Let us consider \u03c6  FX\u03f52 2. If we take h BR(\u03c6), then h is the Bayes Optimal Classi\ufb01er for the distribution characterized by (\u03bd, \u03c61\u00b51, \u03c6-1\u00b5-1). Proof.",
      "For simplicity, we denote f1  (d\u03c61\u00b51) and f1  d(\u03c61\u00b5-1) the Radon-Nikodym derivatives of \u03c61\u00b51 and \u03c61\u00b5-1 w.r.t. \u03b6. The best response h minimizes adversarial risk under attack \u03c6. This minimal risk writes: hH R\u2126mass adv (h, \u03c6) (38)  inf y1 x\u00b5y 1 h(\u03c6y(x))  y \u03bb \u2126(\u03c6) . (39) Since the the penalty function does not depend on h, it suf\ufb01ces to seek inf y1 1 h(x)  y d(\u03c6y\u00b5y)(x). Moreover thanks to the transfer theorem, one gets the following: y1 1 h(x)  y d(\u03c6y\u00b5y)(x) (40)  inf y1 1 h(x)  y fy(x) d\u03b6(x) (41)  inf y1 \u03bdy1 h(x)  y fy(x) d\u03b6(x). (42) Finally, since the integral is bounded we get: y1 \u03bdy1 h(x)  y fy(x) d\u03b6(x) (43) y1 \u03bdy1 h(x)  y fy(x) d\u03b6(x). (44) Hence, the best response h is such that for every x X, and y Y, one has h(x)  y if and only if fy(x) fy(x). Thus, h is the optimal Bayes classi\ufb01er for the distribution (\u03bd, \u03c61\u00b51, \u03c6-1\u00b5-1). Furthermore, for y  1 (resp. y  -1), and for any B Ph (resp. B Nh) one has: P(Y  yX B) P(Y  yX B) Randomization matters with Y \u03bd and for all y Y, X(Y  y) \u03c6y\u00b5y.",
      "Theorem 1 (Non-existence of a pure Nash equilibrium). In our zero-sum game with \u03bb (0, 1) and penalty \u2126 \u2126mass, \u2126norm, there is no Pure Nash Equilibrium. Proof. Let h be a classi\ufb01er, \u03c6 BR\u2126(h) an optimal attack against h. We will show that h BR(\u03c6), i.e. that h does not satisfy the condition from Lemma 3. This suf\ufb01ces for Theorem 1 to hold since it implies that there is no (h, \u03c6) H  FX\u03f52 2 such that h BR(\u03c6) and \u03c6 BR\u2126(h). According to Lemmas 1 and 2, whatever penalty we use, there exists \u03b4  0 such that \u03c61\u00b51 (Ph(\u03b4))  0 or \u03c61\u00b5-1 (Nh(\u03b4))  0. Both cases are symmetrical, so let us assume that Ph(\u03b4) is of null measure for the transported distribution conditioned by y  1. Furthermore we have \u03c61\u00b5-1 (Ph(\u03b4))  \u00b5-1 (Ph(\u03b4))  0 since \u03c61 is the identity function on Ph(\u03b4), and since \u00b5-1 is of full support on X. Hence we get the following: \u03c61\u00b5-1 (Ph(\u03b4))  \u03c61\u00b51 (Ph(\u03b4)) . (45) Since the right side of the inequality is null, we also get: \u03c61\u00b5-1 (Ph(\u03b4)) \u03bd-1  \u03c61\u00b51 (Ph(\u03b4)) \u03bd1.",
      "(46) This inequality is incompatible with the characterization of best response for the Defender of Lemma 3. Hence h BR(\u03c6). Theorem 2. (Randomization matters) Let us consider h1 H, \u03bb (0, 1), \u2126 \u2126mass, \u03c6 BR\u2126(h1) and h2 BR(\u03c6). Then for any \u03b1 (max(\u03bb, 1 \u03bb), 1) and for any \u03c6 BR\u2126(mq h) one has R\u2126mass adv (mq h, \u03c6)  R\u2126mass adv (h1, \u03c6). Where h  (h1, h2), q  (\u03b1, 1 \u03b1), and mq h is the mixture of h by q. Figure 3. Illustration of the notations U, U , and U for proof of Theorem 2. Proof. To demonstrate Theorem 2, let us denote U  Ph1(\u03f52) and de\ufb01ne the \u03f52-dilation of U as U \u03f52 : u  v  (u, v) U  X and vp \u03f52 . We can construct h2 as follows h2(x)   h1(x) if x U h1(x) otherwise. Randomization matters This means that h2 changes the class of all points in U, and do not change the rest, compared to h1. Then taking \u03b1 (0, 1), we can de\ufb01ne mq h, and \u03c6 BR\u2126(mq h). We aim to \ufb01nd a condition on \u03b1 so that the score of mq h is lower than the score of h1.",
      "Finally, let us recall that R\u2126mass adv (mq h, \u03c6)  \u03bd1 essup zB.(x,\u03f52) \u03b11 h1(z)  -1  (1 \u03b1)1 h2(z)  -1 \u03bb1 x  z d\u00b51(x)  \u03bd-1 essup zB.(x,\u03f52) \u03b11 h1(z)  10  (1 \u03b1)1 h2(z)  1 \u03bb1 x  z d\u00b5-1(x). The only terms that may vary between the score of h1 and the score of mq h are the integrals on U, U \u03f52 Ph1 and \u03c61 -1 (U)  inverse image of U by \u03c6-1. These sets represent respectively the points we mix on, the points that may become attacked  when changing from h1 to mq h  by moving them on U, and the ones that were  for h1  attacked before by moving them on U. Hence, for simplicity, we only write those terms. Furthermore, we denote U  : U \u03f52 Ph1  U, U : \u03c61 -1 (U) and recall U : Ph1(\u03f52). One can refer to Figure 3 for visual interpretation of this sets. We can now evaluate the worst case adversarial score for h1 restricted to the above sets.",
      "Thanks to Lemma 1 that characterizes \u03c6, we can write R\u2126mass adv (h1, \u03c6)U, U , U   (1 \u03bb)  \u03bd1\u00b51 (U)  \u03bd-1\u00b5-1(U)  0  \u03bd1\u00b51 U   \u03bd-1\u00b5-1 U   \u03bd1\u00b51 U   (1 \u03bb)  \u03bd-1\u00b5-1 U  Similarly, we can write the worst case adversarial score of the mixture on the sets we consider. Note that the max operator comes from the fact that the adversary has to make a choice between attacking the zone or just take advantage of the error due to randomization. R\u2126mass adv (mq h, \u03c6)U, U , U   max (1 \u03b1, 1 \u03bb)  \u03bd1\u00b51 (U)  max (\u03b1, 1 \u03bb)  \u03bd-1\u00b5-1(U)  max (0, 1 \u03b1 \u03bb)  \u03bd1\u00b51 U   \u03bd-1\u00b5-1 U   \u03bd1\u00b51 U   max (0, \u03b1 \u03bb)  \u03bd-1\u00b5-1 U  Computing the difference between these two terms, we get the following R\u2126mass adv (h1, \u03c6) R\u2126mass adv (mq h, \u03c6) (47)  (1 \u03bb max (1 \u03b1, 1 \u03bb))  \u03bd1\u00b51 (U) (48)  (1 max (\u03b1, 1 \u03bb))  \u03bd-1\u00b5-1 (U) (49) max (0, 1 \u03b1 \u03bb)  \u03bd1\u00b51 U  (50)  (1 \u03bb max (0, \u03b1 \u03bb))  \u03bd-1\u00b5-1 U  (51) Let us now simplify Equation (47) using additional assumptions. First, we have that Equation (49) is equal to min (1 \u03b1, \u03bb) \u00b5-1(U)\u03bd-1  0.",
      "Thus, a suf\ufb01cient condition for the difference between the adversarial scores to be positive is to have the other terms greater or equal to 0. Randomization matters  To have Equation (48) 0 we can always set max (1 \u03b1, 1 \u03bb)  1 \u03bb. This gives us \u03b1 \u03bb. Also note that to get (50) 0, we can force max (1 \u03b1 \u03bb, 0)  0. This gives us \u03b1 1 \u03bb. Finally, since \u03b1 \u03bb, we have that 1 \u03bb max (0, \u03b1 \u03bb)  1 \u03b1 thus Equations (51)  0. With the above simpli\ufb01cations, we have (47)  0 for any \u03b1  max(\u03bb, 1 \u03bb) which concludes the proof. Theorem 3. (Randomization matters) Let us consider h1 H, \u03bb (0, 1), \u2126 \u2126norm, \u03c6 BR\u2126(h1) and h2 BR(\u03c6). Let us take \u03b4 (0, \u03f52), then for any \u03b1 (max(1 \u03bb\u03b4, \u03bb(\u03f52 \u03b4)), 1) and for any \u03c6 BR\u2126(mq h) one has R\u2126norm adv (mq h, \u03c6)  R\u2126norm adv (h1, \u03c6). Where h  (h1, h2), q  (\u03b1, 1 \u03b1), and mq h is the mixture of h by q. Figure 4. Illustration of the notations U, U , U and \u03b4 for proof of Theorem 3. Proof. Let us take U Ph1(\u03f52) such that xU x \u03c0PhPh(\u03f52)(x) \u03b4 (0, \u03f52) . We construct h2 as follows.",
      "h2(x)   h1(x) if x U h1(x) otherwise. This means that h2 changes the class of all points in U, and do not change the rest. Let \u03b1 (0, 1), the corresponding mixture h, and \u03c6 BR\u2126(mq h). We will \ufb01nd a condition on \u03b1 so that the score of mq h is lower than the score of h1. Recall that R\u2126norm adv (mq h, \u03c6)  \u03bd1 essup zB.(x,\u03f52) \u03b11 h1(z)  -1  (1 \u03b1)1 h2(z)  -1 \u03bbx zd\u00b51(x)  \u03bd-1 essup zB.(x,\u03f52) \u03b11 h1(z)  1  (1 \u03b1)1 h2(z)  1 \u03bbx zd\u00b5-1(x). As we discussed in proof of Theorem 2, the only terms that may vary between the score of h1 and the score of mq h are the integrals on U, U \u03f52 Ph1 and \u03c61 -1 (U). Hence, for simplicity, we only write those terms. Furthermore, we denote U  : U \u03f52 Ph1  U, U : \u03c61 -1 (U) and P\u03f52 : Ph1(\u03f52). Randomization matters One can refer to Figure 4 for a visual interpretation of this ensembles. We can now evaluate the worst case adversarial score for h1 restricted to the above sets.",
      "Thanks to Lemma 2 that characterizes \u03c6, we can write R\u2126norm adv (h1, \u03c6)  \u03bd1 1 \u03bbx \u03c0P  h1(x) d\u00b51(x)  \u03bd-1\u00b5-1(U)  \u03bd1 U P\u03f52 0 d\u00b51(x)  \u03bd-1\u00b5-1 U   P\u03f52  \u03bd1 U P\u03f52 1 \u03bbx \u03c0P  h1(x) d\u00b51(x)  \u03bd-1\u00b5-1 U  P\u03f52  \u03bd1\u00b51 U   \u03bd-1 1 \u03bbx \u03c0U(x) d\u00b5-1(x). Similarly we can evaluate the worst case adversarial score for the mixture, R\u2126norm adv (mq h, \u03c6)  \u03bd1 1 \u03b1, 1 \u03bbx \u03c0P  h1(x) d\u00b51(x)  \u03bd-1 max (\u03b1, 1 \u03bbx \u03c0U (x)) d\u00b5-1(x)  \u03bd1 U P\u03f52 max (0, 1 \u03b1 \u03bbx \u03c0U(x)) d\u00b51(x)  \u03bd-1\u00b5-1 U   P\u03f52  \u03bd1 U P\u03f52 1 \u03b1 \u03bbx \u03c0U(x), 1 \u03bbx \u03c0P  h1(x) d\u00b51(x)  \u03bd-1\u00b5-1 U  P\u03f52  \u03bd1\u00b51 U   \u03bd-1 0, 1 \u03bbx \u03c0N h1U(x), \u03b1 \u03bbx \u03c0U(x) d\u00b5-1(x). Note that we need to take into account the special case of the points in the dilation that were already in the attacked zone before, and that can now be attacked in two ways, either by projecting on U  but that works with probability \u03b1, since the classi\ufb01cation on U is now randomized  or by projecting on P  h1, which works with probability 1 but may use more distance and so pay more penalty.",
      "We can now compute the difference between both scores. R\u2126norm adv (h1, \u03c6) R\u2126norm adv (mq h, \u03c6) (52)  \u03bd1 1 \u03bbx \u03c0P  h1(x)max 1 \u03b1, 1 \u03bbx \u03c0P  h1(x) d\u00b51(x) (53)  \u03bd-1 1 max (\u03b1, 1 \u03bbx \u03c0U (x)) d\u00b5-1(x) (54) U P\u03f52 max (1 \u03b1 \u03bbx \u03c0U(x), 0) d\u00b51(x) (55)  \u03bd1 U P\u03f52 1 \u03bbx \u03c0P  h1(x) Randomization matters max 1 \u03b1 \u03bbx \u03c0U(x), 1 \u03bbx \u03c0P  h1(x) d\u00b51(x) (56)  \u03bd-1 1 \u03bbx \u03c0U(x) max 0, 1 \u03bbx \u03c0N h1U(x), \u03b1 \u03bbx \u03c0U(x) d\u00b5-1(x). (57) Let us simplify Equation (52) using using additional hypothesis:  First, note that Equation (54) 0. Then a suf\ufb01cient condition for the difference to be strictly positive is to ensure that other lines are 0. In particular to have (53) 0 it is suf\ufb01cient to have for all x U 1 \u03b1, 1 \u03bbx \u03c0P  h1(x)  1 \u03bbx \u03c0P  h1(x). This gives us \u03b1 \u03bb(\u03f52 \u03b4) \u03bb max xU x \u03c0P  h1(x). Similarly, to have (55) 0, we should set for all x U   P\u03f52 \u03b1 1 \u03bbx \u03c0U(x). Since xU P\u03f52 x \u03c0U(x) \u03b4, we get the condition \u03b1 1 \u03bb\u03b4. Finally (57) 0, since by de\ufb01nition of U , for any x U we have x \u03c0N h1U(x)x \u03c0U(x).",
      "Finally, by summing all these simpli\ufb01cations, we have (52)  0. Hence the result hold for any \u03b1  max(1 \u03bb\u03b4, \u03bb(\u03f52  2. Experimental results In the experimental section, we consider X  0, 133232 to be the set of images, and Y  1, ..., 10 or Y  1, ..., 100 according to the dataset at hand. 2.1. Adversarial attacks Let (x, y) D and h H. We consider the following attacks: (i) \u2113-PGD attack. In this scenario, the Adversary maximizes the loss objective function, under the constraint that the \u2113 norm of the perturbation remains bounded by some value \u03f5. To do so, it recursively computes: xt1  \u03a0B.(x,\u03f5) xt  \u03b2 sgn xL xt (58) where L is some differentiable loss (such as the cross-entropy), \u03b2 is a gradient step size, and \u03a0S is the projection operator on S. One can refer to (Madry et al., 2018) for implementation details. (ii) \u21132-CW attack. In this attack, the Adversary optimizes the following objective: argmin \u03c42  \u03bb  cost(x  \u03c4) (59) where cost(x  \u03c4)  0 if and only if h(x  \u03c4)  y.",
      "The authors use a change of variable \u03c4  1 2(tanh(w) x  1) to ensure that x  \u03c4 X, a binary search to optimize the constant \u03bb, and Adam or SGD to compute an approximated solution. One should refer to (Carlini  Wagner, 2017) for implementation details. Randomization matters 2.2. Experimental setup Datasets. To illustrate our theoretical results we did experiments on the CIFAR10 and CIFAR100 datasets. See (Krizhevsky et al., 2009) for more details. Classi\ufb01ers. All the classi\ufb01ers we use are WideResNets (see (Zagoruyko  Komodakis, 2016)) with 28 layers, a widen factor of 10, a dropout factor of 0.3 and LeakyRelu activations with a 0.1 slope. Natural Training. To train an undefended classi\ufb01er we use the following hyperparameters.",
      "Number of Epochs: 200  Batch size: 128  Loss function: Cross Entropy Loss  Optimizer : SGD algorithm with momentum 0.9, weight decay of 2  104 and a learning rate that decreases during the training as follows: lr  epoch 0.02 epoch 0.004 epoch 0.0008 epoch Adversarial Training. To adversarially train a classi\ufb01er we use the same hyperparameters as above, and generate adversarial examples using the \u2113-PGD attack with 20 iterations. When considering that the input space is 0, 25533232, on CIFAR10 and CIFAR100, a perturbation is considered to be imperceptible for \u03f5 8. Here, we consider X  0, 133232 which is the normalization of the pixel space 0.25533232. Hence, we choose \u03f52  0.031 (8255) for each attack. Moreover, the step size we use for \u2113-PGD is 0.008 (2255), we use a random initialization for the gradient descent and we repeat the procedure three times to take the best perturbation over all the iterations i.e the one that maximises the loss.",
      "For the \u2113-PGD attack against the mixture mq h, we use the same parameters as above, but compute the gradient over the loss of the expected logits (as explained in the main paper). Evaluation Under Attack. At evaluation time, we use 100 iterations instead of 20 for Adaptive-\u2113-PGD, and the same remaining hyperparameters as before. For the Adaptive-\u21132-CW attack, we use 100 iterations, a learning rate equal to 0.01, 9 binary search steps, and an initial constant of 0.001. We give results for several different values of the rejection threshold: \u03f52 0.4, 0.6, 0.8. Computing Adaptive-\u21132-CW on a mixture To attack a randomized model, it is advised in the literature (Tramer et al., 2020) to compute the expected logits returned by this model. However this advice holds for randomized models that return logits in the same range for a same example (e.g. classi\ufb01er with noise injection). Our randomized model is a mixture and returns logits that depend on selected classi\ufb01er.",
      "Hence, for a same example, the logits can be very different. This phenomenon made us notice that for some example in the dataset, computing the expected loss over the classi\ufb01er (instead of the expected logits) performs better to \ufb01nd a good perturbation (it can be seen as computing the expectation of the logits normalized thanks to the loss). To ensure a fair evaluation of our model, in addition of using EOT with the expected logits, we compute in parallel EOT with the expected loss and take the perturbation that maximizes the expected error of the mixture. See the submitted code for more details. Library used. We used the Pytorch and Advertorch libraries for all implementations. Machine used. 6 Tesla V100-SXM2-32GB GPUs 2.3. Experimental details Sanity checks for Adaptive attacks In (Tramer et al., 2020), the authors give a lot of sanity checks and good practices to design an Adaptive attacks.",
      "We follow them and here are the information for Adaptive-\u2113-PGD : Randomization matters  We compute the gradient of the loss by doing the expected logits over the mixture. The attack is repeated 3 times with random start and we take the best perturbation over all the iterations. When adding a constant to the logits, it doesnt change anything to the attack  When doing 200 iterations instead of 100 iterations, it doesnt change the performance of the attack  When increasing the budget \u03f5, the accuracy goes to 0, which ensures that there is no gradient masking. Here are some values to back this statement: Epsilon 0.015 0.031 0.125 0.250 Accuracy 0.638 0.546 0.027 0.000 Table 2. Evolution of the accuracy under Adaptive-\u2113-PGD attack depending on the budget \u03f5  The loss doesnt \ufb02uctuate at the end of the optimization process. Selecting the \ufb01rst element of the mixture. Our algorithm creates classi\ufb01ers in a boosting fashion, starting with an adversarially trained classi\ufb01er.",
      "There are several ways of selecting this \ufb01rst element of the mixture: use the classi\ufb01er with the best accuracy under attack (option 1, called bestAUA), or rather the one with the best natural accuracy (option 2). Table 3 compares both options. Beside the fact that any of the two mixtures outperforms the \ufb01rst classi\ufb01er, we see that the \ufb01srt option always outperforms the second. In fact, when taking option 1 (bestAUA  True) the accuracy under \u2113-PGD attack of the mixture is 3 better than with option 2 (bestAUA  False). One can also note that both mixtures have the same natural accuracy (0.80), which makes the choice of option 1 natural. Training method NA of the 1st clf AUA of the 1st clf NA of the mixture AUA of the mixture BAT (bestAUATrue) 0.77 0.46 0.80 0.55 BAT (bestAUAFalse) 0.83 0.42 0.80 0.52 Table 3. Comparison of the mixture that has as \ufb01rst classi\ufb01er the best one in term of natural accuracy and the mixture that has as \ufb01rst classi\ufb01er the best one in term of Accuracy under attack.",
      "The accuracy under attack is computed with the \u2113-PGD attack. NA means matural accuracy, and AUA means accuracy under attack. 2.4. Extension to more than two classi\ufb01ers As we mention in the main part of the paper, a mixture of more than two classi\ufb01ers can be constructed by adding at each step t a new classi\ufb01er trained naturally on the dataset D that contains adversarial examples against the mixture at step t 1. Since D has to be constructed from a mixture, one would have to use an adaptive attack as Adaptive-\u2113-PGD. Here is the algorithm for the extented version : Randomization matters Algorithm 2 Boosted Adversarial Training Input : n the number of classi\ufb01ers, D the training data set and \u03b1 the weight update parameter. Create and adversarially train h1 on D h  (h1) ; q  (1) for i  2, . . . , n do Generate the adversarial data set D against mq Create and naturally train hi on D qk (1 \u03b1)qk k i 1 qi \u03b1 q (\u03b1, . . . , qi) h (h1, . . .",
      ", hi) return mq Here to \ufb01nd the parameter \u03b1, the grid search is more costly. In fact in the two-classi\ufb01er version we only need to train the \ufb01rst and second classi\ufb01er without taking care of \u03b1, and then test all the values of \u03b1 using the same two classi\ufb01er we trained. For the extended version, the third classi\ufb01er (and all the other ones added after) depends on the \ufb01rst classi\ufb01er, the second one and their weights 1 \u03b1 and \u03b1. Hence the third classi\ufb01er for a certain value of \u03b1 cant be use for another one and, to conduct the grid search, one have to retrain all the classi\ufb01ers from the third one. Naturally the parameters \u03b1 depends on the number of classi\ufb01ers n in the mixtures."
    ],
    "word_count": 12318,
    "page_count": 23
  },
  "OCEAN": {
    "chunks": [
      "Ocean Protocol: Tools for the Web3 Data Economy Ocean Protocol Foundation with BigchainDB GmbH Technical Whitepaper, Version 2022-SEP-01 Abstract Ocean Protocol is an on-ramp for data services into crypto ecosystems, using data NFTs and datatokens. Data NFTs are like master tapes, and datatokens are like CDs: base IP and licenses respectively. A data NFT is a non-fungible ERC721 token representing copyright of a data service; a datatoken is a fungible ERC20 token to access the service. Ocean smart contracts and libraries make it easy to publish data services (deploy and mint data NFTs  datatokens) and consume data services (spend datatokens). Ocean contracts are deployed to Ethereum mainnet and other EVM networks. Ethereum composability enables crypto wallets as data wallets, crypto exchanges as data marketplaces, DAOs as data coops, datatoken-backed stable assets, and more. Ocean Market is an open-source community marketplace for data.",
      "Oceans Compute-to-Data feature gives compute access on privately held data, which never leaves the data owners premises. Ocean-based marketplaces enable monetization of private data while preserving privacy. Token dynamics are designed such that OCEAN health rises with usage volume. The community-driven OceanDAO funds software development, outreach, and more. veOCEAN is a means to lock OCEAN to be able to vote in OceanDAO and earn ecosystem rewards. veOCEAN holders can earn passively, but can earn more by actively allocating veOCEAN to data assets that have high data consume volume (DCV). We envision thousands of data marketplaces, where Ocean Market is just one. In addition to Ocean Market being open-source (and therefore forkable), Ocean includes tools to help developers build their own marketplaces and other apps. These tools are part of a system designed for long-term growth of a permissionless Web3 Data Economy. The Ocean Data Farming program incentivizes a supply of data.",
      "2 bigchaindb.com 1 oceanprotocol.com Contents 1. Overview 1.1. Introduction 1.2. Paper Organization 2. Top-Level Block: Ocean System 2.1. Goals 2.2. The Design 2.3. Kickstarting Growth, and Long-Term Sustainability 3. Subblock: Data Ecosystem Powered by Ocean Tools 3.1. Introduction 3.2. USPs of Ocean Tools 3.3. Ocean Tools Foundation: Data NFTs  Datatokens 3.3.1. Introduction 3.3.2. Mental Model 3.3.3. IP Backgrounder 3.3.4. IP  Data NFTs  Datatokens 3.3.5. Datatokens are ERC20 Access Tokens 3.3.6. Data Access Variants 3.4. Ocean Tools Architecture 3.4.1. Overview 3.4.2. Data NFTs, Datatokens, and Access Control Tools 3.4.3. Market Tools 3.4.4. Metadata Tools 3.4.5. Third-Party Apps  Tools 3.4.6. Actor Identities 3.5. Ocean Tools: Multi-Network Deployment 3.6. Ocean Tools: Data Marketplaces 3.6.1. Introduction 3.6.2. Marketplaces Architecture 3.6.3. Ocean Marketplace Actions 3.6.4. Developer Tools  3rd Party Marketplaces 3.6.5. Sustainability  Fees 3.6.6.",
      "IP Violations and Sensitive Data 3.6.7. Benefits of Ocean Data Marketplaces 3.7. Ocean Tools: Compute-to-Data 3.7.1. Motivation 3.7.2. Conceptual Working 3.7.3. Compute-to-Data Flow Variants 3.7.4. Share, or Monetize 3.7.5. Compute-to-Data Architecture 3.7.6. Marketplaces and Compute-to-Data 3.7.7. Trusting Algorithms 3.7.8. Benefits of Compute-to-Data 3.7.9. Relation to other Privacy-Preserving Technologies 3.8. Ocean Tools: Fine-Grained Permissions 3.8.1. Introduction 3.8.2. Market-Level Permissions 3.8.3. Asset-Level Permissions 4. Subblock: OceanDAO 4.1. Introduction 4.2. OceanDAO Components 4.3. OceanDAO Implementation 5. Ocean Data Farming 5.1. Overview 5.2. Reward Function 6. On OCEAN Token Model 6.1. Introduction 6.2. Token Value  Volume 6.3. Work Token (and Governance Token) 6.4. Burning 6.5. veOCEAN 6.6. User Acquisition 6.7. On OCEAN as Unit-of-Exchange 7. Ocean Applications 7.1. Decentralized Orchestration 7.2. Data Wallets: Data Custody  Data Management 7.3.",
      "Data Auditability 7.4. Data DAOs: Data Co-ops and More 7.5. Fractional Ownership 7.6. Group-Restricted Data Access 7.7. Unlocking Latent Data of Individuals  Enterprises 7.8. Data Marketplaces 7.9. Data as an Asset Class for DeFi 7.10. Data to Optimize DeFi Returns 7.11. Data Management Platforms for Smart Cities and More 7.12. Composable Data NFTs  Datatokens 8. Conclusion 9. References Overview 1.1. Introduction Modern society runs on data Economist2017. Modern artificial intelligence (AI) extracts value from data. More data means more accurate AI models Banko2001 Halevy2009, which in turn means more benefits to society and business. The greatest beneficiaries are companies that have both vast data and internal AI expertise, like Google and Facebook. In contrast, AI startups have excellent AI expertise but are starving for data; and typical enterprises are drowning in data but have less AI expertise. The power of both data and AIand therefore societyis in the hands of few.",
      "The aim of Ocean Protocol is to spread the benefits of AI by equalizing the opportunity to access and monetize data. We accomplish this by creating simple tools to publish data and consume data as decentralized data NFTs  datatokens. Data NFTs  tokens interoperate with ERC721  ERC20 wallets, exchanges, DAOs and more. This data may be held on-premise to preserve privacy. Additionally, Ocean has tools for data marketplaces. We have implemented these tools as Solidity code running on Ethereum mainnet Buterin2013 and other EVM networks EthereumEvm2022; as Python and JavaScript libraries to ease higher-level integration; and as a community data marketplace web application. These tools are encapsulated in a broader system design for long-term growth of an open, permissionless data economy. The Data Farming program incentivizes a supply of data. OceanDAO funds software development, outreach, and more.",
      "OceanDAO will be funded by revenue from apps and services in the Ocean data ecosystem, Ocean network rewards, and Ocean Protocol Foundation. These are all detailed in the paper. Everything described above has been deployed live as of Nov 30, 2020. 1.2. Paper Organization The rest of this paper is organized as follows. Ocean System. Section 2 describes Oceans system-level design, which is designed to facilitate the emergence of an open data ecosystem. Ocean Subblocks. Then, several sections describe key building blocks in the Ocean system: Section 3 describes Ocean Tools to power a Data Ecosystem, including data NFTs  datatokens, tools software architecture, network deployment, data marketplaces, and Compute-to-Data. Section 4 describes OceanDAO. Ocean Data Farming. Section 5 describes a program to kickstart a supply of quality data. OCEAN token. Section 6 describes the OCEAN token model, bringing together information presented in prior sections. Applications.",
      "Section 7 describes applications of Ocean. Final. Section 8 concludes. Section 9 holds references. Top-Level Block: Ocean System 2.1. Goals The top-level goal is to spread the benefits of AI by equalizing the opportunity to access and monetize data. We can make this more specific. The Ocean System has these aims: An overall system that is sustainable and growing, towards ubiquity. Basic design is simple to understand and communicate. In line with Ocean Mission and Values McConaghy2018. These include: unlock data, preserve privacy, spread of power, spread of wealth, stay within the law, censorship-resistant  trustless. It should be permissionless, rent-free, and useful to the world Goldin2017. It should be anti-fragile: get more resilient when kicked, therefore also needing evolvability. It follows time scales of decades not months. 2.2. The Design Figure 1 shows the Ocean System design. At its heart is a loop, designed for snowball effect growth of the ecosystem.",
      "The Workers (center) do work to help grow the Data Ecosystem (right). Marketplaces and other data ecosystem services generate revenue, using Ocean software tools. A tiny fraction of that revenue is looped back (arrow looping from right to left) as Network revenue to the Ocean community: to Buy  Burn OCEAN (bottom left) and back to workers curated by OceanDAO (center-left). To catalyze growth and ensure decent funding in early days, Network rewards (left) also feed to Workers via OceanDAO. Figure 1: Ocean System 2.3. Kickstarting Growth, and Long-Term Sustainability The system design achieves sustainability over decades via funding that goes to teams (Workers) that continually add value to the Data Ecosystem. Funding comes from Network Revenue and from Network Rewards. In the longer term, the bulk of revenue will be from Network Revenue.",
      "To help kickstart growth, 51 of the overall OCEAN token supply is dedicated to Network Rewards, which follow a Bitcoin-like disbursement schedule (but with a ramp-up period). The Ocean token is designed such that as usage of Ocean grows, it grows the value accrued in OCEAN. Heres the loop: more usage of Ocean tools (in the data ecosystem) leads to more Network Revenue, which goes to burning and OceanDAO. Burning OCEAN reduces supply, to grow the value accrued in OCEAN. Funds go through OceanDAO to workers who have the mandate to grow usage of Ocean tools. And the loop repeats. OceanDAO curates funding coming from Network Revenue and Network Rewards. It gets people to do work such as improve Ocean core software, build applications using Ocean, spread awareness, grow data supply, and more. A Data Farming program incentivizes data supply. This system design generalizes beyond Ocean; we can call it the Web3 Sustainability Loop. It draws inspiration from businesses and nations.",
      "The essay McConaghy2020d elaborates. Subblock: Data Ecosystem Powered by Ocean Tools This section and the ones that follow add details on subblocks of the Ocean system. 3.1. Introduction The Data Ecosystem is a major subblock of the Ocean system. Ocean tools power the Data Ecosystem. This section describes Ocean tools. 3.2. USPs of Ocean Tools Towards building something that people want Graham2005, Ocean tools offer these unique selling propositions (USPs): Earn by selling data, and staking on data. Ocean Market makes it easy to sell your data, whether you are an individual, company, or city. Furthermore, users can stake on data: lock OCEAN then allocate veOCEAN to datasets for rewards. An on-ramp and off-ramp for data assets into crypto, allowing: crypto wallets for data custody  data management, DEXes for data exchanges, DAOs for data co-ops, datatoken-backed stable assets like H2O NewOrder2021, securitizing data assets, and more via DeFi composability. Its data legos.",
      "The data itself does not need to be on-chain, just the access control. Quickly launch a data marketplace, with many USPs: buy  sell private data while preserving privacy, non-custodial, censorship-resistant, auto price discovery, data audit trails, and more. Decentralized data exchange platform, enabling these characteristics: improve the visibility, transparency and flexibility in usage of data; share data while avoiding data escapes; needs little dev-ops support and maintenance; has high liveness; is non-custodial; and is censorship-resistant. E.g. Ocean as a traffic data management platform for smart cities. E.g. federated learning without having to trust the orchestration middleman McConaghy2020c. 4 DAO  Decentralized Autonomous Organization 3 DEX  Decentralized Exchange 3.3. Ocean Tools Foundation: Data NFTs  Datatokens 3.3.1. Introduction Ocean is an on-ramp and off-ramp for data assets into crypto ecosystems, using data NFTs and datatokens.",
      "With analogy to music: Data NFTs are like master tapes, and datatokens are like CDs: base IP and licenses respectively. A data NFT is a non-fungible ERC721 token ERC721 representing base IP - copyright or exclusive rights of a data service. This is the equivalent to a music master tape. A datatoken is a fungible ERC20 token Vogelsteller2015; having 1.0 datatokens for a given data service means you can access that service; its a license against the base IP. This is the equivalent to a music CD. Each data service gets its own data NFT; and zero or more datatokens against that data NFT. Ocean smart contracts and libraries make it easy to publish data services (deploy and mint data NFTs  datatokens) and consume data services (spend datatokens). This setup helps keep Ocean as simple as possible (but no simpler), composable, and make Ocean a set of tools (versus a platform that might lock you in). 3.3.2. Mental Model Figure 2 shows the mental model.",
      "Ocean does the beginning (create data assets) and the end (consume data asset). In between are any ERC20-based applications, including Ocean-based marketplaces. Figure 2: Mental model. Data NFTs  datatokens are the interface to connect data assets with DeFi tools. Ocean is an on-ramp for data services into data NFT  datatoken data assets on Ethereum, and an off-ramp to consume data. 3.3.3. IP Backgrounder Heres a quick background on IP, via the example of music. It also applies for books, videos, and data. A more thorough treatment can be found in McConaghy2021a. laws for USA, Germany, and 177 other countries follow the 1983 Berne Convention WIPO2022, which provides creators with the means to control how their work is used, by whom, and on what terms. In it, the creator of a work doesnt need to apply for copyright, or to register it. Rather, tape or as magnetic bits on a hard disk. Exclusive Rights.",
      "When Universal Music bought the rights to Michael Jacksons recording of the song Billie Jean, Universal got an exclusive license (a contract) to use that song for their own profit. The base IP was transferred to Universal. Sub-licenses. When you buy a music CD, youre buying the license to listen to it, for personal consumption. But you cant resell it or play it on the radio, since you dont have the license for that. 3.3.4. IP  Data NFTs  Datatokens Ocean uses both ERC721 data NFTs and ERC20 datatokens. A data NFT is a non-fungible ERC721 token representing base IP - copyright or exclusive rights of a data service. Base IP rights include the ability to create access licenses (deploy ERC20 datatokens, and mint them tokens). You can transfer a data NFT to another person, for them to hold those exclusive rights. A datatoken is a fungible ERC20 token; having 1.0 datatokens for a given data service means you can access that service; its a license against the base IP.",
      "McConaghy2021c has details. Note: Ocean V3 (released fall 2020) used just ERC20 datatokens. The base rightsholder was implicit: the publisher, who can mint more ERC20 tokens. McConaghy2021b has details. We moved to data NFTs and datatokens for Ocean V4 (released spring 2022). Note: We could have used ERC721 for data access control, where you can access the dataset if you hold the token. However, datasets typically get shared among 1 people. For this we need fungibility, which is the realm of ERC20. ERC20 brings many more benefits, especially easy price discovery via various data pricing approaches. 3.3.5. Datatokens are ERC20 Access Tokens Traditional access tokens exist, such as OAuth 2.0 Oauth2020. If you present the token, you can get access to the service. However, the tokens are simply a string of characters, and transfer is basically copying and pasting that string.",
      "This means they can easily be double-spent: if one person gets access, they can share that access with innumerable others, even if that access was only meant for them. These tokens arent the tokens we think of in blockchain. How do we address the double-spend problem? This is where blockchain technology comes in. In short, theres a single shared global database that keeps track of who owns what, and can then easily prevent people from spending the same token twice. This generalizes beyond Bitcoin tokens to other token assets. ERC20 Vogelsteller2015 was developed as a standard for token ownership actions. Its been adopted widely in Ethereum and beyond. Its focus is fungible tokens, where tokens are fully interchangeable. We can connect the idea of access with the ERC20 token standard. Specifically, consider an ERC20 token where you can access the dataset if you hold 1.0 tokens. To access the dataset, you send 1.0 datatokens to the data provider.",
      "You have custody of the data if you have at least 1.0 tokens. To give access to someone else, send them 1.0 datatokens. Thats it! But now, the double-spend problem is solved for access control, and by following a standard, theres a whole ecosystem around it to support that standard. 3.3.6. Data Access Variants At the smart contract level, data NFTs  datatokens dont differ. Variants emerge in the semantic interpretation by libraries run by the data provider, one level up. Here are some variants: Access could be perpetual (access as many times as you like), time-bound (e.g. access for just one day, or within specific date range), or one-time (after you access, the token is burned). Data access is always treated as a data service. This could be a service to access a static dataset (e.g. a single file), a dynamic dataset (stream), or for a compute service (e.g. bring compute to the data). For static data, we can tune variants based on the type of storage: Web2 cloud (e.g.",
      "AWS S3), Web3 non-permanent (e.g. Filecoin), Web3 permanent small-scale (e.g. Ethereum), Web3 permanent large-scale (e.g. Arweave), or go meta using IPFS but pinned (served up) by many places. For dynamic data, variants include Web2 streaming APIs (single-source), Web3 public data oracles (e.g. Chainlink), and Web3 private data oracles (e.g. DECO). The terms of access are specified in the metadata, which is on-chain (more on this later). Figure 3: Ocean tools architecture 3.4. Ocean Tools Architecture 3.4.1. Overview Figure 3 shows the Ocean tools architecture. The lowest level has the smart contracts, which are deployed on Ethereum mainnet and other networks. Above that are libraries and middleware, which expose the contracts to higher-level languages and provide convenience utilities. The top layer is applications.",
      "Left to right are groupings of functionality: tools for data NFTs  datatokens, tools for markets (including pools), tools to consume data services and for metadata, and external ERC20 tools. The following subsections elaborate; more information yet is in the documentation OceanDocs2022 and the open-source code itself OceanContracts2022 OceanLibPy2022 OceanLibJs2022OceanMarket2022. 3.4.2. Data NFTs, Datatokens, and Access Control Tools The publisher actor holds the dataset in Google Drive, Dropbox, AWS S3, on their phone, on their home server, etc. The dataset has a URL. The publisher can optionally use IPFS for a content-addressable URL. Or instead of a file, the publisher may run a compute-to-data service. In the publish step, the publisher invokes Ocean Data NFT Factory to deploy a new data NFT to the chain. The publisher can then invoke Ocean Datatoken Factory to deploy a new datatoken contract (against the data NFT) to the chain), then mints datatokens.",
      "To save gas fees, deployments use ERC1167 proxy approach on templates for the data NFT and datatoken. The publisher runs Ocean Provider. In the consume step, Provider software needs to retrieve the data service URL given a datatoken address. One approach would be for the publisher to run a database; however this adds another dependency. To avoid this, it stores the URL on-chain. So that others dont see that URL, it encrypts it. To initiate the consume step, the data consumer sends 1.0 datatokens to the Provider wallet. Then they make a service request to the Provider. The Provider loads the encrypted URL, decrypts it, and provisions the requested service (send static data, or enable a compute-to-data job). The Appendix has details of this process. Instead of running a Provider themselves, the publisher can have a 3rd party like Ocean Market run it. While more convenient, it means that the 3rd party has custody of the private encryptiondecryption key (more centralized).",
      "Ocean will support more service types and url custody options in the future. Ocean JavaScript and Python libraries act as drivers for the lower-level contracts. Each library integrates with Ocean Provider to provision  consume data services, and Ocean Aquarius for metadata. 3.4.3. Market Tools Once someone has generated datatokens, they can be used in any ERC20 exchange. In addition, Ocean provides Ocean Market, a reference data marketplace for use by the Ocean community. Its decentralized (no single owner or controller), and non-custodial (only the data owner holds the keys for the datatokens). Ocean Market supports fixed pricing. For fixed pricing, theres a simple contract for users to buysell datatokens for OCEAN, while avoiding custodianship during value transfer. Complementary to Ocean Market, Ocean has reference code to ease building third-party data marketplaces, such as for mobility (Daimler OceanBlog2020b). 3.4.4.",
      "Metadata Tools Metadata (name of dataset, dateCreated etc.) is used by marketplaces for data asset discovery. Each data asset can have a decentralized identifier (DID) DID2019 that resolves to a DID document (DDO) for associated metadata. The DDO is essentially JSON filling in metadata fields. Ocean uses the blockchain network as an on-chain metadata store, i.e. to store both DID and DDO. This means that once the write fee is paid, there are no further expenses or dev-ops work needed to ensure metadata availability into the future, aiding in the discoverability of data assets. It also simplifies integration with the rest of the Ocean system, which is Ethereum-based. Storage cost on blockchain networks is not negligible, but not prohibitive and the other benefits are currently worth the tradeoff compared to alternatives. Due to the permissionless, decentralized nature of data on Ethereum mainnet (and other EVM networks), any last-mile tool can access metadata.",
      "Ocean Aquarius supports different metadata fields for each different Ocean-based marketplace. Third-party tool TheGraph sees metadata fields that are common across all marketplaces. To address GDPR right-to-be-forgotten, metadata is stored encrypted. The publisher running a Provider provides a REST service to get decrypted metadata. Typically, an Ocean-powered market will retrieve decrypted metadata from various publishers, then store the decrypted metadata in its local Aquarius. 3.4.5. Third-Party Apps  Tools Because they are ERC721  ERC20, data NFTs  datatokens have great potential composability with other Ethereum tools and apps. This includes including MetaMask and Trezor as data wallets, DEXes as data exchanges, and more. The Applications section expands on this. 3.4.6. Actor Identities Actors like data providers and consumers have Ethereum addresses, aka web3 accounts. These are managed by crypto wallets, as one would expect. For most use cases, this is all thats needed.",
      "In Ocean Market, publishers can opt-in to provide more of their identity information via 3Box 3Box2020, to help data buyers build their trust in the publisher. To go further, the Ocean community could layer on protocols like Verifiable Credentials W3C2019. This subsection has described the Ocean tools architecture at a higher level. McConaghy2020h OceanDocs2022 has further details. 3.5. Ocean Tools: Multi-Network Deployment Towards a broadly open Data Economy, we aim for Ocean deployment across many chains as a thin layer for data assets and permissioning. Ocean is deployed to Ethereum mainnet and other EVM networks (for lower gas costs) including Polygon, Binance Smart Chain (BSC), Moonriver, and Energy Web Chain (EWC). A full list is at OceanDocsNetworks2022. The contracts do not have upgradeability built in. Therefore the only way to upgrade contracts is by community consensus to use a new set of smart contracts.",
      "This principle of governance minimization shrinks the attack surface, and also avoids security issues specific to upgradeable smart contracts. Over time, Ocean will get deployed to more L1 EVM networks, L2 rollups, and eventually, non-EVM networks. On a longer time horizon, we envision every blockchain network to have an Ocean-powered data resource permissioning layer. We envision them being interconnected with data NFTs, datatokens and OCEAN flowing everywhere. In all these deployments, the root OCEAN token deployment will remain on Ethereum mainnet, and bridged to other networks. 3.6. Ocean Tools: Data Marketplaces 3.6.1. Introduction This section drills into details about Ocean-based data marketplaces. Since each data service has its own ERC20 token, any ERC20 exchange can serve as a data marketplace. We can still make it easier for users.",
      "Specifically, marketplaces tuned for data can help users in the whole data flow, including publish data, set price, curate data, discover data, buy data, and consume data. 3.6.2. Marketplaces Architecture Figure 4 shows the conceptual architecture. There are many data marketplace frontends; there is a common backend (for a given network). The frontends include a community market (Ocean Market) top middle and independent third-party markets like those of Daimler top left. Each frontend runs client-side in the browser, using Ocean JS library, which interfaces with the backend. The Decentralized Backend bottom left is Solidity code running on the EVM chain, and includes the data NFT  datatoken and pool contracts, and the on-chain metadata store. When a Buyer purchases a dataset on a frontend far top, far left, most of the revenue goes to the Data seller bottom right. Some fees go to LPs, the marketplace runner, and the broader community top right.",
      "Figure 4: Ocean Marketplaces share a decentralized backend 3.6.3. Ocean Marketplace Actions Functionality covered. A marketplace's core functionality is about connecting buyers to sellers for given assets: to make the assets discoverable, and and buyingselling an asset of interest. For smoother user flow, Ocean Market supports adjacent functionality: publishing the asset in the first place, and consuming it. Each subsection will cover these, in the order that it would happen, with a focus on Ocean Market. Action: publish dataset. When the user (publisher) clicks on \"Publish\", they end up here. They start to fill out metadata, including Title and Description. The publisher then provides the url of where the data asset can be found. This url gets stored encrypted and on-chain. When a buyer later consumes a datatoken, that url will be decrypted. The publisher then fills out price information. It may be fixed price or dynamic (automatic).",
      "If automatic, they add liquidity as desired to be in line with their target price. Finally, they hit publish and Ocean Market will invoke blockchain transactions to deploy a datatoken contract, and publish metadata on-chain. Action: discovery. Ocean Market will have thousands of data assets. To help discovery, Ocean includes support for browsing, searching, and filtering data assets. Action: buysell dataset. Here, a buyer comes to Ocean Market and connects their wallet. Their wallet has some OCEAN. The buyer clicks the \"buy\" button; then Metamask pops up and asks for the buyer to confirm a transaction to swap OCEAN tokens for 1.0 datatokens. The buyer confirms, and the swap happens on-chain. Now, the buyer now has 1.0 more datatokens in their wallet. Action: Consume Dataset. Here, a datatoken owner comes to Ocean Market and connects their wallet. They go to the appropriate sub-page with the asset that they own. They consume the dataset by clicking the use button.",
      "They follow the prompts to end up with a downloaded dataset, or to get results of bringing compute to data. 3.6.4. Developer Tools  3rd Party Marketplaces Ocean Market is just one data marketplace. We envision many data marketplaces. We can catalyze this, by making it easy for developers to create their own marketplaces. Using Ocean JavaScript or Python libraries, each of the following is 13 lines of JS or PY code: Create a data asset (provision data service, deploy datatoken contract, add metadata, mint datatokens) Create a fixed-price market Buy datatokens Submit a datatoken and consume a data asset The libraries interface to Ocean smart contracts. Ocean Provider is a support tool to provision data assets, and Ocean Aquarius and to help store data on-chain and to query metadata (with the help of a local cache).",
      "With these tools in place, there are two main ways that a developer can build an Ocean-based data marketplace: (1) Fork Ocean Market, which uses Ocean tools, and (2) Build up their own marketplace using Ocean tools more directly (eg Javascript library). 3.6.5. Sustainability  Fees A primary aim is to help marketplace operators make money. A data market can readily support many actions: publish, pool-based swap, fixed-rate exchange, consuming (market-side or Provider side). Whenever one of these actions is performed on the market, the market earns a cut. Therefore the market is incentivized to have lots of these actions performed on it, via traditional startup traction activities (improve UX; improve asset supply; etc). In addition: data publishers can make money by publishing useful datasets that people buy  consume. They can also sell their base IP (data NFT). Liquidity providers can make money by taking a cut of swaps in datatoken pools.",
      "Finally, the Ocean Community takes a small cut to help long-term sustainability of the Ocean ecosystem. Fee details are at: https:docs.oceanprotocol.comcore-conceptsfees 3.6.6. IP Violations and Sensitive Data Ocean tooling gives a path for Ocean-powered data markets to handle IP violations and sensitive data. Here are some challenges. What happens when someone publishes a dataset that isnt theirs? What happens when lawyers send a DMCA takedown to your market? How do laws of a jurisdiction (e.g. USA, Germany) relate to blockchain? What happens if someone publishes PII ? These are all thorny questions. Towards a solution, heres a key insight. Its the dapp - e.g. Ocean Market or a third-party market - that is run in a given jurisdiction, typically with a centralized server running the backend (metadata cache, serving up webpage). This has practical implications: the party ensures that their dapp follows the laws of their jurisdiction. Heres the specific solution.",
      "The Ocean Market Purgatory Process is a process to handle IP violations and sensitive data OceanPurgatory2022 in Ocean Market. Its for Ocean Market, which is run by Ocean Protocol Foundation (OPF); 3rd party data markets can leverage this process, or use whatever other process they choose. Heres an overview of the process. Purgatory can be for a data asset or an actor, as shown in Figure 5 and Figure 6 respectively. Purgatory for a data asset is the state in which a published data asset is tagged as in purgatory in Ocean Market. Being in purgatory has implications into how the asset is displayed in Ocean Market, and what actions are permitted to be performed on the asset. Once in purgatory, the asset (actor) may stay there or leave purgatory if certain conditions are fulfilled. 6 PII  Personally Identifiable Information. This is enough info to be able to identify a unique person. GDPR European data protection laws (GDPR) mandate the right to be forgotten for PII.",
      "5 IP  Intellectual Property. This includes copyright and trademark IP. Figure 5: Purgatory for a data asset Purgatory for an actor is a state in which an actor (Ethereum address) is tagged as in purgatory in Ocean Market. Being in purgatory has implications into how the actor profile is displayed in Ocean Market, and what actions the actor is permitted to be perform in Ocean Market. Once in purgatory, the actor may stay there, or leave purgatory if certain conditions are fulfilled. Figure 6: Purgatory for an actor A data asset may enter purgatory if it receives a formal request for takedown due to sensitive data, IP violation, or otherwise. A data asset can either stay in purgatory indefinitely or revert back to default, according to specific conditions. An actor enters purgatory if they have published an asset thats been put into purgatory, or if they rug pull.",
      "The actor can leave purgatory if assets by the actor are on longer in Purgatory; the actor submits a request in writing to OPF, who then decides at its discretion whether to revoke purgatory status. This process is modeled on GitHubs approach. The provenance of assets and actors entering purgatory is all on git, stored in a public repo on Github. This makes it precise and transparent. The file list-assets.json lists all assets in Purgatory. It follows the following schema. \"did\": \"did:op:0000\", \"reason\": \"stolen data\" , ... Theres a similar schema for accounts (actors) in Purgatory. More information is at OceanPurgatory2022. 3.6.7. Benefits of Ocean Data Marketplaces Ocean marketplace tools make it easy to build  launch data marketplaces. Ocean-based data marketplaces have these characteristics: Interoperability - data assets being bought and sold are ERC20 tokens on the Ethereum mainnet and other EVM-compatible chains, which play well with the broader Ethereum ecosystem.",
      "Dont need login - users just connect their Web3 wallet (Metamask, etc). Therefore to buy or sell datatokens, theyre in and out in 2 minutes. This big UX improvement feels similar to DEXes, versus traditional CEXes . Non-custodial  decentralized - no centralized middlemen controls the data NFTs  datatokens. No single point of failure. Censorship-resistant, with flexibility - by default, everyone can transact with the marketplace on the same terms, regardless of their personal identifying characteristics. Or, to meet data regulations or KYC, there is the option of fine-grained permissions. Buy  sell private data while preserving privacy - using Ocean Compute-to-Data. Data wont leave the premises. This also gives sellers the option to make data exclusive (in an economic sense) which can give data a pricing premium. Provenance - sellers and buyers benefit from the auditability of purchase transactions (using e.g. Etherscan).",
      "Monetization - marketplace has the option to take a commission on sales. This helps to ensure that data marketplace businesses can be built that can sustain themselves and grow over time. The essay McConaghy2020f elaborates on more aspects of Ocean-powered marketplaces, especially Ocean Market. Also, McConaghy2020j elaborates on selling data, respectively. 3.7. Ocean Tools: Compute-to-Data Ocean Compute-to-Data provides a means to share or monetize ones data while preserving privacy. This section expands on this. 3.7.1. Motivation Private data is data that people or organizations keep to themselves, or at least want to keep to themselves. It can mean any personal, personally identifiable, medical, lifestyle, financial, sensitive or regulated information. Privacy tools are about asymmetric information sharing: get info to the people you want, for their benefit, while ensuring that others dont see the info. Benefits of Private Data.",
      "Private data can help research, leading to life-altering innovations in science and technology. For example, more data improves the predictive accuracy of modern AI models. Private data is often considered the most valuable data because its so hard to get, and using it can lead to potentially big payoffs. It's often considered as a competitive, or even decisive, advantage in their market by companies. Risks of Private Data. Sharing or selling private data comes with risk. What if you dont get hired because of your private medical history? What if you are persecuted for private lifestyle choices? Large organizations that have massive datasets know their data is valuable  and potentially monetizable  but do not pursue the opportunity for risk of data escaping and the related liability. Resolving the Tradeoff. There appears to be a tradeoff between benefits of using private data, and risks of exposing it. What if there was a way to get the benefits, while minimizing the risks?",
      "This is the idea behind Compute-to-Data: let the data stay on-premise, yet allow 3rd parties to run specific compute jobs on it to get useful analytics results like averaging or building an AI model. The analytics results help in science, technology, or business contexts; yet the compute is sufficiently aggregating or anonymizing that the privacy risk is minimized. 7 CEX  Centralized Exchange 3.7.2. Conceptual Working Figure 7 illustrates Compute-to-Data conceptually. Alice the data scientist goes to a data marketplace and purchases access to private data from seller Bob. She runs her AI modeling algorithm (which Bob has approved) on Bobs private data to privately train a model, which Bob also stores privately. She then runs the trained private model on new input data to get model predictions. Those predictions are the only data she sees. Everyone is satisfied: Alice gets predictions she wants, and Bob keeps his data private.",
      "At the heart, there are datatokens which grant access to run compute next to the data. These datatokens can be used within a marketplace context (like described) or other contexts. Figure 7: Ocean Compute-to-Data 3.7.3. Compute-to-Data Flow Variants This section describes variants of how Compute-to-Data may be used. For further detail yet, we refer the reader to McConaghy2020c. In these variants, there is still model training next to the data: Alice is able to download the trained model, i.e. it can leave Bobs premises. This will happen if Bob believes the model has low risk of leaking personally identifiable information (PII), such as being a linear model or a small neural network. Alice learns a model across many data silos. This is Federated Learning. In the following, the compute that is run next to the data is not for training a model, but something else. A simpler aggregating function is run next to the data, such as an average, median, or simple 1-d density estimation.",
      "This means that Compute-to-Data is useful for simpler business intelligence (BI) use cases, in addition to more complex artificial intelligence (AI) use cases. An aggregating function is computed across many data silos. This is Federated Analytics. A synthetic-data generation algorithm is run next to Bobs data. Alice downloads the synthetic data, which she then visualizes or trains models. A hash is computed for each input variable, input value combination of Bobs data, where the compute is done next to the data. Hashing naturally anonymizes the data. Alice downloads this hashed data and trains the model client-side. This is called Decoupled Hashing. Random noise is added to Bobs dataset, sufficiently so for the data to be considered anonymized. Alice downloads this partly-randomized data and trains the model client-side. This is a variant of Differential Privacy. 3.7.4. Share, or Monetize Compute-to-Data is meant to be useful for data sharing in science or technology contexts.",
      "Its also meant to be useful for monetizing private data, while preserving privacy. This might look like a paradox at first glance but its not! The private data isnt directly sold; rather, specific access to it is sold, access for compute eyes only rather than human eyes. So Compute-to-Data in data marketplaces is an opportunity for companies to monetize their data assets. 3.7.5. Compute-to-Data Architecture New actors. Ocean Protocol has these actors: Data Providers, who want to sell their data; Data Consumers, who want to buy data; and Marketplaces, to facilitate data exchange. Compute-to-Data adds a new actor, the Compute Provider. The Compute Provider sells compute on data, instead of data itself. New Components. Ocean technology has several components. Operator Service and Operator Engine were introduced for Compute-to-Data. Operator Service  a microservice in charge of managing the workflow and executing requests.",
      "It directly communicates and takes orders from Provider (the data providers proxy server) and performs computation on data, provided by Provider. Operator Engine  a backend service in charge of orchestrating the compute infrastructure using Kubernetes as a backend. Typically, the Operator Engine retrieves the workflows created by the Operator Service in Kubernetes. It also manages the infrastructure necessary to complete the execution of the compute workflows. New Asset Type. Before, datasets were the only asset type in metadata (DDO). Compute-to-Data introduces a new asset type  algorithm, which is a script that can be executed on datasets. For further detail, Patel2020 provides a worked example and further references on Compute-to-Data. 3.7.6. Marketplaces and Compute-to-Data Marketplaces can allow their users to publish datasets with Compute-to-Data enabled. Some marketplaces may even require it.",
      "Marketplaces choose what exact compute resources they want to make available to their end users within a K8s cluster, even having them choose from a selection of different images and resources. Likewise, marketplaces can choose and restrict the kind of algorithm they want to allow their users to run on the datasets in a marketplace. 3.7.7. Trusting Algorithms Only trust in a narrow facet is required: does the algorithm have negligible leakage of PII? For example, a simple averaging function aggregates data sufficiently to avoid leaking PII. AI algorithms also aggregate information. The data owner typically chooses which algorithms to trust. Its their judgement call. They might inspect the code and perhaps run it in a sandbox to see what other dependencies it causes, communications it invokes, and resources it uses. Therefore its the same entity that risks private data getting exposed and chooses what algorithm to trust. It is their choice to make, based on their preference of risk vs.",
      "reward. This also points to an opportunity for marketplaces of vetted algorithms: Ocean marketplaces themselves could be used, where liquidity provided is a proxy for quality and trust of the algorithm. Like with all pools, anyone can provide liquidity. This may be quite powerful, as it creates a data science side of the market. 3.7.8. Benefits of Compute-to-Data Compute-to-Data has privacy benefits and other benefits as well: Privacy. Avoid data escapes, never leak personal or sensitive information. Control. Data owners retain control of their data, since the data never leaves the premises. Huge datasets. Data owners can share or sell data without having to move the data, which is ideal for very large datasets that are slow or expensive to move. Compliance. Having only one copy of the data and not moving it makes it easier to comply with data protection regulations like GDPR GDPR2020. Auditability.",
      "Compute-to-Data gives proof that algorithms were properly executed, so that AI practitioners can be confident in the results. 3.7.9. Relation to other Privacy-Preserving Technologies Compute-to-Data is complementary to other technologies such as encryptiondecryption, Multi-Party Compute, Trusted Execution Environments, and Homomorphic Encryption. This compatibility and Oceans tools framing helps make it easy to adopt. McConaghy2020c has details. 3.8. Ocean Tools: Fine-Grained Permissions 3.8.1. Introduction Ocean Fine-Grained Permissions Hewitt2021 enable publishers and data marketplace owners to restrict access to publishing and consuming in an enterprise-oriented data marketplace. A large part of Ocean is about access control, of the decentralized variety. Oceans baseline approach is datatokens: a consumer can access a resource (e.g. a file) by redeeming 1.0 of the datatokens for that resource. Enterprises and other users often need more precise ways to specify and manage access.",
      "Consider a medical data use case, where only an appropriately credentialed EU researcher can browse available medical datasets, or download the datasets themselves. Or what if one wanted to restrict to employees of a specific country, or consortium, region, or a combination thereof? Questions like this were often asked by our enterprise collaborators. This means Ocean must address how to control access more precisely, at these two levels: Market-level fine-grained permissions for browsing, publishing, etc within a marketplace frontend Asset-level fine-grained permissions on consuming the asset itself Ocean supports both. The next two sections describe the high-level approach for each. For further detail yet, we refer the reader to the original article Hewitt2021 and related Ocean documentation OceanDocsPermissions2022. 3.8.2. Market-Level Permissions Back to the medical data use case: only an appropriately credentialed EU researcher can use the marketplace data.",
      "How to specify who can access it? A first idea would be to have a list of all the allowed people. But this has problems. Imagine an enterprise publishing 1000 data assets, and for each asset having a list of all its employees. If someone new is hired, theyd have to update all 1000 assets. Fortunately, theres a cleaner way. Access is based on a role, specified as a credential. E.g. is an employee of x. To continue the example above: new hires might get the credential when they join the company. Since they have the credential, they have permission. Thats it :) This credential-based approach is known in the data industry as Role-Based Access Control (RBAC) RBAC2022 . For market-level permissions, Ocean implements RBAC via an RBAC server. It implements restrictions at the user level, based on the users role (credentials). The RBAC server is run  controlled by the marketplace. Therefore permissions at this level are defined by the marketplace. 3.8.3.",
      "Asset-Level Permissions Lets go back to the medical data use case, where only an appropriately credentialed EU researcher can download the datasets themselves. How do we handle this? Theres a tension: Datatokens on their own arent enough  it would mean that anyone with 1.0 datatokens could access that data. Yet we want to retain datatokens approach since they enable Ocean users to leverage existing crypto infrastructure to get data wallets, data exchanges, data unions, and more out-of-the-box. The thousands of man-years of effort dedicated to create these crypto tools can be used directly as tools for a data ecosystem. We can resolve this tension by drawing on the following mental model: Imagine going to an age 18 rock concert. You can only get in if you show both (a) your concert ticket and (b) an id showing that youre old enough. We can port this model into Ocean, where (a) is a datatoken, and (b) is a credential. The datatoken is the baseline access control.",
      "Its fungible, and something that youve paid for or had shared to you. Its independent of your identity. The credential is something thats a function of your identity or your roles in society (the RBAC part). Ocean now supports fine-grained permissions for asset-based access control. Heres how: a consumer can only access the resource if they have 1.0 datatokens and one of the credentials in the allow list of the DDO. Ocean also has complementary deny functionality: if a consumer is on the deny list, they will not be allowed to access the resource. The rules combine as follows: Access  allowed  denied. Allowed  everyone by default, or allow list if it has entries. Denied  entries in deny list. Initially, the only credential supported is Ethereum public addresses. To be fair, its more a pointer to an individual, not a credential; but it has a low-complexity implementation so makes a good starting point.",
      "For extensibility, the Ocean metadata schema enables the specification of other types of credentials like W3C Verifiable Credentials and more. When this gets implemented, asset-level permissions will be properly RBAC too. Since asset-level permissions are in the DDO, and the DDO is controlled by the publisher, asset-level restrictions are controlled by the publisher. Subblock: OceanDAO 4.1. Introduction The previous section described Ocean Tools to power a data ecosystem. We now discuss the next subblock in the Ocean System: OceanDAO. It addresses the questions: How to kickstart growth? and How can the system not only self-sustain but actually improve over the decades, without guidance by a centralized actor? Our answer to this is OceanDAO: a DAO with its own treasury, active treasury management, and grants program. OceanDAO closes the loop with the rest of the Ocean System, such that growth in Ocean usage leads to more funding to teams, to work on further growing usage.",
      "This helps both short-term growth and long-term sustainability. 4.2. OceanDAO Components OceanDAO has an initial treasury from the 51 of unminted OCEAN. It converts some to ETH, DAI, etc by selling time-locked OCEAN (bonds). It grows its treasury via income from Network Revenue and from active treasury management (LPing into OCEAN-ETH and OCEAN-DAI, yield farming, etc). It spends its treasury towards rewards (e.g. Data Farming) and grants. 4.3. OceanDAO Implementation Many tools for DAOs are emerging. At the smart contract level, these include Aragon Aragon2020a, Moloch V2 Turley2020, DAOStack DaoStack2020, Colony Colony2020, and Collab.Land CollabLand2020. Higher-level interfacesapps for soft-signaling or voting include Snapshot, Boardroom, Pokemol, Daohaus, Discourse, Discord, and Telegram. The DAO  governance space is evolving rapidly. Ocean will need to have flexibility to be able to learn over time.",
      "Therefore rather than deploying one perfect DAO a single time and hope that it stays perfect, we plan to bake slowly to give ample opportunity to change things. OceanDAO was deployed live on Nov 30, 2020 Napheys2020, initially as just a grants DAO. Proposals are posted on a Ocean Port (a Discourse Forum), discussion has text chat on Ocean Discord and videochat in weekly Town Halls, and voting on Snapshot. The OceanDAO wiki is a unifying connector of these tools OceanDAOwiki. OceanDAOs scope is expanding over time. During 2022 this includes expansion to holding its own treasury, active treasury management, and implementation of Rewards programs. Ocean Data Farming 5.1. Overview Ocean Data Farming is a program to incentivize a supply of relevant and high-quality data assets. ODF aims to maximize the data supply reward function (RF), which is a function of liquidity added, dataset usage, and potentially more.",
      "In other words, liquidity providers (LPs) can earn extra OCEAN rewards for providing liquidity, and amount earned will be multiplied by usage of the datasets they stake on, and more. We will start with an initial RF and a program budget of OCEAN over a time interval (e.g. 3 months). Each week at a pre-set time, the program managers will: Off-chain, calculate contribution of each actor to the RF for that week (actor  Ethereum address). Manually airdrop OCEAN to each actor proportional to their contribution, the total being that weeks budgeted OCEAN. Declare the RF for use in the following week, to fine-tune towards the most relevant and high-quality datasets, and away from gaming that is against the spirit of the program. During the week, the program managers will: Gather feedback from the community on possible ways to improve the RF, using soft signaling in Discord or similar. Later generations may use OCEAN token voting.",
      "The OCEAN will budget may draw from OPF treasury and (if the community votes for it) OceanDAO funds. This design allows the program managers to start simple, then to quickly learn and improve the RF every week. This design draws inspiration from the successful Balancer Liquidity Mining Program Martinelli2020. If the initial ODF run (over the time interval) is successful, it may be repeated. 5.2. Reward Function This section describes the RF. is the (non-normalized) contribution for actor i on datatoken j: \ud835\udc45\ud835\udc39\ud835\udc56\ud835\udc57 \ud835\udc45\ud835\udc39\ud835\udc56\ud835\udc57 \ud835\udc46\ud835\udc56\ud835\udc57 \ud835\udc37\ud835\udc57 where:  actor is OCEAN stake in data asset j  (actors  BPTs in datatoken js pool  total  BPTs in pool)  ( OCEAN in pool)10 10 In the first iteration, rewards will only go to liquidity in Ocean-tweaked Balancer pools. Over time, we anticipate this to expand to other exchanges. 9 BPT  Balancer Pool Token. People receive BPTs proportional to the OCEAN or datatoken liquidity they add to a pool.",
      "Conversely, when they remove liquidity, they are essentially selling BPTs for OCEAN and datatokens. 8 E.g. giving a thumbs-up or thumbs-down to questions posted in Discord. times data asset j has been consumed in the last week (  consume transfers to the datatoken js Provider) The first term is . It reflects the actors belief in the relevance, quality, or potential usage of the datatoken. This incentivizes Providers to publish relevant data assets. The second term, , is a direct measure for the usage of the data asset, and a proxy for its relevance or quality. This can be \ud835\udc45\ud835\udc39\ud835\udc56\ud835\udc57 summarized as a binding of predicted popularity  actual popularity in terms of their orders-of-magnitude. As discussed, this is the initial RF. We anticipate feedback from the community to further improve it. To help long-term thinking and drive value to the protocol, we are considering lockup periods to ensure that rewards are not immediately sold by traders Jain2020.",
      "It could even be longer lockup, longer reward Fiskantes2020, like in Filecoins ICO Coinlist2017. Astute readers may recognize this design from the 2018 and 2019 versions of the Ocean whitepaper, to maximize the supply of data. Thats correct. There are two main differences. Weve changed it from smart contracts in the core of the system, into a program to catalyze growth run manually and off-chain for rapid learning and iterations. Also, the rewards formula has a simpler implementation, due to using staking inside datatoken pools and datatoken transfer calls. The blog post \"Ocean Data Farming is Launching\" McConaghy2022a has further details. On OCEAN Token Model 6.1. Introduction This section summarizes the OCEAN token from a crypto-economic perspective. 6.2. Token Value  Volume To ensure long-term funding for the Ocean community, we want OCEAN health to increase as usage volume increases. Ocean does this; heres how.",
      "Heres the loop: more usage of Ocean tools in the Data Ecosystem leads to more OCEAN being staked, leading to more OCEAN demand, growing OCEAN. More usage also leads to more Network Revenue, which goes to (i) burning and (ii) OceanDAO. Burning OCEAN reduces supply, to grow OCEAN. Funds go through OceanDAO to workers who have the mandate to grow usage of Ocean tools. And the loop repeats. 6.3. Work Token (and Governance Token) Work is when actors perform services for the token ecosystem that add value to the system . Ocean incentivizes for work to happen in OceanDAO governance, Ocean Workers block, and Ocean marketplace tools. This gives OCEAN some work token dynamics Samani2018. OceanDAO Governance. In Ocean, governance is about directing the flow of resources towards the teams and projects that offer the best chance for growth.",
      "OCEAN holders are incentivized to do work to learn more about each team and project proposal, and then use OceanDAO to vote for the most promising teamsprojects (subject to Ocean mission  values). If there arent enough value-adding projects for the OCEAN funds available, remaining OCEAN gets burned; this therefore sets a baseline 11 This is more general than the idea that one must buy a work token to have access to revenue streams in the network (like a NYC taxi medallion). for the value-add needed. Note that governance is not about specific decisions for protocols or software, its at a higher level and more directly about value creation. Ocean Workers. Paid by OceanDAO, these teams do a variety of work, including: improving core software (datatokens, etc), building and improving applications, outreach and community building, and unlocking data for use in Ocean.",
      "All of these activities drive value to the Ocean ecosystem (on average) because the work has been filtered by OceanDAO towards the value-adding activities. Ocean marketplace tools. There are several ways for the Ocean ecosystem to gain value via work inside the data ecosystem box of Figure 1. More liquidity in datatoken pools gives lower slippage, for a better data buyer experience, which in turn drives volume, and in turn Ocean value. The promise of LP transaction fees incentivizes people to add OCEAN liquidity to OCEAN-datatoken pools. Curation of assets (amount staked in the corresponding datatoken pool) allows data buyers to surface the best assets more easily, which in turn drives volume, and in turn Ocean value. To have the best returns, LPs are incentivized to seek datatoken pools with the highest volumes, which tend to be the ones with the best datasets. That is, theyre curating. Referrals bring new users to Ocean tools, which in turn drives volume, and in turn Ocean value.",
      "Because LPs  stakers  curators get fees on transactions, they are incentivized to refer people to the pools that they have staked in. More marketplaces with good volumes drives total Ocean volume, and in turn Ocean value. The promise of marketplace fees incentivizes people to run their own marketplaces and grow their volumes. Ocean Data Farming. This program incentivizes workers to add data assets, use data assets, and add liquidity to data assets. Future versions may also use OCEAN-based voting to govern updates to their reward formulae. 6.4. Burning First, 5 of all Network Revenue is burned. This ensures that OCEAN holders will always see some deflationary actions, and as Network Revenue increases, deflation increases. The 5 value was chosen based on running various possible values in agent-based simulation TokenSPICE2020. Second, for a given funding period, OceanDAO burns all non-allocated funds. 6.5.",
      "veOCEAN veOCEAN is a means to lock OCEAN to be able to vote in OceanDAO and earn ecosystem rewards. veOCEAN holders can earn passively, but can earn more by actively allocating veOCEAN to data assets that have high data consume volume (DCV). The blog post \"Introducing veOCEAN\" McConaghy2022b has further details. 6.6. User Acquisition Special OceanDAO projects like Data Supply Mining are not core to the Ocean token design itself, but spend some OCEAN resources in order to help acquisition of users, data, liquidity, and the like. 6.7. On OCEAN as Unit-of-Exchange OCEAN can be used as a unit-of-exchange to buy and sell datatokens, but its not mandated. Being ERC20 tokens, datatokens may be transferred in return for whatever payment is desired, such as fiat, DAI, or ETH. Community participants may add capabilities like this as they see fit.",
      "OCEAN is the default unit-of-exchange in Ocean Market and in Ocean libraries, for both fixed-price and auto-pricing scenarios (the latter also defaults to staking OCEAN). While Ocean Market transacts in OCEAN, it also displays the price in USD and EUR. Community participants may build data marketplaces that transact in fiat or other crypto-tokens. The essay McConaghy2020g elaborates on the Ocean token model further. Ocean Applications 7.1. Decentralized Orchestration Heres an example compute pipeline: input raw training data  clean the data  store cleaned data  build model  store model  input raw test data  run predictions  store predicted result y_test. Leveraging Ocean, a developer can write Solidity code Solidity2020 to define a pipeline and execute it, i.e. do decentralized orchestration: Write a smart contract that uses various data services. Theres a data NFT  datatoken for access control of each data service. Deploy the smart contract to the EVM chain.",
      "For each different datatoken, do a tx that approves the required amount of datatokens to the SEA smart contract. Finally, do a transaction that makes the actual call to the smart contract. With a Set Protocol, ERC998 or similar to have a basket datatoken that holds all the necessary sub-datatokens in the compute pipeline, step 3 becomes simpler yet. A major sub-application is decentralized federated learning (decentralized FL). In traditional FL, a centralized entity (e.g. Google) must perform the orchestration of compute jobs across silos. So, PII can leak to this entity. OpenMined OpenMined2020 could decentralize orchestration, using Ocean to manage computation at each silo in a more secure fashion McConaghy2020c. 7.2. Data Wallets: Data Custody  Data Management Data custody is the act of holding access to the data, which from an Ocean framing is simply holding data NFTs or datatokens in wallets.",
      "Data management also includes sharing access to data, which in Ocean is simply transferring data NFTs or datatokens to others. With data NFTs  datatokens following the ERC721  ERC20 standards, we can leverage existing wallets for ERC721  ERC20. This includes browser wallets (e.g. Metamask), mobile wallets (e.g. Argent, Pillar), hardware wallets (e.g. Trezor, Ledger), multi-sig wallets (e.g. Gnosis Safe), institution-grade wallets (e.g. Riddle  Code), custodial wallets (e.g. Coinbase Custody), and more. Wallets may get tuned specifically for datatokens as well, e.g. to visualize datasets, or long-tail token management (e.g. holding 10,000 different datatoken assets). Existing software could be extended to include data wallets. For example, Brave browser has a built-in crypto wallet that could hold datatokens. There could be browser forks focused on datatokens, with direct connection to user browsing data.",
      "Integrated Development Environments (IDEs) for AI like Azure ML Studio Azure2020 could have built-in wallets to hold  transfer datatokens for training data, models as data, and more. Non-graphical AI tools could integrate; such as scikit-learn or TensorFlow Python libraries using a Web3 wallet (mediated with Oceans Python library). As token custody continues to improve, data custody inherits these improvements. 7.3. Data Auditability Data auditability and provenance is another goal in data management. Thanks to data NFTs  datatokens, blockchain explorers like Etherscan Etherscan2020 now become data audit trail explorers. Just as CoinGecko CoinGecko2020 or CoinMarketCap CoinMarketCap2020 provide services to discover new tokens and track key data like price or exchanges, we anticipate similar services to emerge for data NFTs  datatokens. CoinGecko and CoinMarketCap may even do this themselves, just as theyve done for DeFi tokens. 7.4.",
      "Data DAOs: Data Co-ops and More Decentralized Autonomous Organizations (DAOs) DAO2020 help people coordinate to manage resources. They can be seen as multi-sig wallets, but with significantly more people, and with more flexibility. DAO technology is maturing well, as we reference in the OceanDAO section. A data DAO would own or manage data NFTs or datatokens on behalf of its members. The DAO could have governance processes on what data NFTs or datatokens to create, acquire, hold, sell  license, and so Here are some applications of data DAOs: Co-ops and Unions (Collective Bargaining). Starting in the early 1900s, thousands of farmers in rural Canada grouped into the SWP SWP2020 for clout in negotiating grain prices, marketing grain, and distributing it. Labour unions have done the same for factory workers, teachers, and many other professions. In Ibarra2018, the authors suggest that data creators are currently getting a raw deal, and the solution is to make a labour union for data.",
      "A data DAO could be set up for collective bargaining, as a data coop or data union. For example, there could be a data coop with thousands of members for location data, using FOAM proof-of-location service McConaghy2020a. Manage a single data asset. There could be a DAO attached to a single data asset. One way is: create a Telegram channel dedicated to that dataset. You can only enter the Telegram channel if you have 1.0 of the corresponding datatokens (inspired by Karma DAO Lee2020). This can also be for Discord, Slack, or otherwise. Datatoken pool management. There could be a data DAO to manage a datatoken pools weights, transaction fees, and more, leveraging Balancer Configurable Rights Pools Balancer2020b (inspired by PieDAO which does this for a pool of DeFi assets Delmonti2020). Index Funds for Data Investments. Using e.g.",
      "Melon Melon2020, an investment product can be constructed to allow people to buy a basket of data assets with the current plethora of mutual and index funds as a guide. 7.5. Fractional Ownership Fractional ownership is an exciting sub-niche of Web3, at the intersection of NFTs and DeFi. IT allows co-ownership of data IP. Ocean provides two approaches to fractional ownership: Sharded holding of ERC20 datatokens, where each ERC20 holder has the usual datatoken rights as described above, e.g. 1.0 datatokens to consume an asset. This comes out-of-the-box with Ocean. Sharding ERC721 Data NFT, where each co-holder has right to some earnings against base IP, and co-controls the Data NFT. For example, theres a DAO with the sole purpose to hold the Data NFT; this DAO has its own ERC20 token; DAO members vote with tokens to update Data NFT roles or deploy ERC20 datatokens against the ERC721. Note: For (2), one might consider doing sharding with something like Niftex.",
      "But then there are questions: what rights do the shard-holders get exactly? It could be zero; for example AMZN shareholders dont have the right to walk the hallways of Amazon Inc. just because they hold shares. Secondly, how do the shard-holders control the Data NFT? These questions get resolved by using a tokenized DAO, as described above. 7.6. Group-Restricted Data Access In this operational model, membership rules apply for a group. These rules are governed by a verifiable credential, a custom list of allowed Ethereum addresses, otherwise. Ocean implemented these via the Fine-Grained Permissions feature described earlier. These membership rules enable the following applications for data sharing. Contests, Hackathons, Impromptu Collaborations. A group of hackers or data scientists self-organize to try to solve an AI problem, such as a Kaggle competition or a hackathon.",
      "They want to be able to easily access each others data and compute services as they progress, especially if they are working remotely from each other. Regulatory Sandboxes. A government wants to give a means for organizations to run in a monitored regulatory environment that the government can observe. The organizations are vetted by the government and may have access to specially designated government compute services. Enterprise data access. An enterprise might make some of its data available to only its employees, but want to be able to use Ocean services available in the broader network. Sharing autonomous driving data. Individuals in each membership company of MOBI MOBI2020 need to access automotive data from any one of the MOBI member companies. It could be time-consuming and error-prone to specify permission for each member company individually.",
      "Furthermore, those permissions will fall out of date if MOBI members are added or removed; and updating the permissions one organization at a time could also be time-consuming or error-prone. This involves two levels of permissions: access of member companies into MOBI, and access of individuals in each member company (enterprise). Sharing medical data. Researchers on European soil that wish to directly access German medical data need to demonstrate that they have been accredited by appropriate authorities. This will usually be through their hospital or university. There could be thousands of researchers accessing the data. As with automotive data, it will be time-consuming and error prone to specify and update permissions for each of these thousands of researchers. This may be two levels of permissions (hospitaluniversity into EU authority; individual into hospitaluniversity), or it may be among hospitals and universities in a more networked fashion.",
      "Sharing financial data (while preserving privacy). Small and medium-sized credit unions in the U.S. have a challenge: they don't have large enough datasets to justify using AI. Since the credit unions dont compete with each other, they would find great value to build AI models across their collective datasets. 7.7. Unlocking Latent Data of Individuals  Enterprises Specialized apps could get built to tokenize and earn from latent data assets of individuals or organizations. Individuals. For example, an app that you install on your phone, you give permissions to access data (Compute-to-Data for privacy), then it auto-creates a fixed-price market for your data. After that you earn money from the data on your phone. In a mashup with personal tokens Chaturvedi2020, these would be your sovereign personal datatokens and you could launch your data as an Personal Data Offering (PDO). One could even launch multiple PDOs.",
      "There would be one datatoken for each data type generated - smartphone data, smartwatch data, browser data, shopping data, and so on. Then, bundle the datatokens into a collection of personal datatokens (composable datatokens). One could even have a personal data marketplace . One could do the same with a browser plug-in, to sell the latent data on your browser (cookies, bookmarks, browsing history). New service firms could emerge to help enterprises tokenize and earn from their massive internal data troves. These data assets might be sold one person or entity at a time. Or, they could be brought into a DAO to pool resources for more marketing and distribution muscle. Enterprises. Large enterprises have massive datasets. They know their data is valuable: they spend millions annually to help protect it and insure it due to hacks. But what if rather than data being a liability, data was assets on enterprises books?",
      "Ocean offers this possibility, by making it easy to turn the internal data into fungible assets (via datatokens), while preserving privacy and control (via Compute-to-Data). Data not only becomes a new line of revenue, it becomes a financial asset that can be borrowed against to fund growth, and more. 7.8. Data Marketplaces Earlier, we described Ocean Market, an out-of-the-box data marketplace that Ocean offers. Here are some ways that forks of Ocean Market could differentiate. Focus on a given vertical. For example, the Ocean-based Daimler data marketplace focuses on the mobility vertical. Other verticals include health, logistics, and DeFi. Focus on private data. A marketplace focusing on Ocean compute-to-data, with features for curation of compute algorithms. Different fee structure. When a purchase happens in Ocean Market, the default 0.1 fee goes fully to LPs. Variants include: marketplace operator gets a cut, referrers get a cut, charge higher , charge a flat fee.",
      "Novel pricing mechanism. Many price discovery mechanisms are possible Kuhn2019a Kuhn2019b, including royalties (a  of sales), English  Dutch  Channel auctions Azevedo2018, or income share agreements like Bowie or Dinwiddie bonds McConaghy2019d. This may include a novel initial distribution mechanism, as the Initial Data Offerings section elaborates. Different payment means. Ocean Market takes OCEAN. Variants could take in fiat, DAI, ETH, etc. (Ocean Market may support these directly over time as well. PRs are welcome!) Decentralized dispute resolution using Aragon Court Aragon2020b or Kleros Kleros2020. Different Balancer weighting schemes. For example, make the 9010 weights shift to 5050 over time (Liquidity Bootstrapping). Or, surge pricing pools, which have higher fees when 12 Thanks to Simon Mezgec for this suggestion. there is more demand for liquidity. These could be implemented with Configurable Rights Pools. PRs for Ocean Market are welcome!",
      "Beyond forks of Ocean Market, there is an larger variety of possible marketplaces. Here are some Web2 and Web3 variants. AMM DEXes. This could be a Uniswap or Balancer-like webapp to swap datatokens for DAI, ETH, or OCEAN. Non-data NFT marketplaces. Create an NFT marketplace focusing on photos, digital art, music, utility tokens, physical objects, or otherwise. Its USP would be the ability to sell well-defined licenses to single editions, and pools for staking  swapping on those tokens. (DeFi pools are much more fluid than typical NFT auctions.) Order-book DEXes. It could use 0x, Binance DEX, Kyber, etc. It could leverage platform-specific features such as 0xs shared liquidity across marketplaces. Order-book CEXes. Centralized exchanges like Binance or Coinbase could readily create their own datatoken-based marketplaces, and to kickstart usage could sell datasets that they've generated internally. Marketplaces in AI tools.",
      "This could be an AI-oriented data marketplace app embedded directly in an AI platform or webapp like Azure ML Studio or Anaconda Cloud. It could also be an AI-oriented data marketplace as a Python library call, for usage in any AI flow (since most AI flows are in Python). In fact, this is already live in Oceans Python library. Nocode Data Marketplace builder. Think Shopify Shopify2020 for data marketplaces, where people can deploy their own data marketplaces in just a few clicks. 7.9. Data as an Asset Class for DeFi The data economy is already 377B for Europe alone EuroComm2019. Tokenized data assets have great promise to grow the size of DeFi assets under management. Data can be securitized and used as collateral McConaghy2019d. An example is Bowie Bonds, where a fraction of David Bowie's IP (intellectual property) licensing revenue was paid to bondholders. Data is IP. To use it as a financial asset, one must price it.",
      "In Bowie's case, the value was established from previous years' licensing revenue. Alternatively, we can establish price by selling data assets in data marketplaces. As such, data is an asset class. With datatokens, we can onboard more more data assets into each major DeFi service types: Data assets can be used as collateral in stablecoins and loans, therefore growing total collateral. New Order has released H2O, a stable asset backed by OCEAN in its V1 (early 2022), and by datatokens themselves in its V2 (late 2022) NewOrder2021. Data assets bought and sold in DEXes and CEXes contributes to their  volume and assets under management (AUM). There can be insurance on data assets. As described above, there can be data DAOs, data baskets, and more. Here comes DataFi. 7.10. Data to Optimize DeFi Returns We can close the loop with data helping DeFi, and vice versa. Specifically: data can improve decision-making in DeFi to optimize returns. This will catalyze the growth of DeFi further.",
      "Here are some examples: Yield farming. Data can improve the automated strategies to maximize APR in yield farming. Think yearn.finance robots, but optimized further. Insurance. Data to lower the risk models in insurance. Loans. Better prediction of default for under-collateralized loans. Arb bots. More data for higher-return arbitration bots. Stablecoins. Assessment of assets for inclusion in stablecoins. Data-powered loops. DeFi looping techniques further boost returns. For each of the examples above, we envision loops of buying more data, to get better returns, to buy more data, and so on. To go even further, we could apply this to data assets themselves. 7.11. Data Management Platforms for Smart Cities and More In 2001, the government of Estonia rolled out a data management platform called X-Road XRoad2020. It then deployed an identity system on top; each citizen received an identity card with a digital signature.",
      "Since then, Estonia has rolled out apps for elections, health, taxes, parking, lawmaking, E-Residency and two dozen more government apps, plus 3rd-party apps like banking eEstonia2020. Both Ocean and X-Road can be used as digital infrastructure for smart cities data sharing. X-Road can be seen as a smart city example, since the majority of Estonians live in Tallinn. X-Road has a longer history, but has more centralized control and requires dev-ops effort. While Ocean is younger, by using a global permissionless infrastructure, it has lighter dev-ops requirements. We envision a future with both X-Road and Ocean-based data sharing in smart cities. Ocean framed as a data management platform can be used not only for data sharing within a city (across citizens), but also within a provincestate (across cities), within a nation (across provinces), within an international initiative (across nations, e.g.",
      "GAIA-X GAIAX2020), within a company (across employees), and within a multinational enterprise (across national offices). 7.12. Composable Data NFTs  Datatokens Data NFTs  datatokens can be composed into bundles, sets, or groups using ERC998 Lockyer2018, Set Protocol Set2020, Melon Protocol Melon2020, or others. This helps for the following use cases: Group across time. Package each 10-min chunk of data from the last 24 hours into a single token. Group across data sources. Package 100 data streams from 100 unique Internet-of-Things (IoT) devices, as a single token. Data baskets for asset management. Group together 1000 datasets that each have individual (but small) value, to sell as a single asset to others wanting to hold data assets. Data indexes. Track the top 100 data assets and make it easy for others to invest in those as a single asset, similar to todays index funds. On-chain annotations to metadata.",
      "Use ERC998 in a bottom-up setting to attach tags or other information to the data asset. Uses include: reputation given by a marketplaces users, quality as computed by a marketplaces algorithms, input training data vs output vs a model, industry verticals, and more. The essay McConaghy2019d elaborates further. Conclusion This paper presented Ocean Protocol. Ocean tools help developers build marketplaces and other apps to privately  securely publish, exchange, and consume data. The tools offer an on-ramp and off-ramp for data assets into crypto ecosystems, using data NFTs  datatokens. Composability gives many application opportunities, including data wallets, data marketplaces, data DAOs, and more. Ocean Market is a live reference community marketplace. Ocean tools are encapsulated in a broader system designed for long-term growth of an open, permissionless Web3 Data Economy. Ocean Data Farming incentivizes a supply of quality data.",
      "A key piece is OceanDAO to fund software development, outreach. OceanDAO is funded by revenue from Ocean apps and from Oceans network rewards. 3Box2020 3Box Team, 3Box: User Data Cloud, 3Box Homepage, last accessed Aug 19, 2020, https:3box.io Aragon2020a Aragon, homepage, last accessed Aug. 20, 2020, https:aragon.org Aragon2020b Aragon Court, homepage, last accessed Aug. 20, 2020, https:aragon.orgcourt Azevedo2018 Eduardo M. Azevedo, David M. Pennock, and E. Glen Weyl, Channel Auctions, Aug 31, 2018, Azure2020 Azure ML Team, Microsoft Azure Machine Learning Studio, homepage, last accessed Aug. 19, 2020, Balancer2019 Balancer Team, Interest-Bearing Stablecoin Pools Without Impermanent Loss, Balancer blog, Oct. 30, 2019, https:balancer.finance20191030interest-bearing-stablecoin-pools-without-impermanent-loss Balancer2020a Balancer, homepage, last accessed Aug.",
      "21, 2020, https:balancer.finance Balancer2020b Balancer Team, Configurable Rights Pool, GitHub Repository, published Aug, 2020, Banko2001 M. Banko and E. Brill, Scaling to Very Very Large Corpora for Natural Language Disambiguation, Proc. Annual Meeting on Association for Computational Linguistics, July, 2001, http:www.aclweb.organthologyP01-1005 Bitcoin2020 Bitcoin, Twitter, Apr. 4, 2020 https:twitter.comBitcoinstatus1246482664376414209 Buterin2013 Vitalik Buterin, Ethereum: A Next-Generation Smart Contract and Decentralized Application Platform, Dec. 2013, https:ethereum.orgenwhitepaper Buterin2018 Vitalik Buterin, Explanation of DAICOs, Eth.Research Blog, Jan. 2018, Chaturvedi2020, Vinamrata Chaturvedi, The Man Who Tokenized Himself Gives Holders Power Over His Life, June 30, 2020, https:www.coindesk.comman-who-sells-himself-now-wants-buyers-to-control-his-life CollabLand2020 Collab.Land, Homepage, last accessed Aug.",
      "20, 2020, http:collab.land CoinGecko2020, CoinGecko Team, CoinGecko website, last accessed Aug. 19, 2020 https:www.coingecko.com CoinMarketCap2020, CoinMarketCap Team, CoinMarketCap website, last accessed Aug. 19, 2020, Coinlist2017, Coinlist Team, Filecoin Token Sale Economics, fall 2017, 4dd60de115d67ff1e9047ffa8e.pdf Colony2020 Colony, Homepage, last accessed Aug. 20, 2020, https:colony.io DAO2020 Decentralized Autonomous Organization, Wikipedia, last accessed Aug. 19, 2020, DaoStack2020 DaoStack, homepage, last accessed Aug. 20, 2020, https:daostack.io Delmonti2020 Alessio Delmonti, Introducing PieDAO, the asset allocation DAO, Mar. 3, 2020, DID2019 Decentralized Identifiers(DIDs) v0.11: Data Model and Syntaxes for Decentralized Identifiers, W3C Community Group, Draft Community Group Report 06 February 2019, last accessed Feb.",
      "28, 2019, https:w3c-ccg.github.iodid-spec Economist2017 The Worlds Most Valuable Resource is No Longer Oil, But Data, The Economist, May 6, 2017, valuable-resource eEstonia2020 E-Estonia, homepage, last accessed Aug. 19, 2020, https:e-estonia.com EthereumEvm2022 Ethereum Virtual Machine, Ethereum Foundation website, last accessed Feb. 23, 2022, Etherscan2020 Etherscan, homepage, last accessed Aug. 21, 2020, https:etherscan.io EuroComm2019 European Commission, Building a Data Economy - Brochure, Sep. 2019, ERC721 ERC-721, Homepage, last accessed Aug. 19, 2020, http:erc721.org ERC1167 Peter Murray, Nate Welch, and Joe Messerman, EIP-1167: Minimal Proxy Contract, Ethereum Improvement Proposals, no. 1167, June 2018, https:eips.ethereum.orgEIPSeip-1167 Fiskantes2020 Fiskantes, Twitter, Aug 12, 2020, https:twitter.comfiskantesstatus1293372786002538496?s11 GAIAX2020 GAIA-X, homepage, last accessed Aug.",
      "21, 2020, https:www.data-infrastructure.eu GDPR2020 General Data Protection Regulations, Wikipedia, last accessed Aug. 21, 2020, Goldin2017 Mike Goldin, Mikes Cryptosystems Manifesto, Google Docs, 2017, Graham2005 Paul Graham, How to Start a Startup, Paul Graham Blog, March 2005, Halevy2009 Alon Halevy, Peter Norvig, and Fernando Pereira, The Unreasonable Effectiveness of Data, IEEE Intelligent Systems 24(2), March-April 2009, https:research.google.compubsarchive35179.pdf Hewitt2021 Jamie Hewitt, Fine-Grained Permissions Now Supported in Ocean Protocol, Ocean Protocol Blog, Sep. 22, 2021, https:blog.oceanprotocol.comfine-grained-permissions-now-supported-in-ocean-protocol-4fe434af24b9 Ibarra2018 Imanol Arrieta Ibarra et al, Should We Treat Data as Labor?",
      "Moving Beyond 'Free', AEA Papers and Proceedings, 108 : 3842.DOI: 10.1257pandp.20181003, Jain2020 Tushar Jain, Exploring the Design Space of Liquidity Mining, Multicoin Capital Blog, Aug 13, 2020, KellyCriterion2017 Kelly Criterion, Wikipedia, last accessed Feb. 17, 2018, https:en.wikipedia.orgwikiKelly_criterion Kleros2020 Kleros, homepage, last accessed Aug. 20, 2020, https:kleros.io Kuhn2019a Erwin Kuhn, Lets Talk About Data Pricing  Part I, Aug. 13, 2019, Kuhn2019b Erwin Kuhn, Lets Talk About Data Pricing  Part II, Aug. 21, 2019, Lee2020 Andrew Lee, Announcing Karma DAO: First-Ever Token-Permissioned Networking Chat Group on Telegram, July 20, 2020, m-5feab7a54def Lockyer2018 Matt Lockyer, ERC-998 Composable Non-Fungible Token Standard, Apr.",
      "15, 2018, Martinelli2020 Fernando Martinelli, Balancer Liquidity Mining Begins, Balancer Blog, May 29, 2020, McConaghy2018 Trent McConaghy, Mission  Values for Ocean Protocol, July 23, 2018, McConaghy2019b Trent McConaghy, Datatokens 1: Data Custody, Ocean Protocol Blog, FNov. 8, 2019, McConaghy2019c Trent McConaghy, Datatokens 2: Non-Fungible, Fungible and Composable Datatokens, Ocean Protocol Blog, Nov. 25, 2019, https:blog.oceanprotocol.comdata-tokens-2-fungible-composable-54b6e0d28293 McConaghy2019d Trent McConaghy, Datatokens 3: Data and Decentralized Finance (Data  DeFi), Ocean Protocol Blog, Dec. 3, 2019, https:blog.oceanprotocol.comdata-tokens-3-data-and-decentralized-finance-data-defi-d5c9a6e578b7 McConaghy2020a Trent McConaghy, Radical Markets and the Data Economy, Ocean Protocol Blog, Mar.",
      "5, 2020, McConaghy2020c Trent McConaghy, How Does Ocean Compute-to-Data Related to Other Privacy-Preserving Approaches?, Ocean Protocol Blog, May 28, 2020, McConaghy2020d Trent McConaghy, The Web3 Sustainability Loop. A system design for long-term growth of Web3 projects, with application to Ocean Protocol, Ocean Protocol Blog, Sep. 1, 2020, McConaghy2020e Trent McConaghy, Ocean Datatokens: From Money Legos to Data Legos. How DeFi Helps Data, and Data Helps DeFi, Ocean Protocol Blog, Sep. 8, 2020, McConaghy2020f Trent McConaghy, Ocean Market: An Open-Source Community Marketplace for Data. Featuring OCEAN Staking, Automated Market Makers, and Initial Data Offerings, Ocean Protocol Blog, Sep. 24, 2020, McConaghy2020g Trent McConaghy, Ocean Token Model: System Dynamics for Near-Term Growth and Long-Term Sustainability, Ocean Protocol Blog, Oct.",
      "2, 2020, https:blog.oceanprotocol.comocean-token-model-3e4e7af210f9 McConaghy2020h Trent McConaghy, Ocean Protocol V3 Architecture Overview: Simplicity and Interoperability via a Datatokens Core, Ocean Protocol Blog, Oct. 12, 2020, McConaghy2020i Trent McConaghy, On Selling Data in Ocean Market: Selling Your Proprietary Data, Others Proprietary Data, and Value-Adds to Open Data, Ocean Protocol Blog, Oct. 19, 2020, McConaghy2020j Trent McConaghy, On Staking on Data in Ocean Market: Earning Opportunities  and Risks  in Oceans Community Marketplace, Ocean Protocol Blog, Oct. 22, 2020, McConaghy2021a Trent McConaghy, NFTs  IP 1: Practical Connections of ERC721 with Intellectual Property, Ocean Protocol Blog, Jun. 3, 2021, McConaghy2021b Trent McConaghy, NFTs  IP 2: Leveraging ERC20 Fungibility, Ocean Protocol Blog, Jun. 4, 2021, McConaghy2021c Trent McConaghy, NFTs  IP 3: Combining ERC721  ERC20, Ocean Protocol Blog, Jun.",
      "5, 2021, McConaghy2022a Trent McConaghy, Ocean Data Farming is Launching, Ocean Protocol Blog, Jun. 9, 2022, McConaghy2022b Trent McConaghy, Introducing veOCEAN, Ocean Protocol Blog, Jul. 12, 2022, McDonald2020 Mike McDonald, Building Liquidity into Token Distribution, Mar. 4, 2020, Melon2020 Melon Protocol, homepage, last accessed Aug. 19, 2020, https:melonprotocol.com MOBI2020 MOBI - Mobility Open Blockchain Initiative. Homepage. Last accessed Aug. 20, 2020, https:dlt.mobi Monegro2020 Joel Monegro, Proof of Liquidity, Placeholder blog, May 22, 2020, Napheys2020 Alex Napheys, Introducing OceanDAO: A grants DAO curated by the Ocean community, for growth  long-term sustainability, Ocean Protocol Blog, Nov.",
      "30, 2020, NewOrder2021 Introducing H2O - Our Latest Incubation Project, New Order blog, Nov 30, 2021, OceanBlog2020b Ocean Protocol Team, Ocean Protocol delivers Proof-of-Concept for Daimler AG in collaboration with Daimler South East Asia, July 7, 2020, th-east-564aa7d959ca OceanBlog_Roadmap2019 Ocean Roadmap  Setting sail to 2021, Ocean Protocol Blog, Feb. 13, 2019, OceanBlog_Roadmap2020 Ocean Product Update  2020, Ocean Protocol Blog, Mar .19, 2020, OceanContracts2022 Ocean Contracts, Ocean Protocol GitHub, last accessed Feb. 24, 2022, OceanDocs2022 Ocean Protocol Software Documentation, last accessed Feb. 24, 2022, OceanDocsNetworks2022 Supported Networks, Ocean Protocol documentation, last accessed Sep. 1, 2022, OceanDocsPermissions2022 Fine-Grained Permissions, Ocean Protocol documentation, last accessed Sep. 1te, 2022, OceanDAOwiki OceanDAO Wiki, homepage, last accessed Feb. 24, 2022, OceanLibJs2022 Ocean-lib: JavaScript, Ocean Protocol GitHub, last accessed Feb.",
      "24, 2022, OceanLibPy2022 Ocean-lib: Python, Ocean Protocol GitHub, last accessed Feb. 24, 2022, OceanMarket2022 Ocean Market, Ocean Protocol GitHub, last accessed Feb. 24, 2022, OceanPurgatory2022 Ocean Market Purgatory Process, Ocean Protocol GitHub, last accessed Feb. 23, 2022, OpenMined2020 OpenMined, homepage, last accessed Aug. 20, 2020, https:www.openmined.org Oauth2020 Oauth Team, Oauth 2.0 Spec, Oauth Website, last accessed Aug. 19, 2020, https:oauth.net2 Patel2020 Manan Patel, Technical Guide to Ocean Compute-to-Data, Ocean Protocol Blog, May 19, 2020, RBAC2022 Role-based access control, Wikipedia, last accessed Feb. 23, 2022, Rouviere2017 Simon de la Rouviere, Introducing Curation Markets: Trade Popularity of Memes  Information (with code)!, Medium, May 22, 2017, d9881 Samani2018 Kyle Samani, New Models for Utility Tokens, Feb. 13, 2018, Solidity2020 Solidity, Wikipedia, last accessed Aug.",
      "21, 2020, https:en.wikipedia.orgwikiSolidity Set2020 TokenSets, homepage, last accessed Aug. 19, 2020, https:www.tokensets.com Shopify2020 Shopify, homepage, last accessed Aug. 21, 2020, https:shopify.com Simler2018 Kevin Simler and Robin Hanson, The Elephant in the Brain: Hidden Motives in Everyday Life, Oxford University Press, Jan. 2018, https:www.amazon.comElephant-Brain-Hidden-Motives-Everydaydp0190495995 Simmons2017 Andrew Simmons, Prediction markets: How betting markets can be used to create a robust AI, Sept. 13, 2017, http:dstil.ghost.ioprediction-marketsamp SWP2020 Saskatchewan Wheat Pool, Wikipedia, last accessed Aug. 21, 2020, TokenSPICE2020 TokenSPICE: Token simulator using agent-based modeling, Ocean Protocol GitHub organization, open-sourced in Nov 2020, https:github.comoceanprotocoltokenspice Turley2020 Cooper Turley, Moloch Evolved: V2 Primer, Mar. 27, 2020, Uma2020 Uma Team, UMA Initial Uniswap Listing, Apr.",
      "22, 2020, Unisocks2020 Unisocks, homepage, https:unisocks.exchange Vogelsteller2015 Fabian Vogelsteller and Vitalik Buterin, ERC-20 Token Standard, Nov. 19, 2015, Vogelsteller2020 Fabian Vogelsteller, rICO - The Reversible ICO, Lukso Blog, Apr. 13, 2020, WIPO2022 Summary of the Berne Convention for the Protection of Literary and Artistic Works (1986), World Intellectual Property Organization (WIPO), last accessed Feb. 24, 2022, W3C2019 W3C, Verifiable Credentials Data Model 1.0, W3C Recommendation, November 2019, XRoad2020 X-Road, Wikipedia, last accessed Aug. 19, 2020, https:en.wikipedia.orgwikiX-Road Thanks very much to the following people for reviewing the paper and providing feedback: Simon de la Rouviere, Julien Thevenard, Bruce Pon, Alex Coseru, Monica Botez, Sarah Vallon, David Holtzman, Simon Mezgec, Robin Lehmann, Laurent Rochat, Gary Latta, Andreas Fauler. Thanks to my excellent colleagues at Ocean Protocol for the collaboration in building towards this."
    ],
    "word_count": 13703,
    "page_count": 36
  },
  "RPL": {
    "chunks": [
      "Rocket Pool Guides  Documentation Rocket Pool Search K Overview Guides Website \u7b80\u4f53\u4e2d\u6587 English Search K Overview Guides Website \u7b80\u4f53\u4e2d\u6587 English Rocket Pool Guides  Documentation Decentralised Ethereum Liquid Staking Protocol Get Started  Contribute Overview Learn all about what Rocket Pool is, how it works, and how to use it with an easy-to-read series of articles. Guides Follow our detailed walkthroughs to practice using Rocket Pool on the Hoodi test network, from staking ETH to running a node. Run a node Get started running a node with our detailed walkthroughs. Learn how to secure, maintain, monitor and upgrade your node."
    ],
    "word_count": 98,
    "page_count": 1
  },
  "SAND": {
    "chunks": [
      "Play. Create. Own. Govern. Earn. Welcome to the Metaverse The Sandbox team is building a unique virtual world where players can build, own, and monetize their gaming experiences using SAND, the main utility token of the platform. SAND holders will be also able to participate in governance of the platform via a Decentralized Autonomous Organization (DAO), where they can exercise voting rights on key decisions of The Sandbox ecosystem. As a player, you can create digital assets (Non-Fungible Tokens, aka NFTs), upload them to the marketplace, and drag-and-drop them to create game experiences with The Sandbox Game Maker. The Sandbox has secured over 50 partnerships including Atari, Crypto Kitties, and Shaun the Sheep to build a fun, creative play-to-earn Gaming platform, owned and made by players.",
      "The Sandbox aims to bring blockchain into mainstream gaming, attracting both crypto and non-crypto game enthusiasts by offering the advantages of true-ownership, digital scarcity, monetization capabilities, and interoperability WHITEPAPER www.sandbox.game Page 2 The information in this White Paper is subject to change or update and should not be construed as a commitment, promise or guarantee by The Sandbox or any other individual or organisation mentioned in this white paper relating to the future availability of services related to the use of the tokens or to their future performance or value. The document does not constitute an offer or solicitation to sell shares or securities. It does not constitute or form part of and should not be construed as any offer for sale or subscription of or any invitation to buy or subscribe for any securities not should it or any part of it form the basis of or be relied upon in any connection with any contract or commitment whatsoever.",
      "The Sandbox expressly disclaims any and all responsibility for any direct or consequential loss or damage of any kind whatsoever arising directly or indirectly from reliance on any information contained in the white paper, any error, omission or inaccuracy in any such information or any action resulting therefrom. This is not a recommendation to buy or financial advice, It is strictly informational. Do not trade or invest in any tokens, companies or entities based solely upon this information. Any investment involves substantial risks, including, but not limited to, pricing volatility, inadequate liquidity, and the potential complete loss of principal. Investors should conduct independent due diligence, with assistance from professional financial, legal and tax experts, on topics discussed in this document and develop a stand- alone judgment of the relevant markets prior to making any investment decision.",
      "We have prepared all information herein from sources we believe to be accurate and reliable. However, such information is presented as is, without warranty of any kind  whether expressed or implied. All market prices, data and other information are not warranted as to completeness or accuracy, are based upon selected public market data, reflect prevailing conditions, and our view as of this date, all of which are accordingly subject to change without notice. The graphs, charts and other visual aids are provided for informational purposes only. None of these graphs, charts or visual aids can and of themselves be used to make investment decisions. No representation is made that these will assist any person in making investment decisions and no graph, chart or other visual aid can capture all factors and variables required in making such decisions.",
      "The information contained in this document may include, or incorporate by reference, forward-looking statements, which would include any statements that are not statements of historical fact. No representations or warranties are made as to the accuracy of such forward-looking statements. Any projections, forecasts and estimates contained in this document are necessarily speculative in nature and are based upon certain assumptions. These forward-looking statements may turn out to be wrong and can be affected by inaccurate assumptions or by known or unknown risks, uncertainties and other factors, most of which are beyond control. It can be expected that some or all of such forward-looking assumptions will not materialize or will vary significantly from actual results.",
      "WHITEPAPER www.sandbox.game Page 3 Abbreviations AMLCFT Anti-Money Laundering and Combating the Funding of Terrorism Application Programming Interface Distributed Ledger Technology Intellectual Property Initial Public Offering Monthly Active Users MLFT Money Laundering and Funding of Terrorism Non-Fungible Token Peer-to-peer The Sandbox User Generated Content WHITEPAPER www.sandbox.game Page 4 Abbreviations Executive Summary A User-Generated Content Ecosystem Blockchain Gaming with NFTs What is SAND and what is it used for? What do we plan next?",
      "Associated challenges and risks Historical Background The Sandbox Platform Mission Game Overview LAND and the LANDS Map Avatar SAND SAND Stakeholders SAND Revenue Streams Benefits Market Overview Gaming Creator Market Creator Market Player Market Market Solution Roadmap Previous Milestones Future Milestones Major Features in the Press Platform WHITEPAPER www.sandbox.game Page 5 Gameplay Experience Map - LANDS ASSETS Marketplace VoxEdit Economy Tokenomics Project Financing and Initial Revenue Initial Exchange Offering Other Dynamics Technology Future Technology Integrations Multiple Class Fungible Token (MCFT) Interoperability Meta Transactions Technology Solution Breakdown Program Agents Game Engine Wallets Security The Team Core Team Advisors Services Providers CONCLUSION The Whitepaper 1. Executive Summary The Sandbox is a virtual world where players can build, own, and monetize their gaming experiences in the Ethereum blockchain using SAND, the platforms utility token.",
      "Our vision is to offer a deeply immersive metaverse in which players will create virtual worlds and games collaboratively and without central authority. We are aiming to disrupt the existing game makers like Minecraft and Roblox by providing creators true ownership of their creations as non-fungible tokens (NFTs) and rewarding their participation with our utility token  SAND. In the current game market, the centralized ownership and control of user-generated content limits creator rights and ownership. Central control over the trading of virtual goods created by players restricts them from generating fair value for their creations. Compounding this, it can be difficult to prove creative ownership of works, especially as content is copied, altered, and built upon. With The Sandbox, we aim to overcome these limitations while accelerating blockchain adoption to grow the blockchain gaming market.",
      "We will do this by building a voxel gaming platform where creators are able to craft, play, share, collect, and trade without central control, enjoying secure copyright ownership with the ability to earn cryptocurrency (SAND). This copyright ownership will be established and guaranteed through the use of NFTs, where every in-game item will have a unique and immutable blockchain identifier. WHITEPAPER www.sandbox.game Page 7 1.1. A User-Generated Content Ecosystem The Sandbox gaming ecosystem consists of three integrated products that together provide a comprehensive experience for user-generated content (UGC) content production. VOXEDIT: This simple to use yet powerful free 3D voxel modelling package allows users to create and animate 3D objects such as people, animals, foliage, and tools, and export them into The Sandbox marketplace to become game ASSETS.",
      "MARKETPLACE: The Sandboxs web-based marketplace allows users to upload, publish, and sell their creations (ASSETS) made in VoxEdit, as Tokens (Both ERC-721 and ERC-1155 tokens). GAME MAKER: Anyone who owns ASSETS, either by making them in VoxEdit or purchasing them, can utilize them with the third and most important ecosystem product, the Game Maker and the Game itself. This product, when launched in Game Maker mode, enables users to place and use their ASSETS within a piece of LAND (an ERC-721 token) that they can own in the virtual world. Users can decorate their LAND with ASSETS, and more importantly, implement interesting and nuanced gameplay mechanics by assigning predefined behaviors to the ASSETS through visual scripting nodes, turning a LAND from a decoration experience into a potential full game experience. WHITEPAPER www.sandbox.game Page 8 1.2. Blockchain Gaming with NFTs The Sandbox virtual world uses blockchain technology and NFTs to empower the players and creators.",
      "NFTs are an emerging segment in the global game market: virtual tokens for digital scarcity, security and authenticity. Each NFT is distinct or unique, it is indivisible and it is not interchangeable for another. Comparison of Fungible versus Non-fungible Tokens Through the use of NFTs, The Sandbox users will be able to benefit from: TRUE DIGITAL OWNERSHIP: Gamers are the true and perpetual owners of their digital items, even if the game was shut down or abandoned. With blockchain, every game item can be tokenized, allowing gamers to decide how they want to trade, sell, or gift their items. SECURITY AND IMMUTABILITY: Digital game items can be easily tokenized and traded in primary and secondary markets that are managed and facilitated by blockchain technology. Items based on scarcity and demand usually invite fraud and theft, but these risks are minimised on blockchain because it is a distributed ledger.",
      "TRADING: Blockchain-based gaming platforms can provide users with ultimate control over their digital assets. They can buy and sell items freely without concern that they will be ripped off or that a platform will close and cancel all the value of their in-game items. CROSS-APPLICATION INTEROPERABILITY: Blockchain provides capacity for games to utilize shared assets. ASSETS, avatars, LANDS and any other game elements can be used in other games that allow it. These game items are no longer confined by a narrow digital ecosystem. The Sandbox metaverse uses several tokens to create a circular economy between all the profiles of users who will interact with its platform namely the players, creators, curators and LAND owners. These are the LANDS, ASSETS and SAND, a token based on the ERC20 protocol which will act as the official currency in the ecosystem when purchasing goods and services within the game marketplace (amongst other rights). 1.3. What is SAND and what is it used for?",
      "WHITEPAPER www.sandbox.game Page 9 SAND is an essential part of The Sandbox platform and The Sandbox are working on establishing key mechanics that makes it intrinsically tied to The Sandbox platform and its value. SAND is an ERC-20 utility token built on the Ethereum blockchain that serves as the basis for transactions within The Sandbox, and has the following uses:  Access The Sandbox platform: Players spend SAND in order to play games, buy equipment, or customize their Avatar characterand can potentially collect SAND through gameplay. Creators spend SAND to acquire ASSETS, LANDS, and through Staking. LAND Sales drive demand for SAND to purchase LANDS. Artists spend SAND to upload ASSETS to the Marketplace and buy Gems for defining Rarity and Scarcity. Governance: SAND is a governance token that allows holders to participate in Governance decisions of the platform, using a DAO structure.",
      "They can exercise voting rights on key elements such as Foundation grant attributions to content and game creators and feature prioritisation on the platform Roadmap. SAND owners can vote themselves or delegate voting rights to other players of their choice. Staking: SAND allows for staking, which allows for passive revenues on LANDS: you get more SAND by staking it. This is also the only way to get valuable Gems and Catalysts, needed for ASSET creation. Fee Capture model  5 of all transaction volume carried out in SAND tokens (Transaction Fees) shall be allocated with 50 to the Staking Pool as rewards for token holders that Stake SAND tokens and 50 to the ''Foundation.''  Foundation: The role of the Foundation is to support the ecosystem of The Sandbox, offering grants to incentivize high quality content  game production on the platform. To date, the Foundation has funded over 15 game projects and granted 100 artists to produce NFTs ahead of the public Launch in December 2020.",
      "The overall valuation of the metaverse grows through the valuation of all games funded by the Foundation, creating a virtuous circle to enable funding bigger games. 1.4. What do we plan next? We have a strong product roadmap ahead and a top team to execute a strong vision to build a unique virtual world gaming platform where players can build, own, and monetize their gaming experiences and spread the power of blockchain as the lead technology in the gaming industry. All these together with what we have achieved so far resulted in TSB being awarded a recognition as the most expected blockchain game in 2020. anticipated WHITEPAPER www.sandbox.game Page 10 In the short term, we are launching the fourth LAND presale and Game Maker. This will be complemented with the SAND public launch.",
      "As such, we expect to have SAND available for individuals and corporations in order to increase liquidity and availability of it with the main purpose to collaborate with the community growth and ecosystem flow. In relation to the proposed SAND offering, the community of creators and players will need to get access to SAND. We plan to make it accessible to the community through multiple ways with controllable supply mechanisms, such as purchasing SAND from multiple exchanges. As the community increases in terms of the number of creators, players and ASSETS exchanged in the marketplace, there might be an increased need for SAND within the ecosystem. Therefore, while the total supply of SAND is fixed, the initial amount of SAND offered will provide a scarcity effect reducing the SAND available per capita and therefore fostering demand. 1.5.",
      "Associated challenges and risks It is the responsibility of any purchaser of SAND to inform themselves of, and to observe and comply with, all applicable laws and regulations of any relevant jurisdiction. Prospective purchasers for any SAND shall be expected to consider the risk factors identified in this section. If any of the indicated risks were to materialize, then they could have a negative impact on the Issuers finances and operational performance which can hinder the ability of the Issuer to fulfil its obligations under this whitepaper. The challenges and risks are identified by the board of administration of the Issuer as at the date of registration of this whitepaper. This does not exclude the possibility of there being other risk threats and the purchaser must be aware that the impact on the Issuer may be heightened due to a combination of several risks materializing simultaneously.",
      "One of the main challenges in the project is the rate of adoption of the community of artists and creators to start exchanging assets, which will be a key point behind the demand generation of SAND. As such, we are focused in developing a strong, multi-platform product that can spread massively by offering a tool to create voxel art that is focused on solving problems and community needs. We are therefore developing strong communities in different areas of the world with a strong focus in Asia, where we believe the rate of adoption and learning curve will be steeper. The project success rate will be dependent upon the priority and preference of consumers and its ability to swiftly anticipate, identify and capitalise upon these priorities and preferences. The Issuer operates in an emerging and disruptive industry that is under rapid and dynamic development.",
      "This includes, but is not limited to, the developments relating to the business itself, the underlying technology, and the regulatory and legal implications thereof. Specifically, in relation to public knowledge and understanding of emerging technologies, such distributed ledger technologies, smart contracts and other innovative technology arrangements, are still limited as at the date of registration of this whitepaper. The risks arising out of such may not be fully comprehended as of yet and new additional risks may arise in the future. As the Issuers undertaking is reliant on the continuous and proper functioning of its technology infrastructure, the Issuer is subject to a variety of risks relating to the proper maintenance thereof. These risks include, but are not limited to, cyber-attacks, data theft or other unauthorised use of data, and other malicious interferences. Certain parts of the infrastructure may be outsourced to third parties.",
      "In such WHITEPAPER www.sandbox.game Page 11 cases, the Issuer is reliant upon technology arrangements developed by such third parties for the running of its undertaking, and it is exposed to the risk of failures in such technology arrangements. The Issuer is subject to various laws, including regulations and rules issued thereunder, and is at risk in relation to changes in the laws and the timing and effects of changes in the respective legislation. This includes changes in the interpretation thereof which cannot be predicted by the Issuer. The growth of the Issuer and its undertaking is partially attributable to the efforts and abilities of the members of its management team and other key personnel, particularly its board of administration. If one or more of the members of this team were unable or unwilling to continue in their present position, the Issuer might not be able to replace them within a short time, which could in turn have a material adverse effect on the Issuers business.",
      "2. Historical Background The Sandbox is the latest version of The Sandbox franchise, which has helped create the sandbox world creation games genre on iOS and Android smartphones since its launch in 2012:  40,000,000 players have downloaded The Sandbox franchise;  The latest iteration, The Sandbox Evolution, has over 1,200,000 MAUs, with up to 2,600,000 at peak;  70,000,000 worlds have been created; and  Over 100,000 worlds are created every day.",
      "The new blockchain-based version of The Sandbox has already received several awards: Voted the most anticipated blockchain game of 20192020 by BlockchainGamer.biz - WHITEPAPER www.sandbox.game Page 12 sandbox-most-anticipated Ranked 13 in the Top 50 Blockchain Games Companies of 2020 Report by DappRadar.com - https:www.blockchaingamer.bizfeatures13597top-50-blockchain-game- companies-2020 The Sandbox combines the large, active and enthusiastic franchise audience with a richer gaming environment based on a new 3D voxel1 engine, and the revolutionary ability of blockchain integration with NFTs2 to offer players creative and financial control of the content they make. 3. The Sandbox Platform 3.1. Mission The Sandbox teams mission is to build a system where creators will be able to craft, play, share, and trade without central control, enjoying secure copyright ownership with the ability to earn SAND.",
      "The Sandbox team believes these innovations are important, for while the current market for voxel-based creation games already has 500,000,000 Creators and 160,000,000 MAUs, it suffers from four key problems that could hinder future growth if not addressed:  The centralization of user-generated content in dominant titles such as Minecraft and Roblox limits creator rights and ownership;  Central control over the trading of virtual goods created by players restricts players from generating fair value for their creations, limiting what they can or cannot sell andor taking a significant portion of the sales revenue;  Given the nature of voxel art, it can be difficult to prove creative ownership of works, especially as content is copied, altered and built upon; and  Existing game marketplaces are based on fiat currencies, which do not support true microtransactions and are vulnerable to credit card fraud that can unbalance in-game economies. 3.2.",
      "Game Overview 1 Voxels, or volume pixels, are analogous to 3D pixels, with each voxel representing a value on a defined grid in three-dimensional space 2 Non-fungible items are unique and cannot be replaced by another equal part or quantity, as explained in this article: WHITEPAPER www.sandbox.game Page 13 At its core, The Sandbox is an ecosystem for Players and Creators, consisting of three main components: a Voxel Editor (named VoxEdit), a Marketplace, and the Game itself. The Sandbox offers a unique way to create, assemble, and share 3D voxel models. We empower Creators with intuitive, powerful content creation tools. You can make your own 3D voxel objects, animate them, and publishsell them in our worldwide marketplace. VoxEdit is our 3D tool that allows anyone to create or import their own voxel objects, work on them, and effortlessly export them to the marketplace where users can turn them into limited ERC-1155 tokens called ASSETS.",
      "ASSETS are virtual tokens for digital scarcity, security and authenticity. They are uniquelimited as well as distinct and indivisible. Blockchain allows creators to have true ownership and thanks to this technology we, in our capacity as developers, can finally reward players for their time and the hard work they put into content creation, allowing them to monetize and freely trade their ASSETS. Our ASSET smart contract allows content to become the new platform, creating a new paradigm. This is because they offer multiple uses and are not confined to a single game. This \"second-layer programmability\" gives any developer the power to bring a lasting and ever-growing value to the ASSETS, which can be shared cross-games, cross-platforms and cross-chains. The Marketplace is the trading environment for ASSETS, where they can be given away for free or sold to other PLAYERS. The Game is the overall game system where playable experiences can be enjoyed and shared.",
      "In the Game, not only can creators profit from their creations, players can play-to-earn by collecting resources, rewards, and tokens. 3.3. LAND and the LANDS Map LANDS are blockchain-backed virtual tokens (using the ERC-721 standard for NFTs) representing physical parcels of The Sandbox Metaverse. They allow players to own a portion of the Metaverse and thus be able to host content (ASSETS and GAMES). The Sandbox Metaverse is based on a map of 166,464 LANDS (408408). LANDS are physical spaces in the Metaverse owned by players to create and monetize Games. LANDS are used to publish your game and can be rented to game creators. Every LAND comes with a set of prebuilt terrains, but it can be terraformed and modified by the USER who owns it (or other PLAYERS they invite to work on it). In the future, players will be able to combine LANDS together to form ESTATES In the future, ESTATES will have the potential to be owned by multiple players to form DISTRICTS. 3.4.",
      "Avatar An AVATAR is an in-game voxel representation of a player, which may be modified piece by piece by the owner. An avatar has certain specifications and format and comes with a default set of animations such WHITEPAPER www.sandbox.game Page 14 as walking, running, jumping and fighting. Specifically, this avatar will be modifiable piece by piece (helmet, arms, torso, legs, etc.) by equipping it with different compatible ASSET tokens. 3.5. SAND SAND is an essential part of The Sandbox platform and we are working on establishing key mechanics that makes it intrinsically tied to The Sandbox platform and its value. SAND is an ERC-20 utility token built on the Ethereum blockchain that serves as the basis for transactions within The Sandbox, and has the following uses:  Access The Sandbox platform: Players spend SAND in order to play games, buy equipment, or customize their Avatar character. Creators spend SAND to acquire ASSETS, LANDS, and through Staking.",
      "LAND Sales drive demand for SAND to purchase LANDS. Artists spend SAND to upload ASSETS to the Marketplace and buy Gems for defining Rarity and Scarcity. Governance: SAND is a governance token that allows holders to participate in Governance decisions of the platform, using a DAO structure. They can exercise voting rights on key elements such as Foundation grant attributions to content and game creators and feature prioritisation on the platform Roadmap. SAND owners can vote themselves or delegate voting rights to other players of their choice. Staking: SAND allows for staking, which allows for passive revenues on LANDS: you get more SAND by staking it.",
      "This is also the only way to get valuable Gems and Catalysts, needed for ASSET creation  Fee Capture model  5 of all transaction volume carried out in SAND tokens (Transaction Fees) shall be allocated with 50 to the Staking Pool as rewards for token holders that Stake SAND tokens and 50 to the ''Foundation.''  Foundation: The role of the Foundation is to support the ecosystem of The Sandbox, offering grants to incentivize high quality content  game production on the platform. To date, the Foundation has funded over 15 game projects and granted 100 artists to produce NFTs ahead of the public Launch in December 2020. The overall valuation of the metaverse grows through the valuation of all games funded by the Foundation, creating a virtuous circle to enable funding bigger games. WHITEPAPER www.sandbox.game Page 15 Figure: The Use Case for SAND 3.6. SAND Stakeholders We have developed a 4 Stakeholders approach to ensure that that the success of TSB.",
      "Therefore, we will set through smart contracts, that revenues generated through The Sandbox will be distributed across 4 stakeholders to provide support to the ecosystems of creators and players as well as to provide the resources needed to grow the market and develop high quality gaming experiences. These 4 stakeholders are: Foundation Pool: Is designed to ensure that revenue generated through the game is redistributed to support the growth of the ecosystem. The token allocation of this Foundation pool will be progressively decentralised as we implement a DAOCommunity driven solution towards a decentralized model for the benefit of the token ecosystem. Staking Pool: Is designed to provide incentives to token holders who are actively locking their funds in smart contracts. Its governance will migrate from a centralized decision on year 1 towards a DAO mechanism in the coming years. Token holders that are also active players will generate an extra yield rewarding their activity.",
      "Company Treasury: This represents the SAND that is owned by the company that comes from the proceeds of sales of company owned assets with a 12 months lock-up. The SAND generated through this Treasury will be sold back to the market to pay operational expenses. Company Reserve: This is the initial company reserve of 600mm SAND (20 of total Token Supply). This reserve will be fed with the proceeds of sales of company owned assets with a 6 month lock-up. WHITEPAPER www.sandbox.game Page 16 3.7. SAND Revenue Streams TSB has 5 Revenue Streams where playerscreators will exchange SAND.",
      "That SAND will be allocated as detailed below: (1) LAND Sales 50 locked for 12 months then being sold in auction sales and proceeds go to the Companys Treasury; (2) Company NFTs Sales 25 transferred to the Company Reserve with a lock-up period of 6 months; (3) Player Subscriptions  Services 25 transferred to the Foundation to support the growth of the token ecosystem (4) Transaction Fees  5 any SAND trx Transaction Fees to be restructured as 5 of all transaction volume carried out in SAND tokens, including but not limited to SAND transactions related to sales and rental transactions of LAND (from TSB to players and from players to players in marketplace), NFT transactions (from TSB to players and players to players in marketplace), peer-to-peer in-game payments (e.g., entrance fees for game experiences), player subscription fees and advertising fees 50 allocated to the Staking Pool as rewards for token holders that Stake SAND tokens; 50 of all Transaction Fees shall be allocated to the ''Foundation'' (5) Advertising Revenue 25 allocated to the Staking Pool as rewards for token holders 75 allocated to the ''Foundation'' WHITEPAPER www.sandbox.game Page 17 Figure: The Flows of SAND 3.8.",
      "Benefits The creation of a decentralized blockchain based gaming platform provides multiple benefits when compared to current, non-blockchain systems, as illustrated in the chart below: With Blockchain (The Sandbox) Without Blockchain (Roblox  Minecraft) True Ownership  Assets stored in player wallets through tokenization of UGC;  User retention of copyright in perpetuity;  Ownership not tied to a game; and  Creative authorship is immutable. Limited Ownership  Assets stored in game systems;  Copyright limited and items can be modified by third parties;  Ownership is tied to the game; and  Creative authorship is hard to prove. WHITEPAPER www.sandbox.game Page 18 Secure  Less fraud possibility thanks to blockchains record keeping technology. Insecure  Server-based transactions; and  High possibility of fraud (13 average). Fair Revenue Share  Creators will receive 100 of the selling price they set for their creations; and  Multiple creators can automatically share payments and revenue.",
      "Limited Revenue Share  Creators receive partial payment for items they sell; and  Ownership limited to one player entity. Decentralized Trading  Blockchain allows assets to be shared between users for collaboration; and  Peer-to-peer trading Centralized Trading  Centralized system limits collaboration; and  All trades are controlled by 3rd parties. 4. Market Overview The current gaming market presents favorable conditions that offer The Sandbox a unique opportunity to exploit on both PCMac and mobile platforms, as the opportunity to shift power and earning potential into the hands of players in the growing UGC gaming market will add significant value to their gameplay experience.",
      "The global gaming market is expected to reach: USD 171.96 billion3 by 2025;  The mobile segment, which is one of The Sandboxs target platforms, is projected to gain momentum and exhibit a compounded annual growth rate (CAGR) of 7.3 over the coming years, owing to increasing smartphone penetration across the globe4;  The online segment is estimated to witness substantial growth over the next eight years, expanding at a CAGR of 8.6 from 2020 to 2025. This can be attributed to increasing broadband penetration and growing online betting, gambling, and social network gaming5; and  The Asia Pacific market is expected to gain traction, attaining a market size of USD 86.84 billion by 2025. Emerging countries such as China, India and South Korea are offering lucrative growth opportunities for market augmentation, which can be partly attributed to growing smartphone and Internet penetration in these countries.",
      "3 https:www.grandviewresearch.compress-releaseglobal-gaming-market 4 https:newzoo.cominsightsarticlesthe-global-games-market-will-reach-108-9-billion-in-2017-with-mobile-taking-42 5https:prnewswire.comnews-releasesgaming-market-size-worth-17196-billion-by-2025--cagr-65-grand-view-research-inc-671617663.html WHITEPAPER www.sandbox.game Page 19 4.1. Gaming Creator Market The Sandbox teams mission is to build a system where creators will be able to craft, play, share, and trade without central control, enjoying secure copyright ownership with the ability to earn SAND. The genre of creator games with voxel graphics is dominated by two key industry players, Minecraft and Roblox. Minecraft, the leader in the category, saw player numbers grow over 30 year on year for the last five years, while Roblox placed sixth in the list of top grossing iPhone mobile gaming apps in the United States as of January 2018, ranked by daily revenue.",
      "Roblox is estimated to take in over USD 750,000 per day in revenue. Some creators have reportedly earned over USD 30,000,0006 in 2017 from the 707 revenue share they receive from sales inside their own creations. Both games are now available on a variety of platforms including PC, mobile and console. A few numbers show the scope of the market in tangible terms: Minecraft  Over 100,000,000 MAU in 2019, up 74,000,000 from 20178;  Over 144,000,000 copies sold worldwide;  In 2014, Microsoft purchased the Minecraft franchise for USD 2.5 billion; and  The purchase price equates to roughly USD 17 per copy sold when averaged across all units.",
      "Roblox  Had 100,000,000 MAU in 20209;  Raised USD 150,000,000 series G on a USD 5 billion valuation in February 2020;  11,000,000 game titles were published in the Roblox world in 201710;  Over 1,500 titles have each generated over 1,000,000 user visits from other players; and  Meep City, which was published in Roblox in February 2016, was the first Roblox game to surpass 1 billion player visits. The Sandbox Evolution  Had 1,200,000 MAUs in 2018; and  Over 70,000,000 creations made inside the game by its users.",
      "6 https:www.forbes.comsitesalexknapp20180321young-developers-earned-over-30m-on-roblox-in-2017-the-gaming-site-kids-visit-more-than- youtubee490ac860942 7 https:www.roblox.compremiummembership?cashoutobc 8 https:www.engadget.com20190915minecraft-112-million-monthly-players 9 https:techcrunch.com20190804roblox-hits-100-million-monthly-active-users 10 https:www.businesswire.comnewshome20180221005527enRoblox-Celebrates-Top-2017-Titles-5th-Annual WHITEPAPER www.sandbox.game Page 20 CryptoKitties  Over USD 12,000,000 raised from asset sales11  More than 1,000,000 kitties created12;  Has approximately 500 Daily Active Users; and  Various special themed kitties from partnerships, including the NBA13 Platforms  Sketchfab, one of the largest 3D creators marketplace has approximately 400-1,000 models uploaded per day;  100,000 3D models have been uploaded in over 700 days14; and  TurboSquid, the worlds source for professional 3D models has over 500,000 models and 3,500,000 users. 4.2.",
      "Creator Market A primary challenge facing voxel art creators in the current gaming environment is they have limited or no legal rights to the intellectual property they create. This can result in artists or creators spending days or even months creating their in-game worlds while receiving little or no financial benefit. Another hurdle facing players is establishing creative ownership of their works, especially if another player or entity copies the original work, modifies it or builds on it. Without a system for identifying and tracking the ownership of an item from first creation, its almost impossible to tell if work is original or copied. Lastly, the common reliance on fiat currencies and credit card transactions means the element of credit card fraud is never far away. Currently, the ratio of illegitimate transactions in games can be as high as one out of every 7.5 items sold as the result of credit card fraud, potentially disrupting the entire in-game economy.",
      "By registering every ASSET created as an NFT, The Sandbox allows creators true ownership of everything they create. Creators retain copyright and ownership of everything they create and can sell and trade items while receiving all the benefits for their work. As transactions are made via blockchain in Ethereum, credit card fraud is eliminated. Aside from addressing all of these issues, The Sandbox aims to bring even more value to the crafted items of CREATORS. For the first time ever, CREATORS will be able to see how their uploaded ASSETS come to life when they see them used in a LAND as part of great experiences. 11 https:venturebeat.com20180320cryptokitties-blockchain-sensation-raises-12-million 12 https:medium.comCryptoKittiesthanks-a-million-f422af041d0f 13 https:www.coindesk.comnba-superstar-steph-curry-now-first-celebrity-cryptokitty 14 https:sketchfab.com100k WHITEPAPER www.sandbox.game Page 21 4.3.",
      "Player Market Issues such as ownership or compensation are of less concern to players who primarily come to games to play and buy items rather than create and sell them. However, another issue directly affects these players is that they spend real-world money to buy items in-game, but they have no way to recover this money. For instance, if they stop playing a game, all the items they bought in it immediately become worthless. If the game is shut down for any reason, such as a decline in popularity that makes it less profitable, all the players in-game purchases vanish into nothing. This is especially true in free-to-play (F2P) games. Purchases are a one-way street. Players spend money to buy things in the game, but have no way to recover their value if they no longer use an item, and have no way to sell it to another player legally so they receive no compensation if the game is taken off the market. 4.4.",
      "Market Solution The games player-centric innovations and improvements over current titles in the market make it probable that The Sandbox can expand its audience beyond the 40,000,000 players who have downloaded previous versions of The Sandbox and the games current 1,200,000 MAUs. The first stage of this process is the creation of a semi-decentralized gaming community with some elements under the central control of The Sandbox and other elements decentralized to the creators and players themselves. Roadmap The Sandbox has been operating since 2012, and it follows that this issue of SAND is one of the next steps in its journey towards creating this decentralized game 5.1. Previous Milestones May 2012  The Sandbox franchise launches on iOS. February 2013  The Sandbox is released for Android (1st title). WHITEPAPER www.sandbox.game Page 22 March 2014  The Sandbox surpasses 10,000,000 downloads. March 2015  The Sandbox Evolution is announced at GDC conference (2nd title).",
      "June 2016  The Sandbox Evolution released on iOS, Android  Steam. July 2017  The Sandbox Evolution receives two big partnerships:  Bandai Namco brings PAC-MAN to the game;  Dreamworks brings Shrek to the game; and  The Sandbox Evolution announced free download in Steam. April 2018  The Sandbox surpasses 40,000,000 downloads and 70,000,000 player creations; and  The Sandbox Surpasses 750,000 followers on Facebook  Funding: We have closed a seed round with a total of USD 3,410,000 raised;  Team: We have assembled the right senior team to execute our vision: 42 resources located in our 3 offices with 28 FTE in Argentina, 11 in France, 2 in Korea and 1 in Japan;  First revenue: The first two rounds of LANDS presale (2.5 then 5 of the total map supply) sold out in almost 1 hour  exciting success despite the bearish crypto market in December 19 then February 2020. Total Revenue generated of 1,400 ETH (approximately USD 300,000);  Community: Growing with over 10,000 members on our Discord.",
      "Our NFT maker VoxEdit has been downloaded overall 65,000 times and the 100 artists forming part of our Creators Fund have made 3,000 ASSETS already. Over 3,500 accounts on our Marketplace;  Smart Contracts: All our smart contracts is available publicly on Github and has been audited by Certik, a leader in security trusted by Binance, Upbit, Bithumb, etc, We scored 93100, among the highest in the industry;  Korea Presence: Growing over 1,000 highly engaged creators and members. Officialized our partnership with SBS Seoul Game Academy (reaching out to 2,000 students on 8 campuses in Korea); WHITEPAPER www.sandbox.game Page 23  Japan Presence: Started collaboration with Geekhash (a Japanese crypto-marketing agency, who is also representing Dapp.com), ran several meetups (Devcon5 Blockchain Game meetup and Tokyo Blockchain Games Meetup) and began localizing our Deck, Website and Medium articles. Opened Twitter with 600 followers and reaching 50,000 impressions per month.",
      "Q1 2020  VoxEdit  Launch of VoxEdit Beta Release  LAND Pre Sales  Launch Rounds 2 and Round 3, selling 5 and 10 of the Map in hours Q2 2020  Game Maker Closed Alpha  Private release of the Game Maker Alpha to few early testers  LAND MoonSale  6 rounds of LAND sales over 5 weeks Q3 2020  Game Maker Public Beta  Public Release of the Game Maker Beta  LAND Presales  Round 4 Opening  Marketplace  SAND purchases, P2P sales, ASSET minting with Gems  Catalysts  SAND Public Listing  Binance IEO and listing on public exchanges 5.2.",
      "Future Milestones Q4 2020  Game Public Beta  Public Beta Launch of the Games Platform, with dozens of games available to play (supported by the Foundation)  Events  Launch of the First Season event with play-to-earn mechanisms  Multiplayer  Socialize, Chat and Interact with friends in The Sandbox hubs  LAND Public sale  Regular LAND sales and Support for Game Maker  VoxEdit  Official Release 1.0 2021-2023 WHITEPAPER www.sandbox.game Page 24 We are aiming to become one of the leading established key players in the space of virtual worlds with blockchain-based digital LANDS, reaching over 1,000,000 MAUs over the next five years.",
      "To achieve this, we will be working on:  VoxEdit: We will be working directly with communities of artists to add the extra features they are asking for and facilitate the creation of more advanced models, while keeping in mind to remain accessible to all;  Marketplace: We will be working on engaging features that will be driving long term engagement and interactions with The Sandbox platform, allowing creators to work and collaborate together in teams establishing more advanced collaborative workflows, as well as features enabling any creator to run its own presale for games, organize its own contest with prize pools, define rules of each DAOs with its sub-community and more;  Game MakerPlayer: We will be releasing the Game Player on multiple platforms, starting with mobile (iOS and Android) in 2021 and expanding to console thereafter.",
      "We will improve the meta-gameplay to create come back design and providing incentives for players to explore and participate in the economic loop (Create, Trade, Play), fostering the Play-to-Earn (Monetize your skill and time by collecting resources, rewards, and tokens) as pillars of the economy. Players will have individual goals and objectives for the meta- gameplay but we will also include collective objectives to create communities and competition. The Sandbox LANDS can work as common hubs for players, provide different experiences, advertise  shops, chat rooms, and other featured experiences;  Blockchain and Tokens: We will be opening even more our blockchain technology to other Developers (marketplace, smart contracts, etc.) so they can build additional services, dApps smart contracts.",
      "A more detailed roadmap can be found below:  Partnerships  New IPs and Premium NFTs to be announced  Foundation  Launch of the Foundation DAO with SAND staking for voting on Creators Fund management  Game Maker  Official Release 1.0 and major updates with more gaming packages, Behaviors and Visual Scripting  Game  Official Release 1.0 and major updates  LAND Public sale  Regular LAND sales will continue  Events  Additional Seasons with big rewards to earn  Multi-platforms  Availability of Game on smartphones and tablets devices  Multiplayer  First user-generated games with multiplayer modes  SAND  Staking mechanisms for Yield generation and curation  Partnerships  New IPs and Premium NFTs to be announced WHITEPAPER www.sandbox.game Page 25  Games Platform  Over 1,000 Games available  Multi-platforms  Availability of Game on consoles  LAND Public sale  Regular LAND sales will continue  Game Maker  Co-Building Mode - creating games with Friends, Custom Scripting with programming 3rd layer and major updates with more gaming packages and Behaviors  Events  Additional Seasons with big rewards to earn  Games Platform  Over 5,000 Games available  Partnerships  New IPs and Premium NFTs to be announced  Game Maker  Major updates with more gaming packages and behaviors added  Events  Additional Seasons with big rewards to earn  DAO  The Sandbox DAO will allow to participate in major decisions on the Game Platform 5.3.",
      "Major Features in the Press  Our CEO, Arthur Madrid, published a Medium article titled Blockchain Metaverse: Where Players Can Build, Own, and Control a Platonic Republic of Gaming, which has been clapped over 500 times. control-a-platonic-republic-of-gaming-65e23f059e37  Our COO, Sebastien Borget, was quoted repeatedly on CoinTelegraph among 50 Crypto Experts on his perspectives for 2020 and looked back on 2019 articles. blockchain-topics-to-follow-in-2020  The Sandbox ranked 13th among the Top 50 Blockchain Games Companies of 2020 report by DappRadar, while Animoca Brands is ranking 5th. This confirms our leadership position in the space. The Sandbox raises 2 million more to build out blockchain-based game world from B Cryptos, True Global Ventures, Square Enix more. blockchain-based-game-world WHITEPAPER www.sandbox.game Page 26  The Sandbox Virtual World sold out third Virtual Lands Presale generating 3,330 ETH440,000 USD.",
      "t_third_Virtual_Lands_Presale_after_6_Hours_generating_3330_ETH440000_USD_in_Atari_The me_Park_Neighborhood.php Platform 6.1. Gameplay Experience The gaming experience on the new The Sandbox platform will be launched through several intermediary phases, with increasingly more complex features in each. When the alpha version is launched, players will be able to make transactions and store their digital voxel ASSETS while also playing and interacting with other voxel asset creators and players in different user generated worlds. The gaming experience will make The Sandbox a strong blockchain-based alternative to existing platforms such as Minecraft and Roblox. WHITEPAPER www.sandbox.game Page 27 Even for players who do not create discrete items to sell on the UGC store, the combination of creativity and security can pay off for players who create LANDS with compelling attractions. This is because a LAND might have different revenue streams implemented on it, like microtransactions.",
      "Additionally, players can play-to-earn by collecting rewards through gameplay. We expect a wide variety of audiences to enjoy gaming experiences in The Sandbox marketplace. We have identified some of the types of games creators will be able to craft for them:  Social Experiences: The ASSET marketplace will allow creators to fully decorate their spaces with beautiful sceneries and structures such as a fairy wonderland, a medieval town, or even a futuristic space station. These types of experiences can attract communities of in-game friends that hang out for activities that combine social and gameplay elements. Build  Craft: Our voxel map is ideal for creators to make Building  Crafting experiences. Creators will be able to facilitate players with interesting experiences like building competitions, creating their own play spaces. All these tools and mechanics will be able to interact with the terrain to modify it, build over it or even completely destroy it.",
      "Exploration  Adventure: Solve the mystery of the cursed swamp, slay the mercenary king of the endless desert, help the rebellion of the exiled prince of the elder LANDS or even discover all the hidden secrets of the ancient temple! Creators will be able to bring narrative, exploration and adventure gameplay to their LAND. Each new LAND will be different, offering new potential to discover new LANDS, meet new people and discover never before seen secrets! WHITEPAPER www.sandbox.game Page 28 6.2. Map - LANDS The persistent map determines a players specific LAND coordinate. In most blockchain games where player domains are sold like real estate, the specific location of a domain is of paramount importance as it will determine who and what you are next to. Similarly, in The Sandbox there is a finite amount of LANDS, and their location is determined initially in the map, and cannot be moved from one coordinate of the world map to the other.",
      "There are several advantages to this structure:  Players have the agency to combine LANDS to create a bigger gaming space with and near their friends; and  Friends can cooperate in the creation of these larger experiences by building experiences together within the combined LAND and thus sharing the rewards (whether financial or purely social or artistic). There is also an economic incentive for players to work together via a combined LAND. For instance, if several players band together they can create a theme park experience, with each member focusing on a key attraction, or members can focus on distinct neighborhoods that together create a city experience. In each case, the team effort allows players to cooperatively create a stronger incentive for players to visit and enjoy their creationsand more visits results in more revenue to be shared.",
      "Its true that a single player with one LAND can keep all the revenue from that LAND, but a larger, more diverse collection of LANDS combined can boost financial rewards to be shared by charging, for example, for a SUPER PASS that allows a player to visit all the gaming experiences in a combined LAND, with the revenue from the SUPER PASS shared between all the owners of said LAND. There will be limits on joining LANDS together to keep certain conglomerations from having too much control and power against solo players. Recent information has been released on The Sandbox Transportation System. 6.3. ASSETS Marketplace The marketplace will use smart contracts to control item auctions and transactions. This open marketplace for creators and developers will allow for the free trade of in-game assets between the platform users.",
      "Additional benefits of this marketplace are that economic principles of supply and demand become automatically integrated into the system, allowing the community to determine the fair price of user- generated ASSETS. The true strength of this marketplace, however, is that creators of such ASSETS can finally generate true tangible real-world value from the voxel content they create. WHITEPAPER www.sandbox.game Page 29 The ASSET tokens in our marketplace will require several pieces of information in order to properly be displayed as a sellable good.",
      "These pieces of info are:  Name  Genre: In order to better identify the ASSET being sold, and to help potential buyers in finding it in the marketplace, the ASSET will require a name and a genre;  Description: The description will help creators add specifications like size, model, potential uses or even attach a short piece of fictional lore to the ASSET;  Thumbnail: The marketplace will show what the ASSET itself looks like via several thumbnails of the ASSET. We will include a feature in our voxel editor that will automatically generate these thumbnails for the creators;  Price: The creator will have total control over the price the ASSET will have on the marketplace. This price will be set in SAND. Value: Creators will define Rarity, Scarcity and Attributes thanks to the Gems  Catalytsts system. Once all these requirements are met, the ASSET will be successfully displayed on the marketplace and be available for purchase. 6.4.",
      "VoxEdit WHITEPAPER www.sandbox.game Page 30 Under the philosophy of CREATE, ANIMATE, PLAY VoxEdit is the voxel modeling software that allows the artist to rig models and animate them using a user-friendly keyframe interpolation timeline. Modeling: Having other software that also lets you model your own voxel art  what makes the modeling in VoxEdit different is that it is aimed at artists. That commodity is thanks to a user-friendly interface, powerful tools to let the artists model anything they want, but also stay connected with them all the time to hear their feedback, what would be nice to have, and especially what would make their life using VoxEdit much easier. Animation: Here is where VoxEdit stands out. VoxEdit uses a skeletal system with hierarchies, like the ones used in professional 3D software like Maya, 3D Max, Cinema 4D, etc, allowing the Artist to rig anything they want and be ready for animation.",
      "It is the only voxel modeling software that allows to model AND animate your ASSETS. Once the skeleton is made, the artist can create any type of animation moving the different parts of their model with an easy-to-use timeline. The timeline works like a video editing software timeline, using layers to represent what part of the model you are moving and creating key-frames to set the position, rotation and scale. The magic happens thanks to the interpolation of these key-frames that the timeline does in the background. Exporting: VoxEdit allows the artist to export their creation in .VXM and .VXR, formats that our Marketplace and games use to create a super smooth communication between the three parts. Besides that, it also lets export in the standard formats such as .OBJ and also .DAE and .GLTF for the models with animations on it.",
      "Economy We are excited by the opportunity to build new kinds of business models with this player-powered economy, both in terms of revenue model and token model. SAND will provide immediate liquidity to players and investors. The valuation of the SAND token is calculated on what players will be able to buildplaytradewin inside our metaverse. We are aiming at replicating real-world economy systems, with players able to buy, rent, hire, vote, stake, etc.",
      "The main flows of income will be the following:  Company ASSETs token: Sales of ASSETS created and developed by the company (LANDS, ASSETS, Attributes, others);  Transaction Fees: TSB to charges 5 of all transaction volume carried out in SAND tokens (Transaction Fees) to be allocated 50 to the Staking Pool as rewards for token holders that Stake SAND tokens and 50 to the ''Foundation.''  Player Subscription Fees: Revenues that we will generate via various subscription models to gain access to exclusive in-game features, Premium ASSETS etc. WHITEPAPER www.sandbox.game Page 31 7.1. Tokenomics SAND is the utility token used throughout The Sandbox ecosystem as the basis of transactions and interactions. It is an ERC-20 utility token built on the Ethereum blockchain.",
      "It will be used across the ecosystem by gamers, developers, and publishers, allowing Creators and Players to exchange ASSETS and Games and build a user-based platform of rewards while developing an ecosystem where Creators and Players will share various truly unique gaming experiences. User generated digital ASSETS are represented as ERC-1155 tokens. To ensure the everlasting scarcity and verifiable scarcity of non-fungible items, we use the ERC-1155 token standard. This standard allows a smart contract to track token-ownership at the individual token level: each item has a unique identifier and, optionally, unique properties stored as metadata. SAND can be used and acquired by Artists, Creators, Players and LAND Owners through multiple ways as described below: ARTISTS CREATORS PLAYERS LAND OWNERS INVESTORS What each profile will USE SAND tokens for?",
      "Year 1 Mint ASSETS Buy Gems  Catalysts Acquire ASSETS Publish Games Buy LANDS Buy LANDS Combine LANDS in Estates Acquire SAND on Exchanges Years 2-5 Rent LANDS Create Prize Pools Play Games Buy ASSETS, Avatar Skins  Equipments Buy Portals Stake SAND What each profile will GET SAND tokens with? WHITEPAPER www.sandbox.game Page 32 Year 1 Sell ASSETS Stake SAND Event Rewards Stake SAND Event Rewards Sell Gems  Catalysts Stake SAND in his LAND Rent LANDS Sell LANDS Stake SAND in his LAND Stake SAND Years 2-5 Work For Hire Monetize Games Work For Hire  Players: Playing: Players will be able to earn SAND tokens while playing the different games uploaded to The Sandbox ecosystem.",
      "They might win SAND by slaying a legendary monster, opening a chest, or even completing a quest; nearly endless possibilities can be created in games; Skill-Based Challenge: Players will be able to pay an entry fee to enter a challenge such as a race, a card game, a battle royale, or any other type of competitive contest. The winner takes all of the prize pool. The experience creator might take a  of the total pool for creating and hosting the experience; and Tips System: A player who also happens to be a streamer, might be tipped with small amounts of SAND based on his performance either for entertaining his audience or by being good at whichever game he or she is playing. Creators: ASSET Sales Revenue: Creators will be able to sell their assets in the marketplace, receiving 100 of the SAND price of the ASSET; In-Game Purchases: Experience creators will be able to set up many different monetization systems that will let them charge players SAND for exclusive content.",
      "These monetization techniques range from item purchases to stat boosters, subscriptions, or even entry fees to the game itself; Creators Fund: At the first stages of the ecosystem, we will have a rewards program to incentivize creators to fill the marketplace with amazing voxel creations and in return be rewarded with a generous sum of SAND tokens; Art Challenges: We intend to keep a community-centered focus towards the marketplace, and with that in mind we decided that we will hold art challenges in which creators will be able to upload one of their ASSETS for the possibility to win a prize pool of SAND tokens if their ASSET received the most votes from the community; and WHITEPAPER www.sandbox.game Page 33 Crowdfunded Requests: We shall enable a section of the marketplace that allows individuals and groups of users to request a certain type of ASSET in exchange for a payment of SAND, contributed by all the requesters.",
      "The flow of SAND within the ecosystem could be represented as follows: What The Sandbox does and How WHITEPAPER www.sandbox.game Page 34 7.2. Project Financing and Initial Revenue During the period May 2019 to November 2019, the Company completed a successful private placement round with 13 Investors for an aggregate amount of USD 3,410,000. The private placement round was done through equity and token sale agreements: The token distribution and the SAND acquired by each investor during the private place took place in 2019 and resulted in the total sale of 515,277,777 SAND which is equivalent to 17.18 of the token allocation through token sale. SAND allocated to investors through the Private Sale, Founders, Tem and Advisors are 100 locked with 5 years vesting and 12-month cliff. TSB has also generated its first LAND presales successfully during December 2019 and July 2020. All LAND assigned for sale have been sold for over 6,000 ETH. 7.3.",
      "Initial Exchange Offering WHITEPAPER www.sandbox.game Page 35 TSB is aiming to raise around USD 3,000,000 during the public sale, through the allocation of 12 of the total supply of SAND. There shall not be a soft cap and therefore there will be no refunds made of funds contributed towards the project and allocated as detailed below: Grow the development team and infrastructure for the game platform Marketing  Creators acquisition and IP licences for NFT developments Security, Legal and Compliance Expenses General and Administrative Expenses Investors for SAND include, but are not limited to:  Traditional Venture Capital;  Cryptocurrency Investors;  Gaming Companies;  Gamers;  Game Developers 7.4. Other Dynamics As the community increases in terms of the number of creators, players and ASSETS in the marketplace, there will be an increase in the need for a utility token to reward the growing number of stakeholders participating in the platform.",
      "Growth Model and KPIs The key success factors for building the ecosystem consist of ensuring an organic community growth of Creators and Players invested in the value proposition offered by The Sandbox while ensuring customer satisfaction.",
      "In order to ensure a progressive growth of the community and its model, we have identified key variables that will be measured and incentivized through marketing, promotional, and communication tactics to ensure the growth of the ecosystem: o Creators growth rate; o Players growth rate; o New Assets created and uploaded per Creator; o New Assets created and uploaded by TSB; o Assets sold per month created by Creators; o Assets sold per month created by TSB; o Inflation rate of Assets in the marketplace; o Average price per Asset developed by Creators (in SAND); o Average price per Asset developed by TSB (in SAND); o Asset price growth; o Initial supply of SAND; o Initial supply of SAND in the hands of Creators and Players; o Price of SAND (in USD or ETH); o  Commission at the marketplace.",
      "WHITEPAPER www.sandbox.game Page 36 Therefore, while the total supply of SAND is fixed, the initial amount of SAND offered to the ecosystem through creators and players will be limited to add a scarcity effect due to the following key factors: o A growing number of Creators and Players which eventually will reduce the SAND available per capita; and o Staking effect, increasing the time holding SAND. Technology Blockchain technology is used to record ownership of tokens and allow owners to transferselluse them without restriction. IPFS is used to store the actual digital asset and ensure the asset cannot be changed without owner permission. Three different blockchain protocols will be integrated into The Sandbox gaming stack:  ERC-20 for SAND; and  ERC-1155 and ERC-721 for the Digital Assets storage and trading. 8.1. Future Technology Integrations Why are we using Ethereum?",
      "We found many benefits to do so as detailed below:  Ease of use: Ethereum has been built with flexibility in mind, which perfectly fits our use case with Asset tokens. Ethereum also has the largest number of developers, which allows for well-established standards, best practices and support;  Robustness: It is, as of today, among the most robust blockchain protocols: its wide adoption makes the network validated by many and thus more secure; and  Interoperability: Ethereum is a protocol, a base layer on top of which all applications can build and interact with each other. The main issue faced by Ethereum today is often said to be its scaling issue. Blockchains are based on three pillars: Security, Decentralization and Scalability (throughput). Historically, Ethereum always put security and decentralization first, at the cost of scalability and some technological improvements need to be made at the base layer to support traffic at scale.",
      "On most public blockchains like Ethereum, this is one of the main current concerns, and lots of efforts are in progress to solve those scaling issues, without sacrificing the blockchain trilemma. Ethereum is a blockchain that has been proven by thousands of dApps and developers for its robustness and offering a large ecosystem, resources and support from developers. With the current system we are building, escrow less auctions, meta transaction, subscriptions (cie) we can offer a great experience on Ethereum for non-crypto users WHITEPAPER www.sandbox.game Page 37 The Sandbox team is looking at layer-2 solutions (that use the main ethereum chain as an anchor for security)  at the moment they are not yet ready for implementation or have not reached a production stage so that we could reliably integrate them. We will closely follow technical progresses on this topic and decide to integrate the solution that combines the best advantages for our product, use-case and users. 8.2.",
      "Multiple Class Fungible Token (MCFT) At The Sandbox, we spend a lot of time looking at how to preserve security and integrity for ASSETS on the blockchain while still allowing the speed and flexibility that players and creators demand. As with many aspects of game design, you cant get everything you want easily, so its a matter of balancing elements to implement the best solution The ERC-1155 standard that we co-created with other game companies in the blockchain space, was designed to support the management of millions of tokens.",
      "On top of that our ASSET contract is also ERC- 721 compliant and allows multiple unique items to be created by a single smart contract with the benefit of keeping the ERC-721 interface to allow interoperability with other Marketplaces and games In The Sandbox, users will have full control over their ASSETS, by being able to create them, but also having the possibility to buy, sell, and trade those same ASSETS in The Sandbox Marketplace using our SAND token currency. 8.2.1. Interoperability As we use Ethereum as the substrate of our ERC-1155ERC721 implementation, any platform  wallet that supports ERC-1155ERC-721 will be able to inspect and use our ASSET. This means independent developers could rely on the plethora of ASSETS our players will be creating.",
      "The Sandbox gaming environment will be divided into 5 types of tokens:  SAND (our game currency used across the system)  LANDS (Worlds in which players play that are created by the community)  ASSETS (Voxel models created by players and traded in the marketplace)  GEMS burnt to give attributes to ASSETS. They are ERC20 tokens  CATALYSTS burnt to mint ASSETS. Depending on their power, they allow users to associate a certain number of GEMS to an ASSET. They are ERC20 tokens WHITEPAPER www.sandbox.game Page 38 Figure: The Sandbox Tokens Token contracts are responsible for:  Keeping track of creator (the address that minted the token)  ownership  transfers  Emitting events when the state changes 8.2.2. Meta Transactions The Sandbox is aiming to bring non-crypto users to the blockchain world. In order to achieve that we believe that the system should be transparent to the user as much as possible.",
      "Meta Transactions allow users that own SAND to interact with our platform without the need to own or know what ETH is or for that matter any of the tokens that will be used. 8.3. Technology Solution Breakdown The Sandbox platform architecture is composed of several components. In terms of blockchain integration, we have a traditionally backend running on the cloud (currently using AWS) to support our web frontend. An S3 bucket is used to store the asset of the artist before they get minted. While we currently guarantee the privacy of the artist work (to protect their work from being copied prior to minting), we envision later a system that would prevent even us to peek at the artist work pre-minting. Once an UGC asset is minted, our backend releases the ASSET on IPFS so that it becomes public.",
      "Our smart contract records the hash of an ASSET so that the owner of the ASSET will always be able to prove ownership of not only the number recorded on the blockchain (which many projects in the blockchain gaming space stop there) but also the voxel model itself as well as the various render. WHITEPAPER www.sandbox.game Page 39 Here is an overview of the overall architecture with all components represented. Creators can create their ASSETS with VoxEdit, and upload those ASSETS through the browser  Metamask is a web3 provider which acts as an intermediary between the User and the blockchain  A browser in which the user will interact with the smart contracts and our server  The smart contracts running on the blockchain  Our server currently hosted on AWS  The backend is being developed with Node.js, which manages the creation of an asset on S3, generating a Hash ID for that ASSET, which is sent back to the frontend so it can begin the transaction with the smart contract.",
      "Finally, once the transaction is confirmed we released the asset on the ipds network. ensuring that future owner will always be able to prove that what graphical representation they hold is indeed the original one. IPFS17 (The InterPlanetary File System) is a peer-to-peer distributed \ufb01le system that seeks to connect all computing devices with the same system of \ufb01les. 8.4. Program Agents WHITEPAPER www.sandbox.game Page 40 The Sandbox platform architecture includes a server on our AWS infrastructure that is monitoring the blockchain and registering change of state. Changes are then propagated by our API into our backend, and our centralized database is updating its records, so that the frontend can pull the recent changes and display them accordingly to the players and users of The Sandbox platforms. These changes of states include transfer of ownerships from the tokens (LANDS, ASSETS, or SAND).",
      "Additionally, the following are the various situations that will trigger external data inputs to our smart contracts. The data received in inputs is always sanitized and several security layers are implemented to make sure these are protected. Our smart contracts have been duly audited. LAND Sale Setup: we submit the LAND configuration to each new sale contract at the time of deployment. LAND Sale: the buyer submits their LAND of choice inclusive of the payment, following which the contract mints the LAND in exchange. We provide a 2D map to allow the buyer to easily choose their LAND;  LAND Sale Stable Price Oracle: We do not have any oracle system that sends information to our smart contract.",
      "Instead we utilise MakerDAO medianizer which receives updates from the Oracle and makes a median estimation of the price of the dollar: https:developer.makerdao.comfeeds  Asset Sale setup: o the creator submits a 3D voxel model to our backend, we hash the result and submit that on the smart contract, that in turn mints a token to represent it; o the creator submits a signature specifying the selling price and our backend shall store it;  Asset Sale: the buyer submits a payment for a particular token to the smart contract along with the seller signature. It receives the token in exchange. 8.5. Game Engine We have developed a custom Voxel Engine on top of Unity engine. Our engine leverages the power of the new Unity Data Oriented Tech Stack (a.k.a. DOTS) by making extensive use of the Entity Component System (ECS) and the Jobs system.",
      "By using Unity's new Universal Render Pipeline (URP) we are able to support Mobile platforms in the future, without sacrificing the render quality of our game for Desktop platforms. Our engine also implements support for our custom Voxel Models, Rigging and Animation formats (VXM, VXR, VXA) from our in-house Voxel Editor (VoxEdit). WHITEPAPER www.sandbox.game Page 41 8.6. Wallets We minimize the security risk on our backend by reducing the responsibility of hot wallets that are only used for referral and whitelisting. The other wallets are cold storage wallets that hold responsibility for updating the platform with new contracts and new parameters. These could later be part of a governance mechanism. these cold wallets, multi-signature wallet (https:github.comgnosisMultiSigWallet) and all of the owners of those multi-signatures are three hardware wallets. This wallet has the possibility to upgrade our SAND token, as well as the ability to extend functionality via super operators.",
      "In relation to the methods of payment, if the user wishes to pay with:  an ERC-20 based token (ETH or DAI), it will then be processed by a smart contract on-chain and the user shall receive the SAND directly into his wallet; andor  BTC, credit card or fiat payment, it will then be processed by a third party service merchant solution, whereby upon confirmation by the latter, the SAND shall be released. 8.7. Security The Sandbox marketplace is relying on the security of Ethereum for the functioning of its smart contracts. The only wallet that can affect the logic of our smart contract is our multi-signature wallet which uses the gnosis MultiSig wallet backed with three hardware wallets. On our backend side, we manage a database that cache the smart contract events to keep track of ownership. Our backend also stores temporary assets. On that side, its responsibility is thus minimal.",
      "As for fiat payment hot wallet, we are designing a daily auto-refill to ensure that only a minimum amount of token is present, minimizing the consequence of a breach. WHITEPAPER www.sandbox.game Page 42 9. The Team We have assembled the right senior team to execute our vision: 42 resources located in our 3 offices with 28 full time employees in Argentina, 11 in France, 2 in Korean and 1 in Japan. 9.1. Core Team Arthur Madrid - CEO and Director of The Sandbox Arthur Madrid is the Co-founder and CEO of Pixowl (acquired by Animoca Brands in 2018) as well as The Sandbox (40M players). Board Member of Animoca Brands and a longtime social- gaming entrepreneur. He sold two software companies (Wixi Inc. and 1-Click Media) and is an advisor for startups in gaming, social media and software. Arthur began his career building a P2P delivery platform for Media companies, one of the first Distributed Computing Software (DCIA) platforms.",
      "Now, he is focusing on Blockchain Gaming as CEO  Co-Founder of The Sandbox. Sebastien Borget  COO and Director of The Sandbox S\u00e9bastien Borget is the Co-founder and COO of Pixowl (acquired by Animoca Brands in 2018) as well as The Sandbox (40M players). Passionate for blockchain technology, gaming and education, he is a very active speaker and evangelist on the opportunity Non-Fungible Tokens brings to gaming. He is now building the metaverse with The Sandbox, one of the 2019s most anticipated and top ranked blockchain games (13 in the Top 50), that is empowering players through NFTs, enabling them to own, buy, sell and trade their 3D creations on their Marketplace and use them in their Game Maker. Sebastien also became President of the Blockchain Game Alliance in 2020. Marcelo Santurio - CFO of The Sandbox Marcelo Santurio is the Co-founder of the 1st online payment company in Latam and has 20 years experience in Finance, Tech, and Gaming.",
      "MBA from the London Business School with focus on Finance and Execute Program at MIT in Entrepreneurship. Pablo Iglesias - Inventor of The Sandbox Game Pablo Iglesias has over ten years of experience in researching and developing emergent procedural systems while leading teams of professionals through several successful games. Lucas Shrewsbury - CTO of The Sandbox, ex-CTO of Gameloft Lucas Shrewbusyr has spent more than a decade leading mobile gaming studios. While at Gameloft Argentina, he managed the studio and a team of 200 people. WHITEPAPER www.sandbox.game Page 43 9.2. Advisors Hashed Based in South Korea and San Francisco, Hashed has realized meaningful gains in the crypto-industry. Led by serial entrepreneurs and engineers, Hashed has expedited global Blockchain adoption through strategic investment and community building.",
      "Mikhael Naayem Mik Naayem is Chief Business Officer at Axiom Zen and CryptoKitties, the worlds most successful game built on blockchain technology and Axiom Zens first public blockchain project. Mik focuses on directing business strategy, growing partnerships, and evolving the company platform. Yat Siu Co-founder and CEO at Animoca Brands. He is a serial entrepreneur and angel investor. He is the CEO and founder of Outblaze (sold B2B business to IBM in 2009) and he has received recognition for his role as an entrepreneur focused on Internet and technology companies. Alexis Bonte Group COO of Stillfront Group, Co-founder and board member of eRepublik Labs. He is a co-founder and angel investor with a strong general management background and ovber 20 years 360 degrees experience in all types of start ups from early stage to post-IPO. Ed Fries Ed Fries created his first video games for the Atari 800 in the early 1980s.",
      "He joined Microsoft in 1986, and spent the next ten years as one of the early developers of Excel and Word. He left the Office team to pursue his passion for interactive entertainment and created Microsoft Game Studios. Over the next eight years he grew the team from 50 people to over 1200, published more than 100 games including more than a dozen million sellers, co-founded the Xbox project, and made Microsoft one of the leaders in the video game business. 9.3. Services Providers Legal, Accounts  Tax Centrium Advisory System Auditor(s) Fact Group CertiK Solidified WHITEPAPER www.sandbox.game Page 44 CONCLUSION The Sandbox decentralized platform allows players and creators to own a piece of our gaming metaverse (LAND), participate in the governance and economy (SAND), while creating and enjoying a simple way to benefit from their playing time.",
      "In the coming year, our experienced and dedicated team will build an unique way to play in a virtual world where you can play, create, collect, earn, govern, and own anything in the game. Join us now."
    ],
    "word_count": 11982,
    "page_count": 44
  },
  "SC": {
    "chunks": [
      "Sia: Simple Decentralized Storage David Vorick Nebulous Inc. davidnebulouslabs.com Luke Champine Nebulous Inc. lukenebulouslabs.com November 29, 2014 Abstract The authors introduce Sia, a platform for decentral- ized storage. Sia enables the formation of storage con- tracts between peers. Contracts are agreements be- tween a storage provider and their client, de\ufb01ning what data will be stored and at what price. They require the storage provider to prove, at regular in- tervals, that they are still storing their clients data. Contracts are stored in a blockchain, making them publicly auditable. In this respect, Sia can be viewed as a Bitcoin derivative that includes support for such contracts. Sia will initially be implemented as an alt- coin, and later \ufb01nancially connected to Bitcoin via a two-way peg. Introduction Sia is a decentralized cloud storage platform that in- tends to compete with existing storage solutions, at both the P2P and enterprise level.",
      "Instead of renting storage from a centralized provider, peers on Sia rent storage from each other. Sia itself stores only the stor- age contracts formed between parties, de\ufb01ning the terms of their arrangement. A blockchain, similar to Bitcoin 1, 12, is used for this purpose. By forming a contract, a storage provider (also known as a host) agrees to store a clients data, and to periodically submit proof of their continued stor- age until the contract expires. The host is compen- sated for every proof they submit, and penalized for missing a proof. Since these proofs are publicly veri- \ufb01able (and are publicly available in the blockchain), network consensus can be used to automatically en- force storage contracts. Importantly, this means that clients do not need to personally verify storage proofs; they can simply upload their \ufb01le and let the network do the rest.",
      "We acknowledge that storing data on a single un- trusted host guarantees little in the way of availabil- ity, bandwidth, or general quality of service. Instead, we recommend storing data redundantly across mul- tiple hosts. In particular, the use of erasure codes can enable high availability without excessive redun- dancy. Sia will initially be implemented as a blockchain- based altcoin. Future support for a two-way peg with Bitcoin is planned, as discussed in Enabling Blockchain Innovations with Pegged Sidechains 5. The Sia protocol largely resembles Bitcoin except for the changes noted below. General Structure Sias primary departure from Bitcoin lies in its trans- actions. Bitcoin uses a scripting system to enable a range of transaction types, such as pay-to-public-key- hash and pay-to-script-hash. Sia opts instead to use an MofN multi-signature scheme for all transac- tions, eschewing the scripting system entirely. This reduces complexity and attack surface.",
      "Sia also extends transactions to enable the creation and enforcement of storage contracts. Three exten- sions are used to accomplish this: contracts, proofs, and contract updates. Contracts declare the inten- tion of a host to store a \ufb01le with a certain size and hash. They de\ufb01ne the regularity with which a host must submit storage proofs. Once established, con- tracts can be modi\ufb01ed later via contract updates. The speci\ufb01cs of these transaction types are de\ufb01ned in sections 4 and 5. Transactions A transaction contains the following \ufb01elds: Field Description Version Protocol version number Arbitrary Data Used for metadata or otherwise Miner Fee Reward given to miner Inputs Incoming funds Outputs Outgoing funds (optional) File Contract See: File Contracts (optional) Storage Proof See: Proof of Storage (optional) Signatures Signatures from each input Inputs and Outputs An output comprises a volume of coins.",
      "Each output has an associated identi\ufb01er, which is derived from the transaction that the output appeared in. The ID of output i in transaction t is de\ufb01ned as: H(toutputi) where H is a cryptographic hashing function, and output is a string literal. The block reward and miner fees have special output IDs, given by: H(H(Block Header)blockreward) Every input must come from a prior output, so an input is simply an output ID. Inputs and outputs are also paired with a set of spend conditions. Inputs contain the spend conditions themselves, while outputs contain their Merkle root hash 2. Spend Conditions Spend conditions are properties that must be met before coins are unlocked and can be spent. The spend conditions include a time lock and a set of pub- lic keys, and the number of signatures required. An output cannot be spent until the time lock has ex- pired and enough of the speci\ufb01ed keys have added their signature.",
      "The spend conditions are hashed into a Merkle tree, using the time lock, the number of signatures required, and the public keys as leaves. The root hash of this tree is used as the address to which the coins are sent. In order to spend the coins, the spend con- ditions corresponding to the address hash must be provided. The use of a Merkle tree allows parties to selectively reveal information in the spend conditions. For example, the time lock can be revealed without revealing the number of public keys or the number of signatures required. It should be noted that the time lock and number of signatures have low entropy, making their hashes vulnerable to brute-forcing. This could be resolved by adding a random nonce to these \ufb01elds, increasing their entropy at the cost of space e\ufb03ciency. Signatures Each input in a transaction must be signed.",
      "The cryp- tographic signature itself is paired with an input ID, a time lock, and a set of \ufb02ags indicating which parts of the transaction have been signed. The input ID in- dicates which input the signature is being applied to. The time lock speci\ufb01es when the signature becomes valid. Any subset of \ufb01elds in the transaction can be signed, with the exception of the signature itself (as this would be impossible). There is also a \ufb02ag to in- dicate that the whole transaction should be signed, except for the signatures. This allows for more nu- anced transaction schemes. The actual data being signed, then, is a concate- nation of the time lock, input ID, \ufb02ags, and every \ufb02agged \ufb01eld. Every such signature in the transaction must be valid for the transaction to be accepted. File Contracts A \ufb01le contract is an agreement between a storage provider and their client. At the core of a \ufb01le contract is the \ufb01les Merkle root hash.",
      "To construct this hash, the \ufb01le is split into segments of constant size and hashed into a Merkle tree. The root hash, along with the total size of the \ufb01le, can be used to verify storage proofs. File contracts also specify a duration, challenge fre- quency, and payout parameters, including the reward for a valid proof, the reward for an invalid or missing proof, and the maximum number of proofs that can be missed. The challenge frequency speci\ufb01es how of- ten a storage proof must be submitted, and creates discrete challenge windows during which a host must submit storage proofs (one proof per window). Sub- mitting a valid proof during the challenge window triggers an automatic payment to the valid proof address (presumably the host). If, at the end of the challenge window, no valid proof has been submitted, coins are instead sent to the missed proof address (likely an unspendable address in order to disincen- tivize DoS attacks; see section 7.1).",
      "Contracts de\ufb01ne a maximum number of proofs that can be missed; if this number is exceeded, the contract becomes in- valid. If the contract is still valid at the end of the con- tract duration, it successfully terminates and any re- maining coins are sent to the valid proof address. Conversely, if the contract funds are exhausted be- fore the duration elapses, or if the maximum number of missed proofs is exceeded, the contract unsuccess- fully terminates and any remaining coins are sent to the missed proof address. Completing or missing a proof results in a new transaction output belonging to the recipient speci- \ufb01ed in the contract. The output ID of a proof depends on the contract ID, de\ufb01ned as: H(transactioncontracti) where i is the index of the contract within the trans- action. The output ID of the proof can then be de- termined from: H(contract IDoutcomeWi) Where Wi is the window index, i.e. the number of windows that have elapsed since the contract was formed.",
      "The outcome is a string literal: either valid- proof and missedproof, corresponding to the va- lidity of the proof. The output ID of a contract termination is de\ufb01ned H(contract IDoutcome) Where outcome has the potential values success- fultermination and unsucessfultermination, corre- sponding to the termination status of the contract. File contracts are also created with a list of edit conditions, analogous to the spend conditions of a transaction. If the edit conditions are ful\ufb01lled, the contract may be modi\ufb01ed. Any of the values can be modi\ufb01ed, including the contract funds, \ufb01le hash, and output addresses. As these modi\ufb01cations can a\ufb00ect the validity of subsequent storage proofs, contract ed- its must specify a future challenge window at which they will become e\ufb00ective. Theoretically, peers could create micro-edit chan- nels to facilitate frequent edits; see discussion of micropayment channels, section 7.3.",
      "Proof of Storage Storage proof transactions are periodically submitted in order to ful\ufb01ll \ufb01le contracts. Each storage proof targets a speci\ufb01c \ufb01le contract. A storage proof does not need to have any inputs or outputs; only a con- tract ID and the proof data are required. Algorithm Hosts prove their storage by providing a segment of the original \ufb01le and a list of hashes from the \ufb01les Merkle tree. This information is su\ufb03cient to prove that the segment came from the original \ufb01le. Because proofs are submitted to the blockchain, anyone can verify their validity or invalidity. Each storage proof uses a randomly selected segment. The random seed for challenge window Wi is given by: H(contract IDH(Bi1)) where Bi1 is the block immediately prior to the be- ginning of Wi. If the host is consistently able to demonstrate pos- session of a random segment, then they are very likely storing the whole \ufb01le. A host storing only 50 of the \ufb01le will be unable to complete approximately 50 of the proofs.",
      "Block Withholding Attacks The random number generator is subject to manip- ulation via block withholding attacks, in which the attacker withholds blocks until they \ufb01nd one that will produce a favorable random number. However, the attacker has only one chance to manipulate the random number for a particular challenge. Further- more, withholding a block to manipulate the random number will cost the attacker the block reward. If an attacker is able to mine 50 of the blocks, then 50 of the challenges can be manipulated. Nev- ertheless, the remaining 50 are still random, so the attacker will still fail some storage proofs. Speci\ufb01cally, they will fail half as many as they would without the withholding attack. To protect against such attacks, clients can spec- ify a high challenge frequency and large penalties for missing proofs. These precautions should be su\ufb03cient to deter any \ufb01nancially-motivated attacker that con- trols less than 50 of the networks hashing power.",
      "Regardless, clients are advised to plan around poten- tial Byzantine attacks, which may not be \ufb01nancially motivated. Closed Window Attacks Hosts can only complete a storage proof if their proof transaction makes it into the blockchain. Miners could maliciously exclude storage proofs from blocks, depriving themselves of transaction fees but forcing a penalty on hosts. Alternatively, miners could ex- tort hosts by requiring large fees to include storage proofs, knowing that they are more important than the average transaction. This is termed a closed win- dow attack, because the malicious miner has arti\ufb01- cially closed the window. The defense for this is to use a large window size. Hosts can reasonably assume that some percentage of miners will include their proofs in return for a trans- action fee. Because hosts consent to all \ufb01le contracts, they are free to reject any contract that they feel leaves them vulnerable to closed window attacks.",
      "Arbitrary Transaction Data Each transaction has an arbitrary data \ufb01eld which can be used for any type of information. Nodes will be required to store the arbitrary data if it is signed by any signature in the transaction. Nodes will initially accept up to 64 KB of arbitrary data per block. This arbitrary data provides hosts and clients with a decentralized way to organize themselves. It can be used to advertise available space or \ufb01les seeking a host, or to create a decentralized \ufb01le tracker. Arbitrary data could also be used to implement other types of soft forks. This would be done by cre- ating an anyone-can-spend output but with restric- tions speci\ufb01ed in the arbitrary data. Miners that un- derstand the restrictions can block any transaction that spends the output without satisfying the neces- sary stipulations. Naive nodes will stay synchronized without needing to be able to parse the arbitrary data.",
      "Storage Ecosystem Sia relies on an ecosystem that facilitates decentral- ized storage. Storage providers can use the arbitrary data \ufb01eld to announce themselves to the network. This can be done using standardized template that clients will be able to read. Clients can use these an- nouncements to create a database of potential hosts, and form contracts with only those they trust. Host Protections A contract requires consent from both the storage provider and their client, allowing the provider to re- ject unfavorable terms or unwanted (e.g. illegal) \ufb01les. The provider may also refuse to sign a contract until the entire \ufb01le has been uploaded to them. Contract terms give storage providers some \ufb02ex- ibility. They can advertise themselves as minimally reliable, o\ufb00ering a low price and a agreeing to min- imal penalties for losing \ufb01les; or they can advertise themselves as highly reliable, o\ufb00ering a higher price and agreeing to harsher penalties for losing \ufb01les.",
      "An e\ufb03cient market will optimize storage strategies. Hosts are vulnerable to denial of service attacks, which could prevent them from submitting storage proofs or transferring \ufb01les. It is the responsibility of the host to protect themselves from such attacks. Client Protections Clients can use erasure codes, such as regenerating codes 4, to safeguard against hosts going o\ufb04ine. These codes typically operate by splitting a \ufb01le into n pieces, such that the \ufb01le can be recovered from any subset of m unique pieces. (The values of n and m vary based on the speci\ufb01c erasure code and re- dundancy factor.) Each piece is then encrypted and stored across many hosts. This allows a client to at- tain high \ufb01le availability even if the average network reliability is low. As an extreme example, if only 10 out of 100 pieces are needed to recover the \ufb01le, then the client is actually relying on the 10 most reliable hosts, rather than the average reliability.",
      "Availabil- ity can be further improved by rehosting \ufb01le pieces whose hosts have gone o\ufb04ine. Other metrics bene\ufb01t from this strategy as well; the client can reduce la- tency by downloading from the closest 10 hosts, or increase download speed by downloading from the 10 fastest hosts. These downloads can be run in parallel to maximize available bandwidth. Uptime Incentives The storage proofs contain no mechanism to enforce constant uptime. There are also no provisions that require hosts to transfer \ufb01les to clients upon request. One might expect, then, to see hosts holding their clients \ufb01les hostage and demanding exorbitant fees to download them. However, this attack is mitigated through the use of erasure codes, as described in sec- tion 7.2. The strategy gives clients the freedom to ignore uncooperative hosts and work only with those that are cooperative. As a result, power shifts from the host to the client, and the download fee be- comes an upload incentive.",
      "In this scenario, clients o\ufb00er a reward for being sent a \ufb01le, and hosts must compete to provide the best quality of service. Clients may request a \ufb01le at any time, which incentivizes hosts to maximize uptime in order to collect as many rewards as possible. Clients can also incentivize greater throughput and lower la- tency via proportionally larger rewards. Clients could even perform random checkups that reward hosts simply for being online, even if they do not wish to download anything. However, we reiterate that up- time incentives are not part of the Sia protocol; they are entirely dependent on client behavior. Payment for downloads is expected to be o\ufb00ered through preexisting micropayment channels 11. Mi- cropayment channels allow clients to make many con- secutive small payments with minimal latency and blockchain bloat. Hosts could transfer a small seg- ment of the \ufb01le and wait to receive a micropayment before proceeding.",
      "The use of many consecutive pay- ments allows each party to minimize the risk of being cheated. Micropayments are small enough and fast enough that payments could be made every few sec- onds without having any major e\ufb00ect on throughput. Basic Reputation System Clients need a reliable method for picking quality hosts. Analyzing their history is insu\ufb03cient, because the history could be spoofed. A host could repeat- edly form contracts with itself, agreeing to store large fake \ufb01les, such as a \ufb01le containing only zeros. It would be trivial to perform storage proofs on such data without actually storing anything. To mitigate this Sybil attack, clients can require that hosts that announce themselves in the arbitrary data section also include a large volume of time locked coins. If 10 coins are time locked 14 days into the future, then the host can be said to have created a lock valued at 140 coin-days.",
      "By favoring hosts that have created high-value locks, clients can mitigate the risk of Sybil attacks, as valuable locks are not trivial to create. Each client can choose their own equation for pick- ing hosts, and can use a large number of factors, in- cluding price, lock value, volume of storage being of- fered, and the penalties hosts are willing to pay for losing \ufb01les. More complex systems, such as those that use human review or other metrics, could be imple- mented out-of-band in a more centralized setting. Siafunds Sia is a product of Nebulous Incorporated. Nebulous is a for-pro\ufb01t company, and Sia is intended to be- come a primary source of income for the company. Currency premining is not a stable source of income, as it requires creating a new currency and tethering the companys revenue to the currencys increasing value. When the company needs to spend money, it must trade away portions of its source of income.",
      "Ad- ditionally, premining means that one entity has con- trol over a large volume of the currency, and therefore potentially large and disruptive control over the mar- ket. Instead, Nebulous intends to generate revenue from Sia in a manner proportional to the value added by Sia, as determined by the value of the contracts set up between clients and hosts. This is accomplished by imposing a fee on all contracts. When a contract is created, 3.9 of the contract fund is removed and distributed to the holders of siafunds. Nebulous Inc. will initially hold approx. 88 of the siafunds, and the early crowd-fund backers of Sia will hold the rest. Siafunds can be sent to other addresses, in the same way that siacoins can be sent to other addresses. They cannot, however, be used to fund contracts or miner fees. When siafunds are transferred to a new address, an additional unspent output is created, containing all of the siacoins that have been earned by the sia- funds since their previous transfer.",
      "These siacoins are sent to the same address as the siafunds. Economics of Sia The primary currency of Sia is the siacoin. The supply of siacoins will increase permanently, and all fresh supply will be given to miners as a block subisdy. The \ufb01rst block will have 300,000 coins minted. This number will decrease by 1 coin per block, until a minimum of 30,000 coins per block is reached. Following a target of 10 minutes between blocks, the annual growth in supply is: Year Growth 90 39 21 11.5 4.4 3.2 2.3 There are ine\ufb03ciencies within the Sia incentive scheme. The primary goal of Sia is to provide a blockchain that enforces storage contracts. The min- ing reward, however, is only indirectly linked to the total value of contracts being created. The siacoin, especially initially, is likely to have high volatility. Hosts can be adversely a\ufb00ected if the value of the currency shifts mid-contract.",
      "As a re- sult, we expect to see hosts increasing the price of long-term contracts as a hedge against volatility. Ad- ditionally, hosts can advertise their prices in a more stable currency (like USD) and convert to siacoin im- mediately before \ufb01nalizing a contract. Eventually, the use of two-way pegs with other crypto-assets will give hosts additional means to insulate themselves from volatility. Conclusion Sia is a variant on the Bitcoin protocol that enables decentralized \ufb01le storage via cryptographic contracts. These contracts can be used to enforce storage agree- ments between clients and hosts. After agreeing to store a \ufb01le, a host must regularly submit storage proofs to the network. The host will automatically be compensated for storing the \ufb01le regardless of the behavior of the client. Importantly, contracts do not require hosts to transfer \ufb01les back to their client when requested. In- stead, an out-of-band ecosystem must be created to reward hosts for uploading.",
      "Clients and hosts must also \ufb01nd a way to coordinate; one mechanism would be the arbitrary data \ufb01eld in the blockchain. Vari- ous precautions have been enumerated which miti- gate Sybil attacks and the unreliability of hosts. Siafunds are used as a mechanism of generating revenue for Nebulous Inc., the company responsible for the release and maintenance of Sia. By using Sia- funds instead of premining, Nebulous more directly correlates revenue to actual use of the network, and is largely una\ufb00ected by market games that malicious entities may play with the network currency. Miners may also derive a part of their block subsidy from siafunds, with similar bene\ufb01ts. Long term, we hope to add support for two-way-pegs with various curren- cies, which would enable consumers to insulate them- selves from the instability of a single currency. We believe Sia will provide a fertile platform for decentralized cloud storage in trustless environments.",
      "1 Satoshi Nakamoto, Bitcoin: A Peer-to-Peer Electronic Cash System. 2 R.C. Merkle, Protocols for public key cryptosystems, In Proc. 1980 Symposium on Security and Privacy, IEEE Computer Society, pages 122-133, April 1980. 3 Hovav Shacham, Brent Waters, Compact Proofs of Retrievability, Proc. of Asiacrypt 2008, vol. 5350, Dec 2008, pp. 90-107. 4 K. V. Rashmi, Nihar B. Shah, and P. Vijay Kumar, Optimal Exact-Regenerating Codes for Distributed Storage at the MSR and MBR Points via a Product-Matrix Construction. 5 Adam Back, Matt Corallo, Luke Dashjr, Mark Friedenbach, Gregory Maxwell, Andrew Miller, Andrew Peolstra, Jorge Timon, Pieter Wuille, Enabling Blockchain Innovations with Pegged Sidechains. 6 Andrew Poelstra, A Treatise on Altcoins.",
      "7 Gavin Andresen, O(1) Block Propagation, https:gist.github.comgavinandresene20c3b5a1d4b97f79ac2 8 Gregory Maxwell, Deterministic Wallets, https:bitcointalk.orgindex.php?topic19137.0 9 etotheipi, Ultimate blockchain compression w trust-free lite nodes, 10 Gregory Maxwell, Proof of Storage to make distributed resource consumption costly. 11 Mike Hearn, Rapidly-adjusted (micro)payments to a pre-determined party, determined party 12 Bitcoin Developer Guide, https:bitcoin.orgendeveloper-guide"
    ],
    "word_count": 3876,
    "page_count": 8
  },
  "SOL": {
    "chunks": [
      "Solana: A new architecture for a high performance blockchain v0.8.13 Anatoly Yakovenko anatolysolana.io Legal Disclaimer Nothing in this White Paper is an o\ufb00er to sell, or the solicitation of an o\ufb00er to buy, any tokens. Solana is publishing this White Paper solely to receive feedback and comments from the public. If and when Solana o\ufb00ers for sale any tokens (or a Simple Agreement for Future Tokens), it will do so through de\ufb01nitive o\ufb00ering documents, including a disclosure document and risk factors. Those de\ufb01nitive documents also are expected to include an updated version of this White Paper, which may di\ufb00er signi\ufb01cantly from the current version. If and when Solana makes such an o\ufb00ering in the United States, the o\ufb00ering likely will be available solely to accredited investors. Nothing in this White Paper should be treated or read as a guarantee or promise of how Solanas business or the tokens will develop or of the utility or value of the tokens.",
      "This White Paper outlines current plans, which could change at its discretion, and the success of which will depend on many factors outside Solanas control, including market-based factors and factors within the data and cryptocurrency industries, among others. Any statements about future events are based solely on Solanas analysis of the issues described in this White Paper. That analysis may prove to be incorrect. Abstract This paper proposes a new blockchain architecture based on Proof of History (PoH) - a proof for verifying order and passage of time between events. PoH is used to encode trustless passage of time into a ledger - an append only data structure. When used alongside a consensus algorithm such as Proof of Work (PoW) or Proof of Stake (PoS), PoH can reduce messaging overhead in a Byzantine Fault Tol- erant replicated state machine, resulting inn sub-second \ufb01nality times.",
      "This paper also proposes two algorithms that leverage the time keep- ing properties of the PoH ledger - a PoS algorithm that can recover from partitions of any size and an e\ufb03cient streaming Proof of Replica- tion (PoRep). The combination of PoRep and PoH provides a defense against forgery of the ledger with respect to time (ordering) and stor- age. The protocol is analyzed on a 1 gbps network, and this paper shows that throughput up to 710k transactions per second is possible with todays hardware. Introduction Blockchain is an implementation of a fault tolerant replicated state machine. Current publicly available blockchains do not rely on time, or make a weak assumption about the participants abilities to keep time 4, 5. Each node in the network usually relies on their own local clock without knowledge of any other participants clocks in the network.",
      "The lack of a trusted source of time means that when a message timestamp is used to accept or reject a message, there is no guarantee that every other participant in the network will make the exact same choice. The PoH presented here is designed to create a ledger with veri\ufb01able passage of time, i.e. duration between events and message ordering. It is anticipated that every node in the network will be able to rely on the recorded passage of time in the ledger without trust. Outline The remainder of this article is organized as follows. Overall system design is described in Section 3. In depth description of Proof of History is described in Section 4. In depth description of the proposed Proof of Stake consensus algorithm is described in Section 5. In depth description of the proposed fast Proof of Replication is described in Section 6. System Architecture and performance limits are analyzed in Section 7.",
      "A high performance GPU friendly smart contracts engine is described in Section 7.5 Network Design As shown in Figure 1, at any given time a system node is designated as Leader to generate a Proof of History sequence, providing the network global read consistency and a veri\ufb01able passage of time. The Leader sequences user messages and orders them such that they can be e\ufb03ciently processed by other nodes in the system, maximizing throughput. It executes the transactions on the current state that is stored in RAM and publishes the transactions and a signature of the \ufb01nal state to the replications nodes called Veri\ufb01ers. Veri\ufb01ers execute the same transactions on their copies of the state, and pub- lish their computed signatures of the state as con\ufb01rmations. The published con\ufb01rmations serve as votes for the consensus algorithm. Figure 1: Transaction \ufb02ow throughout the network. In a non-partitioned state, at any given time, there is one Leader in the network.",
      "Each Veri\ufb01er node has the same hardware capabilities as a Leader and can be elected as a Leader, this is done through PoS based elections. Elections for the proposed PoS algorithm are covered in depth in Section 5.6. In terms of CAP theorem, Consistency is almost always picked over Avail- ability in an event of a Partition. In case of a large partition, this paper proposes a mechanism to recover control of the network from a partition of any size. This is covered in depth in Section 5.12. Proof of History Proof of History is a sequence of computation that can provide a way to cryptographically verify passage of time between two events. It uses a cryp- tographically secure function written so that output cannot be predicted from the input, and must be completely executed to generate the output. The function is run in a sequence on a single core, its previous output as the current input, periodically recording the current output, and how many times its been called.",
      "The output can then be re-computed and veri\ufb01ed by external computers in parallel by checking each sequence segment on a separate core. Data can be timestamped into this sequence by appending the data (or a hash of some data) into the state of the function. The recording of the state, index and data as it was appended into the sequences provides a timestamp that can guarantee that the data was created sometime before the next hash was generated in the sequence. This design also supports horizontal scaling as multiple generators can synchronize amongst each other by mixing their state into each others sequences. Horizontal scaling is discussed in depth in Section 4.4 Description The system is designed to work as follows. With a cryptographic hash func- tion, whose output cannot be predicted without running the function (e.g. sha256, ripemd, etc.), run the function from some random starting value and take its output and pass it as the input into the same function again.",
      "Record the number of times the function has been called and the output at each call. The starting random value chosen could be any string, like the headline of the New York times for the day. For example: PoH Sequence Index Operation Output Hash sha256(any random starting value) hash1 sha256(hash1) hash2 sha256(hash2) hash3 Where hashN represents the actual hash output. It is only necessary to publish a subset of the hashes and indices at an interval. For example: PoH Sequence Index Operation Output Hash sha256(any random starting value) hash1 sha256(hash199) hash200 sha256(hash299) hash300 As long as the hash function chosen is collision resistant, this set of hashes can only be computed in sequence by a single computer thread. This follows from the fact that there is no way to predict what the hash value at index 300 is going to be without actually running the algorithm from the starting value 300 times.",
      "Thus we can thus infer from the data structure that real time has passed between index 0 and index 300. In the example in Figure 2, hash 62f51643c1 was produced on count 510144806912 and hash c43d862d88 was produced on count 510146904064. Following the previously discussed properties of the PoH algorithm, we can trust that real time passed between count 510144806912 and count 510146904064. Figure 2: Proof of History sequence Timestamp for Events This sequence of hashes can also be used to record that some piece of data was created before a particular hash index was generated. Using a combine function to combine the piece of data with the current hash at the current index. The data can simply be a cryptographically unique hash of arbitrary event data. The combine function can be a simple append of data, or any operation that is collision resistant.",
      "The next generated hash represents a timestamp of the data, because it could have only been generated after that speci\ufb01c piece of data was inserted. For example: PoH Sequence Index Operation Output Hash sha256(any random starting value) hash1 sha256(hash199) hash200 sha256(hash299) hash300 Some external event occurs, like a photograph was taken, or any arbitrary digital data was created: PoH Sequence With Data Index Operation Output Hash sha256(any random starting value) hash1 sha256(hash199) hash200 sha256(hash299) hash300 sha256(append(hash335, photograph sha256)) hash336 Hash336 is computed from the appended binary data of hash335 and the sha256 of the photograph. The index and the sha256 of the photograph are recorded as part of the sequence output. So anyone verifying this sequence can then recreate this change to the sequence.",
      "The verifying can still be done in parallel and its discussed in Section 4.3 Because the initial process is still sequential, we can then tell that things entered into the sequence must have occurred sometime before the future hashed value was computed. POH Sequence Index Operation Output Hash sha256(any random starting value) hash1 sha256(hash199) hash200 sha256(hash299) hash300 sha256(append(hash335, photograph1 sha256)) hash336 sha256(hash399) hash400 sha256(hash499) hash500 sha256(append(hash599, photograph2 sha256)) hash600 sha256(hash699) hash700 Table 1: PoH Sequence With 2 Events In the sequence represented by Table 1, photograph2 was created before hash600, and photograph1 was created before hash336. Inserting the data into the sequence of hashes results in a change to all subsequent values in the sequence.",
      "As long as the hash function used is collision resistant, and the data was appended, it should be computationally impossible to pre-compute any future sequences based on prior knowledge of what data will be inte- grated into the sequence. The data that is mixed into the sequence can be the raw data itself, or just a hash of the data with accompanying metadata. In the example in Figure 3, input cfd40df8... was inserted into the Proof of History sequence. The count at which it was inserted is 510145855488 and the state at which it was inserted it is 3d039eef3. All the future gen- erated hashes are modi\ufb01ed by this change to the sequence, this change is indicated by the color change in the \ufb01gure. Every node observing this sequence can determine the order at which all events have been inserted and estimate the real time between the insertions.",
      "Figure 3: Inserting data into Proof of History Veri\ufb01cation The sequence can be veri\ufb01ed correct by a multicore computer in signi\ufb01cantly less time than it took to generate it. For example: Core 1 Index Data Output Hash sha256(hash199) hash200 sha256(hash299) hash300 Core 2 Index Data Output Hash sha256(hash299) hash300 sha256(hash399) hash400 Given some number of cores, like a modern GPU with 4000 cores, the veri\ufb01er can split up the sequence of hashes and their indexes into 4000 slices, and in parallel make sure that each slice is correct from the starting hash to the last hash in the slice.",
      "If the expected time to produce the sequence is going to be: Figure 4: Veri\ufb01cation using multiple cores Total number of hashes Hashes per second for 1 core The expected time to verify that the sequence is correct is going to be: Total number of hashes (Hashes per second per core  Number of cores available to verify) In the example in Figure 4, each core is able to verify each slice of the sequence in parallel. Since all input strings are recorded into the output, with the counter and state that they are appended to, the veri\ufb01ers can replicate each slice in parallel. The red colored hashes indicate that the sequence was modi\ufb01ed by a data insertion. Horizontal Scaling Its possible to synchronize multiple Proof of History generators by mixing the sequence state from each generator to each other generator, and thus achieve horizontal scaling of the Proof of History generator. This scaling is done without sharding.",
      "The output of both generators is necessary to reconstruct the full order of events in the system. PoH Generator A Index Hash Data hash1a hash2a hash1b hash3a hash4a PoH Generator B Index Hash Data hash1b hash2b hash1a hash3b hash4b Given generators A and B, A receives a data packet from B (hash1b), which contains the last state from Generator B, and the last state generator B observed from Generator A. The next state hash in Generator A then de- pends on the state from Generator B, so we can derive that hash1b happened sometime before hash3a. This property can be transitive, so if three gener- ators are synchronized through a single common generator A B C, we can trace the dependency between A and C even though they were not synchronized directly.",
      "By periodically synchronizing the generators, each generator can then handle a portion of external tra\ufb03c, thus the overall system can handle a larger amount of events to track at the cost of true time accuracy due to network latencies between the generators. A global order can still be achieved by picking some deterministic function to order any events that are within the synchronization window, such as by the value of the hash itself. In Figure 5, the two generators insert each others output state and record the operation. The color change indicates that data from the peer had mod- i\ufb01ed the sequence. The generated hashes that are mixed into each stream are highlighted in bold. The synchronization is transitive. A B C There is a provable order of events between A and C through B. Scaling in this way comes at the cost of availability. 10  1 gbps connec- tions with availability of 0.999 would have 0.99910  0.99 availability.",
      "Consistency Users are expected to be able to enforce consistency of the generated se- quence and make it resistant to attacks by inserting the last observed output of the sequence they consider valid into their input. Figure 5: Two generators synchronizing PoH Sequence A Index Data Output Hash hash10a Event1 hash20a Event2 hash30a Event3 hash40a PoH Hidden Sequence B Index Data Output Hash hash10b Event3 hash20b Event2 hash30b Event1 hash40b A malicious PoH generator could produce a second hidden sequence with the events in reverse order, if it has access to all the events at once, or is able to generate a faster hidden sequence. To prevent this attack, each client-generated Event should contain within itself the latest hash that the client observed from what it considers to be a valid sequence. So when a client creates the Event1 data, they should append the last hash they have observed.",
      "PoH Sequence A Index Data Output Hash hash10a Event1  append(event1 data, hash10a) hash20a Event2  append(event2 data, hash20a) hash30a Event3  append(event3 data, hash30a) hash40a When the sequence is published, Event3 would be referencing hash30a, and if its not in the sequence prior to this Event, the consumers of the sequence know that its an invalid sequence. The partial reordering attack would then be limited to the number of hashes produced while the client has observed an event and when the event was entered. Clients should then be able to write software that does not assume the order is correct for the short period of hashes between the last observed and inserted hash. To prevent a malicious PoH generator from rewriting the client Event hashes, the clients can submit a signature of the event data and the last observed hash instead of just the data.",
      "PoH Sequence A Index Data Output Hash hash10a Event1  sign(append(event1 data, hash10a), Client Private Key) hash20a Event2  sign(append(event2 data, hash20a), Client Private Key) hash30a Event3  sign(append(event3 data, hash30a), Client Private Key) hash40a Veri\ufb01cation of this data requires a signature veri\ufb01cation, and a lookup of the hash in the sequence of hashes prior to this one. Verify: (Signature, PublicKey, hash30a, event3 data)  Event3 Verify(Signature, PublicKey, Event3) Lookup(hash30a, PoHSequence) In Figure 6, the user-supplied input is dependent on hash 0xdeadbeef... existing in the generated sequence sometime before its inserted. The blue Figure 6: Input with a back reference. top left arrow indicates that the client is referencing a previously produced hash. The clients message is only valid in a sequence that contains the hash 0xdeadbeef.... The red color in the sequence indicates that the sequence has been modi\ufb01ed by the clients data.",
      "Overhead 4000 hashes per second would generate an additional 160 kilobytes of data, and would require access to a GPU with 4000 cores and roughly 0.25-0.75 milliseconds of time to verify. Attacks 4.7.1 Reversal Generating a reverse order would require an attacker to start the malicious sequence after the second event. This delay should allow any non malicious peer to peer nodes to communicate about the original order. 4.7.2 Speed Having multiple generators may make deployment more resistant to attacks. One generator could be high bandwidth, and receive many events to mix into its sequence, another generator could be high speed low bandwidth that periodically mixes with the high bandwidth generator. The high speed sequence would create a secondary sequence of data that an attacker would have to reverse. 4.7.3 Long Range Attacks Long range attacks involve acquiring old discarded client Private Keys, and generating a falsi\ufb01ed ledger 10.",
      "Proof of History provides some protection against long range attacks. A malicious user that gains access to old private keys would have to recreate a historical record that takes as much time as the original one they are trying to forge. This would require access to a faster processor than the network is currently using, otherwise the attacker would never catch up in history length. Additionally, a single source of time allows for construction of a simpler Proof of Replication (more on that in Section 6). Since the network is de- signed so that all participants in the network will rely on a single historical record of events. PoRep and PoH together should provide a defense of both space and time against a forged ledger.",
      "Proof of Stake Consensus Description This speci\ufb01c instance of Proof of Stake is designed for quick con\ufb01rmation of the current sequence produced by the Proof of History generator, for voting and selecting the next Proof of History generator, and for punishing any mis- behaving validators. This algorithm depends on messages eventually arriving to all participating nodes within a certain timeout. Terminology bonds Bonds are equivalent to a capital expense in Proof of Work. A miner buys hardware and electricity, and commits it to a single branch in a Proof of Work blockchain. A bond is coin that the validator commits as collateral while they are validating transactions. slashing The proposed solution to the nothing at stake problem in Proof of Stake systems 7. When a proof of voting for a di\ufb00erent branch is published, that branch can destroy the validators bond. This is an economic incentive designed to discourage validators from con\ufb01rming multiple branches.",
      "super majority A super majority is 2 3rds of the validators weighted by their bonds. A super majority vote indicates that the network has reached consensus, and at least 3rd of the network would have had to vote maliciously for this branch to be invalid. This would put the economic cost of an attack at 1 3rd of the market cap of the coin. Bonding A bonding transaction takes a amount of coin and moves it to a bonding account under the users identity. Coins in the bonding account cannot be spent and have to remain in the account until the user removes them. The user can only remove stale coins that have timed out. Bonds are valid after super majority of the current stakeholders have con\ufb01rmed the sequence. Voting It is anticipated that the Proof of History generator will be able to publish a signature of the state at a prede\ufb01ned period. Each bonded identity must con\ufb01rm that signature by publishing their own signed signature of the state. The vote is a simple yes vote, without a no.",
      "If super majority of the bonded identities have voted within a timeout, then this branch would be accepted as valid. Unbonding Missing N number of votes marks the coins as stale and no longer eligible for voting. The user can issue an unbonding transaction to remove them. N is a dynamic value based on the ratio of stale to active votes. increases as the number of stale votes increases. In an event of a large network partition, this allows the larger branch to recover faster then the smaller branch. Elections Election for a new PoH generator occur when the PoH generator failure is detected. The validator with the largest voting power, or highest public key address if there is a tie is picked as the new PoH generator. A super majority of con\ufb01rmations are required on the new sequence. If the new leader fails before a super majority con\ufb01rmations are available, the next highest validator is selected, and a new set of con\ufb01rmations is required.",
      "To switch votes, a validator needs to vote at a higher PoH sequence counter, and the new vote needs to contain the votes it wants to switch. Otherwise the second vote will be slashable. Vote switching is expected to be designed so that it can only occur at a height that does not have a super majority. Once a PoH generator is established, a Secondary can be elected to take over the transactional processing duties. If a Secondary exists, it will be considered as the next leader during a Primary failure. The platform is designed so that the Secondary becomes Primary and lower rank generators are promoted if an exception is detected or on a pre- de\ufb01ned schedule. Election Triggers 5.7.1 Forked Proof of History generator PoH generators are designed with an identity that signs the generated se- quence. A fork can only occur in case the PoH generators identity has been compromised. A fork is detected because two di\ufb00erent historical records have been published on the same PoH identity.",
      "5.7.2 Runtime Exceptions A hardware failure or a bug, or a intentional error in the PoH generator could cause it to generate an invalid state and publish a signature of the state that does not match the local validators result. Validators will publish the correct signature via gossip and this event would trigger a new round of elections. Any validators who accept an invalid state will have their bonds slashed. 5.7.3 Network Timeouts A network timeout would trigger an election. Slashing Slashing occurs when a validator votes two separate sequences. A proof of malicious vote will remove the bonded coins from circulation and add them to the mining pool. A vote that includes a previous vote on a contending sequence is not eligible as proof of malicious voting. Instead of slashing the bonds, this vote removes remove the currently cast vote on the contending sequence. Slashing also occurs if a vote is cast for an invalid hash generated by the PoH generator.",
      "The generator is expected to randomly generate an invalid state, which would trigger a fallback to Secondary. Secondary Elections Secondary and lower ranked Proof of History generators can be proposed and approved. A proposal is cast on the primary generators sequence. The proposal contains a timeout, if the motion is approved by a super majority of the vote before the timeout, the Secondary is considered elected, and will take over duties as scheduled. Primary can do a soft handover to Secondary by inserting a message into the generated sequence indicating that a handover will occur, or inserting an invalid state and forcing the network to fallback to Secondary. If a Secondary is elected, and the primary fails, the Secondary will be considered as the \ufb01rst fallback during an election. 5.10 Availability CAP systems that deal with partitions have to pick Consistency or Avail- ability.",
      "Our approach eventually picks Availability, but because we have an objective measure of time, Consistency is picked with reasonable human timeouts. Proof of Stake veri\ufb01ers lock up some amount of coin in a stake, which allows them to vote for a particular set of transactions. Locking up coin is a transaction that is entered into a PoH stream, just like any other transaction. To vote, a PoS veri\ufb01er has to sign the hash of the state, as it was computed after processing all the transactions to a speci\ufb01c position in the PoH ledger. This vote is also entered as a transaction into the PoH stream. Looking at the PoH ledger, we can then infer how much time passed between each vote, and if a partition occurs, for how long each veri\ufb01er has been unavailable. To deal with partitions with reasonable human timeframes, we propose a dynamic approach to unstake unavailable veri\ufb01ers. When the number of veri\ufb01ers is high and above 2 3, the unstaking process can be fast.",
      "The number of hashes that must be generated into the ledger is low before the unavailable veri\ufb01ers stake is fully unstaked and they are no longer counted for consensus. When the number of veri\ufb01ers is below 2 3rds but above 1 2, the unstaking timer is slower, requiring a larger number of hashes to be generated before the missing veri\ufb01ers are unstaked. In a large partition, like a partition that is missing 1 or more of the veri\ufb01ers, the unstaking process is very very slow. Transactions can still be entered into the stream, and veri\ufb01ers can still vote, but full 2 3rds consensus will not be achieved until a very large amount of hashes have been generated and the unavailable veri\ufb01ers have been unstaked. The di\ufb00erence in time for a network to regain liveness allows us as customers of the network human timeframes to pick a partition that we want to continue using. 5.11 Recovery In the system we propose, the ledger can be fully recovered from any failure.",
      "That means, anyone in the world can pick any random spot in the ledger and create a valid fork by appending newly generated hashes and transactions. If all the veri\ufb01ers are missing from this fork, it would take a very very long time for any additional bonds to become valid and for this branch to achieve 3rds super majority consensus. So full recovery with zero available validators would require a very large amount of hashes to be appended to the ledger, and only after all the unavailable validators have been unstaked will any new bonds be able to validate the ledger. 5.12 Finality PoH allows veri\ufb01ers of the network to observe what happened in the past with some degree of certainty of the time of those events. As the PoH generator is producing a stream of messages, all the veri\ufb01ers are required to submit their signatures of the state within 500ms. This number can be reduced further depending on network conditions.",
      "Since each veri\ufb01cation is entered into the stream, everyone in the network can validate that every veri\ufb01er submitted their votes within the required timeout without actually observing the voting directly. 5.13 Attacks 5.13.1 Tragedy of Commons The PoS veri\ufb01ers simply con\ufb01rm the state hash generated by the PoH gen- erator. There is an economic incentive for them to do no work and simply approve every generated state hash. To avoid this condition, the PoH gener- ator should inject an invalid hash at a random interval. Any voters for this hash should be slashed. When the hash is generated, the network should immediately promote the Secondary elected PoH generator. Each veri\ufb01er is required to respond within a small timeout - 500ms for example. The timeout should be set low enough that a malicious veri\ufb01er has a low probability of observing another veri\ufb01ers vote and getting their votes into the stream fast enough.",
      "5.13.2 Collusion with the PoH generator A veri\ufb01er that is colluding with the PoH generator would know in advance when the invalid hash is going to be produced and not vote for it. This scenario is really no di\ufb00erent than the PoH identity having a larger veri\ufb01er stake. The PoH generator still has to do all the work to produce the state hash. 5.13.3 Censorship Censorship or denial of service could occur when a 1 3rd of the bond holders refuse to validate any sequences with new bonds. The protocol can defend against this form of attack by dynamically adjusting how fast bonds become stale. In the event of a denial of service, the larger partition will be designed to fork and censor the Byzantine bond holders. The larger network will re- cover as the Byzantine bonds become stale with time. The smaller Byzantine partition would not be able to move forward for a longer period of time. The algorithm would work as follows. A majority of the network would elect a new Leader.",
      "The Leader would then censor the Byzantine bond holders from participating. Proof of History generator would have to continue generating a sequence, to prove the passage of time, until enough Byzantine bonds have become stale so the bigger network has a super majority. The rate at which bonds become stale would be dynamically based on what percentage of bonds are active. So the Byzantine minority fork of the network would have to wait much longer than the majority fork to recover a super majority. Once a super majority has been established, slashing could be used to permanently punish the Byzantine bond holders. 5.13.4 Long Range Attacks PoH provides a natural defense against long range attacks. Recovering the ledger from any point in the past would require the attacker to overtake the valid ledger in time by outpacing the speed of the PoH generator.",
      "The consensus protocol provides a second layer of defense, as any attack would have to take longer then the time it takes to unstake all the valid validators. It also creates an availability gap in the history of the ledger. When comparing two ledgers of the same height, the one with the smallest maximum partition can be objectively considered as valid. 5.13.5 ASIC Attacks Two opportunities for an ASIC attacks exist in this protocol - during parti- tion, and cheating timeouts in Finality. For ASIC attacks during Partitions, the Rate at which bonds are unstaked is non-linear, and for networks with large partitions the rate is orders of magnitude slower then expected gains from an ASIC attack. For ASIC attacks during Finality, the vulnerability allows for byzantine validators who have a bonded stake wait for con\ufb01rmations from other nodes and inject their votes with a collaborating PoH generator.",
      "The PoH gener- ator can then use its faster ASIC to generate 500ms worth of hashes in less time, and allow for network communication between PoH generator and the collaborating nodes. But, if the PoH generator is also byzantine, there is no reason why the byzantine generator wouldnt have communicated the exact counter when they expect to insert the failure. This scenario is no di\ufb00erent than a PoH generator and all the collaborators sharing the same identity and having a single combined stake and only using 1 set of hardware. Streaming Proof of Replication Description Filecoin proposed a version of Proof of Replication 6. The goal of this version is to have fast and streaming veri\ufb01cations of Proof of Replication, which are enabled by keeping track of time in Proof of History generated sequence. Replication is not used as a consensus algorithm, but is a useful tool to account for the cost of storing the blockchain history or state at a high availability.",
      "Algorithm As shown in Figure 7 CBC encryption encrypts each block of data in se- quence, using the previously encrypted block to XOR the input data. Each replication identity generates a key by signing a hash that has been generated Proof of History sequence. This ties the key to a replicators iden- tity, and to a speci\ufb01c Proof of History sequence. Only speci\ufb01c hashes can be selected. (See Section 6.5 on Hash Selection) The data set is fully encrypted block by block. Then to generate a proof, the key is used to seed a pseudorandom number generator that selects a random 32 bytes from each block. A merkle hash is computed with the selected PoH hash prepended to the each slice. The root is published, along with the key, and the selected hash that was generated.",
      "The replication node is required to publish another proof Figure 7: Sequential CBC encryption Figure 8: Fast Proof of Replication in N hashes as they are generated by Proof of History generator, where N is approximately 2 the time it takes to encrypt the data. The Proof of History generator will publish speci\ufb01c hashes for Proof of Replication at a prede\ufb01ned periods. The replicator node must select the next published hash for generating the proof. Again, the hash is signed, and random slices are selected from the blocks to create the merkle root. After a period of N proofs, the data is re-encrypted with a new CBC key. Veri\ufb01cation With N cores, each core can stream encryption for each identity. Total space required is 2blocks Ncores, since the previous encrypted block is necessary to generate the next one. Each core can then be used to generate all the proofs that derived from the current encrypted block. Total time to verify proofs is expected to be equal to the time it takes to encrypt.",
      "The proofs themselves consume few random bytes from the block, so the amount of data to hash is signi\ufb01cantly lower then the encrypted block size. The number of replication identities that can be veri\ufb01ed at the same time is equal to the number of available cores. Modern GPUs have 3500 cores available to them, albeit at 1 2- 1 3 the clock speed of a CPU. Key Rotation Without key rotation the same encrypted replication can generate cheap proofs for multiple Proof of History sequences. Keys are rotated periodically and each replication is re-encrypted with a new key that is tied to a unique Proof of History sequence. Rotation needs to be slow enough that its practical to verify replication proofs on GPU hardware, which is slower per core than CPUs. Hash Selection Proof of History generator publishes a hash to be used by the entire network for encrypting Proofs of Replication, and for using as the pseudorandom number generator for byte selection in fast proofs.",
      "Hash is published at a periodic counter that is roughly equal to 1 2 the time it takes to encrypt the data set. Each replication identity must use the same hash, and use the signed result of the hash as the seed for byte selection, or the encryption key. The period that each replicator must provide a proof must be smaller than the encryption time. Otherwise the replicator can stream the encryption and delete it for each proof. A malicious generator could inject data into the sequence prior to this hash to generate a speci\ufb01c hash. This attack is discussed more in 5.13.2. Proof Validation The Proof of History node is not expected to validate the submitted Proof of Replication proofs. It is expected to keep track of number of pending and veri\ufb01ed proofs submitted by the replicators identity. A proof is expected to be veri\ufb01ed when the replicator is able to sign the proof by a super majority of the validators in the network.",
      "The veri\ufb01cations are collected by the replicator via p2p gossip network, and submitted as one packet that contains a super majority of the validators in the network. This packet veri\ufb01es all the proofs prior to a speci\ufb01c hash gen- erated by the Proof of History sequence, and can contain multiple replicator identities at once. Attacks 6.7.1 Spam A malicious user could create many replicator identities and spam the net- work with bad proofs. To facilitate faster veri\ufb01cation, nodes are required to provide the encrypted data and the entire merkle tree to the rest of the network when they request veri\ufb01cation. The Proof of Replication that is designed in this paper allows for cheap veri\ufb01cation of any additional proofs, as they take no additional space. But each identity would consume 1 core of encryption time. The replication target should be set to a maximum size of readily available cores. Modern GPUs ship with 3500 cores.",
      "6.7.2 Partial Erasure A replicator node could attempt to partially erase some of the data to avoid storing the entire state. The number of proofs and the randomness of the seed should make this attack di\ufb03cult. For example, a user storing 1 terabyte of data erases a single byte from each 1 megabyte block. A single proof that samples 1 byte out of every megabyte would have a likelihood of collision with any erased byte 1 (1  11, 000, 0000)1,000,000  0.63. After 5 proofs the likelihood is 0.99. 6.7.3 Collusion with PoH generator The signed hash is expected to be used to seed the sample. If a replicator could select a speci\ufb01c hash in advance then the replicator could erase all bytes that are not going to be sampled. A replicator identity that is colluding with the Proof of History generator could inject a speci\ufb01c transaction at the end of the sequence before the pre- de\ufb01ned hash for random byte selection is generated.",
      "With enough cores, an attacker could generate a hash that is preferable to the replicators identity. This attack could only bene\ufb01t a single replicator identity. Since all the identities have to use the same exact hash that is cryptographically signed with ECDSA (or equivalent), the resulting signature is unique for each repli- cator identity, and collision resistant. A single replicator identity would only have marginal gains. 6.7.4 Denial of Service The cost of adding an additional replicator identity is expected to be equal to the cost of storage. The cost of adding extra computational capacity to verify all the replicator identities is expected to be equal to the cost of a CPU or GPU core per replication identity. This creates an opportunity for a denial of service attack on the network by creating a large number of valid replicator identities.",
      "To limit this attack, the consensus protocol chosen for the network can select a replication target, and award the replication proofs that meet the de- sired characteristics, like availability on the network, bandwidth, geolocation etc... 6.7.5 Tragedy of Commons The PoS veri\ufb01ers could simply con\ufb01rm PoRep without doing any work. The economic incentives should be lined up with the PoS veri\ufb01ers to do work, Figure 9: System Architecture like by splitting the mining payout between the PoS veri\ufb01ers and the PoRep replication nodes. To further avoid this scenario, the PoRep veri\ufb01ers can submit false proofs a small percentage of the time. They can prove the proof is false by providing the function that generated the false data. Any PoS veri\ufb01er that con\ufb01rmed a false proof would be slashed. System Architecture Components 7.1.1 Leader, Proof of History generator The Leader is an elected Proof of History generator.",
      "It consumes arbitrary user transactions and outputs a Proof of History sequence of all the transac- tions that guarantees a unique global order in the system. After each batch of transactions the Leader outputs a signature of the state that is the result of running the transactions in that order. This signature is signed with the identity of the Leader. 7.1.2 State A naive hash table indexed by the users address. Each cell contains the full users address and the memory required for this computation. For example Transaction table contains: Ripemd of Users Public Key Account unused For a total of 32 bytes. Proof of Stake bonds table contains: Ripemd of Users Public Key Bond Last Vote unused For a total of 64 bytes. 7.1.3 Veri\ufb01er, State Replication The Veri\ufb01er nodes replicate the blockchain state and provide high availability of the blockchain state.",
      "The replication target is selected by the consensus algorithm, and the validators in the consensus algorithm select and vote the Proof of Replication nodes they approve of based on o\ufb00-chain de\ufb01ned criteria. The network could be con\ufb01gured with a minimum Proof of Stake bond size, and a requirement for a single replicator identity per bond. 7.1.4 Validators These nodes are consuming bandwidth from Veri\ufb01ers. They are virtual nodes, and can run on the same machines as the Veri\ufb01ers or the Leader, or on separate machines that are speci\ufb01c to the consensus algorithm con\ufb01gured for this network. Network Limits Leader is expected to be able to take incoming user packets, orders them the most e\ufb03cient way possible, and sequences them into a Proof of History sequence that is published to downstream Veri\ufb01ers. E\ufb03ciency is based on Figure 10: Generator network limits memory access patterns of the transactions, so the transactions are ordered to minimize faults and to maximize prefetching.",
      "Incoming packet format: Last Valid Hash Counter From Signed Signature 1 of 2 Signature 2 of 2 Size 20  8  16  8  32  3232  148 bytes. The minimal payload that can be supported would be 1 destination ac- count. With payload: Last Valid Hash Counter Amount Counter From Signed Signature 1 of 2 Signature 2 of 2 With payload the minimum size: 176 bytes The Proof of History sequence packet contains the current hash, counter, and the hash of all the new messages added to the PoH sequence and the state signature after processing all the messages. This packet is sent once every N messages are broadcast. Proof of History packet: Current Hash Counter Messages Hash State Hash Signed Signature 1 of 2 Signature 2 of 2 Minimum size of the output packet is: 132 bytes On a 1gbps network connection the maximum number of transactions possible is 1 gigabit per second  176 bytes  710k tps max. Some loss 14 is expected due to Ethernet framing.",
      "The spare capacity over the target amount for the network can be used to increase availability by coding the output with Reed-Solomon codes and striping it to the available downstream Veri\ufb01ers. Computational Limits Each transaction requires a digest veri\ufb01cation. This operation does not use any memory outside of the transaction message itself and can be parallelized independently. Thus throughput is expected to be limited by the number of cores available on the system. GPU based ECDSA veri\ufb01cation servers have had experimental results of 900k operations per second 9. Memory Limits A naive implementation of the state as a 50 full hashtable with 32 byte en- tries for each account, would theoretically \ufb01t 10 billion accounts into 640GB. Steady state random access to this table is measured at 1.1 107 writes or reads per second. Based on 2 reads and two writes per transaction, memory throughput can handle 2.75m transactions per second.",
      "This was measured on an Amazon Web Services 1TB x1.16xlarge instance. High Performance Smart Contracts Smart contracts are a generalized form of transactions. These are programs that run on each node and modify the state. This design leverages extended Berkeley Packet Filter bytecode as fast and easy to analyze and JIT bytecode as the smart contracts language. One of its main advantages is a zero cost Foreign Function Interface. Intrinsics, or functions that are implemented on the platform directly, are callable by programs. Calling the intrinsics suspends that program and schedules the intrinsic on a high performance server. Intrinsics are batched together to execute in parallel on the GPU. In the above example, two di\ufb00erent user programs call the same intrinsic. Each program is suspended until the batch execution of the intrinsics is Figure 11: Executing BPF programs. complete. An example intrinsic is ECDSA veri\ufb01cation.",
      "Batching these calls to execute on the GPU can increase throughput by thousands of times. This trampoline requires no native operating system thread context switches, since the BPF bytecode has a well de\ufb01ned context for all the memory that it is using. eBPF backend has been included in LLVM since 2015, so any LLVM frontend language can be used to write smart contracts. Its been in the Linux kernel since 2015, and the \ufb01rst iterations of the bytecode have been around since 1992. A single pass can check eBPF for correctness, ascertain its runtime and memory requirements and convert it to x86 instructions.",
      "1 Liskov, Practical use of Clocks 2 Google Spanner TrueTime consistency 3 Solving Agreement with Ordering Oracles 4 Tendermint: Consensus without Mining 5 Hedera: A Governing Council  Public Hashgraph Network 6 Filecoin, proof of replication, 7 Slasher, A punative Proof of Stake algorithm 8 BitShares Delegated Proof of Stake 9 An E\ufb03cient Elliptic Curve Cryptography Signature Server With GPU Acceleration 10 Casper the Friendly Finality Gadget"
    ],
    "word_count": 7454,
    "page_count": 32
  },
  "STORJ": {
    "chunks": [
      "Storj: A Decentralized Cloud Storage Network Framework Storj Labs, Inc. October 30, 2018 v3.0 March 7, 2024 v3.1 This work is licensed under a Creative Commons Attribution-ShareAlike 3.0 license (CC BY-SA 3.0). All product names, logos, and brands used or cited in this document are property of their respective own- ers. All company, product, and service names used herein are for identification purposes only. Use of these names, logos, and brands does not imply endorsement. Contents Abstract Contributors Changelog Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 Storj design constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 Security and privacy Decentralization Marketplace and economics Amazon S3 compatibility Durability, device failure, and churn Latency Bandwidth Object size Byzantine fault tolerance 2.10 Coordination avoidance Framework . . . . . . . . . . . . . . . . . . . . .",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 Framework overview Storage nodes Peer-to-peer communication Redundancy Metadata Encryption Audits and reputation Data repair Payments Concrete implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 Definitions Peer classes Storage node Node identity Peer-to-peer communication Node discovery Redundancy Structured file storage Metadata 4.10 Satellite 4.11 Encryption 4.12 Authorization 4.13 Audits 4.14 Data repair 4.15 Storage node reputation 4.16 Payments 4.17 Bandwidth allocation 4.18 Satellite reputation 4.19 Garbage collection 4.20 Uplink Walkthroughs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 Upload Download Delete Move List Audit Data repair Payment Future work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "63 Hot files and content delivery Improving user experience around metadata Selected calculations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 Object repair costs Audit false positive risk Choosing erasure parameters Distributed consensus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 Attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 Primary user benefits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79 CONTENTS Abstract Decentralized cloud storage represents a fundamental shift in the efficiency and eco- nomics of large-scale storage. Eliminating central control allows users to store and share data without reliance on a third-party storage provider. Decentralization mitigates the risk of data failures and outages while simultaneously increasing the security and pri- vacy of object storage.",
      "It also allows market forces to optimize for less expensive storage at a greater rate than any single provider could afford. Although there are many ways to build such a system, there are some specific responsibilities any given implementation should address. Based on our experience with petabyte-scale storage systems, we in- troduce a modular framework for considering these responsibilities and for building our distributed storage network. Additionally, we describe an initial concrete implementa- tion for the entire framework. Contributors This paper represents the combined efforts of many individuals. Contributors affiliated with Storj Labs, Inc.",
      "include but are not limited to: Tim Adams, Kishore Aligeti, Cameron Ayer, Atikh Bana, Alexander Bender, Stefan Benten, Maximillian von Briesen, Paul Can- non, Gina Cooley, Dennis Coyle, Egon Elbre, Nadine Farah, Patrick Gerbes, John Gleeson, Ben Golub, James Hagans, Jens Heimb\u00fcrge, Faris Huskovic, Philip Hutchins, Brandon Iglesias, Viktor Ihnatiuk, Jennifer Johnson, Kevin Leffew, Alexander Leitner, Richard Lit- tauer, Dylan Lott, JT Olio, Kaloyan Raev, Garrett Ransom, Matthew Robinson, Jon Sander- son, Benjamin Sirb, Dan Sorensen, Helene Unland, Natalie Villasana, Bryan White, and Shawn Wilkinson. Wed also like to thank the other authors and contributors of the previous Storj and Metadisk white papers: Tome Boshevski, Josh Brandoff, Vitalik Buterin, Braydon Fuller, Gordy Hall, Jim Lowry, Chris Pollard, and James Prestwich.",
      "Wed like to especially thank Petar Maymounkov, Anand Babu Periasamy, Tim Kosse, Roberto Galoppini, Steven Willoughby, and Aaron Boodman for their helpful review of and contributions to an early draft of this paper. We would like to acknowledge the efforts, white papers, and communications of oth- ers in the distributed computing, blockchain, distributed storage, and decentralized stor- age space, whose work has informed our efforts. A more comprehensive list of sources is in the bibliography, but we would like to provide particular acknowledgement for the guidance and inspiration provided by the teams that designed and built Allmydata, Ceph, CoralCDN, Ethereum, Farsite, Filecoin, Freenet, Gluster, GFS, Hadoop, IPFS, Kademlia, Lustre, Maidsafe, Minio, MojoNation, OceanStore, Scality, Siacoin, and Tahoe-LAFS. Finally, we extend a huge thank you to everyone we talked to during the design and architecture of this system for their valuable thoughts, feedback, input, and suggestions.",
      "CONTENTS Please address correspondence to paperstorj.io. Changelog This section describes updates from the past editions of this white paper. Beyond a few trivial wording tweaks, we changed the following aspects in version 3.1:  Clarified encryption blocks in section 4.1.2. Replaced Kademlia for storage node discovery with a direct node-to-satellite indi- cation of network participation (section 4.6). Simplified the Audits service and containment mode (section 4.13). Added the ability for storage node operators to select the Satellites they would like to work with, eliminating the need for Satellite vetting and opt-out (section 4.18). Removed section 4.21 Quality Control and Branding about obsolete branding ideas. Updated appendix B about anticipated attacks to reflect the removal of Kademlia. With these changes in mind, we expect that this paper once again matches our in- production service at the time of publication.",
      "For more a detailed changelog, please see https:github.comstorjwhitepapercompare v3.0-merged...v3.1. 1. Introduction The Internet is a massive decentralized and distributed network consisting of billions of devices which are not controlled by a single group or entity. Much of the data currently available through the Internet is quite centralized and is stored with a handful of tech- nology companies that have the experience and capital to build massive data centers capable of handling this vast amount of information. A few of the challenges faced by data centers are: data breaches, periods of unavailability on a grand scale, storage costs, and expanding and upgrading quickly enough to meet user demand for faster data and larger formats. Decentralized storage has emerged as an answer to the challenge of providing a per- formant, secure, private, and economical cloud storage solution.",
      "Decentralized storage is better positioned to achieve these outcomes as the architecture has a more natural alignment to the decentralized architecture of the Internet as a whole, as opposed to massive centralized data centers. News coverage of data breaches over the past few years has shown us that the fre- quency of such breaches has been increasing by as much as a factor of 10 between 2005 and 2017 1. Decentralized storages process of protecting data makes data breaches more difficult than current methods used by data centers while, at the same time, cost- ing less than current storage methods. This model can address the rapidly expanding amount of data for which current solu- tions struggle. With an anticipated 44 zettabytes of data expected to exist by 2020 and a market that will grow to 92 billion USD in the same time frame 2, we have identified several key market segments that decentralized cloud storage has the potential to ad- dress.",
      "As decentralized cloud storage capabilities evolve, it will be able to address a much wider range of use cases from basic object storage to content delivery networks (CDN). Decentralized cloud storage is rapidly advancing in maturity, but its evolution is sub- ject to a specific set of design constraints which define the overall requirements and im- plementation of the network. When designing a distributed storage system, there are many parameters to be optimized such as speed, capacity, trustlessness, Byzantine fault tolerance, cost, bandwidth, and latency. We propose a framework that scales horizontally to exabytes of data storage across the globe. Our system, the Storj Network, is a robust object store that encrypts, shards, and distributes data to nodes around the world for storage. Data is stored and served in a manner purposefully designed to prevent breaches.",
      "In order to accomplish this task, weve designed our system to be modular, consisting of independent components with task-specific jobs. Weve integrated these components to implement a decentralized ob- ject storage system that is not only secure, performant, and reliable but also significantly more economical than either on-premise or traditional, centralized cloud storage. Chapter 1. Introduction We have organized the rest of this paper into six additional chapters. Chapter 2 dis- cusses the design space in which Storj operates and the specific constraints on which our optimization efforts are based. Chapter 3 covers our framework. Chapter 4 describes the concrete implementation of the framework, while chapter 5 explains what happens during each operation in the network. Chapter 6 covers future work. Finally, chapter 7 covers selected calculations. 2. Storj design constraints Before designing a system, its important to first define its requirements.",
      "There are many different ways to design a decentralized storage system. However, with the addition of a few requirements, the potential design space shrinks significantly. Our design con- straints are heavily influenced by our product and market fit goals. By carefully consid- ering each requirement, we ensure the framework we choose is as universal as possible, given the constraints. Security and privacy Any object storage platform must ensure both the privacy and security of data stored regardless of whether it is centralized or decentralized. Decentralized storage platforms must mitigate an additional layer of complexity and risk associated with the storage of data on inherently untrusted nodes. Because decentralized storage platforms cannot take many of the same shortcuts data center based approaches can (e.g.",
      "firewalls, DMZs, etc.), decentralized storage must be designed from the ground up to support not only end-to-end encryption but also enhanced security and privacy at all levels of the system. Certain categories of data are also subject to specific regulatory compliance. For ex- ample, the United States legislation for the Health Insurance Portability and Accountabil- ity Act (HIPAA) has specific requirements for data center compatibility. European coun- tries have to consider the General Data Protection Regulation (GDPR) regarding how in- dividual information must be protected and secured. Many customers outside of the United States may feel they have significant geopolitical reasons to consider storing data in a way that limits the ability for US-based entities to impact their privacy 3. There are many other regulations in other sectors regarding users data privacy.",
      "Customers should be able to evaluate that our software is implemented correctly, is resistant to attack vectors (known or unknown), is secure, and otherwise fulfills all of the customers requirements. The code for the Storj network is open source software and provides the level of transparency and assurance needed to prove that the behaviors of the system are as advertised. Decentralization Informally, a decentralized application is a service that has no single operator. Further- more, no single entity should be solely responsible for the cost associated with running the service or be able to cause a service interruption for other users. One of the main motivations for preferring decentralization is to drive down infras- tructure costs for maintenance, utilities, and bandwidth. We believe that there are sig- Chapter 2. Storj design constraints nificant underutilized resources at the edge of the network for many smaller operators.",
      "In our experience building decentralized storage networks, we have found a long tail of resources that are presently unused or underused that could provide affordable and ge- ographically distributed cloud storage. Conceivably, some small operator might have ac- cess to less-expensive electricity than standard data centers or another small operator could have access to less-expensive cooling. Many of these small operator environments are not substantial enough to run an entire datacenter-like storage system. For example, perhaps a small business or home Network Attached Storage (NAS) operator has enough excess electricity to run ten hard drives but not more. We have found that in aggregate, enough small operator environments exist such that their combination over the internet constitutes significant opportunity and advantage for less-expensive and faster storage.",
      "Our decentralization goals for fundamental infrastructure, such as storage, are also driven by our desire to provide a viable alternative to the few major centralized storage entities who dominate the market at present. We believe that there exists inherent risk in trusting a single entity, company, or organization with a significant percentage of the worlds data. In fact, we believe that there is an implicit cost associated with the risk of trusting any third party with custodianship of personal data. Some possible costly outcomes include changes to the companys roadmap that could result in the product becoming less useful, changes to the companys position on data collection that could cause it to sell customer metadata to advertisers, or even the company could go out of business or otherwise fail to keep customer data safe. By creating an equivalent or better decentralized system, many users concerned about single-entity risk will have a viable alternative.",
      "With decentralized architecture, Storj could cease operating and the data would continue to be available. We have decided to adopt a decentralized architecture because, despite the trade- offs, we believe decentralization better addresses the needs of cloud storage and resolves many core limitations, risks, and cost factors that result from centralization. Within this context, decentralization results in a globally distributed network that can serve a wide range of storage use cases from archival to CDN. However, centralized storage systems require different architectures, implementations, and infrastructure to address each of those same use cases. Marketplace and economics Public cloud computing, and public cloud storage in particular, has proven to be an at- tractive business model for the large centralized cloud providers. Cloud computing is estimated to be a 186.4 billion dollar market in 2018, and is expected to reach 302.5 billion by 2021 4.",
      "The public cloud storage model has provided a compelling economic model to end users. Not only does it enable end users to scale on demand but also allows them to avoid the significant fixed costs of facilities, power, and data center personnel. Public Chapter 2. Storj design constraints cloud storage has generally proven to be an economical, durable, and performant option for many end users when compared to on-premise solutions. However, the public cloud storage model has, by its nature, led to a high degree of con- centration. Fixed costs are born by the network operators, who invest billions of dollars in building out a network of data centers and then enjoy significant economies of scale. The combination of large upfront costs and economies of scale means that there is an ex- tremely limited number of viable suppliers of public cloud storage (arguably, fewer than five major operators worldwide). These few suppliers are also the primary beneficiaries of the economic return.",
      "We believe that decentralized storage can provide a viable alternative to centralized cloud. However, to encourage partners or customers to bring data to the network, the price charged for storage and bandwidthcombined with the other benefits of decen- tralized storagemust be more compelling and economically beneficial than competing storage solutions. In our design of Storj, we seek to create an economically advantageous situation for four different groups: End users - We must provide the same economically compelling characteristics of pub- lic cloud storage with no upfront costs and scale on demand. In addition, end users must experience meaningfully better value for given levels of capacity, durability, security, and performance. Storage node operators - It must be economically attractive for storage node operators to help build out the network. They must be paid fairly, transparently, and be able to make a reasonable profit relative to any marginal costs they incur.",
      "It should be economically advantageous to be a storage node operator not only by utilizing un- derused capacity but also by creating new capacity, so that we can grow the network beyond the capacity that currently exists. Since node availability and reliability has a large impact on network availability, cost, and durability, it is required that stor- age node operators have sufficient incentive to maintain reliable and continuous connections to the network. Demand providers - It must be economically attractive for developers and businesses to drive customers and data onto the Storj network. We must design the system to fairly and transparently deliver margin to partners. We believe that there is a unique opportunity to provide open-source software (OSS) companies and projects, which drive over two-thirds of the public cloud workloads today without receiving direct revenue, a source of sustainable revenue.",
      "Network operator - To sustain continued investment in code, functionality, network maintenance, and demand generation, the network operator, currently Storj Labs, Inc., must be able to retain a reasonable profit. The operator must maintain this profit while not only charging end users less than the public cloud providers but also margin sharing with storage node operators and demand providers. Additionally, the network must be able to account for ensuring efficient, timely billing and payment processes as well as regulatory compliance for tax and other reporting. To be as globally versatile as possible with payments, our network must be robust to accom- Chapter 2. Storj design constraints modate several types of transactions (such as cryptocurrency, bank payments, and other forms of barter). Lastly, the Storj roadmap must be aligned with the economic drivers of the network.",
      "New features and changes to the concrete implementations of framework components must be driven by applicability to specific object storage use cases and the relationship between features and performance to the price of storage and bandwidth relative to those use cases. Amazon S3 compatibility At the time of this papers publication, the most widely deployed public cloud is Amazon Web Services 5. Amazon Web Services not only is the largest cloud services ecosystem but also has the benefit of first mover advantage. Amazons first cloud services product was Amazon Simple Storage Service, or Amazon S3 for short. Public numbers are hard to come by but Amazon S3 is likely the most widely deployed cloud storage protocol in existence. Most cloud storage products provide some form of compatibility with the Amazon S3 application program interface (API) architecture.",
      "Our objective is to aggressively compete in the wider cloud storage industry and bring decentralized cloud storage into the mainstream. Until a decentralized cloud storage protocol becomes widely adopted, Amazon S3 compatibility creates a graceful transi- tion path from centralized providers by alleviating many switching costs for our users. To achieve this, the Storj implementation allows applications previously built against Ama- zon S3 to work with Storj with minimal friction or changes. S3 compatibility adds aggres- sive requirements for feature set, performance, and durability. At a bare minimum, this requires the methods described in Figure 2.1 to be implemented.",
      "Bucket operations CreateBucket(bucketName) DeleteBucket(bucketName) ListBuckets ()  Object operations GetObject(bucketName , objectPath , offset , length) PutObject(bucketName , objectPath , data , metadata) DeleteObject(bucketName , objectPath) ListObjects(bucketName , prefix , startKey , limit , delimiter) Figure 2.1: Minimum S3 API The Storj service supports the majority of the S3 protocol via two different integration patterns that emerged as specific customer requirements: Single tenant gateway - a self-hosted, end-to-end encrypted S3 compatible gateway where the customers equipment is responsible for encryption, erasure coding and Chapter 2. Storj design constraints direct peer-to-peer transmission of data to storage nodes, including data expansion from erasure coded redundancy and long tail mitigation. Multi-tenant gateway - a hosted S3 compatible gateway where a trusted provider is re- sponsible for encryption, erasure coding, and transmission of data to storage nodes.",
      "This gateway requires a key from the customer as part of every request, since the customers encryption keys are not preserved in the hosted environment. Durability, device failure, and churn A storage platform is useless unless it also functions as a retrieval platform. For any stor- age platform to be valuable, it must be careful not to lose the data it was given, even in the presence of a variety of possible failures within the system. Our system must store data with high durability and have negligible risk of data loss. For all devices, component failure is a guarantee. All hard drives fail after enough wear 6 and servers providing network access to these hard drives will also eventually fail. Network links may die, power failures could cause havoc sporadically, and storage media become unreliable over time. Data must be stored with enough redundancy to recover from individual component failures. Perhaps more importantly, no data can be left in a single location indefinitely.",
      "In such an environment, redundancy, data mainte- nance, repair, and replacement of lost redundancy must be considered inevitable, and the system must account for these issues. Furthermore, decentralized systems are susceptible to high churn rates where par- ticipants join the network and then leave for various reasons, well before their hardware has actually failed. For instance, Rhea et al. found that in many real world peer-to-peer systems, the median time a participant lasts in the network ranges from hours to mere minutes 7. Maymounkov et al. found that the probability of a node staying connected to a decentralized network for an additional hour is an increasing function of uptime (Fig- ure 2.2 8). In other words, nodes that have been online for a long time are less likely to contribute to overall node churn. Churn could be caused by any number of factors.",
      "Storage nodes may go offline due to hardware or software failure, intermittent internet connectivity, power loss, complete disk failure, or software shutdown or removal. The more network churn that exists, the more redundancy is required to make up for the greater rate of node loss. The more re- dundancy that is required, the more bandwidth is needed for correct operation of the sys- tem. In fact, there is a tight relationship between network churn, additional redundancy, and bandwidth availability 9. To keep background bandwidth usage and redundancy low, our network must have low network churn and a strong incentive to favor long-lived, stable nodes. See section 7.3.3 and Blake et al. 9 for a discussion of how repair bandwidth varies as a function of node churn. Chapter 2. Storj design constraints Figure 2.2: Probability of remaining online an additional hour as a function of uptime. The x axis represents minutes.",
      "The y axis shows the fraction of nodes that stayed online at least x minutes that also stayed online at least x  60 minutes. Source: Maymounkov et al. 8 Latency Decentralized storage systems can potentially capitalize on massive opportunities for parallelism. Some of these opportunities include increased transfer rates, processing ca- pabilities, and overall throughput even when individual network links are slow. However, parallelism cannot, by itself, improve latency. If an individual network link is utilized as part of an operation, its latency will be the lower bound for the overall operation. There- fore, any distributed system intended for high performance applications must continu- ously and aggressively optimize for low latency not only on an individual process scale but also for the systems entire architecture. Bandwidth Global bandwidth availability is increasing year after year.",
      "Unfortunately, access to high- bandwidth internet connections is unevenly distributed across the world. While some users can easily access symmetric, high-speed, unlimited bandwidth connections, others have significant difficulty obtaining the same type of access. In the United States and other countries, the method in which many residential in- ternet service providers (ISPs) operate presents two specific challenges for designers of a decentralized network protocol. The first challenge is the asymmetric internet connec- tions offered by many ISPs. Customers subscribe to internet service based on an adver- tised download speed, but the upload speed is potentially an order of magnitude or two slower. The second challenge is that bandwidth is sometimes capped by the ISP at a fixed amount of allowed traffic per month. For example, in many US markets, the ISP Comcast imposes a one terabyte per month bandwidth cap with stiff fines for customers Chapter 2.",
      "Storj design constraints who go over this limit 10. An internet connection with a cap of 1 TBmonth cannot av- erage more than 385 KBs over the month without exceeding the monthly bandwidth allowance, even if the ISP advertises speeds of 10 MBs or higher. Such caps impose sig- nificant limitations on the bandwidth available to the network at any given moment. With device failure and churn guaranteed, any decentralized system will have a cor- responding amount of repair traffic. As a result, it is important to account for the band- width required not only for data storage and retrieval but also for data maintenance and repair 9. Designing a storage system that is careless with bandwidth usage would not only give undue preference to storage node operators with access to unlimited high- speed bandwidth but also centralize the system to some degree.",
      "In order to keep the storage system as decentralized as possible and working in as many environments as possible, bandwidth usage must be aggressively minimized. Please see section 7.1.1 for a discussion on how bandwidth availability and repair traffic limit usable space. Object size We can broadly classify large storage systems into two groups by average object size. To differentiate between the two groups, we classify a large file as a few megabytes or greater in size. A database is the preferred solution for storing many small pieces of information, whereas an object store or file system is ideal for storing many large files. The initial product offering by Storj Labs is designed to function primarily as a decen- tralized object store for larger files. While future improvements may enable database-like use cases, object storage is the predominant initial use case described in this paper.",
      "We made protocol design decisions with the assumption that the vast majority of stored ob- jects will be 4MB or larger. While smaller files are supported, they may simply be more costly to store. It is worth noting that this will not negatively impact use cases that require reading lots of files smaller than a megabyte. Users can address this with a packing strategy by aggregating and storing many small files as one large file. The protocol supports seek- ing and streaming, which will allow users to download small files without requiring full retrieval of the aggregated object. Byzantine fault tolerance Unlike centralized solutions like Amazon S3, Storj operates in an untrusted environment where individual storage providers are not necessarily assumed to be trustworthy. Storj operates over the public internet, allowing anyone to sign up to become a storage provider. Chapter 2.",
      "Storj design constraints We adopt the Byzantine, Altruistic, Rational (BAR) model 11 to discuss participants in the network. Byzantine nodes may deviate arbitrarily from the suggested protocol for any reason. Some examples include nodes that are broken or nodes that are actively trying to sabotage the protocol. In general, a Byzantine node is a bad actor, or one that op- timizes for a utility function that is independent of the one given for the suggested protocol. Inevitable hardware failures aside, Altruistic nodes are good actors and participate in a proposed protocol even if the rational choice is to deviate. Rational nodes are neutral actors and participate or deviate only when it is in their net best interest. Some distributed storage systems (e.g. datacenter-based cloud object storage sys- tems) operate in an environment where all nodes are considered altruistic.",
      "For example, absent hardware failure or security breaches, Amazons storage nodes will not do any- thing besides what they were explicitly programmed to do, because Amazon owns and runs all of them. In contrast, Storj operates in an environment where every node is managed by its own independent operator. In this environment, we can expect that a majority of storage nodes are rational and a minority are Byzantine. Storj assumes no altruistic nodes. We must include incentives that encourage the network to ensure that the rational nodes on the network (the majority of operators) behave as similarly as possible to the expected behavior of altruistic nodes. Likewise, the effects of Byzantine behavior must be minimized or eliminated. Note that creating a system that is robust in the face of Byzantine behavior does not require a Byzantine fault tolerant consensus protocolwe avoid Byzantine consensus. See sections 4.9, 6.2, and appendix A for more details.",
      "2.10 Coordination avoidance A growing body of distributed database research shows that systems that avoid coordina- tion wherever possible have far better throughput than systems where subcomponents are forced to coordinate to achieve correctness 1219. We use Bailis et al.s informal def- inition that coordination is the requirement that concurrently executing operations syn- chronously communicate or otherwise stall in order to complete 16. This observation happens at all scales and applies not only to distributed networks but also to concurrent threads of execution coordinating within the same computer. As soon as coordination is needed, actors in the system will need to wait for other actors, and waitingdue to coordination issuescan have a significant cost. While many types of operations in a network may require coordination (e.g., opera- Chapter 2.",
      "Storj design constraints tions that require linearizability1 15, 20, 21), choosing strategies that avoid coordination (such as Highly Available Transactions 15) can offer performance gains of two to three orders of magnitude over wide area networks. In fact, by carefully avoiding coordination as much as possible, the Anna database 17 is able to be 10 times faster than both Cas- sandra and Redis in their corresponding environments and 700 to 800 times faster than performance-focused in-memory databases such as Masstree or Intels TBB 22. Not all coordination can be avoided, but new frameworks (such as Invariant Confluence 16 or the CALM principle 18,19) allow system architects to understand when coordination is re- quired for consistency and correctness. As evidenced by Annas performance successes, it is most efficient to avoid coordination where possible. Systems that minimize coordination are much better at scaling from small to large workloads.",
      "Adding more resources to a coordination-avoidant system will directly in- crease throughput and performance. However, adding more resources to a coordination- dependent system (such as Bitcoin 23 or even Raft 24) will not result in much additional throughput or overall performance. To get to exabyte scale, minimizing coordination is one of the key components of our strategy. Surprisingly, many decentralized storage platforms are working towards archi- tectures that require significant amounts of coordination, where most if not all opera- tions must be accounted for by a single global ledger. For us to achieve exabyte scale, it is a fundamental requirement to limit hotpath coordination domains to small spheres which are entirely controllable by each user. This limits the applicability of blockchain-like solutions for our use case. 1Linearizable operations are atomic operations on a specific object where the order of operations is equiva- lent to the order given original wall clock time.",
      "3. Framework After having considered our design constraints, this chapter outlines the design of a framework consisting of only the most fundamental components. The framework de- scribes all of the components that must exist to satisfy our constraints. As long as our design constraints remain constant, this framework will, as much as is feasible, describe Storj both now and ten years from now. While there will be some design freedom within the framework, this framework will obviate the need for future rearchitectures entirely, as independent components will be able to be replaced without affecting other compo- nents. Framework overview All designs within our framework will do the following things: Store data When data is stored with the network, a client encrypts and breaks it up into multiple pieces. The pieces are distributed to peers across the network. When this occurs, metadata is generated that contains information on where to find the data again.",
      "Retrieve data When data is retrieved from the network, the client will first reference the metadata to identify the locations of the previously stored pieces. Then the pieces will be retrieved and the original data will be reassembled on the clients local ma- chine. Maintain data When the amount of redundancy drops below a certain threshold, the necessary data for the missing pieces is regenerated and replaced. Pay for usage A unit of value should be sent in exchange for services rendered. To improve understandability, we break up the design into a collection of eight inde- pendent components and then combine them to form the desired framework. The individual components are: 1. Storage nodes 2. Peer-to-peer communication 3. Redundancy 4. Metadata 5. Encryption 6. Audits and reputation 7. Data repair 8. Payments Chapter 3. Framework Storage nodes The storage nodes role is to store and return data.",
      "Aside from reliably storing data, nodes should provide network bandwidth and appropriate responsiveness. Storage nodes are selected to store data based on various criteria: ping time, latency, throughput, band- width caps, sufficient disk space, geographic location, uptime, history of responding ac- curately to audits, and so forth. In return for their service, nodes are paid for both data egress and data at rest. Because storage nodes are selected via changing variables external to the protocol, node selection is an explicit, non-deterministic process in our framework. This means that we must keep track of which nodes were selected for each upload via a small amount of metadata; we cant select nodes for storing data implicitly or deterministically as in a sys- tem like Dynamo 25. As with GFS 26, HDFS 27, or Lustre 28, this decision implies the requirement of a metadata storage system to keep track of selected nodes (see section 3.5).",
      "Peer-to-peer communication All peers on the network communicate via a standarized protocol. The framework re- quires that this protocol:  provides peer reachability, even in the face of firewalls and NATs where possible. This may require techniques like STUN 29, UPnP 30, NAT-PMP 31, etc. provides authentication as in SKademlia 32, where each participant cryptograph- ically proves the identity of the peer with whom they are speaking to avoid man-in- the-middle attacks. provides complete privacy. In cases such as bandwidth measurement (see section 4.17), the client and storage node must be able to communicate without any risk of eavesdroppers. The protocol should ensure that all communications are private by default. Additionally, the framework requires a way to look up peer network addresses by a unique identifier so that, given a peers unique identifier, any other peer can connect to it.",
      "This responsibility is similar to the internets standard domain name system (DNS) 33, which is a mapping of an identifier to an ephemeral connection address, but unlike DNS, there can be no centralized registration process. A network overlay can be built on top of our chosen peer-to-peer communication protocol to achieve these goals. See Section 4.6 for implementation details. Chapter 3. Framework Redundancy We assume that at any moment, any storage node could go offline permanently. Our redundancy strategy must store data in a way that provides access to the data with high probability, even though any given number of individual nodes may be in an offline state. To achieve a specific level of durability (defined as the probability that data remains avail- able in the face of failures), many products in this space use simple replication. Unfortu- nately, this ties durability to the network expansion factor, which is the storage overhead for reliably storing data.",
      "This significantly increases the total cost relative to the stored data. For example, suppose a certain desired level of durability requires a replication strat- egy that makes eight copies of the data. This yields an expansion factor of 8x, or 800. This data then needs to be stored on the network, using bandwidth in the process. Thus, more replication results in more bandwidth usage for a fixed amount of data. As dis- cussed in the protocol design constraints (section 2.7) and Blake et al. 9, high bandwidth usage prevents scaling, so this is an undesirable strategy for ensuring a high degree of file durability. As an alternative to simple replication, erasure codes provide a much more efficient method to achieve redundancy. Erasure codes are well-established in use for both dis- tributed and peer-to-peer storage systems 3440.",
      "Erasure codes are an encoding scheme for manipulating data durability without tying it to bandwidth usage, and have been found to improve repair traffic significantly over replication 9. Importantly, they allow changes in durability without changes in expansion factor. An erasure code is often described by two numbers, k and n. If a block of data is en- coded with a (k, n) erasure code, there are n total generated erasure shares, where only any k of them are required to recover the original block of data. If a block of data is s bytes, each of the n erasure shares is roughly sk bytes. Besides the case when k  1 (replication), all erasure shares are unique. Interestingly, the durability of a (k  20, n  40) erasure code is better than a (k  10, n  20) erasure code, even though the expansion factor (2x) is the same for both. This is because the risk is spread across more nodes in the (k  20, n  40) case. These consid- erations make erasure codes an important part of our general framework.",
      "To better understand how erasure codes increase durability without increasing expan- sion factors, the following table shows various choices of k and n, along with the expan- sion factor and associated durability: Chapter 3. Framework Exp. factor P(D  p  10) 99.207366813274616 99.858868985411326 99.995462406878260 99.999994620652776 99.999999807694154 99.999999999990544 In contrast, replication requires significantly higher expansion factors for the same durability. The following table shows durability with a replication scheme: Exp. factor P(D  p  10) 90.483741803595962 98.247690369357827 99.640050681691051 99.999988857452166 99.999999998036174 To see how these tables were calculated, well start with the simplifying assumption that p is the monthly node churn rate (that is, the fraction of nodes that will go offline in a month on average).",
      "Mathematically, time-dependent processes are modeled according to the Poisson distribution, where it is assumed that \u03bb events are observed in the given unit of time. As a result, we model durability as the cumulative distribution function (CDF) of the Poisson distribution with mean \u03bb  pn, where we expect \u03bb pieces of the file to be lost monthly. To estimate durability, we consider the CDF up to nk, looking at the probability that at most n  k pieces of the file are lost in a month and the file can still be rebuilt. The CDF is given by: P(D)  e\u03bb The expansion factor still plays a big role in durability, as seen in the following table: Exp. factor P(D  p  10) 97.688471224736705 99.999514117129605 99.970766304935266 99.999999999999548 99.999999999973570 By being able to tweak the durability independently of the expansion factor, erasure coding allows very high durability to be achieved with surprisingly low expansion factors.",
      "Because of how limited bandwidth is as a resource, completely eliminating replication Chapter 3. Framework as a strategy and using erasure codes only for redundancy causes a drastic decrease in bandwidth footprint. Erasure coding also results in storage nodes getting paid more. High expansion fac- tors dilute the incoming funds per byte across more storage nodes; therefore, low ex- pansion factors, such as those provided by erasure coding, allow for a much more direct passthrough of income to storage node operators. 3.4.1 Erasure codes effect on streaming Erasure codes are used in many streaming contexts such as audio CDs and satellite com- munications 36, so its important to point out that using erasure coding in general does not make our streaming design requirement (required by Amazon S3 compatibility, see section 2.4) more challenging.",
      "Whatever erasure code is chosen for our framework, as with CDs, streaming can be added on top by encoding small portions at a time, instead of attempting to encode a file all at once. See section 4.8 for more details. 3.4.2 Erasure codes effect on long tails Erasure codes enable an enormous performance benefit, which is the ability to avoid waiting for long-tail response times 41. A long-tail response occurs in situations where a needed server has an unreasonably slow operation time due to a confluence of un- predictable factors. Long-tail responses are so-named due to their rare average rate of occurrence but highly variable nature, which in a probability density graph looks like a long tail. In aggregate, long-tail responses are a big issue in distributed system design. In MapReduce, long-tail responses are called stragglers.",
      "MapReduce executes re- dundant requests called backup tasks to make sure that if specific stragglers take too long, the overall operation can still proceed without waiting. If the backup task mecha- nism is disabled in MapReduce, basic operations can take 44 longer to complete, even though the backup task mechanism is causing duplicated work 42. By using erasure codes, we are in a position to create MapReduce-like backup tasks for storage 37,38. For uploads, a file can be encoded to a higher (k, n) ratio than necessary for desired durability guarantees. During an upload, after enough pieces have uploaded to gain required redundancy, the remaining additional uploads can be canceled. This cancellation allows the upload to continue as fast as the fastest nodes in a set, instead of waiting for the slowest nodes. Downloads are similarly improved.",
      "Since more redundancy exists than is needed, downloads can be served from the fastest peers, eliminating a wait for temporarily slow or offline peers. The outcome is that every request is satisfiable by the fastest nodes participating in any given transaction, without needing to wait for a slower subset. Focusing on opera- Chapter 3. Framework Figure 3.1: Various outcomes during upload and download tions where the result is only dependent on the fastest nodes of a random subpopulation turns what could be a potential liability (highly variable performance from individual ac- tors) into a great source of strength for a distributed storage network, while still providing great load balancing characteristics. This ability to over-encode a file greatly assists dynamic load balancing of popular con- tent on the network. See section 6.1 for a discussion on how we plan to address load balancing very active files.",
      "Metadata Once we split an object up with erasure codes and select storage nodes on which to store the new pieces, we now need to keep track of which storage nodes we selected. We allow users to choose storage based on geographic location, performance characteristics, available space, and other features. Therefore, instead of implicit node selection such as a scheme using consistent hashing like Dynamo 25, we must use an explicit node selection scheme such as directory-based lookups 43. Additionally, to maintain Amazon S3 compatibility, the user must be able to choose an arbitrary key, often treated like a path, to identify this mapping of data pieces to node. These features imply the necessity of a metadata storage system. Amazon S3 compatibility once again imposes some tight requirements. We should support: hierarchical objects (paths with prefixes), per-object keyvalue storage, arbitrarily large files, arbitrarily large amounts of files, and so forth.",
      "Objects should be able to be Chapter 3. Framework stored and retrieved by arbitrary key; in addition, deterministic iteration over those keys will be required to allow for paginated listing. Every time an object is added, edited, or removed, one or more entries in this meta- data storage system will need to be adjusted. As a result, there could be heavy churn in this metadata system, and across the entire userbase the metadata itself could end up being sizable. For example, suppose in a few years the network stores one total exabyte of data, where the average object size is 50MB and our erasure code is selected such that n  40. One exabyte of 50MB objects is 20 billion objects. This metadata system will need to keep track of which 40 nodes were selected for each object. If each metadata element is roughly 40  64  192 bytes (info for each selected node plus the path and some general overhead), there are over 55 terabytes of metadata of which to keep track.",
      "Fortunately, the metadata can be heavily partitioned by the user. A user storing 100 terabytes of 50 megabyte objects will only incur a metadata overhead of 5.5 gigabytes. Its worth pointing out that these numbers vary heavily with object size: the larger the average object size, the less the metadata overhead. An additional framework focus is enabling this componentmetadata storageto be interchangeable. Specifically, we expect the platform to incorporate multiple implemen- tations of metadata storage that users will be allowed to choose between. This greatly assists with our design goal of coordination avoidance between users (see section 2.10).",
      "As metadata retrieval is a prerequisite to data retrieval in any operation, availability of services responsible for metadata storage and retrieval are designed and implemented to be distributed over multiple regions within a geography to provide maximum resis- tance to any single point of failure, whether is it a device, server, network connection, or entire region within a geography. Aside from scale requirements, to implement Amazon S3 compatibility, the desired API is straightforward and simple: Put (store metadata at a given path), Get (retrieve metadata at a given a path), List (paginated, deterministic listing of existing paths), and Delete (remove a path). See Figure 2.1 for more details. Encryption Regardless of storage system, our design constraints require total security and privacy. All data or metadata will be encrypted. Data must be encrypted as early as possible in the data storage pipeline, ideally before the data ever leaves the source computer.",
      "This means that an Amazon S3-compatible interface or appropriate similar client library should run colocated on the same computer as the users application. Encryption should use a pluggable mechanism that allows users to choose their de- sired encryption scheme. It should also store metadata about that encryption scheme to Chapter 3. Framework allow users to recover their data using the appropriate decryption mechanism in cases where their encryption choices are changed or upgraded. To support rich access management features, the same encryption key should not be used for every file, as having access to one file would result in access to decryption keys for all files. Instead, each file should be encrypted with a unique key. This should allow users to share access to certain selected files without giving up encryption details for others.",
      "Because each file should be encrypted differently with different keys and potentially different algorithms, the metadata about that encryption must be stored somewhere in a manner that is secure and reliable. This metadata, along with other metadata about the file, including its path, will be stored in the previously discussed metadata storage system, encrypted by a deterministic, hierarchical encryption scheme. A hierarchical en- cryption scheme based on BIP32 44 will allow subtrees to be shared without sharing their parents and will allow some files to be shared without sharing other files. See sec- tion 4.11 for a discussion of our path-based hierarchical deterministic encryption scheme. Audits and reputation Incentivizing storage nodes to accurately store data is of paramount importance to the viability of this whole system. It is essential to be able to validate and verify that storage nodes are accurately storing what they have been asked to store.",
      "When storage nodes initially join the network, they generate a unique identity via a small proof of work function. Storage nodes build up an initial reputation score during a vetting period during which they are subject to an increased level of audit and uptime checks to ensure the nodes are properly operated. During the first 9 months of opera- tion, a portion of storage node earnings are held by a Satellite as an offset against the possibility that a node may leave the network with data stored on the storage node such that some portion of that data may require repair. At all times while a storage node is operating on the network, it is subject to an audit process for each Satellite for which it stores data. Many storage systems use probabilistic per-file audits, called proofs of retrievability, as a way of determining when and where to repair files 45, 46.",
      "We are extending the probabilistic nature of common per-file proofs of retrievability to range across all possi- ble files stored by a specific node. Audits, in this case, are probabilistic challenges that confirm, with a high degree of certainty and a low amount of overhead, that a storage node is well-behaved, keeping the data it claims, and not susceptible to hardware failure or malintent. Audits function as spot checks 47 to help calculate the future usefulness of a given storage node. In our storage system, audits are simply a mechanism used to determine a nodes de- gree of stability. Failed audits will result in a storage node being marked as bad, which will result in redistributing data to new nodes and avoiding that node altogether in the fu- Chapter 3. Framework ture. Storage node uptime and overall health are the primary metrics used to determine which files need repair.",
      "As is the case with proofs of retrievability 45, 46, this auditing mechanism does not audit all bytes in all files. This can leave room for false positives, where the verifier believes the storage node retains the intact data when it has actually been modified or partially deleted. Fortunately, the probability of a false positive on an individual partial audit is easily calculable (see section 7.2). When applied iteratively to a storage node as a whole, detection of missing or altered data becomes certain to within a known and modifiable error threshold. A reputation system is needed to persist the history of audit outcomes for given node identities. Our overall framework has flexible requirements on the use of such a system, but see section 4.15 for a discussion of our initial approach. Data repair Data loss is an ever-present risk in any distributed storage system.",
      "While there are many potential causes for file loss, storage node churn (storage nodes joining and leaving the network) is the largest leading risk by a significant degree compared to other causes. As discussed in section 2.5, network session time in many real world systems range from hours to mere minutes 7. While there are many other ways data might get lost, such as corruption, malicious behavior, bad hardware, software error, or user initiated space reclamation, these issues are less serious than full node churn. We expect node churn to be the dominant cause of data loss in our network. Because audits are validating that conforming nodes store data correctly, all that re- mains is to detect when a storage node stops storing data correctly or goes offline and then repair the data it had to new nodes.",
      "To repair the data, we will recover the original data via an erasure code reconstruction from the remaining pieces and then regenerate the missing pieces and store them back in the network on new storage nodes. It is vital in our system to incentivize storage node participants to remain online for much longer than a few hours. To encourage this behavior, our payment strategy will involve rewarding storage node operators that keep their nodes participating for months and years at a time. Payments Payments, value attribution, and billing in decentralized networks are a critical part of maintaining a healthy ecosystem of both supply and demand. Of course, decentralized payment systems are still in their infancy in a number of ways. For our framework to achieve low latency and high throughput, we must not have Chapter 3. Framework transactional dependencies on a blockchain (see section 2.10).",
      "This means that an ad- equately performant storage system cannot afford to wait for blockchain operations. When operations should be measured in milliseconds, waiting for a cluster of nodes to probabilistically come to agreement on a shared global ledger is a non-starter. Our framework instead emphasizes game theoretic models to ensure that partici- pants in the network are properly incentivized to remain in the network and behave ra- tionally to get paid. Many of our decisions are modeled after real-world financial relation- ships. Payments will be transferred during a background settlement process in which well-behaved participants within the network cooperate. Storage nodes in our frame- work should limit their exposure to untrusted payers until confidence is gained that those payers are likely to pay for services rendered. In addition, the framework also tracks and aggregates the value of the consumption of those services by those who own the data stored on the network.",
      "By charging for usage, the framework is able to support the end-to-end economics of the storage marketplace ecosystem. Although the Storj network is payment agnostic and the protocol does not require a specific payment type, the network assumes the Ethereum-based STORJ token as the default mechanism for payment. While we intend for the STORJ token to be the primary form of payment, in the future other alternate payment types could be implemented, including Bitcoin, Ether, credit or debit card, ACH transfer, or even physical transfer of live goats. 4. Concrete implementation We believe the framework weve described to be relatively fundamental given our design constraints. However, within the framework there still remains some freedom in choosing how to implement each component. In this section, we lay out our initial implementation strategy. We expect the details contained within this section to change gradually over time.",
      "However, we believe the de- tails outlined here are viable and support a working implementation of our framework ca- pable of providing highly secure, performant, and durable production-grade cloud stor- age. Definitions The following defined terms are used throughout the description of the concrete imple- mentation that follows: 4.1.1 Actors Client A user or application that will upload or download data from the network. Peer class A cohesive collection of network services and responsibilities. There are three different peer classes that represent services in our network: storage nodes, Satel- lites, and Uplinks. Storage node This peer class stores data for others and gets paid for storage and band- width. It registers itself directly with Satellites it trusts. Uplink This peer class represents any application or service that implements libuplink and wants to store andor retrieve data.",
      "This peer class is not expected to remain online like the other two classes and is relatively lightweight. This peer class per- forms encryption, erasure encoding, and coordinates with the other peer classes on behalf of the customerclient. libuplink A library which provides all necessary functions to interact with storage nodes and Satellites directly. This library will be available in a number of differ- ent programming languages. Gateway A service which provides a compatibility layer between other object stor- age services such as Amazon S3 and libuplink exposing an Amazon S3-compatible API. Uplink CLI A command line interface for uploading and downloading files from the network, managing permissions and sharing, and managing accounts.",
      "Satellite This peer class allows storage nodes to register with it, caches node address information, stores per-object metadata, maintains storage node reputation, ag- gregates billing data, pays storage nodes, performs audits and repair, and manages authorization and user accounts. Users have accounts on and trust specific Satel- Chapter 4. Concrete implementation lites. Any user can run their own Satellite, but we expect many users to elect to avoid the operational complexity and create an account on another Satellite hosted by a trusted third party such as Storj Labs, a friend, group, or workplace. Figure 4.1: The three different peer classes 4.1.2 Data Bucket A bucket is an unbounded but named collection of objects (or files) identified by object keys. Every object has a unique key within a bucket. Object Key An object key is a unique identifier for an object (or file) within a bucket. An object key is an arbitrary string of bytes.",
      "Object keys, like traditional filesystem paths, contain forward slashes at access control boundaries. Forward slashes (re- ferred to as the path separator) separate object key path components. An example object key might be videoscarlsagangloriousdawn.mp4, where the path compo- nents are videos, carlsagan, and gloriousdawn.mp4. Unless otherwise requested, we encrypt paths before they ever leave the customers applications computer. Object (or File) An object (or file) is the main data type in our system. An object is re- ferred to by an object key, contains an arbitrary amount of bytes, and has no mini- mum or maximum size. An object is represented by an ordered collection of one or more segments. Segments have a fixed maximum size. An object also supports a limited amount of keyvalue user-defined fields called object metadata. Like object keys, the data contained in an object is encrypted before it ever leaves the client computer.",
      "Object metadata Object metadata are user defined keyvalue fields that are associated with an object. Object metadata are stored encrypted. Segment A segment represents a single array of bytes, between 0 and a Satellite-defined maximum segment size. See section 4.8.2 for more details. Remote Segment A remote segment is a segment that will be erasure encoded and distributed across the network. A remote segment is larger than the metadata re- quired to keep track of its bookkeeping, which includes information such as the IDs of the nodes that the data is stored on. Chapter 4. Concrete implementation Figure 4.2: Files, segments, stripes, erasure shares, and pieces Chapter 4. Concrete implementation Inline Segment An inline segment is a segment that is small enough where the data it represents takes less space than the corresponding data a remote segment will need to keep track of which nodes had the data. In these cases, the data is stored inline instead of being stored on nodes.",
      "Encryption block An encryption block is a small fixed amount of bytes that is used as an encryption boundary size. Authenticated encryption happens on encryption blocks individually (with monotonically increasing nonces). All segments are encrypted in encryption blocks. Encryption blocks are often an integer multiple of the stripe size for alignment reasons. Stripe A stripe is a further subdivision of a segment. A stripe is a fixed amount of bytes that is used as an erasure encoding boundary size. Erasure encoding happens on stripes individually. Inline segments do not have erasure encoding, and thus only remote segments erasure encode stripes. A stripe is the unit on which audits are performed. See section 4.8.3 for more details. Erasure Share When a stripe is erasure encoded, it generates multiple pieces called era- sure shares. Only a subset of the erasure shares are needed to recover the original stripe.",
      "Each erasure share has an index identifying which erasure share it is (e.g., the first, the second, etc.). Piece When a remote segments stripes are erasure encoded into erasure shares, the erasure shares for that remote segment with the same index are concatenated to- gether, and that concatenated group of erasure shares is called a piece. If there are n erasure shares after erasure encoding a stripe, then there are n pieces after pro- cessing a remote segment. The ith piece is the concatenation of all of the ith erasure shares from that segments stripes. See section 4.8.5 for more details. Pointer A pointer is a data structure that either contains the inline segment data, or keeps track of which storage nodes the pieces of a remote segment were stored on, along with other per-file metadata.",
      "Peer classes Our overall strategy extends from our previous version 35 and also heavily mirrors dis- tributed storage systems such as the Google File System 26 (and other GFS-like sys- tems 27,48,49) and the Lustre distributed file system 28. In every case, there are three major actors in the network: metadata servers, object storage servers, and clients. Ob- ject storage servers hold the bulk of the data stored in the system. Metadata servers keep track of per-object metadata and where the objects are located on object storage servers. Clients provide a coherent view and easy access to files by communicating with both the metadata and object storage servers. Lustres architecture is proven for high performance. The majority of the top 100 fastest supercomputers use Lustre for their high-performance, scalable storage 28. While we dont expect to achieve equal performance over a wide-area network, we expect dramat- ically better performance than other architectures.",
      "Any limitation, if any, we experience in performance will be due to factors besides our overall architecture. Chapter 4. Concrete implementation Our previous version used different names for each component. What we previously referred to as Storj Share, we now refer to as simply storage nodes. Our formerly central- ized single Bridge instance can now be run by anyone and is referred to as a Satellite. Our libstorj library will be made to be backwards compatible where possible, but we now refer to client software as Uplinks. Storage node The main duty of the storage node is to reliably store and return data. Node operators are individuals or entities that have excess hard drive space and want to earn income by renting their space to others. These operators will download, install, and configure Storj software locally, with no account required anywhere.1 They will then configure disk space and per-Satellite bandwidth allowance.",
      "During Satellite registration and check-in, storage nodes will advertise how much bandwidth and hard drive space is available, and their designated STORJ token wallet address. To simplify lifecycle management for ephemeral files, storage nodes also keep track of optional per-piece time-to-live, or TTL, designations. Pieces may be stored with a specific TTL expiry where data is expected to be deleted after the expiration date. If no TTL is provided, data is expected to be stored indefinitely. This means storage nodes have a database of expiration times and must occasionally clear out old data. Storage nodes must additionally keep track of signed bandwidth allocations (see sec- tion 4.17) to send to Satellites for later settlement and payment. This also requires a small database. Storage nodes can choose with which Satellites to work. If they work with multiple Satellites (the default behavior), then payment may come from multiple sources on vary- ing payment schedules.",
      "Storage nodes are paid by specific Satellites for (1) returning data when requested in the form of egress bandwidth payment, and for (2) storing data at rest. Storage nodes are expected to reliably store all data sent to them and are paid with the assumption that they are faithfully storing all data. Storage nodes that fail random audits will be \"disqualified\" and thus removed from the pool, can lose held funds to cover addi- tional costs, and will receive limited to no future payments. Storage nodes are not paid for the initial transfer of data to store (ingress bandwidth). This is to discourage storage nodes from deleting data only to be paid for storing more, which became a problem with our previous version 35. While storage nodes are paid for repair egress bandwidth us- age, some Satellites may opt to pay less than normal retrieval egress bandwidth usage. Storage nodes are not paid for Satellite registration operations or any other maintenance traffic.",
      "Storage nodes will support three primary methods: get, put, and delete. Each method will take a piece ID, a Satellite ID, a signature from the associated Satellite instance, and a 1Registration with a US-1099 tax form service may be required. Chapter 4. Concrete implementation bandwidth allocation (see section 4.17). The Satellite ID forms a namespace. An identical piece ID with a different Satellite ID refers to a different piece. The put operation will take a stream of bytes and an optional TTL and store the bytes such that any subrange of bytes can be retrieved again via a get operation. Get opera- tions are expected to work until the TTL expires (if a TTL was provided) or until a delete or garbage collection (4.19) operation is received, whichever comes first. Storage nodes will allow administrators to configure maximum allowed disk space. They will keep track of how much is remaining, and reject operations that do not have a valid signature from the appropriate Satellite.",
      "Storage nodes also support a garbage collection system, where they can retrieve a probabilistic data structure called a Bloom filter 50 that indicates which pieces are no longer tracked and can be safely deleted. The storage node has been released as open source software. Node identity During setup, storage nodes, Satellites, and Uplinks all generate their own identity and certificates for use in the network. Each node will operate its own certificate authority, which requires a publicprivate key pair and a self-signed certificate. The certificate authoritys private key will ideally be kept in cold storage to prevent key compromise. Its important that the certificate authority private key be managed with good operational security because key rotation for the certificate authority will require a brand new node ID. Figure 4.3: The different keys and certificates that compose a storage nodes overall identity. Each row represents a privatepublic key pair. Chapter 4.",
      "Concrete implementation The public key of the nodes certificate authority determines its node ID. As in SKadem- lia 32, the node ID will be the hash of the public key and will serve as a proof of work for joining the network. Unlike Bitcoins proof of work 23, the proof of work will be depen- dent on how many trailing zero bits one can find in the hash output. This cost is meant to make Sybil attacks prohibitively expensive and time consuming. Each node will have a revocable leaf certificate and key pair that is signed by the nodes certificate authority. Nodes use the leaf key pair for communication. Each leaf has a signed timestamp that Satellites keep track of per node. Should the leaf become com- promised, the node can issue a new leaf with a later timestamp. Interested peers will make note of newly seen leaf timestamps and reject connections from nodes with older leaf certificates.",
      "As an optimized special case, peers will not need to make a note when the leaf certificate and certificate authority share the same timestamp. Peer-to-peer communication We are using a self-made gRPC-like 51 application protocol (DRPC). For security, it runs on top of either Transport Layer Security (TLS) 52 or the Noise Protocol Framework in IK mode 53. That layer then runs on top of TCP or QUIC (over UDP). TCP and QUIC provide reliable, ordered delivery; TLS and Noise provide privacy and authentication; and DRPC provides multiplexing and a convenient programmer interface. Noise is used in certain cases to reduce round trips due to connection handshakes in situations where the data is already encrypted and forward secrecy isnt necessary. When using authenticated communication such as TLS or Noise, every peer can as- certain the ID of the node with which it is speaking by validating the certificate chain and hashing its peers certificate authoritys public key.",
      "It can then be estimated how much work went into constructing the node ID by considering the number of trailing zero bits at the end of the ID. Satellites can configure a minimum proof of work required to pass an audit (section 4.13) such that, over time, the network will require greater proofs of work due to natural user intervention. Node discovery At this point, we have storage nodes and we have the means to identify and commu- nicate with them if we know their address. We must account for the fact that storage nodes will often be on consumer internet connections and behind routers with con- stantly changing IP addresses. Therefore, the node discovery systems goal is to imple- ment a means to look up a nodes latest address by node ID, somewhat similar to the role DNS provides for the public internet. In our per-Satellite node discovery cache, each Satellite stores information to be able to communicate with the nodes in its network, as well as data needed to select nodes on Chapter 4.",
      "Concrete implementation which to store data. This caching service will live independently in each Satellite and is our primary source of truth for DNS-like functionality for node lookups. When a node joins the network, it reaches out to each Satellite in its Satellite trustlist (see 4.18 for details) indicating that it is available and willing to accept data from the Satellite. In return, the Satellite confirms or denies the success of the connection. If it is successful, the Satellite stores the nodes available disk space, STORJ wallet address, IP address, and any other metadata the Satellite needs in its node discovery cache. The node will subsequently check-in with each of its Satellites on an ongoing basis, perhaps once per hour, to notify the Satellites of any updates. If a node has not commu- nicated with a Satellite after a certain amount of time, the Satellite will reach out to the node to check its status.",
      "If it can no longer successfully reach the node, the Satellite will stop suggesting that node to clients. Redundancy We use the Reed-Solomon erasure code 54. To implement our solution for reducing the effects of long-tails (see section 3.4.2), we choose 4 numbers for each object that we store, k, m, o, and n, such that k m o n. The standard Reed-Solomon numbers are k and n, where k is the minimum required number of pieces for reconstruction, and n is the total number of pieces generated during creation. Figure 4.4: The relationship between k, m, o, and n. The minimum safe and optimal values, respectively, are m and o. The value m is cho- sen such that if a Satellite notices the amount of available pieces has fallen below m, it triggers a repair immediately in an attempt to make sure we always maintain k or more pieces (m is called r0 in Giroire et al. 34).",
      "To achieve our long-tail performance improve- ments 37,38,41,42, the value o is chosen such that during uploads and repairs, as soon as o pieces have finished uploading, remaining pieces up to n are canceled. Furthermore, o is chosen such that storing o pieces is all that is needed to achieve the desired durability goals; n is thus chosen such that storing n pieces will be excess durability. The amount of long tail uploads we can tolerate is no, and thus is the amount of slow nodes to which we are immune. The amount of nodes that can go temporarily offline at Chapter 4. Concrete implementation the same time without triggering a repair is o  m. The safety buffer to avoid losing the data between the time we recognize the data requires a repair and the actual repair is executed is m  k. See section 7.3 for how we select our Reed-Solomon numbers. Also see section 4.14 for a discussion about how we repair data as its durability drops over time.",
      "Structured file storage 4.8.1 Object metadata Many applications benefit from being able to keep metadata alongside files. Amazon S3 supports object metadata 55 to assist with this need. This functionality is called ex- tended attributes in many POSIX compatible systems. Every object will include a limited set of user-specified key-value pairs that will be stored alongside other metadata about the object. 4.8.2 Objects as Segments In our previous version 35, the term shard referred to pieces on storage nodes, whereas sharding referred to segmenting a file into smaller chunks for easier processing. With the addition of erasure coding in our previous version, these terms became somewhat con- fusing, so we have decided to distinguish each meaning with new words. The sharding process is now called segmenting, and the highest level subdivision of an objects stream of data is called a segment. Unfortunately, there is general inconsis- tency using these terms in the literature.",
      "GFS refers to segments as chunks 26. Lustre refers to segments as stripes 28, but we use the term stripes for a further subdelineation. A file may be small enough that it consists of only one segment. If that segment is smaller than the metadata required to store it on the network, the data will be stored inline with the metadata.2 We call this an inline segment. For larger files, the data will be broken into one or more large remote segments. Seg- menting in this manner offers numerous advantages to security, privacy, performance, and availability. As in other distributed storage systems 2628,48,49, segmenting large files (e.g. videos) and distributing the segments across the network reduces the impact of 2The Linux file system Ext4 performs the same optimization with inline inodes 56. Chapter 4. Concrete implementation content delivery on any given node, as bandwidth demands are distributed more evenly across the network.",
      "As with our previous version 35, standardized sizes help frustrate attempts to determine the content of a given segment and can help obscure the flow of data through the network. In addition, the end user can take advantage of parallel trans- fer, similar to BitTorrent 57 or other peer-to-peer networks. Lastly, capping the size of segments allows for more uniform storage node filling. Thus, a node only needs enough space to store a segment to participate in the network, and a client doesnt need to find nodes that have enough space for a large file. 4.8.3 Segments as Stripes In many situations, its important to access a subsection of a larger piece of data. Some file formats, such as video files or disk images, support seeking, where only a subset of the data is needed for read operations. As the creators of audio CDs discovered, its useful to be able to decode small parts of a segment to support these operations 36.",
      "For this purpose, a stripe defines a subset of a segment and should be no more than a couple of kilobytes in size. Encryption happens on an encryption block boundary, which may be a small multiple of stripes, whereas erasure encoding happens on a single stripe at a time. Because we use authenticated encryption, every encryption block has a slight overhead, so slightly larger encryption sizes are preferred. However, audits happen on stripes, and we want audit bandwidth usage to be small. For the reader familiar with the zfec library, in filefec mode, zfec refers to a stripe as a chunk 40. Chapter 4. Concrete implementation 4.8.4 Stripes as Erasure Shares As discussed in sections 3.4 and 4.7, erasure codes give us the chance to control net- work durability in the face of unreliable storage nodes. Stripes are the boundary by which we perform erasure encoding. In a (k, n) erasure code scheme, n erasure shares are generated for every stripe 54.",
      "For example, perhaps a stripe is broken into 40 erasure shares (n  40), where any 20 (k  20) are needed to reconstruct the stripe. Each of the 40 erasure shares will be 120th the size of the original stripe. Erasure encoding a single stripe at a time allows us to read small portions of a large segment without retrieving the entire segment first 36. It also allows us to stream data into the network without staging it beforehand, and it enables a number of other useful features. See section 7.3.3 for a breakdown of how varying the erasure code parameters affects availability and redundancy. 4.8.5 Erasure Shares as Pieces Because stripes are already small, erasure shares are often much smaller, and the metadata to keep track of all of them separately will be immense relative to their size. All n erasure shares have a well-defined index associated with them. More specifically, for a fixed stripe and any given n, the ith share of an erasure code will always be the same.",
      "As with the zfec librarys filefec mode 40, instead of keeping track of all of the erasure Chapter 4. Concrete implementation shares separately, we pack all of the erasure shares with the same index into a piece. In a (k, n) scheme, there are n pieces, where each piece i is the ordered concatenation of all of the erasure shares with index i. As a result, where each erasure share is 1kth of a stripe, each piece is 1kth of a segment, and only k pieces are needed to recover the full segment. A piece is what we store on a storage node. Satellites generate a brand-new, randomly chosen root piece ID each time a new up- load begins. The Uplink will keep the root piece ID secret and send a node-specific piece ID to each storage node, formed by taking the Hash-based Message Authentication Code (HMAC) of the root piece ID and the nodes ID. This serves to obscure what pieces belong together from storage nodes. The root piece ID is stored in the pointer.",
      "Storage nodes namespace pieces by Satellite ID. If a piece ID used by one Satellite is reused by another Satellite, each Satellite can safely assume the shared piece ID refers to a different piece than the other Satellite, with different content and lifecycle. 4.8.6 Pointers The data owner will need knowledge of how a remote segment is broken up and where in the network the pieces are located to recover it. This is contained in the pointer data structure. A pointer includes: which nodes are storing the pieces, encryption information, era- sure coding details, the repair threshold amount that determines how much redundancy a segment must lose before triggering a repair, the amount of pieces that must be stored to consider a repair to be successful, and other details. If the segment is an inline seg- ment, the pointer contains the entire segments binary data instead of which nodes store the pieces.",
      "In our previous version 35, we used two data structures to keep track of the afore- mentioned kinds of information: frames and pointers. In this version, we have combined these data structures into a single data structure and elected to call the new combined data structure a pointer. Metadata The metadata storage system in the Storj network predominantly stores pointers. Other individual components of the Storj network communicate with the pointer database to store and retrieve pointers by path to perform actions. The most trivial implementation for the metadata storage functionality we require will be to simply have each user use their preferred trusted database, such as MongoDB, Mari- aDB, Couchbase, PostgreSQL, SQLite 58, Cassandra 59, Spanner 60, or CockroachDB, to name a few. In many cases, this will be acceptable for specific users, provided those Chapter 4. Concrete implementation users are managing appropriate backups of their metadata.",
      "Indeed, the types of users who have petabytes of data to store can most likely manage reliable backups of a single relational database storing only metadata. On one hand, there are a few downsides to letting clients manage their metadata in a traditional database system, such as:  Availability - The availability of the users data is tied entirely to the availability of their metadata server. The counterpoint is that availability can be made arbitrar- ily good with existing trusted distributed solutions, such as Cassandra, Spanner, or CockroachDB, assuming an appropriate amount of effort is put into maintaining operations. Furthermore, any individual metadata service downtime does not af- fect the rest of the network. In fact, the network as a whole can never go down. Durability - If the metadata server suffers a catastrophic failure without backups, all of the users data will be lost.",
      "This is already true with encryption keys, but a tra- ditional database solution considerably increases the risk area from using encryp- tion keys. Fortunately, the metadata itself can be periodically backed up into the Storj network. This in turn allows us to only keep track of the metadata of this meta- data, further decreasing the amount of critical information that must be stored else- where. Trust - The user has to trust the metadata server. On the other hand, there are a few upsides:  Control - The user is in complete control of all of their data. There is no organiza- tional single point of failure. The user is free to choose whatever metadata store with whatever trade-offs they prefer and can even run their own. Like Mastodon 61, this solution is still decentralized. Furthermore, in a catastrophic scenario, this design is no worse than most other technologies or techniques application developers fre- quently use (databases).",
      "Simplicity - Other projects have spent multiple years on shaky implementations of Byzantine-fault tolerant consensus metadata storage, with expected performance and complexity trade-offs (see appendix A). We can get a useful product to market without doing this work at all. This is a considerable advantage. Coordination Avoidance - Users only need to coordinate with other users on their Satellite. If a user has high throughput demands, they can set up their own Satellite and avoid coordination overhead from any other user. By allowing Satellite oper- ators to select their own database, this will allow a user to choose a Satellite with weaker consistency semantics, such as Highly Available Transactions 15, that re- duce coordination overhead within their own Satellite and increase performance even further. Our launch goal is to allow customers to store their metadata in a database of their choosing.",
      "We expect and look forward to new systems and improvements specifically in this component of our framework. Please see appendix A for more about why weve chosen to currently avoid trying to Chapter 4. Concrete implementation solve the problem of Byzantine distributed consensus. See section 6.2 for a discussion of future plans. 4.10 Satellite The collection of services that hold this metadata is called the Satellite. Users of the net- work will have accounts on a specific Satellite instance, which will: store their file meta- data, manage authorization to data, keep track of storage node reliability, repair and maintain data when redundancy is reduced, and issue payments to storage nodes on the users behalf. Notably, a specific Satellite instance does not necessarily constitute one server. A Satellite may be run as a collection of servers and be backed by a horizontally scalable trusted database for higher uptime.",
      "Storj implements a thin-client model that delegates trust around managing files lo- cation metadata to the Satellite service which manages data ownership. Uplinks are thus able to support the widest possible array of client applications, while Satellites re- quire high uptime and potentially significant infrastructure, especially for an active set of files. Like storage nodes, the Satellite service has been developed and is released as open source software. Any individual or organization can run their own Satellite to facilitate network access. The Satellite is, at its core, one of the most complex and yet straightforward compo- nents of our initial release that fulfills our framework. Notwithstanding future framework- conforming releases, the initial Satellite is a standard application server that wraps a trusted database, such as PostgreSQL, Cassandra, or whichever solution the metadata system chooses (section 4.9). Users sign in to a specific Satellite with account credentials.",
      "Data available through one Satellite instance is not available through another Satellite instance, though various levels of export and import are planned (section 6.2). With respect to customer data, the Satellite is never given data unencrypted and does not hold encryption keys. The only knowledge of an object that the Satellite is able to share with third parties is its existence, rough size, and other metadata such as access patterns. This system protects the clients privacy and gives the client complete control over access to the data, while delegating the responsibility of keeping files available on the network to the Satellite. Clients may use Satellites run by a third-party. Because Satellites store almost no data and have no access to keys, this is a large improvement over the traditional data-center model. Many of the features Satellites provide, like storage node selection and reputation, leverage considerable network effects.",
      "Reputation data sets grow more useful as they in- crease in size, indicating that there are strong economic incentives to share infrastructure and information in a Satellite. Providers may choose to operate public Satellites as a service. Application develop- ers then delegate trust regarding the location of their data on the network to a specific Chapter 4. Concrete implementation Satellite, as they would to a traditional object store but to a lesser degree. Future updates will allow for various distributions of responsibilities, and thus levels of trust, between cus- tomer applications and Satellites.",
      "The Satellite instance is made up of these components:  A full node discovery cache (section 4.6)  A per-object metadata database indexed by encrypted path (section 4.9)  An account management and authorization system (section 4.12)  A storage node reputation, statistics, and auditing system (section 4.13)  A data repair service (section 4.14)  A storage node payment service (section 4.16) While our launch goal of many Satellites is a step ahead of our previous systems Bridge implementation 35, this is still just one point on our decentralization journey and we expect to continue to find ways to decentralize our components further. 4.11 Encryption Our encryption choice is authenticated encryption, with support for both the AES-GCM cipher and the Salsa20 and Poly1305 combination NaCl calls Secretbox 62. Authenti- cated encryption is used so that the user can know if anything has tampered with the data.",
      "Data is encrypted in blocks that may be small batches of stripes, recommended to be 4KB or less 63. While the same encryption key is used for every encryption batch in a segment, segments may have different encryption keys. However, the nonce for each encryption batch must be monotonically incrementing from the previous batch throughout the entire segment. The nonce wraps around to 0 if the counter reaches the maximum representable nonce. To prevent reordering attacks, the starting nonce of each segment is deterministically chosen based on the segment number. When mul- tiple segments are uploaded in parallel, such as in the case of Amazon S3s multipart- upload feature, the starting nonce for each segment can be calculated from the starting nonce of the file and the segment number. This scheme protects the content of the data from the storage node housing the data. The data owner retains complete control over the encryption key, and thus over access to the data. Paths are also encrypted.",
      "Like BIP32 44, the encryption is hierarchical and determin- istic, and each path component is encrypted separately. To explain how we do this, we start with a scheme for determining a secret value for each path component. Lets say a given path p has unencrypted path components p1, p2, . . . , pn and we want to determine an encrypted path e with path components e1, e2, . . . , en. We assume a predetermined root secret, s0. This root secret is chosen by the user and, like all other encryption secrets, never leaves the client computer. We recursively define si  HMAC(si1, pi). A key K(si) can be deterministically generated from si. We then define the encrypted path component Chapter 4. Concrete implementation ei  enc(K(si1), pi), such that the new path e is e1, e2, . . . , en. HMAC-SHA256 or HMAC- SHA512 are used for key derivations. This construction allows a client to share access to some subtree of the path without access to its parents or other paths of the same depth.",
      "For example, suppose a client would like to share access to all paths with the same prefix p1, p2, p3 with another client. The client would give the other client e1, e2, e3 and s3. This allows the client to decrypt and access any arbitrary e4, as K(s3) is known to them, without allowing the client to decrypt e3 or earlier. More generally in this case, the client could decrypt and access any arbitrary ei, if and only if i  3. Path encryption is enabled by default but is otherwise optional, as encrypted paths make efficient sorted path listing challenging. When path encryption is in use (a per- bucket feature), objects are sorted by their encrypted path name, which is deterministic but otherwise relatively unhelpful when the client application is interested in sorted, un- encrypted paths. For this reason, users can opt out of path encryption. When path en- cryption is disabled, unencrypted paths are only revealed to the users chosen Satellite, but not to the storage nodes.",
      "Storage nodes continue to have no information about the path and metadata of the pieces they store. 4.12 Authorization Encryption protects the privacy of data while allowing for the identification of tamper- ing, but authorization allows for the prevention of tampering by disallowing clients from making unauthorized edits. Users who are authorized will be able to add, remove, and edit files, while users who are not authorized will not have those abilities. Metadata op- erations will be authorized. Users will authenticate with their Satellite, which will allow them access to various operations according to their authorization configuration. Our initial metadata authorization scheme uses macaroons 64. Macaroons are a type of bearer token that authorizes the bearer to some restricted resources. Macaroons are especially interesting in that they allow for rich contextual decentralized delegation.",
      "In other words, they provide the property that anyone can add restrictions in a way in which those restrictions cannot later be removed, without coordination with a central party. We use macaroons to restrict which operations can be applied and to which en- crypted paths they can be applied. In this way, macaroons provide a mechanism to restrict delegated access to specific encrypted path prefixes, specific files, and specific operations, such as read only access or perhaps append only access. Each account has a root macaroon and operations are validated against a supplied macaroons set of caveats. Our macaroons are further caveated with optional expirations and revocation tokens, which allow users to revoke macaroons programmatically. Because we want to restrict Satellite operations, and Satellites only have access to en- crypted paths, our authorization scheme must work on encrypted paths. For access del- Chapter 4.",
      "Concrete implementation egation to specific path prefixes, path separation boundaries between path components must remain across encryption. This implies reduced functionality andor performance for path delimiters other than a forward slash. Once the Uplink is authorized with the Satellite, the Satellite will approve and sign for operations to storage nodes, including bandwidth allocations (section 4.17). The Uplink must retrieve valid signatures from the Satellite prior to operations with storage nodes. All operations on a storage node require a specific Satellite ID and associated signature. A storage node will reject operations not signed by the appropriate Satellite ID. Storage nodes will not allow operations signed by one Satellite to apply to objects owned by an- other, unless explicitly granted by the owning Satellite. Our initial implementation does not detect or attempt to mitigate unexpected file re- moval or rollback by a misbehaving Satellite.",
      "Our trust model expects that a users Satel- lite is well-behaved and stores and repairs data reliably. If a Satellite cannot be trusted, it is unlikely to repair data on a clients behalf anyway. However, a future implementa- tion could add more thorough detection for Satellite-based file system tampering, via a scheme as in systems such as SUNDR, SiRiUS, or Plutus 6567. 4.13 Audits In a network with untrusted nodes, validating that those nodes are returning data ac- curately and otherwise behaving as expected is vital to ensuring a properly functioning system. Audits are a way to confirm that nodes have the data they claim to have. Auditors, such as Satellites, will send a challenge to a storage node and expect a valid response. A challenge is a request to the storage node in order to prove it has the expected data.",
      "Some distributed storage systems, including the previous version of Storj 35, discuss Merkle tree proofs, in which audit challenges and expected responses are generated at the time of storage as a form of proof of retrievability 45. By using a Merkle tree 68, the amount of metadata needed to store these challenges and responses is negligible. Proofs of retrievability can be broadly classified into limited and unlimited schemes 47. The Merkle tree variety used in our previous version is one such limited scheme. Unfortunately, in such a scheme, the challenges and expected responses must be pre- generated. As we learned with our previous version, without a periodic regeneration of these challenges, a storage node can begin to pass most audits without storing all of the requested data by keeping track of which challenges exist and then saving only the expected responses. During our previous version, we began to consider Reed-Solomon erasure coding to help us solve this problem.",
      "An assumption in our storage system is that most storage nodes behave rationally, and incentives are aligned such that most data is stored faithfully. As long as that assumption holds, Reed-Solomon is able to detect errors and even correct them, via mechanisms such as the Berlekamp-Welch error correction algorithm 37, 69. We are already using Chapter 4. Concrete implementation Reed-Solomon erasure coding 54 on small ranges (stripes), so as discussed in the HAIL system 39, we use erasure coding to read a single stripe at a time as a challenge and then validate the erasure share responses. This allows us to run arbitrary audits without pre-generated challenges. To perform an audit, we first choose a stripe. We request that stripes erasure shares from all storage nodes responsible. We then run the Berlekamp-Welch algorithm 37,69 across all the erasure shares. When enough storage nodes return correct information, any faulty or missing responses can easily be identified.",
      "Given a specific storage node, an audit might reveal that it is offline or incorrect, or un- available. If a node is offline, it is simply marked as offline. A node is moved into what we call containment mode if it responds to the initial audit services dial but fails to send the requested data, perhaps because its busy fulfilling other requests and doesnt respond to the audit in time. In this mode, the Satellite will calculate and save the expected response, then continue to try the same audit with that node until the node either responds suc- cessfully, actively fails the audit, or is disqualified for being offline too long. Once the node responds successfully, it leaves containment mode. All audit failures will be stored and saved in the reputation system. Audits addition- ally serve as opportunity to test storage node latency, throughput, responsiveness, and uptime. This data will also be saved in the reputation system.",
      "It is important that every storage node has a frequent set of random audits to gain statistical power on how well-behaved that storage node is operating. However, as dis- cussed in section 3.7, it is not a requirement that audits are performed on every byte, or even on every file. Additionally, it is important that every byte stored in the system has an equal probability of being checked for a future audit to every other byte in the system. See section 7.2 for a discussion on how many audits are required to be confident data is stored correctly. 4.14 Data repair As storage nodes go offlinetaking their pieces with themit will be necessary for the missing pieces to be rebuilt once each segments pieces fall below the predetermined threshold, m. If a node goes offline, the Satellite will mark that nodes file pieces as miss- ing. The node discovery systems caches have reasonably accurate and up-to-date infor- mation about which storage nodes have been online recently.",
      "When a storage node changes state from recently online to offline, this can trigger a lookup in a reverse in- dex within a users metadata database, identifying all segment pointers that were stored on that node. For every segment that drops below the appropriate minimum safety threshold, m, Chapter 4. Concrete implementation the segment will be downloaded and reconstructed, and the missing pieces will be re- generated and uploaded to new nodes. Finally, the pointer will be updated to include the new information. Users will choose their desired durability with their Satellite which may impact price and other considerations. This desired durability (along with statistics from ongoing au- dits) will directly inform what Reed-Solomon erasure code choices will be made for new and repaired files, and what thresholds will be set for when uploads are successful and when repair is needed. See sections 3.4 and 7.3 for how we calculate these values given user inputs.",
      "A direct implication of this design is that, for now, the Satellite must constantly stay running. If the users Satellite stops running, repairs will stop, and data will eventually disappear from the network due to node churn. This is similar to the design of how value storing and republishing works in Kademlia 8, which requires the owner to stay online. The ingress (or inbound) bandwidth demands of the audit and repair system are large, but given standard configuration, the egress (or outbound) demands are relatively small. A large amount of data comes into the system for audits and repairs, but only the formerly missing pieces are sent back out. While the repair and audit system can run anywhere, the bandwidth usage asymmetry means that hosting providers which offer free ingress make for an especially attractive hosting location for users of this system.",
      "4.14.1 Piece hashes Data repair is an ongoing, costly operation that will use significant bandwidth, memory, and processing power, often impacting a single operator. As a result, repair resource us- age should be aggressively minimized as much as possible. For repairing a segment to be effective at minimizing bandwidth usage, only as few pieces as needed for reconstruction should be downloaded. Unfortunately, Reed-Solomon is insufficient on its own for correcting errors when only a few redundant pieces are pro- vided. Instead, piece hashes provide a better way to be confident that were repairing the data correctly. To solve this problem, hashes of every piece will be stored alongside each piece on each storage node. A validation hash that the set of hashes is correct will be stored in the pointer. During repair, the hashes of every piece can be retrieved and validated for cor- rectness against the pointer, thus allowing each piece to be validated in its entirety.",
      "This allows the repair system to correctly assess whether or not repair has been completed successfully without using extra redundancy for the same task. Chapter 4. Concrete implementation 4.15 Storage node reputation Reputation metrics on decentralized networks are a critical part of enabling cooperation between nodes where progress would be challenging otherwise. Reputation metrics are used to ensure that bad actors within the network are eliminated as participants, improving security, reliability, and durability. Storage node reputation can be divided into four subsystems. The first subsystem is a proof of work identity system, the second subsystem is the initial vetting process, the third subsystem is a filtering system, and finally, the fourth system is a preference system. The goal of the first system is to require a short proof that the storage node operator is invested, through time, stake, or resources. Initially, we are using proof of work.",
      "As men- tioned in section 4.3, storage nodes require a proof of work as part of identity generation. This helps the network avoid some Sybil attacks 70, but we glossed over how proof of work difficulty is set. We will let Satellite operators set per-Satellite minimum difficulty required for new data storage. If a storage node has an identity generated with a lower difficulty than the Satellites configured minimum, that storage node will not be a candi- date for new data. We expect Satellite operators to naturally increase the minimum proof of work difficulty requirements over time until a reasonable balance is found. In the case of a changing difficulty configuration, Satellites will leave existing data on existing nodes where possible. Other investment proof schemes are possible, such as a form of proof of stake as we proposed in our previous work 71. The second subsystem slowly allows nodes to join the network. When a storage node first joins the network, its reliability is unknown.",
      "As a result, it will be placed into a vetting process until enough data is known about it. We propose the following way to gather data about new nodes without compromising the integrity of the network. Every time a file is uploaded, the Satellite will select a small number of additional unvetted storage nodes to include in the list of target nodes. The Reed-Solomon parameters will be chosen such that these unvetted storage nodes will not affect the durability of the file, but will allow the network to test the node with a small fraction of data until we are sure the node is reliable. After the storage node has successfully stored enough data for a long enough period (at least one payment period), the Satellite will then start including that storage node in the standard selection process used for general uploads. It will also give the node a signed message claiming that the vetting process is completed.",
      "Importantly, storage nodes get paid during this vetting period, but dont receive as much data. The filtering system is the third subsystem; it blocks bad storage nodes from partici- pating. In addition to simply not having done a sufficient proof of work, certain actions a storage node can take are disqualifying events. The reputation system will be used to filter these nodes out from future uploads, regardless of where the node is in the vetting process. Actions that are disqualifying include: failing too many audits; failing to return data, with reasonable speed; and failing too many uptime checks. If a storage node is disqualified, that node will no longer be selected for future data Chapter 4. Concrete implementation storage and the data that node stores will be moved to new storage nodes. Likewise, if a client attempts to download a piece from a storage node that the node should have stored and the node fails to return it, the node will be disqualified.",
      "Importantly, storage nodes will be allowed to reject and fail put operations without penalty, as nodes will be allowed to choose which Satellite operators to work with and which data to store. Its worth reiterating that failing too many uptime checks is a disqualifying event. Stor- age nodes can be taken down for maintenance, but if a storage node is offline too much, it can have an adverse impact on the network. If a node is offline during an audit, that specific audit should be retried until the node responds successfully or is disqualified, to prevent nodes from selectively failing to respond to audits. After a storage node is disqualified, the node must go back through the entire vetting process again. If the node decides to start over with a brand-new identity, the node must restart the vetting process from the beginning (in addition to generating a new node ID via the proof of work system). This strongly disincentivizes storage nodes from being cavalier with their reputation.",
      "The last subsystem is a preference system. After disqualified storage nodes have been filtered out, remaining statistics collected during audits will be used to establish a prefer- ence for better storage nodes during uploads. These statistics include performance char- acteristics such as throughput and latency, history of reliability and uptime, geographic location, and other desirable qualities. They will be combined into a load-balancing se- lection process, such that all uploads are sent to qualified nodes, with a higher likelihood of uploads to preferred nodes, but with a non-zero chance for any qualified node. Ini- tially, well be load balancing with these preferences via a randomized scheme, such as the Power of Two Choices 72, which selects two options entirely at random, and then chooses the more qualified between those two.",
      "On the Storj network, preferential storage node reputation is only used to select where new data will be stored, both during repair and during the upload of new files, unlike disqualifying events. If a storage nodes preferential reputation decreases, its file pieces will not be moved or repaired to other nodes. There is no process planned in our system for storage nodes to contest their reputa- tion scores. It is in the best interest of storage nodes to have good uptime, pass audits, and return data. Storage nodes that dont do these things are not useful to the network. Storage nodes that are treated by Satellites unfairly will not accept future data from those Satellites. Initially, storage node reputation will be individually determined by each Satellite. If a node is disqualified by one Satellite, it may still store data for other Satellites. Reputation will not initially be shared between Satellites. Over time, reputation will be determined globally. Chapter 4.",
      "Concrete implementation 4.16 Payments In the Storj network, payments are made by clients who store data on the platform to the Satellite they utilize. The Satellites then pays storage nodes for the amount of storage and bandwidth they provide on the network. Payments by clients may be through any mechanism (STORJ, credit card, invoice, etc.), but payments to storage nodes are via the Ethereum-based ERC20 73 STORJ token. Previous distributed systems have handled payments as hard-coded contracts. For example, the previous Storj network utilized 90-day contracts to maintain data on the network. After that period of time, the file was deleted. Other distributed storage plat- forms use 15-day renewable contracts that delete data if the user does not login every 15 days. Others use 30-day contracts. We believe that the most common use case is in- definite storage. To best solve this use case, our network will no longer use contracts to manage payments and file storage durations.",
      "The default assumption is that data will last indefinitely. Satellites will pay storage nodes for the data they store and for piece downloads. Stor- age nodes will not be paid for the initial transfer of data, but they will be paid for storing the data month-by-month. At the end of the payment period, a Satellite will calculate earnings for each of its storage nodes. Provided the storage node hasnt been disquali- fied, the storage node will be paid by the Satellite for the data it has stored over the course of the month, per the Satellites records. Satellites have a strong incentive to prefer long-lived storage nodes. If storage node churn is too high, Satellites will hold back a portion of a storage nodes payment until the storage node has maintained good participation and uptime for some minimum amount of time, on the order of greater than half a year. If a storage node leaves the network prematurely, the Satellite will reclaim held payments to it.",
      "If a storage node misses a delete command due to the node being offline, it will be storing more data than the Satellite credits it for. Storage nodes are not paid for storing such file pieces, but will eventually be cleaned up through the garbage collection pro- cess (see section 4.19). This means that storage nodes who maintain higher availability can maximize their profits by deleting files on request, which minimizes the amount of garbage data they store. The Satellite maintains a database of all file pieces it is responsible for and the storage nodes it believes are storing these pieces. Each day, the Satellite adds another days worth of accounting to each storage node for every file piece it will be storing. Satellites will track utilized bandwidth (see section 4.17). At the end of the month, each Satellite adds up all bandwidth and storage payments each storage node has earned and makes the payments to the appropriate storage nodes.",
      "Satellites will also earn revenue from account holders for executing audits, repairing segments, and storing metadata. Satellites charge a per-segment and per-byte cost, in addition to charging for access and retrieval. Per-segment charges cover the cost of Chapter 4. Concrete implementation pointer metadata, whereas per-byte charges cover the cost of data maintenance on the network. Every day, each Satellite will execute a number of audits across all of its storage nodes on the network. The Satellite will charge for both completing audits and repairs, once segments fall below the piece threshold needed for repair. When it is detected that a storage node acts maliciously and does not store files prop- erly or maintain sufficient availability, it will not be paid for the services rendered, and the funds allocated to it will instead be used to repair any missing file pieces and to pay new storage nodes for storing the data.",
      "To reduce transaction fees and other overhead as much as possible, payments must be worth at least some minimum value. Certain Satellites may elect to use a portion of the storage nodes payout to cover transaction fees in part or in whole. See the Satellite reputation section (4.18) for details on how storage nodes will be able to choose which Satellites they trust. 4.17 Bandwidth allocation A core component of our system requires knowing how much bandwidth is used be- tween two peers. In our previous version 35,74, we used exchange reports to gather information about what transpired between two peers. At the end of an operation, both peers would send reports to a central collection service for settlement. When both peers mutually agreed, it was straightforward to determine how much bandwidth had been used.",
      "When they disagreed, however, we resorted to data analysis and regression to determine which peer had a greater propensity for dishonesty in an effort to catch cheaters (or, self-rational nodes). With our new version, we want to make cheating impossible from the protocol level. To solve this problem, we turn to Neumans Proxy-based authorization and account- ing for distributed systems 75. This accounting protocol more correctly measures re- source usage in a delegated and decentralized way. In Neumans accounting protocol, if an account holder has enough funds to cover the operation, an account server will create a signed, digital check and transfer it to the ac- count holder. The protocol refers to this check as a proxy, but we refer to it as a bandwidth allocation in this paper and as an order limit in our code.",
      "This check contains information identifying the account server, the payer, the payee, the maximum amount of resources available to be used in the operation, a check number to prevent any double spending problems 76, and an expiration date. In our case, the account server is the Satellite, the payer is the Uplink, the payee is the storage node, and the resource in question is bandwidth. The Satellite will only create a bandwidth allocation if the Uplink is authorized for the request. At the beginning of a Chapter 4. Concrete implementation storage operation, the Uplink can transfer bandwidth allocation to a storage node. The storage node can validate the Satellites signature and perform the requested operation up to the allowed bandwidth limit, storing and later sending the bandwidth allocation to the Satellite for payment. Were further inspired by Filecoins off-chain retrieval market, wherein only small amounts of data are transferred at a time 77.",
      "Instead of allowing the storage node to cheat and save the bandwidth allocation without performing the requested operation, we break each operation into smaller requests such that if either the storage node or Uplink stop participating in the protocol prematurely, neither peer class is exposed to too much loss. This is similar to an optimistic, gradual-release, fair-exchange protocol 76. To support this with Neumans accounting protocol and little Satellite overhead, we use restricted bandwidth allocations (referred to as restricted proxies in 75 and referred to as orders in our codebase, contrasted with the Satellites order limits). Neumans re- stricted proxies work much like Macaroons 64 in which further caveats can be added in a way that cant be removed, limiting the capabilities of the proxy. Proxies can use pub- licprivate key cryptography, which means that anyone can validate the proxy, instead of just the original issuer.",
      "Because each Uplink already has a key pair as part of its iden- tity (section 4.4), we use the existing key pair instead of creating a new key pair for every restriction. Restricted bandwidth allocations, in our case, are restricted by the Uplink to limit the bandwidth allocations value to only what has transferred so far. In this way, the storage node will only keep the largest bandwidth allocation it has received up to that point, and the Uplink will only send bandwidth allocations that are slightly larger than what it has received. The storage node has no incentive to keep more than the largest allocation, as they all share the same check number, which can only be cashed once. In the case of a Get operation, assume the Satellite-signed bandwidth allocation al- lows up to x bytes total. The Uplink will start by sending a restricted allocation for some small amount (y bytes), perhaps only a few kilobytes, so the storage node can verify the Uplinks authorization.",
      "If the allocation is signed correctly, the storage node will transfer up to the amount listed in the restricted allocation (y bytes) before awaiting another allo- cation. The Uplink will then send another allocation where y is larger, continuing to send allocations for data until y has grown to the full x value. For each transaction, the storage node only sends previously-unsent data, so that the storage node only sends x bytes to- tal. As seen in Figure 4.6, we pipeline these requests to avoid pipeline stall performance penalties. If the request is terminated at any time, either planned or unexpectedly, the storage node will keep the largest restricted bandwidth allocation it has received. This largest restricted bandwidth allocation is the signed confirmation by the Uplink that the Uplink agreed to bandwidth usage of up to y bytes, along with the Satellites confirmation of the Uplinks bandwidth allowance x. The storage node will periodically send the largest Chapter 4.",
      "Concrete implementation Figure 4.5: Diagram of a put operation Figure 4.6: Diagram of a get operation Chapter 4. Concrete implementation restricted bandwidth allocations it has received to appropriate Satellites, at which point Satellites will add to the overall owed totals for later payment. If the Uplink cant afford the bandwidth usage, the Satellite will not sign an bandwidth allocation, protecting the Satellites reputation. Likewise, if the Uplink tries to use more bandwidth than allocated, the storage node will decline the request. The storage node can only get paid for the maximum amount a client has agreed to, as it otherwise has no valid bandwidth allocations to return for payment. As before, we dont measure all peer-to-peer traffic. This bandwidth traffic measure- ment system only tracks bandwidth used during storage operations (storage and re- trievals of pieces). However, it does not apply to node discovery traffic or other generic maintenance overhead.",
      "4.18 Satellite reputation Whenever a Satellite on the Storj network has a less than stellar payment, demand gen- eration, or performance history, there is a strong incentive for the storage nodes to avoid accepting its data. Storage nodes can select which Satellites they want to work with and can remove those that they dont. If a Satellite misbehaves, storage nodes can indicate they no longer trust that Satellite. Storage node operators can elect to automatically trust a Storj Labs provided collec- tion of recommended Satellites that adhere to a strict set of quality controls and payment service level agreements (SLAs). To protect storage node operators, if a Satellite operator wants to be included in the Storj-provided approved list, the Satellite operator may be required to adhere to a set of operating, payment, and pricing parameters and to sign a business arrangement with Storj Labs.",
      "4.19 Garbage collection When clients move, replace, or delete data, Satellites, or clients on behalf of Satellites, will notify storage nodes that they are no longer required to store that data. In configurations where delete messages are issued by the client, the metadata system will require proof that deletes were issued to a configurable minimum number of storage nodes. In such a configuration, every time data is deleted, storage nodes that are online and reachable will receive notifications right away. Storage nodes will sometimes be temporarily unavailable and will miss delete mes- sages. In these cases, and in configurations where delete messages are not issued by the client, unneeded data is considered garbage. Satellites only pay for data that they expect to be stored. Storage nodes with lots of garbage will earn less than they otherwise would Chapter 4. Concrete implementation unless a garbage collection system is employed.",
      "For this reason, we introduce garbage collection to free up space on storage nodes. A garbage collection algorithm is a method for freeing no-longer used resources. A precise garbage collector collects all garbage exactly and leaves no additional garbage. A conservative garbage collector, on the other hand, may leave some small proportion of garbage around given some other trade-offs, often with the aim of improving perfor- mance. As long as a conservative garbage collector is used in our system, the payment for storage owed to a storage node will be high enough to amortize the cost of storing the garbage. For the nodes that miss initial delete messages if they are sent, our first release will start with a conservative garbage collection strategy, though we anticipate a precise strategy in the future. Periodically, storage nodes will request a data structure to detect differences.",
      "In the simplest form, it can be a hash of stored keys, which allows efficient detection of out-of-sync state. After detecting out-of-sync state, collection can use an- other structure, such as a Bloom filter 50, to find out what data has not been deleted. By returning a data structure tailored to each node on a periodic schedule, a Satellite can give a storage node the ability to clean up garbage data to a configurable tolerance. Satellites will reject overly frequent requests for these data structures. 4.20 Uplink Uplink is the term which we use to identify any software or service that invokes libuplink in order to interact with Satellites and storage nodes. It comes in a few forms: Libuplink - libuplink is a library that provides access to storing and retrieving data in the Storj network. Gateways - Gateways act as compatibility layers between a service or application and the Storj network.",
      "They run as a service co-located with wherever data is generated, and will communicate directly with storage nodes so as to avoid central bandwidth costs. The Gateway is a simple service layer on top of libuplink. Our first gateway is an Amazon S3 gateway. It provides an S3-compatible, drop-in interface for users and applications that need to store data but dont want to bother with the complexities of distributed storage directly. Uplink CLI - The Uplink CLI is a command line application which invokes libuplink, al- lowing its user to upload and download files, create and remove buckets, manage file permissions, and other related tasks. It aims to provide an experience familiar to what you might expect when using LinuxUNIX tools such as scp or rsync. Like storage nodes and Satellites, the Uplink software in all three forms has been de- veloped and released as open source software. 5.",
      "Walkthroughs The following is a collection of common use case examples of different types of transac- tions of data through the system. Upload When a user wants to upload a file, the user first begins transferring data to an instance of the Uplink. The Uplink chooses an encryption key and starting nonce for the first segment and begins encrypting incoming data with authenticated encryption as it flows to the network. The Uplink buffers data until it knows whether the incoming segment is short enough to be an inline segment or a remote segment. Inline segments are small enough to be stored on the Satellite itself. The rest of this walkthrough will assume a remote segment because remote seg- ments involve the full technology stack. The Uplink sends a request to the Satellite to prepare for the storage of this first seg- ment. The request object contains API credentials, such as macaroons, and identity certificates.",
      "Upon receiving the request, the Satellite will:  Confirm that the Uplink has appropriate authorization and funds for the request. The Uplink must have an account with this specific Satellite already. Make a selection of nodes with adequate resources that conform to the buckets configured durability, performance, geographic, and reputation requirements. Return a list of nodes, along with their contact information and unrestricted band- width allocations (section 4.17), and a chosen root piece ID. Next, the Uplink will take this information and begin parallel connections to all of the chosen storage nodes while measuring bandwidth (section 4.17). The Uplink will begin breaking the segment into stripes and then erasure encode each stripe. The generated erasure shares will be concatenated into pieces as they transfer to each storage node in parallel. The erasure encoding will be configured to over-encode to more pieces than needed.",
      "This will eliminate the long tail effect and lead to a significant improvement of visible performance by allowing the Uplink to cancel the slowest uploads. Chapter 5. Walkthroughs  The data will continue to transfer until the maximum segment size is hit or the stream ends, whichever is sooner. All of the hashes of every piece will be written to the end of each piece stream. After that, the storage node will store: the largest restricted bandwidth allocation (sec- tion 4.17); the TTL of the segment, if one exists; and the data itself. The data will be iden- tified by the storage node-specific piece ID and the delegating Satellite ID. If the upload is aborted for any reason, the storage node will keep the largest restricted bandwidth allocation it received from the client Uplink on behalf of the Satellite, but will throw away all other relevant request data.",
      "Assuming success:  The Uplink encrypts the random encryption key it chose for this file, utilizing a de- terministic hierarchical key. The Uplink will upload a pointer object back to the Satellite, which contains the fol- lowing information:  which storage nodes were ultimately successful  what encrypted path was chosen for this segment  which erasure code algorithm was used  the chosen piece ID  the encrypted encryption key and other metadata  the hash of the piece hashes  a signature Finally, the Uplink will then proceed with the next segment, continuing to process segments until the entire stream has completed. Each segment gets a new encryption key, but the segments starting nonce monotonically increases from the previous seg- ment.",
      "Once the last segment stored, the Uplink will send additional metadata:  how many segments the stream contained  how large the segments are, in bytes  the starting nonce of the first segment  any other object metadata Periodically, the storage nodes will later send the largest restricted bandwidth allo- cation (section 4.17) they received as part of the upload to the appropriate Satellite for payment. If an upload happens via the Amazon S3 multipart-upload interface, each part is up- loaded as a segment individually. Chapter 5. Walkthroughs Download When a user wants to download an object, first the user sends a request for data to the Uplink. The Uplink then tries to reduce the number of round trips to the Satellite by spec- ulatively requesting metadata about the object along with the first segments pointer.",
      "If the Uplink knows about the requested byte range, the Uplink may just tell the Satellite which byte ranges are needed and the Satellite can respond with the appropriate seg- ment pointers. For every segment pointer requested, the Satellite will:  Validate that the Uplink has access to download the segment pointer and has enough funds to pay for the download. Generate an unrestricted bandwidth allocation (section 4.17) for each piece that makes up the segment. Look up the contact information for the storage nodes listed in the pointer. Return the requested segment pointer, the bandwidth allocations, and node con- tact info for each piece. The Uplink will determine whether more segments are necessary for the data request it received, and will request the remaining segment pointers if needed.",
      "Once all necessary segment pointers have been returned, if the requested seg- ments are not inline, the Uplink will initiate parallel requests while measuring band- width (section 4.17) to all appropriate storage nodes for the appropriate erasure share ranges inside of each stored piece. Because not all erasure shares are necessary for recovery, long tails will be eliminated and a significant and visible performance improvement will be gained by allowing the Uplink to cancel the slowest downloads. The Uplink will combine the retrieved erasure shares into stripes and decrypt the data. If the download is aborted for any reason, each storage node will keep the largest restricted bandwidth allocation (section 4.17) it received, but it will throw away all other relevant request data. Either way, the storage nodes will later send the largest restricted bandwidth allocation they received as part of the download to the appropriate Satellite for later payment.",
      "Delete When a user wants to delete a file, the delete operation is first received by the Uplink. The Uplink then requests all of the segment pointers for the file. If the client and Satellite are configured to issue direct deletes to nodes, for every seg- ment pointer, the Satellite will: Chapter 5. Walkthroughs  Validate that the Uplink has access to delete the segment pointer. Generate a signed agreement for the deletion of the segment, so the storage node knows the Satellite is expecting the delete to proceed. Look up the contact information for the storage nodes listed in the pointer. Return the segments, the agreements, and contact information. For all of the remote segments, the Uplink will initiate parallel requests to all appro- priate storage nodes to signal that the pieces are being removed.",
      "The storage nodes will return a signed message indicating either that the storage node received the delete operation and will delete both the file and its bookkeeping information or that it was already removed. The Uplink will upload all of the signed messages that it received from working stor- age nodes back to the Satellite. The Satellite will require an adjustable percent of the total storage nodes to successfully sign messages to ensure that the Uplink did its part in notifying the storage nodes that the object was deleted. The Satellite will remove the segment pointers and stop charging the customer and stop paying the storage nodes for them. The Uplink will return a success status. Some Satellite and Uplink configurations may elect to simply delete the metadata directly and let the garbage collection system recover the free space on storage nodes.",
      "Periodically, storage nodes will ask the Satellite for generated garbage collection mes- sages that will update storage nodes who were offline during the main deletion event. Satellites will reject requests for garbage collection messages that happen too frequently. See section 4.19 for more details. Move When a user wants to move a file, first, the Uplink receives a request for moving the file to a new path. Then, the Uplink requests all of the segment pointers of that file. For every segment pointer, the Satellite:  Validates that the Uplink has access to download it. Returns the requested segment metadata. For every segment pointer, the Uplink:  Decrypts the metadata with an encryption key derived from the path. Calculates the path at the new destination. Re-encrypts the metadata with a new encryption key derived from the new path. The Uplink requests that the Satellite add all modified segment pointers and remove all old segment pointers in an atomic compare-and-swap operation.",
      "Chapter 5. Walkthroughs The Satellite will validate that:  The Uplink has appropriate authorization to remove the old path and create the new path. The content of the old path hasnt changed since the overall operation started. If the validation is successful, the Satellite will perform the operation. No storage node will receive any request related to the file move. Because of the complexity around atomic pointer batch modifications, efficient move operations may not be implemented in the first release of this network. List When a user wants to list files:  First, a request for listing a page of objects is received by the Uplink. Then, the Uplink will translate the request on unencrypted paths to encrypted paths. Next, the Uplink will request from the Satellite the appropriate page of encrypted paths. After that, the Satellite will validate that the Uplink has appropriate access and then return the requested list page. Finally, the Uplink will decrypt the results and return them.",
      "Audit Each Satellite has a queue of segment stripes that will be audited across a set of stor- age nodes. This queue is filled when the Satellite chooses a stripe to audit by identifying storage nodes that have had fewer recent audits than other storage nodes. The Satellite will select a stripe at random from the data contained by that storage node. Because the audit must also retrieve data from the other nodes in the erasure coded set, many other nodes are audited as well. Because segments have a maximum size, this also sufficiently approximates our goal of choosing a byte to audit uniformly at random. Satellites will then work to process the queue and report errors. For each stripe request, the Satellite will perform the entire download operation for that small stripe range, filtering out nodes that are in containment mode. Unlike standard downloads, the stripe request does not need to be performant.",
      "The Satel- lite will attempt to download all of the erasure shares for the stripe and will wait for slow storage nodes. After receiving as many shares as possible within a generous timeout, the erasure shares will be analyzed to discover which, if any, are wrong. Satellites will take note of storage nodes that return invalid data, and if a storage node returns too much invalid Chapter 5. Walkthroughs data, the Satellite will disqualify the storage node from future exchanges. In the case of a disqualification, the Satellite will not pay the storage node going forward, and it will not select the storage node for new data. For storage nodes that did not respond, a cryptographic checksum of the expected audit result will be created and stored, placing the unresponsive nodes in contain- ment. While in containment, a node will continue to be given only the audit it was unresponsive for until it passes or is disqualified.",
      "For storage nodes that do respond but the response is judged to be incorrect, the full piece will be requested from the storage node and then compared with the expected hash of that piece 4.14.1. This allows the Satellite to differentiate between dishonest storage nodes and dishonest Uplinks. Data repair The repair process has two parts. The first part detects unhealthy files, and the second part repairs them. Detection is straightforward. Each Satellite will periodically ping every storage node it knows about, either as part of the audit process or via standard node discovery ping operations. The Satellite will keep track of nodes that fail to respond and mark them as down. When a node is marked down or is marked bad via the audit process, the pointers that point to that storage node will be considered for repair. Pointers keep track of their minimum allowable redundancy. If a pointer is not stored on enough good, online storage nodes, it will be added to the repair queue.",
      "A worker process will take segment pointers off the repair queue. When a segment pointer is taken off the repair queue:  The worker will download enough pieces to reconstruct the entire segment, along with the piece hashes stored with those pieces (see section 4.14.1). Unlike audits, only enough pieces for accurate repair are needed. Unlike streaming downloads, the repair system can wait for the entire segment before starting. The piece hashes are validated against the signature in the pointer, and then the downloaded pieces are validated against the validated piece hashes. Incorrect pieces are thrown away and count against the source as failed audits. Once enough correct pieces are recovered, the missing pieces are regenerated. The Satellite selects some new nodes and uploads the new pieces to those new nodes via the normal upload process. The Satellite updates the pointers metadata. Payment The payment process works as follows: Chapter 5.",
      "Walkthroughs  First, the Satellite will choose a rollup period. This is a period of timedefaulting to a daythat payment for data at rest is calculated. This is purely a period chosen for accounting; actual payments will happen on a less frequent schedule. During each roll-up period, a Satellite will consider all of the files it believes are cur- rently stored on each storage node. Satellites will keep track of payments owed to each storage node for each rollup period, based on the data kept on each storage node. Storage nodes will periodically send in bandwidth allocation reports (section 4.17). Finally, when Satellite are ready to pay storage nodes, the Satellite will calculate the owed funds due to bandwidth allocation along with the outstanding data at rest calculations. It then sends the funds to the storage nodes requested wallet address. 6. Future work Storj is a work in progress, and many features are planned for future versions.",
      "In this chapter, we discuss a few especially interesting areas in which we want to consider im- provements to our concrete implementation. Hot files and content delivery Occasionally, users of our system may end up delivering files that are more popular than anticipated. While storage node operators might welcome the opportunity to be paid for more bandwidth usage for the data they already have, demand for these popular files might outstrip available bandwidth capacity, and a form of dynamic scaling is needed. Fortunately, Satellites already authorize all accesses to pieces, and can therefore meter and rate limit access to popular files. If a files demand starts to grow more than current resources can serve, the Satellite has an opportunity to temporarily pause accesses if nec- essary, increase the redundancy of the file over more storage nodes, and then continue allowing access. Reed-Solomon erasure coding has a very useful property.",
      "Assume a (k, n) encoding, where any k pieces are needed of n total. For any non-negative integer number x, the first n pieces of a (k, n  x) encoding are the exact same pieces as a (k, n) encoding. This means that redundancy can easily be scaled with little overhead. As a practical example, suppose a file was encoded via a (k  20, n  40) scheme, and a Satellite discovers that it needs to double bandwidth resources to meet demand. The Satellite can download any 20 pieces of the 40, generate just the last 40 pieces of a new (k  20, n  80) scheme, store the new pieces on 40 new nodes, andwithout changing any data on the original 40 nodesstore the file as a (k  20, n  80) scheme, where any 20 out of 80 pieces are needed. This allows all requests to adequately load balance across the 80 pieces. If demand outstrips supply again, only 20 pieces are needed to generate even more redundancy.",
      "In this manner, a Satellite could temporarily increase redundancy to (20, 250), where requests are load balanced across 250 nodes, such that every piece of all 250 are unique, and any 20 of those pieces are all that is required to regenerate the original file. On one hand, the Satellite will need to pay storage nodes for the increased redun- dancy, so content delivery in this manner has increased at-rest costs during high de- mand, in addition to bandwidth costs. On the other hand, content delivery is often de- sired to be highly geographically redundant, which this scheme provides naturally. Chapter 6. Future work Improving user experience around metadata In our initial concrete implementation, we place significant burdens on the Satellite op- erator to maintain a good service level with high availability, high durability, regular pay- ments, and regular backups.",
      "Over time, clients of Satellites will want to reduce their dependence on Satellite oper- ators and enjoy more efficient data portability between Satellites besides downloading and uploading their data manually. We plan to spend significant time on improving this user experience in a number of ways. In the short term, we plan to build a metadata importexport system, so users can make backups of their metadata on their own and transfer their metadata between Satellites. In the medium term, we plan to reduce the size of these exports considerably and make as much of this backup process as automatic and seamless as possible. We expect to build a system to periodically back up the major portion of the metadata directly to the network. In the long term, we plan to architect the Satellite out of the platform. We hope to eliminate Satellite control of the metadata entirely via a viable Byzantine-fault tolerant consensus algorithm, should one arise.",
      "The biggest challenge to this is finding the right balance between coordination avoidance and Byzantine fault tolerant consensus, where storage nodes can interact with one another and share encoded pieces of files while still operating within the performance levels users will expect from a platform that is com- peting with traditional cloud storage providers. Our team will continue to research viable means to achieve this end. See section 2.10 and appendix A for discussions on why we arent tackling the Byzan- tine fault tolerant consensus problem right away. 7. Selected calculations Object repair costs A fundamental challenge in our system is how to not only choose the system parameters that keep the expansion factor and repair bandwidth to a minimum but also provide an acceptable level of durability. Fortunately, we are not alone in wondering about this, and there is a good amount of prior research on the problem.",
      "Peer-to-Peer Storage Systems: a Practical Guideline to be Lazy 34 is an excellent guide, and much of our work follows from their conclusions. The end result is a mathematical framework which determines network durability and repair bandwidth given Reed-Solomon parameters, average node lifetime, and reconstruction rate. The following is a summary of results and explanation of their implications. Variable Description MTTF Mean time to failure 1MTTF Mean reconstruction time 1MRT Total bytes on the network Total number of pieces per segment (RS encoding) Pieces needed to rebuild a segment (RS encoding) Repair threshold Loss rate 1-LR Durability Expansion factor Ratio of data that is repair bandwidth Total repair bandwidth in the network LR  (m  1) ln(nm) (k  1)! mk2 ED  nk BR  \u03b1(n  m  k) k ln(nm) BWR  D  BR The equations demonstrate that repair bandwidth is impacted by node churn linearly, which is expected. Lower mean time to node failure triggers more frequent rebuilds and, Chapter 7.",
      "Selected calculations therefore, more bandwidth usage. Loss rate is much more sensitive to high node churn, as it increases exponentially with \u03b1. This necessitates very stable nodes, with lifetimes of several months, to achieve acceptable network durability. See section 7.3 for a more in-depth discussion of how node churn affects erasure code parameters. 7.1.1 Bandwidth limits usable space Repair affects storage nodes participation beyond their bandwidth usage; it also con- strains the amount of usable disk space. Consider a storage node with 1 TB of available space, with a stated monthly bandwidth limit of 500 GB. If its known (via the above frame- work) that a storage node can expect to repair 50 of its data in a given month, and as- suming each stored object is served at least once, then we can store no more than 333 GB on this node since anything more than that causes more bandwidth than allowed.",
      "In other words, paid bandwidth plus repair bandwidth must always be less than or equal to the bandwidth limit. Higher repair rates equal lower effective storage size, but nodes serving paid data more frequently are more sensitive to this effect. In practice, the paid bandwidth rate will vary with the type of data being stored on each node. These ratios must be moni- tored closely to determine appropriate usable space limits as the network evolves over time. Chapter 7. Selected calculations Audit false positive risk We rely on a Bayesian approach to determine the probability that a storage node is main- taining stored pieces faithfully. At a high level, we seek to answer the following question: how do consecutive successful audits change our estimate of the probability that a node will continue to return successful audits? We model the audit process as being a binomial random variable with an unknown probability of success p 0, 1, with each audit being an independent Bernoulli trial.",
      "It is well-known that the conjugate prior of the binomial distribution is the beta distribution \u03b2(a, b), and that the posterior also follows the beta distribution. As in 78, we use the mean of the posterior distribution as our Bayes estimator, which is given by P  (a  x)(a  b  n) where a, b are the parameters of the prior distribution, and x is the number of successes observed in n audits. Under our assumption that each audit is successful, we arrive at the Bayes estimate of the success probability P  (a  n)(a  b  n). Audit success probability Jeffrey's prior Number of audits, each assumed to be successful 0.75 0.80 0.85 0.90 0.95 1.00 Probability estimate Audit success probability estimate Figure 7.1: In Jeffreys prior, we see the estimate for audit success probability is heavily weighted to be near 0 or near 1.",
      "Audit success probability 0.96 0.98 1.00 1.02 1.04 Uniform prior Number of audits, each assumed to be successful 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 Probability estimate Audit success probability estimate Figure 7.2: Using a Uniform prior, there is no assumption placed on the estimated audit success probability, and all probabilities are assumed to be equally likely. We now choose a prior to derive a numerical estimate of the audit success proba- bility based on the number of audits performed. There are many reasonable choices of Bayesian priors, but we restrict our attention to two popular choices: the Uniform prior Chapter 7. Selected calculations and Jeffreys prior 79. Using the Uniform prior \u03b2(1, 1) initializes the experiment by as- signing an equal probability to all possible outcomes; that is, the probability of success is drawn from the uniform distribution on (0, 1).",
      "Under Jeffreys prior \u03b2(0.5, 0.5), it is assumed that the probability of success falls towards either extreme, so that a node will return a successful audit either with probability near 0 or with probability near 1. Number of audits Audit success estimate given uniform prior Audit success estimate given Jeffreys prior 0.9545 0.9762 0.9762 0.9878 0.9878 0.9938 0.99505 0.99751 Table 7.1: Estimate of audit success probability by number of audits, each assumed to be successful. We find that the estimated probability of success begins at 0.5 when there is no information known about the node (no audits have been performed), with the estimate quickly jumping to above 99 in as few as 80 audits using Jeffreys prior. In Table 7.1, we present results obtained from using both priors.",
      "We remark that the well-established Bayesian approach allows us to rapidly gain more confidence in a nodes ability to return a successful audit, given that the success probability estimate tends closer to 1 with each consecutive audit success. Chapter 7. Selected calculations Choosing erasure parameters In the context of storing an erasure-coded segment on a decentralized network, we con- sider the loss of a piece from two different perspectives. 7.3.1 Direct piece loss With direct piece loss, we assume that for a specific segment, its erasure pieces are lost according to a certain rate. We point out that modeling this is straightforward: if pieces are lost at a rate 0  p  1 and we start with n pieces, then piece decay follows an expo- nential decay pattern of the form n(1  p)t, with t being the time elapsed according to the units used for the rate.1 To account for a multiple checks per month, we may extend this to n(1  pa)at.",
      "If m is the rebuild threshold which controls when a segment is rebuilt, we may solve n(1pa)at  m for t (taking the ceiling when necessary) to determine how long it will take for the n pieces of a segment to decay to less than m pieces. This works out to the smallest t for which t  ln(mn) a ln(1pa). Thus it becomes clear, given parameters n, m, a and p, how long we expect a segment to last between repairs. 7.3.2 Indirect piece loss When modeling indirect piece loss, we suppose that a fixed rate of nodes drop out of the network each month,2 whether or not they are holding pieces of the segment under consideration. To describe the probability that d of the dropped nodes were each storing one of the n pieces of a specific segment, we turn to the hypergeometric probability dis- tribution. Suppose c nodes are replaced per month out of C total nodes on the network. Then the probability that d nodes were each storing a piece of the segment is given by P(X  d)  Cn (7.1) which has mean ncC.",
      "We then determine how long it will take for the number of pieces to fall below the desired threshold m by iterating, holding the overall churn c fixed but reducing the number of existing pieces by the distributions mean in each iteration and counting the number of iterations required. For example, after one iteration, the num- ber of existing pieces is reduced by ncC, so instead of n pieces on the network (as the parameter in (7.1)), there are nncC pieces, changing both the parameter and the mean for (7.1) in iteration 2. We may extend this model by considering multiple checks per month (as in the di- rect piece loss case), assuming that ca nodes are lost every 1a-th of a month instead of assuming that c nodes are lost per month, where a is the number of checks per month. This yields an initial hypergeometric probability distribution with mean ncaC. 1So if we assume a proportion of p  0.1 pieces are lost per month, t is given in months.",
      "2Though the rate may be taken over any desired time interval. Chapter 7. Selected calculations In either of these two cases (single or multiple segment integrity checks per month), we track the number of iterations until the number of available pieces fall below the repair threshold. This number may then be used to determine the expected number of rebuilds per month for any given segment. 7.3.3 Numerical simulations for indirect piece loss We produce decision tables (Table 7.2) showcasing worst-case mean segment rebuild outcomes based on simulating piece loss for segments encoded with varying Reed- Solomon parameters. We assume a (k, n) RS encoding scheme, where n pieces are gen- erated, with k pieces needed for reconstruction, using three different values for n. We also assume that a segment undergoes the process of repair when less than m pieces remain on the network, using three different values of m for each n.",
      "For the initial table, we use a simplifying assumption that pieces on the network are lost at a constant rate per month,3 which may be due to node churn, data corruption, or other problems. To arrive at the value for mean rebuilds per month, we consider a single segment that is encoded with n pieces which are distributed uniformly randomly to nodes on the net- work. To simulate conditions leading to a rebuild, we uniformly randomly select a subset of nodes from the total population and designate them as failed. We do this multiple times per (simulated) month, scaling the piece loss rate linearly according to the num- ber of segment integrity checks per month.4 Once enough nodes have failed to bring the number of pieces above the repair thresh- old m, the segment is rebuilt, and we track the number of rebuilds over the course of 24 months. We repeat this simulation for 1000 iterations, simulating 1000 two-year periods for a single segment.",
      "We then take the number of rebuilds at the 99th percentile (or higher) of the number of rebuilds occurring over these 1000 iterations. In other words, we choose the value for which the value of the observed cumulative distribution func- tion (CDF), describing the number of rebuilds over this two-year period, is at least 0.99. This value is then divided by the number of months to arrive at the mean rebuildsmonth value. An example of the approach is shown in Figure 7.3. We perform the experiment on a network of 10,000 nodes, observing that the network size will not directly impact the mean rebuildsmonth value for a single segment under our working assumption of a constant rate of loss per month.5 In forming the decision tables, we consider as part of our calculations how different choices of k, n, m, and mean time to failure affect durability and repair bandwidth. What 3This constant rate may be viewed as the mean of the Poisson distribution modeling piece loss per month.",
      "4For example, if the monthly network piece loss rate is assumed to be 0.1 of the network size (or 10), and if 10 segment integrity checks are performed per month, we assume that, on average, 1 of pieces are lost between checks. 5We represent piece loss as a proportion of nodes selected uniformly randomly from the total network. The proportion scales directly with network size, so the overall number of pieces lost stays the same for networks of different sizes. Chapter 7. Selected calculations Number of rebuilds 0.00 0.05 0.10 0.15 0.20 0.25 Observed density 1000 iterations Rebuilds over 24 months Number of rebuilds Cumulative observed density 1000 iterations Rebuilds over 24 months Figure 7.3: Left: Density for the number of rebuilds over a 24 month period, repeated for 1000 iterations. Right: CDF of the number of rebuilds.",
      "In this case, the mean rebuildsmonth value would be taken as 2624 1.083, with there being a 99.7 chance that a segment is rebuilt at most 26 times over the course of 24 months. we are looking for is the lowest repair bandwidth that also meets our durability require- ments. MTTF (months) Repair Bandwidth Ratio Durability ( nines) 9.36 0.9999 (8) 0.87 0.9999 (17) 0.31 0.9999 (13) 3.40 0.9999 (4) 0.60 0.9999 (15) 0.31 0.9999 (25) 5.21 0.9999 (4) 0.52 0.9999(14) 0.24 0.9999 (11) Table 7.2: Decision tables showing the relationship between churn (MTTF), Reed-Solomon parameters (k, n, m), repair bandwidth ratio, and durability 7.3.4 Conclusion We conclude by observing that these models may be tuned to target specific network scenarios and requirements. One network may require one set of Reed-Solomon param- eters, while a different network may require another. In general, the closer mn is to 1, the more rebuilds per month should be expected under a fixed churn rate.",
      "While having a larger ratio for mn increases file durability for any given churn rate, it comes at the ex- pense of more bandwidth used since repairs are triggered more often. To maintain a low mean rebuildsmonth value while also maintaining a higher file durability, the aim should be to increase the value of n as much as feasible given other network conditions (latency, download speed, etc.), which allows for a lower relative value of m while still not jeopardizing file durability. Chapter 7. Selected calculations Informally, it takes longer to lose more pieces under a given fixed network size and churn rate. Therefore, to maximize durability while minimizing repair bandwidth usage, n should be as large as existing network conditions allow. This allows for a value of m that is relatively closer to k, reducing the mean rebuildsmonth value, which in turn lowers the amount of repair bandwidth used. For example, assume we have a network with a mean time to failure of six months.",
      "Suppose we consider the same file encoded with two different RS parameters: one under a (20, 40) schema and the other as an (30, 80) schema. If we set m so that m  k  10 for both cases, we observe from the above table that the bandwidth repair ratio is 0.87 in the (20, 40) case and is 0.60 in the (40, 80) case. Both encoding schemes have similar durability, as a repair in both cases is triggered when there are k  10 pieces left; even though the mean of rebuilds per month is empirically and theoretically lower for the (40, 80) case using m  k  10. A. Distributed consensus To explain why we are not trying to solve Byzantine distributed consensus, its worth a brief discussion of the history of distributed consensus. Non-Byzantine distributed consensus Computerized data storage systems began by necessity with single computers storing and retrieving data on their own.",
      "Unfortunately, in environments where the system must continue operating at all times, a single computer failure can grind an important process to a halt. As a result, researchers have often sought ways to enable groups of computers to manage data without any specific computer being required for operation. Spreading ownership of data across multiple computers could increase uptime in the face of fail- ures, increase throughput by spreading work across more processors, and so forth. This research field has been long and challenging; but, fortunately, it has led to some really exciting technology. The biggest issue with getting a group of computers to agree is that messages can be lost. How this impacts decision making is succinctly described by the Two Generals Problem 80,1 in which two armies try to communicate in the face of potentially lost messages. Both armies have already agreed to attack a shared enemy, but have yet to decide on a time.",
      "Both armies must attack at the same time or else failure is assured. Both armies can send messengers, but the messengers are often captured by the enemy. Both armies must know what time to attack and that the other army has also agreed to this time. Ultimately, a generic solution to the two generals problem with a finite number of messages is impossible, so engineering approaches have had to embrace uncertainty by necessity. Many distributed systems make trade-offs to deal with this uncertainty. Some systems embrace consistency, which means that the system will choose downtime over inconsistent answers. Other systems embrace availability, which means that the sys- tem chooses potentially inconsistent answers over downtime. The widely-cited CAP the- orem 12, 13 states that every system must choose only two of consistency, availability, and partition tolerance.",
      "Due to the inevitability of network failures, partition tolerance is non-negotiable, so when a partition happens, every system must choose to sacrifice either consistency or availability. Many systems sacrifice both (sometimes by accident). In the CAP theorem, consistency (specifically, linearizability) means that every read receives the most recent write or an error, so an inconsistent answer means the system returned something besides the most recent write without obviously failing. More gener- ally, there are a number of other consistency models that may be acceptable by making various trade-offs. Linearizability, sequential consistency, causal consistency, PRAM con- 1earlier described as a problem between groups of gangsters 81 Chapter A.",
      "Distributed consensus sistency, eventual consistency, read-after-write consistency, etc., are all models for dis- cussing how a history of events appears to various participants in a distributed system.2 Amazon S3 generally provides read-after-write consistency, though in some cases will provide eventual consistency instead 84. Many distributed databases provide eventual consistency by default, such as Dynamo 25 and Cassandra 59. Linearizability in a distributed system is often much more desirable than more weakly consistent models, as it is useful as a building block for many higher level data structures and operations (such as distributed locks and other coordination techniques). Initially, early efforts to build linearizable distributed consensus centered around two-phase com- mit, then three-phase commit, which both suffered due to issues similar to the two gen- erals problem.",
      "In 1985, the FLP-impossibility paper 85 proved that no algorithm could reach linearizable consensus in bounded time. Then in 1988, Barbara Liskov and Brian Oki published the Viewstamped Replication algorithm 86 which was the first lineariz- able distributed consensus algorithm. Unaware of the VR publication, Leslie Lamport set out to prove linearizable distributed consensus was impossible 87, but instead in 1989 proved it was possible by publishing his own Paxos algorithm 88, which became sig- nificantly more popular, even though it wasnt officially published in a journal until 1998. Ultimately, both algorithms have a large amount in common. Despite Lamports claims that Paxos is simple 89, many papers have been published since then challenging that assertion. Googles description of their attempts to imple- ment Paxos are described in Paxos Made Live 90, and Paxos Made Moderately Com- plex 91 is an attempt to try and fill in all the details of the protocol.",
      "The entire basis of the Raft algorithm is rooted in trying to wrangle and simplify the complexity of Paxos 24. Ul- timately, after an upsetting few decades, reliable implementations of Paxos, Raft, View- stamped Replication 92, Chain Replication 93, and Zab 94 now exist, with ongoing work to improve the situation further 95, 96. Arguably, part of Googles early success was in spending the time to build their internal Paxos-as-a-service distributed lock sys- tem, Chubby 97. Most of Googles famous early internal data storage tools, such as Bigtable 98, depend on Chubby for correctness. Spanner 60perhaps one of the most incredible distributed databases in the worldis largely just two-phase commit on top of multiple Paxos groups. Byzantine distributed consensus As mentioned in our design constraints, we expect most nodes to be rational and some to be Byzantine, but few-to-none to be altruistic.",
      "Unfortunately, all of the previous algo- rithms we discussed assume a collection of altruistic nodes. Reliable distributed consen- sus algorithms have been game-changing for many applications requiring fault-tolerant 2If differing consistency models are new to you, it may be worth reading about them in Kyle Kingburys excellent tutorial 82. If youre wondering why computers cant just use the current time to order events, keep in mind it is exceedingly difficult to get computers to even agree on that 83. Chapter A. Distributed consensus storage. However, success has been much more mixed in the Byzantine fault tolerant world. There have been a number of attempts to solve the Byzantine fault tolerant distributed consensus problem. The field exploded after the release of Bitcoin 23, and is still in its early stages.",
      "Of note, we are particularly interested in PBFT 99 (Barbara Liskov once again with the first solution), QU 100, FaB 101 (but see 102), Bitcoin, Zyzzyva 103 (but also see 102), RBFT 104, Tangaroa 105, Tendermint 106, Aliph 107, Hashgraph 108, HoneybadgerBFT 109, Algorand 110, Casper 111, Tangle 112, Avalanche 113, PARSEC 114, and others 115. Each of these algorithms make additional trade-offs, that non-Byzantine distributed consensus algorithms dont require, to deal with the potential for uncooperative nodes. For example, PBFT 99 causes a significant amount of network overhead. In PBFT, every client must attempt to talk to a majority of participants, which must all individually reply to the client. Bitcoin 23 intentionally limits the transaction rate with changing proof-of- work difficulty. Many other post-Bitcoin protocols require all participants to keep a full copy of all change histories.",
      "Why were avoiding Byzantine distributed consensus Ultimately, all of the existing solutions fall short of our goal of minimizing coordination (see section 2.10). Flexible Paxos 96 does significantly better than normal Paxos in the steady-state for avoiding coordination, but is completely unusable in a Byzantine envi- ronment. Distributed ledger or tangle-like approaches suffer from an inability to prune history and retain significant global coordination overhead. We are excited about and look forward to a fast, scalable Byzantine fault tolerant so- lution. The building blocks of one may already be listed in the previous discussion. Until it is clear that one has arisen, we are reducing our risk by avoiding the problem entirely. B. Attacks As with any distributed system, a variety of attack vectors exist. Many of these are com- mon to all distributed systems. Some are storage-specific and will apply to any distributed storage system.",
      "Spartacus Spartacus attacks, or identity hijacking, can occur when any node assumes the identity of another node and receive some fraction of messages intended for that node by sim- ply copying its node ID. This allows for targeted attacks against specific nodes and data. Spartacus attacks can be mitgated by implementing node IDs as public key hashes and requiring messages to be signed 32. A Spartacus attacker in this system would be un- able to generate the corresponding private key, and thus unable to sign messages and participate in the network. Sybil Sybil attacks 70 involve the creation of large amounts of nodes in an attempt to disrupt network operation by hijacking or dropping messages. Our adoption of proof of work identity generation (section 4.4) reduces the vulnerability to a degree 32. Further, our storage node reputation system involves a prolonged initial vetting pe- riod nodes must complete before they are trusted with significant amounts of data.",
      "This vetting system, discussed more in section 4.15, prevents a large influx of new nodes from taking incoming data from existing reputable storage nodes without first proving their longevity. Eclipse An eclipse attack attempts to isolate a node or set of nodes in the network graph by ensuring that all outbound connections reach malicious nodes. Eclipse attacks can be hard to identify, as malicious nodes can be made to function normally in most cases, only eclipsing certain important messages or information. Storj addresses eclipse attacks by using public key hashes as node IDs and signatures based on those public keys 32. The larger the network is, the harder it will be to prevent a node from finding a portion of the network uncontrolled by an attacker. As long as a storage node or Satellite has been introduced to a portion of the network that is not controlled by the attacker at any Chapter B.",
      "Attacks point, the public key hashes and signatures ensure that man-in-the-middle attacks are impossible. Honest Geppetto In this attack, the attacker operates a large number of puppet storage nodes on the net- work, accumulating reputation and data over time. Once a certain threshold is reached, she pulls the strings on each puppet to execute a hostage attack with the data involved, or simply drops each storage node from the network. The best defense against this attack is to create a network of sufficient scale that this attack is ineffective. In the meantime, this can be partially prevented by relatedness analysis of storage nodes. Bayesian infer- ence across downtime, latency, network route, and other attributes can be used to assess the likelihood that two storage nodes are operated by the same organization. Satellites can and should attempt to distribute pieces across as many unrelated storage nodes as possible.",
      "Hostage bytes The hostage byte attack is a storage-specific attack where malicious storage nodes refuse to transfer pieces, or portions of pieces, in order to extort additional payments from clients. The Reed-Solomon encoding ought to be sufficient to defeat attacks of this sort (as the client can simply download the necessary number of pieces from other nodes) unless multiple malicious nodes collude to gain control of many pieces of the same file. The same mitigations discussed under the Honest Geppetto attack can apply here to help avoid this situation. Cheating storage nodes, Uplinks, or Satellites Measuring bandwidth with signatures minimizes the risk for Uplink and storage nodes. The Uplink can only interact with the storage node by sending a signed restricted band- width allocation (section 4.17). The restriction limits the risk to a very low level. The storage node has to comply with the protocol as expected in order to get more restricted allo- cations.",
      "Storage nodes and Satellites will commence a vetting process that limits their exposure. Storage nodes are allowed to decline requests from untrusted Satellites. Faithless storage nodes and Satellites While storage nodes and Satellites are built to require authentication via signatures be- fore serving download requests, it is reasonable to imagine a modification of the storage Chapter B. Attacks node or Satellite that will provide downloads to any paying requestor. Even in a network with a faithless Satellite, data privacy is not significantly compromised. Strong client-side encryption protects the contents of the file from inspection. Storj is not designed to pro- tect against compromised clients. Defeated audit attacks A typical Merkle proof verification requires pre-generated challenges and responses. Without a periodic regeneration of these challenges, a storage node can begin to pass most audits without storing all of the requested data.",
      "Instead, we request a random stripe of erasure shares from all storage nodes. We run the Berlekamp-Welch algorithm 69 across all the erasure shares. When enough storage nodes return correct information, any faulty or missing responses can easily be identified. New storage nodes will be placed into a vetting process until enough audits have passed. See section 4.15 for more details. C. Primary user benefits We have designed the Storj network to provide users better security, availability, perfor- mance, and economicsacross a wide variety of use casesthan either on-premise stor- age solutions or traditional, centralized cloud storage. While the bulk of this paper de- scribes the design considerations to overcome the challenges of a highly decentralized system, this appendix describes why the end result should be a significant improvement over traditional approaches. Security We have designed our system to be the equivalent of spreading encrypted sand on an encrypted beach.",
      "All data is encrypted client-side before reaching our system. Data is sharded and distributed across a large number of independently operated disk drives which are part of a much larger network of independently operated storage nodes. In a typical scenario (with a 2040 Reed-Solomon setup), each file is distributed across 40 different disk drives in a global network of over 100,000 independently operated nodes. (The previous version of the Storj network had over 150,000 independently operated nodes.) To compromise an individual file, a would-be bad actor would have to locate and compromise roughly 40 different drives, each operated by a different provider, in a network of over 100,000 drives. Even if the actor were somehow able to compromise those drives, to reconstruct the file, the would-be bad actor would then have to decrypt 256-bit AES encrypted data, with keys that are only held by the end user.",
      "And, the would- be bad actor would have to repeat this process with an entirely different set of potential drives for the next file they wish to obtain. By design, it is not possible for Storj, Satellite operators, storage node operators, or would-be bad actors to mine or compromise end user data. The level of decentralization on the network creates powerful disincentives for malicious actors, as there is no central- ized trove of data to target. Availability While most centralized cloud providers employ various strategies to provide protection against individual drive failures, they are not immune to system-wide events. Storms, power outages, floods, earthquakes, operator error, design flaws, network overload, or attacks can compromise entire data centers. While the centralized providers may calculate and publish theoretically high availabil- ity numbers, these calculations depend on drive failures being uncorrelated. In fact, in Chapter C.",
      "Primary user benefits any data center, the chances of an individual drive failing is highly correlated with the chances of another drive failing. In a decentralized system, by contrast, each node is operated by a different individ- ual, in a different location, with separate personnel, power, network access, and so forth. Therefore, the chance of an individual node failing is almost entirely uncorrelated with the chances of other drives failing. As a result, the kinds of availability we obtain are not subject to storms, power outages, or other black swan events. Even if the chance of an individual drive failing in the Storj network is higher than in a centralized cloud, the chance of collective failure (e.g. losing 20 out of 40 independent drives) is vanishingly small. In addition, the chance of losing one file is not correlated with the chances of los- ing a second file.",
      "Performance For read-intensive use cases, the Storj network can deliver superior performance by tak- ing advantage of parallelism. The storage nodes are located close to the edge, reducing the latency experienced when recipients of data are physically far from the data center that houses the data. Read performance benefits from parallelism. The particular era- sure coding scheme that we use ensures that slow drives, slow networks, or networks and drives experiencing temporarily high load do not limit throughput. We can adjust the kn ratio so that we dramatically improve download and streaming speeds, without imposing the kinds of high costs associated with CDN networks. Economics While the amount of data created around the world has doubled every year, the price of cloud storage has only declined about 10 per year over the last three years. There are a number of potential explanations, both on the supply and demand side.",
      "Public cloud storage operators must make large capital investments in building out a network of data centers and must incur significant costs for power, personnel, security, fire suppression, and so forth. Their pricing structure must allow them to recoup those costs. Moreover, the structure of the industry is such that it is inherently oligopolistic: there are only a handful of public cloud companies, and they comprise the largest com- panies by market cap on the planet (Microsoft, Google, Amazon, Alibaba). As any price decreases by one provider are quickly matched by the other providers, there has been little incentive for providers to drop prices to gain market share. In a decentralized network, by contrast, there is little marginal cost to being a stor- age node operator. In our experience, the vast majority of operators are using existing live equipment with significant spare capacity. There is no additional cost to a storage node operator in terms of capital or personnel.",
      "Running a drive at full capacity does not Chapter C. Primary user benefits consume significantly more power than running a drive with excess space. And, with careful management relative to caps, most operators should not experience increased bandwidth costs. Consequently, operating a node represents nearly pure margin, and these supply cost savings can be passed on to end users. We have designed market mechanisms on the demand side as well, to prevent any Satellite operator from cornering the market. Even after providing a healthy margin to farmers, demand partners, and Satellite operators, we believe we should be able to pro- vide profitable storage services at a fraction of the cost of equivalent centralized cloud storage providers. Bibliography 1 Identity Theft Resource Center and CyberScout. Annual number of data breaches and exposed records in the United States from 2005 to 2018 (in millions). united-states-by-number-of-breaches-and-records-exposed, 2018.",
      "2 Knowledge Sourcing Intelligence LLP. Cloud storage market - forecasts from 2017 to 2022. https:www.researchandmarkets.comresearchlf8wbxcloud_storage, 2017. 3 Dan Shearer. EU-US Cloud Privacy Crash. 4 Gartner Inc. Gartner forecasts worldwide public cloud revenue to grow 21.4 percent in 2018. forecasts-worldwide-public-cloud-revenue-to-grow-21-percent-in-2018, 2018. 5 Synergy Research Group. Cloud Growth Rate Increased Again in Q1; Amazon Maintains Market Share Dominance. https:www.srgresearch.comarticlescloud- growth-rate-increased-again-q1-amazon-maintains-market-share-dominance, 2018. 6 Backblaze Inc. How Long Do Hard Drives Last: 2018 Hard Drive Stats. 7 Sean Rhea, Dennis Geels, Timothy Roscoe, and John Kubiatowicz. Handling Churn in a DHT. In Proceedings of the Annual Conference on USENIX Annual Technical Conference, ATEC 04, page 10, Berkeley, CA, USA, 2004. USENIX Association. 8 Petar Maymounkov and David Mazi\u00e8res.",
      "Kademlia: A Peer-to-Peer Information System Based on the XOR Metric. In Revised Papers from the First International Workshop on Peer-to-Peer Systems, IPTPS 01, pages 5365, London, UK, 2002. Springer-Verlag. 9 Charles Blake and Rodrigo Rodrigues. High availability, scalable storage, dynamic peer networks: Pick two. In Proceedings of the 9th Conference on Hot Topics in Operating Systems - Volume 9, HOTOS03, page 1, Berkeley, CA, USA, 2003. USENIX Association. 10 Comcast Inc. XFINITY Data Usage CenterFAQ. https:dataplan.xfinity.comfaq, 2018. 11 Amitanand S. Aiyer, Lorenzo Alvisi, Allen Clement, Mike Dahlin, Jean-Philippe Martin, and Carl Porth. BAR Fault Tolerance for Cooperative Services. In Proceedings of the Twentieth ACM Symposium on Operating Systems Principles, SOSP 05, pages 4558, New York, NY, USA, 2005. ACM. BIBLIOGRAPHY 12 Seth Gilbert and Nancy Lynch. Brewers conjecture and the feasibility of consistent, available, partition-tolerant web services.",
      "SIGACT News, 33(2):5159, June 2002. 13 Seth Gilbert and Nancy Lynch. Perspectives on the CAP Theorem. Computer, 45(2):3036, February 2012. 14 Daniel Abadi. Consistency Tradeoffs in Modern Distributed Database System Design: CAP is Only Part of the Story. Computer, 45(2):3742, February 2012. 15 Peter Bailis, Aaron Davidson, Alan Fekete, Ali Ghodsi, Joseph M. Hellerstein, and Ion Stoica. Highly available transactions: Virtues and limitations. Proc. VLDB Endow., 7(3):181192, November 2013. 16 Peter Bailis, Alan Fekete, Michael J. Franklin, Ali Ghodsi, Joseph M. Hellerstein, and Ion Stoica. Coordination avoidance in database systems. Proc. VLDB Endow., 8(3):185196, November 2014. 17 Chenggang Wu, Jose M. Faleiro, Yihan Lin, and Joseph M. Hellerstein. Anna: A KVS for any scale. ICDE, 2018. 18 Joseph M. Hellerstein. The declarative imperative: Experiences and conjectures in distributed logic. SIGMOD Rec., 39(1):519, September 2010. 19 Peter Alvaro, Neil Conway, Joseph M.",
      "Hellerstein, and William R. Marczak. Consistency Analysis in Bloom: a CALM and Collected Approach. CIDR, 2011. 20 Kyle Kingsbury. Consistency models clickable map. https:jepsen.ioconsistency, 2018. 21 Paolo Viotti and Marko Vukoli\u0107. Consistency in non-transactional distributed storage systems. ACM Comput. Surv., 49(1):19:119:34, June 2016. 22 Joseph Hellerstein. Anna: A crazy fast, super-scalable, flexibly consistent KVS. 23 Satoshi Nakamoto. Bitcoin: A peer-to-peer electronic cash system. 24 Diego Ongaro and John Ousterhout. In search of an understandable consensus algorithm. In Proceedings of the 2014 USENIX Conference on USENIX Annual Technical Conference, USENIX ATC14, pages 305320, Berkeley, CA, USA, 2014. USENIX Association. 25 Giuseppe DeCandia, Deniz Hastorun, Madan Jampani, Gunavardhan Kakulapati, Avinash Lakshman, Alex Pilchin, Swaminathan Sivasubramanian, Peter Vosshall, and Werner Vogels. Dynamo: Amazons highly available key-value store.",
      "In Proceedings of Twenty-first ACM SIGOPS Symposium on Operating Systems Principles, SOSP 07, pages 205220, New York, NY, USA, 2007. ACM. BIBLIOGRAPHY 26 Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung. The Google File System. In Proceedings of the Nineteenth ACM Symposium on Operating Systems Principles, SOSP 03, pages 2943, New York, NY, USA, 2003. ACM. 27 Konstantin Shvachko, Hairong Kuang, Sanjay Radia, and Robert Chansler. The Hadoop Distributed File System. In Proceedings of the 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST), MSST 10, pages 110, Washington, DC, USA, 2010. IEEE Computer Society. 28 Lustre. Introduction to Lustre Architecture. 29 J. Rosenberg, R. Mahy, P. Matthews, and D. Wing. Session Traversal Utilities for NAT (STUN). RFC 5389, RFC Editor, October 2008. 30 ISO. ISOIEC 29341-1:2011: Information technology  UPnP Device Architecture, 2011. 31 S. Cheshire and M. Krochmal. NAT Port Mapping Protocol (NAT-PMP).",
      "RFC 6886, RFC Editor, April 2013. http:www.rfc-editor.orgrfcrfc6886.txt. 32 Ingmar Baumgart and Sebastian Mies. SKademlia: A practicable approach towards secure key-based routing. In ICPADS, pages 18. IEEE Computer Society, 2007. 33 P. Mockapetris. Domain names - implementation and specification. STD 13, RFC Editor, November 1987. http:www.rfc-editor.orgrfcrfc1035.txt. 34 Fr\u00e9d\u00e9ric Giroire, Julian Monteiro, and St\u00e9phane P\u00e9rennes. Peer-to-peer storage systems: A practical guideline to be lazy. IEEE Global Communications Conference (GlobeCom), 12 2010. 35 Shawn Wilkinson, Tome Boshevski, Josh Brandoff, James Prestwich, Gordon Hall, Patrick Gerbes, Philip Hutchins, and Chris Pollard. Storj: A peer-to-peer cloud storage network v2.0. https:storj.iostorjv2.pdf, 2016. 36 Vijay K. Bhargava, Stephen B. Wicker, IEEE Communications Society., and IEEE Information Theory Society. Reed-Solomon codes and their applications  edited by Stephen B. Wicker, Vijay K.",
      "Bhargava ; IEEE Communications Society and IEEE Information Theory Society, co-sponsors. IEEE Press Piscataway, NJ, 1994. 37 Jeff Wendling and JT Olds. Introduction to Reed-Solomon. 38 Netanel Raviv, Yuval Cassuto, Rami Cohen, and Moshe Schwartz. Erasure correction of scalar codes in the presence of stragglers. CoRR, abs1802.02265, 2018. BIBLIOGRAPHY 39 Kevin D. Bowers, Ari Juels, and Alina Oprea. HAIL: A High-availability and Integrity Layer for Cloud Storage. In Proceedings of the 16th ACM Conference on Computer and Communications Security, CCS 09, pages 187198, New York, NY, USA, 2009. ACM. 40 Zooko Wilcox. zfec: filefec.pys encode_file. 41 Jeffrey Dean and Luiz Andr\u00e9 Barroso. The Tail at Scale. Communications of the ACM, 56:7480, 2013. 42 Jeffrey Dean and Sanjay Ghemawat. MapReduce: Simplified Data Processing on Large Clusters. Commun. ACM, 51(1):107113, January 2008. 43 J. Paiva and L. Rodrigues. Policies for Efficient Data Replication in P2P Systems.",
      "In 2013 International Conference on Parallel and Distributed Systems, pages 404411, Dec 2013. 44 Peter Wuille. BIP32: Hierarchical Deterministic Wallets. 45 Ari Juels and Burton S. Kaliski, Jr. PORs: Proofs of Retrievability for Large Files. In Proceedings of the 14th ACM Conference on Computer and Communications Security, CCS 07, pages 584597, New York, NY, USA, 2007. ACM. 46 Hovav Shacham and Brent Waters. Compact proofs of retrievability. In Proceedings of the 14th International Conference on the Theory and Application of Cryptology and Information Security: Advances in Cryptology, ASIACRYPT 08, pages 90107, Berlin, Heidelberg, 2008. Springer-Verlag. 47 Kevin D. Bowers, Ari Juels, and Alina Oprea. Proofs of retrievability: Theory and implementation. In Proceedings of the 2009 ACM Workshop on Cloud Computing Security, CCSW 09, pages 4354, New York, NY, USA, 2009. ACM. 48 Michael Ovsiannikov, Silvius Rus, Damian Reeves, Paul Sutter, Sriram Rao, and Jim Kelly. The Quantcast File System.",
      "Proc. VLDB Endow., 6(11):10921101, August 2013. 49 Salman Niazi, Mahmoud Ismail, Seif Haridi, Jim Dowling, Steffen Grohsschmiedt, and Mikael Ronstr\u00f6m. HopsFS: Scaling Hierarchical File System Metadata Using newSQL Databases. In Proceedings of the 15th Usenix Conference on File and Storage Technologies, FAST17, pages 89103, Berkeley, CA, USA, 2017. USENIX Association. 50 Burton H. Bloom. Spacetime trade-offs in hash coding with allowable errors. Commun. ACM, 13(7):422426, July 1970. 51 Google Inc. What is gRPC? https:grpc.iodocsguidesindex.html, Accessed 2018. 52 T. Dierks and E. Rescorla. The Transport Layer Security (TLS) Protocol Version 1.2. RFC 5246, RFC Editor, August 2008. http:www.rfc-editor.orgrfcrfc5246.txt. BIBLIOGRAPHY 53 Trevor Perrin. The Noise Protocol Framework. https:noiseprotocol.orgnoise.pdf, 2018. 54 Irving S. Reed and Gustave Solomon. Polynomial codes over certain finite fields. Journal of the Society for Industrial and Applied Mathematics, 8(2):300304, 1960.",
      "55 Amazon Inc. Amazon Simple Storage Service - Object Metadata. Accessed 2018. 56 Tao Ma. ext4: Add inline data support. https:lwn.netArticles468678, 2011. 57 Bram Cohen. The BitTorrent Protocol Specification. 58 D. Richard Hipp et al. SQLite. https:www.sqlite.org, 2000. 59 Avinash Lakshman and Prashant Malik. Cassandra: A decentralized structured storage system. SIGOPS Oper. Syst. Rev., 44(2):3540, April 2010. 60 James C. Corbett, Jeffrey Dean, Michael Epstein, Andrew Fikes, Christopher Frost, JJ Furman, Sanjay Ghemawat, Andrey Gubarev, Christopher Heiser, Peter Hochschild, Wilson Hsieh, Sebastian Kanthak, Eugene Kogan, Hongyi Li, Alexander Lloyd, Sergey Melnik, David Mwaura, David Nagle, Sean Quinlan, Rajesh Rao, Lindsay Rolig, Dale Woodford, Yasushi Saito, Christopher Taylor, Michal Szymaniak, and Ruth Wang. Spanner: Googles globally-distributed database. In OSDI, 2012. 61 Eugen Rochko and others. Mastodon: Your self-hosted, globally interconnected microblogging community.",
      "https:github.comtootsuitemastodon, 2016. 62 Daniel J. Bernstein. Cryptography in NaCl. 63 Daniel J. Bernstein. NaCl: Validation and verification. 64 Arnar Birgisson, Joe Gibbs Politz, \u00dalfar Erlingsson, Ankur Taly, Michael Vrable, and Mark Lentczner. Macaroons: Cookies with contextual caveats for decentralized authorization in the cloud. In Network and Distributed System Security Symposium, 2014. 65 Jinyuan Li, Maxwell Krohn, David Mazi\u00e8res, and Dennis Shasha. Secure Untrusted Data Repository (SUNDR). In Proceedings of the 6th Conference on Symposium on Opearting Systems Design  Implementation - Volume 6, OSDI04, pages 99, Berkeley, CA, USA, 2004. USENIX Association. 66 Eu-Jin Goh, Hovav Shacham, Nagendra Modadugu, and Dan Boneh. SiRiUS: Securing Remote Untrusted Storage. In NDSS, volume 3, pages 131145, 2003. 67 Mahesh Kallahalla, Erik Riedel, Ram Swaminathan, Qian Wang, and Kevin Fu. Plutus: Scalable Secure File Sharing on Untrusted Storage.",
      "In Proceedings of the 2Nd USENIX Conference on File and Storage Technologies, FAST 03, pages 2942, Berkeley, CA, USA, 2003. USENIX Association. BIBLIOGRAPHY 68 Ralph C. Merkle. A digital signature based on a conventional encryption function. In Carl Pomerance, editor, Advances in Cryptology  CRYPTO 87, pages 369378, Berlin, Heidelberg, 1988. Springer. 69 Lloyd R. Welch and Elwyn R. Berlekamp. Error correction for algebraic block codes. US Patent US4633470A, 1986. 70 John R. Douceur. The Sybil Attack. In Revised Papers from the First International Workshop on Peer-to-Peer Systems, IPTPS 01, pages 251260, London, UK, 2002. Springer-Verlag. 71 Shawn Wilkinson and James Prestwich. SIP02: Bounding Sybil Attacks with Identity Cost, (2016). https:github.comstorjsipsblobmainsip-0002.md. 72 Michael Mitzenmacher. The Power of Two Choices in Randomized Load Balancing. IEEE Trans. Parallel Distrib. Syst., 12(10):10941104, October 2001. 73 Fabian Vogelsteller and Vitalik Buterin.",
      "ERC-20 Token Standard, (2015). 74 Braydon Fuller. SIP09: Bandwidth Reputation and Accounting, (2017). 75 B. C. Neuman. Proxy-based authorization and accounting for distributed systems. In The 13th International Conference on Distributed Computing Systems, pages 283291, May 1993. 76 Bruce Schneier. Applied Cryptography (2nd Ed.): Protocols, Algorithms, and Source Code in C. John Wiley  Sons, Inc., New York, NY, USA, 1995. 77 Protocol Labs. Filecoin: A decentralized storage network. 78 Asit P. Basu, David W. Gaylor, and James J. Chen. Estimating the probability of occurrence of tumor for a rare cancer with zero occurrence in a sample. Regulatory Toxicology and Pharmacology, 23(2):139  144, 1996. 79 Harold Jeffreys. An invariant form for the prior probability in estimation problems. Proceedings of the Royal Society of London. Series A, Mathematical and Physical Sciences, 186(1007):453461, 1946. 80 Jim Gray. Notes on data base operating systems.",
      "In Operating Systems, An Advanced Course, pages 393481, London, UK, 1978. Springer-Verlag. 81 E. A. Akkoyunlu, K. Ekanadham, and R. V. Huber. Some constraints and tradeoffs in the design of network communications. In Proceedings of the Fifth ACM Symposium on Operating Systems Principles, SOSP 75, pages 6774, New York, NY, USA, 1975. ACM. 82 Kyle Kingsbury. Strong consistency models. BIBLIOGRAPHY 83 Justin Sheehy. There is no now. Queue, 13(3):20:2020:27, March 2015. 84 Amazon Inc. Amazon Simple Storage Service - Data Consistency Model. Introduction.htmlConsistencyModel, Accessed 2018. 85 Michael J. Fischer, Nancy A. Lynch, and Michael S. Paterson. Impossibility of distributed consensus with one faulty process. J. ACM, 32(2):374382, April 1985. 86 Brian M. Oki and Barbara H. Liskov. Viewstamped replication: A new primary copy method to support highly-available distributed systems.",
      "In Proceedings of the Seventh Annual ACM Symposium on Principles of Distributed Computing, PODC 88, pages 817, New York, NY, USA, 1988. ACM. 87 Leslie Lamport. The part-time parliament website note. Accessed 2018. 88 Leslie Lamport. The part-time parliament. ACM Trans. Comput. Syst., 16(2):133169, May 1998. 89 Leslie Lamport. Paxos made simple. 2001. 90 Tushar Deepak Chandra, Robert Griesemer, and Joshua Redstone. Paxos made live - an engineering perspective (2006 invited talk). In Proceedings of the 26th Annual ACM Symposium on Principles of Distributed Computing, 2007. 91 Robbert Van Renesse and Deniz Altinbuken. Paxos made moderately complex. ACM Comput. Surv., 47(3):42:142:36, February 2015. 92 Barbara Liskov and James Cowling. Viewstamped replication revisited. Technical Report MIT-CSAIL-TR-2012-021, MIT, July 2012. 93 Robbert van Renesse and Fred B. Schneider. Chain replication for supporting high throughput and availability.",
      "In Proceedings of the 6th Conference on Symposium on Operating Systems Design  Implementation - Volume 6, OSDI04, page 7, Berkeley, CA, USA, 2004. USENIX Association. 94 Flavio Paiva Junqueira, Benjamin C. Reed, and Marco Serafini. Zab: High-performance broadcast for primary-backup systems. IEEEIFIP 41st International Conference on Dependable Systems  Networks (DSN), pages 245256, 2011. 95 Iulian Moraru, David G. Andersen, and Michael Kaminsky. There is more consensus in egalitarian parliaments. In Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles, SOSP 13, pages 358372, New York, NY, USA, 2013. ACM. BIBLIOGRAPHY 96 H. Howard, D. Malkhi, and A. Spiegelman. Flexible Paxos: Quorum intersection revisited. ArXiv e-prints, August 2016. 97 Mike Burrows. The Chubby Lock Service for Loosely-coupled Distributed Systems. In Proceedings of the 7th Symposium on Operating Systems Design and Implementation, OSDI 06, pages 335350, Berkeley, CA, USA, 2006.",
      "USENIX Association. 98 Fay Chang, Jeffrey Dean, Sanjay Ghemawat, Wilson C. Hsieh, Deborah A. Wallach, Mike Burrows, Tushar Chandra, Andrew Fikes, and Robert E. Gruber. Bigtable: A distributed storage system for structured data. In 7th USENIX Symposium on Operating Systems Design and Implementation (OSDI), pages 205218, 2006. 99 Miguel Castro and Barbara Liskov. Practical byzantine fault tolerance. In Proceedings of the Third Symposium on Operating Systems Design and Implementation, OSDI 99, pages 173186, Berkeley, CA, USA, 1999. USENIX Association. 100 Michael Abd-El-Malek, Gregory R. Ganger, Garth R. Goodson, Michael K. Reiter, and Jay J. Wylie. Fault-scalable byzantine fault-tolerant services. In Proceedings of the Twentieth ACM Symposium on Operating Systems Principles, SOSP 05, pages 5974, New York, NY, USA, 2005. ACM. 101 Jean-Philippe Martin and Lorenzo Alvisi. Fast byzantine consensus. IEEE Trans. Dependable Secur. Comput., 3(3):202215, July 2006. 102 I. Abraham, G. Gueta, D.",
      "Malkhi, L. Alvisi, R. Kotla, and J.-P. Martin. Revisiting Fast Practical Byzantine Fault Tolerance. ArXiv e-prints, December 2017. 103 Ramakrishna Kotla. Zyzzyva: Speculative byzantine fault tolerance. ACM Transactions on Computer Systems (TOCS), 27, Issue 4, Article No. 7, December 2009. 104 P. L. Aublin, S. B. Mokhtar, and V. Qu\u00e9ma. RBFT: Redundant Byzantine Fault Tolerance. In 2013 IEEE 33rd International Conference on Distributed Computing Systems, pages 297306, July 2013. 105 Christopher N. Copeland and Hongxia Zhong. Tangaroa: a Byzantine Fault Tolerant Raft, 2014. 106 Jae Kwon. Tendermint: Consensus without mining. 107 Pierre-Louis Aublin, Rachid Guerraoui, Nikola Kne\u017eevi\u0107, Vivien Qu\u00e9ma, and Marko Vukoli\u0107. The Next 700 BFT Protocols. ACM Trans. Comput. Syst., 32(4):12:112:45, January 2015. 108 Leemon Baird. The Swirlds hashgraph consensus algorithm: Fair, fast, byzantine fault tolerance, 2016. BIBLIOGRAPHY 109 Andrew Miller, Yu Xia, Kyle Croman, Elaine Shi, and Dawn Song.",
      "The Honey Badger of BFT Protocols. Cryptology ePrint Archive, Report 2016199, 2016. 110 Yossi Gilad, Rotem Hemo, Silvio Micali, Georgios Vlachos, and Nickolai Zeldovich. Algorand: Scaling byzantine agreements for cryptocurrencies. In Proceedings of the 26th Symposium on Operating Systems Principles, SOSP 17, pages 5168, New York, NY, USA, 2017. ACM. 111 Vitalik Buterin and Virgil Griffith. Casper the friendly finality gadget. CoRR, abs1710.09437, 2017. 112 Serguei Popov. The Tangle. https:iota.orgIOTA_Whitepaper.pdf, 2018. 113 Team Rocket. Snowflake to Avalanche: A Novel Metastable Consensus Protocol Family for Cryptocurrencies. 2018. 114 Pierre Chevalier, Bart\u0142omiej Kami\u0144ski, Fraser Hutchison, Qi Ma, and Spandan Sharma. Protocol for Asynchronous, Reliable, Secure and Efficient Consensus (PARSEC). http:docs.maidsafe.netWhitepaperspdfPARSEC.pdf, 2018. 115 James Mickens. The saddest moment. ;login: logout, May 2013."
    ],
    "word_count": 31363,
    "page_count": 90
  },
  "SUI": {
    "chunks": [
      "The Sui Smart Contracts Platform The MystenLabs Team hellomystenlabs.com INTRODUCTION Sui is a decentralized permissionless smart contract platform biased towards low-latency management of assets. It uses the Move pro- gramming language to define assets as objects that may be owned by an address. Move programs define operations on these typed objects including custom rules for their creation, the transfer of these assets to new owners, and operations that mutate assets. Sui is maintained by a permissionless set of authorities that play a role similar to validators or miners in other blockchain systems. It uses a Byzantine consistent broadcast protocol between authorities to ensure the safety of common operations on assets, ensuring lower latency and better scalability as compared to Byzantine agreement. It only relies on Byzantine agreement for the safety of shared objects. As well as governance operations and check-pointing, performed off the critical latency path.",
      "Execution of smart contracts is also naturally parallelized when possible. Sui supports light clients that can authenticate reads as well as full clients that may audit all transitions for integrity. These facilities allow for trust-minimized bridges to other blockchains. A native asset SUI is used to pay for gas for all operations. It is also used by its owners to delegate stake to authorities to operate Sui within epochs, and periodically, authorities are reconfigured according to the stake delegated to them. Used gas is distributed to authorities and their delegates according to their stake and their contribution to the operation of Sui. This whitepaper is organized in two parts, with Sect. 2 describing the Sui programming model using the Move language, and Sect. 4 describing the operations of the permissionless decentralized sys- tem that ensures safety, liveness and performance for Sui. SUI SMART CONTRACT PROGRAMMING Sui smart contracts are written in the Move4 language.",
      "Move is safe and expressive, and its type system and data model natu- rally support the parallel agreementexecution strategies that make Sui scalable. Move is an open-source programming language for building smart contracts originally developed at Facebook for the Diem blockchain. The language is platform-agnostic, and in ad- dition to being adopted by Sui, it has been gaining popularity on other platforms (e.g., 0L, StarCoin). In this section we will discuss the main features of the Move language and explain how it is used to create and manage assets on Sui. A more thorough explanation of Moves features can be found in the Move Programming Language book1 and more Sui-specific Move content can be found in the Sui Developer Portal2, and a more formal description of Move in the context of Sui can be found in Section 3.",
      "1https:diem.github.iomove 2https:github.comMystenLabsfastnftblobmaindocSUMMARY.md Overview Suis global state includes a pool of programmable objects created and managed by Move packages that are collections of Move mod- ules (see Section 2.1.1 for details) containing Move functions and types. Move packages themselves are also objects. Thus, Sui objects can be partitioned into two categories:  Struct data values: Typed data governed by Move modules. Each object is a struct value with fields that can contain primitive types (e.g. integers, addresses), other objects, and non-object structs. Package code values: a set of related Move bytecode mod- ules published as an atomic unit. Each module in a package can depend both on other modules in that package and on modules in previously published packages.",
      "Objects can encode assets (e.g., fungible or non-fungible tokens), capabilities granting the permission to call certain functions or create other objects, smart contracts that manage other assets, and so onits up to the programmer to decide. The Move code to declare a custom Sui object type looks like this: struct Obj has key  id: VersionedID,  globally unique ID and version f: u64  objects can have primitive fields g: OtherObj  fields can also store other objects All structs representing Sui objects (but not all Move struct values) must have the id field and the key ability3 indicating that the value can be stored in Suis global object pool. 2.1.1 Modules. A Move program is organized as a set of modules, each consisting of a list of struct declarations and function declara- tions. A module can import struct types from other modules and invoke functions declared by other modules.",
      "Values declared in one Move module can flow into another e.g., module OtherObj in the example above could be defined in a different module than the module defining Obj. This is different from most smart contract languages, which allow only unstructured bytes to flow across contract boundaries. However, Move is able to support this because it provides encapsulation features to help programmers write robustly safe 14 code. Specifically, Moves type system ensures that a type like Obj above can only be created, destroyed, copied, read, and written by functions inside the module that declares the type. This allows a module to enforce strong invariants on its declared types that continue to hold even when they flow across smart contract trust boundaries. 2.1.2 Transactions and Entrypoints. The global object pool is up- dated via transactions that can create, destroy, read, and write objects. A transaction must take each existing object it wishes to operate on as an input.",
      "In addition, a transaction must include the 3https:diem.github.iomoveabilities.html and The MystenLabs Team versioned ID of a package object, the name of a module and func- tion inside that package, and arguments to the function (including input objects). For example, to call the function public fun entrypoint( o1: Obj, o2: mut Obj, o3: Obj, x: u64, ctx: mut TxContext )  ... a transaction must supply IDs for three distinct objects whose type is Obj and an integer to bind to x. The TxContext is a special parameter filled in by the runtime that contains the sender address and information required to create new objects. Inputs to an entrypoint (and more generally, to any Move func- tion) can be passed with different mutability permissions encoded in the type. An Obj input can be read, written, transferred, or de- stroyed. A mut Obj input can only be read or written, and a Obj can only be read.",
      "The transaction sender must be authorized to use each of the input objects with the specified mutability permissions see Section 4.4 for more detail. 2.1.3 Creating and Transferring Objects. Programmers can create objects by using the TxContext passed into the entrypoint to gener- ate a fresh ID for the object: public fun create_then_transfer( f: u64, g: OtherObj, o1: Obj, ctx: mut TxContext let o2  Obj  id: TxContext::fresh_id(ctx), f, g ; Transfer::transfer(o1, TxContext:sender()); Transfer::transfer(o2, TxContext:sender()); This code takes two objects of type OtherObj and Obj as input, uses the first one and the generated ID to create a new Obj, and then transfers both Obj objects to the transaction sender. Once an object has been transferred, it flows into the global object pool and cannot be accessed by code in the remainder of the transaction.",
      "The Transfer module is part of the Sui standard library, which includes functions for transferring objects to user addresses and to other objects. We note that if the programmer code neglected to include one of the transfer calls, this code would be rejected by the Move type system. Move enforces resource safety 5 protections to ensure that objects cannot be created without permission, copied, or acci- dentally destroyed. Another example of resource safety would be an attempt to transfer the same object twice, which would also be rejected by the Move type system. THE SUI PROGRAMMING MODEL In this section, we expand on the informal description of the Sui programming model from Section 2 by presenting detailed seman- tic definitions. The previous section showed examples of Move source code; here we define the structure of Move bytecode. De- velopers write, test, and formally verify 10, 16 Move source code locally, then compile it to Move bytecode before publishing it to the blockchain.",
      "Any Move bytecode be published on-chain must pass through a bytecode verifier4, 5 to ensure that it satisfies key properties such as type, memory, and resource safety. As mentioned in Section 2, Move is a platform-agnostic language which can be adapted to fit specific needs of different systems without forking the core language. In the following description, we define both concepts from core Move language (denoted in black text) and Sui-specific features extending the core Move language (denoted with orange text). Modules Module  ModuleName (StructName StructDecl) (FunName FunDecl)  FunDecl GenericParam  Ability StructDecl  (FieldName StorableType) Ability  GenericParam FunDecl  TypeType  Instr  GenericParam Instr  TransferToAddr  TransferToObj  ShareMut  ShareImmut  . . . Table 1: Module Move code is organized into modules whose structure is defined in Table 1.",
      "A module consists of a collection of named struct declara- tions and a collection of named function declarations (examples of these declaration are provided in Section 2.1). A module also con- tains a special function declaration serving as the module initializer. This function is invoked exactly once at the time the module is published on-chain. A struct declaration is a collection of named fields, where a field name is mapped to a storeable type. Its declaration also includes an optional list of abilities (see Section 2 for a description of storeable types and abilities). A struct declaration may also include a list of generic parameters with ability constraints, in which case we call it a generic struct declaration, for example struct WrapperT: copy t: T . A generic parameter represents a type to be used when declaring struct fields  it is unknown at the time of struct declara- tion, with a concrete type provided when the struct is instantiated (i.e., as struct value is created).",
      "A function declaration includes a list of parameter types, a list of return types, and a list of instructions forming the functions body. A function declaration may also include a list of generic parameters with ability constraints, in which case we call it a generic function declaration, for example fun unwrapT: copy(p: WrapperT). Similarly to struct declarations, a generic parameter represents a type unknown at function declaration time, but which is never- theless used when declaring function parameters, return values and a function body (concrete type is provided when a function is called). Instructions that can appear in a function body include all or- dinary Move instructions with the exception of global storage in- structions (e.g., move_to, move_from, borrow_global). See 14 for a complete list of core Moves instructions and their semantics. In Sui persistent storage is supported via Suis global object pool rather than the account-based global storage of core Move.",
      "There are four Sui-specific object operations. Each of these oper- ations changes the ownership metadata of the object (see Section 3.3) and returns it to the global object pool. Most simply, a Sui object can be transferred to the address of a Sui end-user. An object can also be transferred to another parent objectthis operation requires the caller to supply a mutable reference to the parent object in The Sui Smart Contracts Platform addition to the child object. An object can be mutably shared so it can be readwritten by anyone in the Sui system. Finally, an object can be immutably shared so it can be read by anyone in the Sui system, but not written by anyone. The ability to distinguish between different kinds of ownership is a unique feature of Sui. In other blockchain platforms we are aware of, every contract and object is mutably shared.",
      "As we will explain in Section 4, Sui leverages this information for parallel transaction execution (for all transactions) and parallel agreement (for transactions involving objects without shared mutability). Types and Abilities PrimType  address, id, bool, u8, u64, . . . StructType  ModuleName  StructName StorableType StorableType  PrimType StructType GenericType VectorType VectorType  StorableType GenericType  MutabilityQual  mut, immut ReferenceType  StorableType  MutabilityQual Type  ReferenceType StorableType Ability  key, store, copy, drop Table 2: Types and Abilities A Move program manipulates both data stored in Sui global object pool and transient data created when the Move program executes. Both objects and transient data are Move values at the language level. However, not all values are created equal  they may have different properties and different structure as prescribed by their types. The types used in Move are defined in Table 2.",
      "Move supports many of the same primitive types supported in other programming languages, such as a boolean type or unsigned integer types of var- ious sizes. In addition, core Move has an address type representing an end-user in the system that is also used to identify the sender of a transaction and (in Sui) the owner of an object. Finally, Sui defines an id type representing an identity of a Sui object see Section 3.3 for details. A struct type describes an instance (i.e., a value) of a struct de- clared in a given module (see Section 3.1 for information on struct declarations). A struct type representing a generic struct declara- tion (i.e., generic struct type) includes a list of storeable types  this list is the counterpart of the generic parameter list in the struct dec- laration. A storeable type can be either a concrete type (a primitive or a struct) or a generic type.",
      "We call such types storeable because they can appear as fields of structs and in objects stored persistently on-chain, whereas reference types cannot. For example, the Wrapperu64 struct type is a generic struct type parameterized with a concrete (primitive) storeable type u64  this kind of type can be used to create a struct instance (i.e.,value). On the other hand, the same generic struct type can be parameterized with a generic type (e.g., struct ParentT  w: WrapperT ) coming from a generic parameter of the enclosing struct or function dec- laration  this kind of type can be used to declare struct fields, function params, etc. Structurally, a generic type is an integer index (defined as N in Table 5) into the list of generic parameters in the enclosing struct or function declaration. A vector type in Move describes a variable length collection of homogenous values. A Move vector can only contain storeable types, and it is also a storeable type itself.",
      "A Move program can operate directly on values or access them indirectly via references. A reference type includes both the storeable type referenced and a mutability qualifier used to determine (and enforce) whether a value of a given type can be read and written (mut) or only read (immut). Consequently, the most general form of a Move value type (Type in Table 2) can be either a storeable type or a reference type. Finally, abilities in Move control what actions are permissible for values of a given type, such as whether a value of a given type can be copied (duplicated). Abilities constraint struct declarations and generic type parameters. The Move bytecode verifier is respon- sible for ensuring that sensitive operations like copies can only be performed on types with the corresponding ability.",
      "Objects and Ownership TxDigest  \ud835\udc36\ud835\udc5c\ud835\udc5a(Tx) ObjID  \ud835\udc36\ud835\udc5c\ud835\udc5a(TxDigest  N) SingleOwner  Addr ObjID Shared  shared_mut, shared_immut Ownership  SingleOwner Shared StructObj  StructType  Struct ObjContents  StructObj Package Obj  ObjContents  ObjID  Ownership  Version Table 3: Objects and Ownership Each Sui object has a globally unique identifier (ObjID in Table 3) that serves as the persistent identity of the object as it flows between owners and into and out of other objects. This ID is assigned to the object by the transaction that creates it. An object ID is created by applying a collision-resistant hash function to the contents of the current transaction and to a counter recording how many objects the transaction has created. A transaction (and thus its digest) is guaranteed to be unique due to constraints on the input objects of the transaction, as we will explain subsequently. In addition to an ID, each object carries metadata about its own- ership.",
      "An object is either uniquely owned by an address or another object, shared with writeread permissions, or shared with only read permissions. The ownership of an object determines whether and how a transaction can use it as an input. Broadly, a uniquely owned object can only be used in a transaction initiated by its owner or including its parent object as an input, whereas a shared object can be used by any transaction, but only with the specified mutability permissions. See Section 4.4 for a full explanation. There are two types of objects: package code objects, and struct data objects. A package object contains of a list of modules. A struct object contains a Move struct value and the Move type of that value. and The MystenLabs Team The contents of an object may change, but its ID, object type (pack- age vs struct) and Move struct type are immutable. This ensures that objects are strongly typed and have a persistent identity. Finally, an object contains a version.",
      "Freshly created objects have version 0, and an objects version is incremented each time a transaction takes the object as an input. Addresses and Authenticators Authenticator  Ed25519PubKey ECDSAPubKey . . . Addr  \ud835\udc36\ud835\udc5c\ud835\udc5a(Authenticator) Table 4: Addresses and Authenticators An address is the persistent identity of a Sui end-user (although note that a single user can have an arbitrary number of addresses). To transfer an object to another user, the sender must know the address of the recipient. As we will discuss shortly, a Sui transaction must contain the address of the user sending (i.e., initiating) the transaction and an authenticator whose digest matches the address. The separation between addresses and authenticators enables cryptographic agility. An authenticator can be a public key from any signature scheme, even if the schemes use different key lengths (e.g., to support post- quantum signatures).",
      "In addition, an authenticator need not be a single public keyit could also be (e.g.) a K-of-N multisig key. Transactions ObjRef  ObjID  Version  \ud835\udc36\ud835\udc5c\ud835\udc5a(Obj) CallTarget  ObjRef  ModuleName  FunName CallArg  ObjRef ObjID PrimType Package  Module Publish  Package  ObjRef Call  CallTarget  StorableType  CallArg GasInfo  ObjRef  MaxGas  BaseFee  Tip Tx  (Call Publish)  GasInfo  Addr  Authenticator Table 5: Transactions Sui has two different transaction types: publishing a new Move package, and calling a previously published Move package. A pub- lish transaction contains a packagea set of modules that will be published together as a single object, as well as the dependencies of all the modules in this package (encoded as a list of object references that must refer to already-published package objects). To execute a publish transaction, the Sui runtime will run the Move bytecode verifier on each package, link the package against its dependencies, and run the module initializer of each module.",
      "Module initializ- ers are useful for bootstrapping the initial state of an application implemented by the package. A call transactions most important arguments are object inputs. Object arguments are either specified via an object reference (for single-owner and shared immutable objects) or an object ID (for shared mutable objects). An object reference consists of an object ID, an object version, and the hash of the object value. The Sui runtime will resolve both object IDs and object references to object values stored in the global object pool. For object references, the runtime will check the version of the reference against the version of the object in the pool, as well as checking that the references hash matches the pool object. This ensures that the runtimes view of the object matches the transaction senders view of the object. In addition, a call transaction accepts type arguments and pure value arguments.",
      "Type arguments instantiate generic type parame- ters of the entrypoint function to be invoked (e.g., if the entrypoint function is send_coinT(c: CoinT, ...), the generic type pa- rameter T could be instantiated with the type argument SUI to send the Sui native token). Pure values can include primitive types and vectors of primitive types, but not struct types. The function to be invoked by the call is specified via an object reference (which must refer to a package object), a name of a module in that package, and a name of a function in that package. To execute a call transaction, the Sui runtime will resolve the function, bind the type, object, and value arguments to the function parameters, and use the Move VM to execute the function. Both call and publish transactions are subject to gas metering and gas fees. The metering limit is expressed by a maximum gas budget.",
      "The runtime will execute the transaction until the budget is reached, and will abort with no effects (other than deducting fees and reporting the abort code) if the budget is exhausted. The fees are deducted from a gas object specified as an object reference. This object must be a Sui native token (i.e., its type must be CoinSUI). Sui uses EIP15594-style fees: the protocol defines a base fee (denominated in gas units per Sui token) that is algorith- mically adjusted at epoch boundaries, and the transaction sender can also include an optional tip (denominated in Sui tokens). Under normal system load, transactions will be processed promptly even with no tip. However, if the system is congested, transactions with a larger tip will be prioritized. The total fee deduced from the gas object is (GasUsed BaseFee)  Tip.",
      "Transaction Effects Event  StructType  Struct Create  Update  Wrap  ObjID  Version Delete  ObjID  Version ObjEffect  Create Update Wrap Delete AbortCode  N  ModuleName SuccessEffects  ObjEffect  Event AbortEffects  AbortCode TxEffects  SuccessEffects AbortEffects Table 6: Transaction Effects Transaction execution generates transaction effects which are dif- ferent in the case when execution of a transaction is successful (SuccessEffects in Table 6) and when it is not (AbortEffects in Table 6). 4https:github.comethereumEIPsblobmasterEIPSeip-1559.md The Sui Smart Contracts Platform Upon successful transaction execution, transaction effects in- clude information about changes made to Suis global object pool (including both updates to existing objects and freshly created ob- jects) and events generated during transaction execution.",
      "Another effect of successful transaction execution could be object removal (i.e., deletion) from the global pool and also wrapping (i.e., embed- ding) one object into another, which has a similar effect to removal  a wrapped object disappears from the global pool and exists only as a part of the object that wraps it. Since deleted and wrapped objects are no longer accessible in the global pool, these effects are represented by the ID and version of the object. Events encode side effects of successful transaction execution beyond updates to the global object pool. Structurally, an event consists of a Move struct and its type. Events are intended to be consumed by actors outside the blockchain, but cannot be read by Move programs.",
      "Transactions in Move have an all-or-nothing semantics  if ex- ecution of a transaction aborts at some point (e.g., due to an un- expected failure), even if some changes to objects had happened (or some events had been generated) prior to this point, none of these effects persist in an aborted transaction. Instead, an aborted transaction effect includes a numeric abort code and the name of a module where the transaction abort occurred. Gas fees are still charged for aborted transactions. THE SUI SYSTEM In this section we describe Sui from a systems perspective, includ- ing the mechanisms to ensure safety and liveness across authorities despite Byzantine failures. We also explain the operation of clients, including light clients that need some assurance about the system state without validating its full state. Brief background.",
      "At a systems level Sui is an evolution of the FastPay 3 low-latency settlement system, extended to operate on arbitrary objects through user-defined smart contracts, and with a permissionless delegated proof of stake committee composition 2. Basic asset management by object owners is based on a variant of Byzantine consistent broadcast 6 that has lower latency and is easier to scale across many machines as compared to traditional implementations of Byzantine consensus 8, 11, 12.When full agree- ment is required we use a high-throughput DAG-based consensus, e.g. 9 to manage locks, while execution on different shared objects is parallelized. Protocol outline. Figure 1 illustrates the high-level interactions between a client and Sui authorities to commit a transaction. We describe them here briefly:  A user with a private signing key creates and signs a user transaction to mutate objects they own, or shared objects, within Sui.",
      "Subsequently, user signature keys are not needed, and the remaining of the process may be performed by the user client, or a gateway on behalf of the user (denoted as keyless operation in the diagram). The user transaction is sent to the Sui authorities, that each check it for validity, and upon success sign it and return the signed transaction to the client. The client collects the re- sponses from a quorum of authorities to form a transaction certificate. The transaction certificate is then sent back to all author- ities, and if the transaction involves shared objects it is also sent to a Byzantine agreement protocol operated by the Sui authorities. Authorities check the certificate, and in case shared objects are involved also wait for the agree- ment protocol to sequence it in relation to other shared object transactions, and then execute the transaction and summarize its effects into a signed effects response.",
      "Once a quorum of authorities has executed the certificate its effects are final (denoted as finality in the diagram). Clients can collect a quorum of authority responses and create an effects certificate and use it as a proof of the finality of the transactions effects. This section describes each of these operations in detail, as well as operations to reconfigure and manage state across authorities. System Model Sui operates in a sequence of epochs denoted by \ud835\udc520, . . .. Each epoch is managed by a committee \ud835\udc36\ud835\udc52 (\ud835\udc49\ud835\udc52,\ud835\udc46\ud835\udc52()), where \ud835\udc49\ud835\udc52is a set of authorities with known public verification keys and network end-points. The function \ud835\udc46\ud835\udc52(\ud835\udc63) maps each authority \ud835\udc63\ud835\udc49\ud835\udc52to a number of units of delegated stake. We assume that \ud835\udc36\ud835\udc52for each epoch is signed by a quorum (see below) of authority stake at epoch \ud835\udc521. (Sect. 4.7 discusses the formation and management of commit- tees).",
      "Within an epoch, some authorities are correct (they follow the protocol faithfully and are live), while others are Byzantine (they de- viate arbitrarily from the protocol). The security assumption is that the set of honest authorities \ud835\udc3b\ud835\udc52\ud835\udc49\ud835\udc52is assigned a quorum of stake within the epoch, i.e. \u00cd \u210e\ud835\udc3b\ud835\udc52\ud835\udc46\ud835\udc52(\u210e)  23 \u00cd \ud835\udc63\ud835\udc49\ud835\udc52\ud835\udc46\ud835\udc52(\ud835\udc63) (and refer to any set of authorities with over two-thirds stake as a quorum). There exists at least one live and correct party that acts as a relay for each certificate (see Sect. 4.3) between honest authorities. This ensures liveness, and provides an eventual delivery property to the Byzantine broadcast (see totality of reliable broadcast in 6). Each authority operates such a relay, either individually or through a col- lective dissemination protocol. External entities, including Sui light clients, replicas and services may also take on this role.",
      "The distinc- tion between the passive authority core, and an internal or external active relay component that is less reliable or trusted, ensures a clear demarcation and minimization of the Trusted Computing Base 15 on which Suis safety and liveness relies. Authority  Replica Data Structures Sui authorities rely on a number of data structures to represent state. We define these structures based on the operations they support. They all have a deterministic byte representation. An Object (Obj) stores user smart contracts and data within Sui. They are the Sui system-level encoding of the Move objects introduced in Sect. 2. They support the following set of operations:  ref(Obj) returns the reference (ObjRef) of the object, namely a triplet (ObjID, Version, ObjDigest).",
      "ObjID is practically and The MystenLabs Team Key Interaction to Process Transaction Client User Transaction Collect Transaction Certificate Effects Certificate Finality Collect Keyless Operation Authorities Process Transaction Process Certificate Shared Objects only Consensus Byzantine Agreement Figure 1: Outline of interactions to commit a transaction. unique for all new objects created, and Version is an in- creasing positive integer representing the object version as it is being mutated. owner(Obj) returns the authenticator Auth of the owner of the object. In the simplest case, Auth is an address, repre- senting a public key that may use this object. More complex authenticators are also available (see Sect. 4.4). read-only(Obj) returns true if the object is read-only. Read- only objects may never be mutated, wrapped or deleted. They may also be used by anyone, not just their owners. parent(Obj) returns the transaction digest (TxDigest) that last mutated or created the object.",
      "contents(Obj) returns the object type Type and data Data that can be used to check the validity of transactions and carry the application-specific information of the object. The object reference (ObjRef) is used to index objects. It is also used to authenticate objects since ObjDigest is a commitment to their full contents. A transaction (Tx) is a structure representing a state transition for one or more objects. They support the following set of operations:  digest(Tx) returns the TxDigest, which is a binding crypto- graphic commitment to the transaction. epoch(Tx) returns the EpochID during which this transac- tion may be executed. inputs(Tx) returns a sequence of object ObjRef the trans- action needs to execute. payment(Tx) returns a reference to an ObjRef to be used to pay for gas, as well as the maximum gas limit, and a conversion rate between a unit of gas and the unit of value in the gas payment object.",
      "valid(Tx, Obj) returns true if the transaction is valid, given the requested input objects provided. Validity is discussed in Sect. 4.4, and relates to the transactions being authorized to act on the input objects, as well as sufficient gas being available to cover the costs of its execution. exec(Tx, Obj) executes the transaction and returns a struc- ture Effects representing its effects. A valid transaction ex- ecution is infallible, and its output is deterministic. A transaction is indexed by its TxDigest, which may also be used to authenticate its full contents. All valid transactions (except the special hard-coded genesis transaction) have at least one owned input, namely the objects used to pay for gas. A transaction effects (Effects) structure summarizes the outcome of a transaction execution. It supports the following operations:  digest(Effects) is a commitment EffDigest to the Effects structure, that may be used to index or authenticate it.",
      "transaction(Effects) returns the TxDigest of the executed transaction yielding the effects. dependencies(Effects) returns a sequence of dependencies TxDigest that should be executed before the transaction with these effects may execute. contents(Effects) returns a summary of the execution. Status reports the outcome of the smart contract execution. The lists Created, Mutated, Wrapped, Unwrapped and Deleted, list the object references that underwent the respective op- erations. And Events lists the events emitted by the execu- tion. A transaction certificate TxCert on a transaction contains the transaction itself as well as the identifiers and signatures from a quorum of authorities. Note that a certificate may not be unique, in that the same logical certificate may be represented by a different set of authorities forming a quorum. Additionally, a certificate might not strictly be signed by exactly a 23 quorum, but possibly more if more authorities are responsive.",
      "However, two different valid certificates on the same transaction should be treated as represent- ing semantically the same certificate. A partial certificate (TxSign) contains the same information, but signatures from a set of author- ities representing stake lower than the required quorum, usually a single authority. The identifiers of signers are included in the certificate (i.e., accountable signatures ? ) to identify authorities The Sui Smart Contracts Platform ready to process the certificate, or that may be used to download past information required to process the certificate (see Sect. 4.8). Similarly, an effects certificate EffCert on an effects structure contains the effects structure itself, and signatures from authorities5 that represent a quorum for the epoch in which the transaction is valid. The same caveats, about non-uniqueness and identity apply as for transaction certificates.",
      "A partial effects certificate, usually containing a single authority signature and the effects structure is denoted as EffSign. Persistent Stores. Each authority and replica maintains a set of persistent stores. The stores implement persistent map seman- tics and can be represented as a set of key-value pairs (denoted \ud835\udc5a\ud835\udc4e\ud835\udc5d\ud835\udc58\ud835\udc52\ud835\udc66 \ud835\udc63\ud835\udc4e\ud835\udc59\ud835\udc62\ud835\udc52), such that only one pair has a given key. Before a pair is inserted a contains(\ud835\udc58\ud835\udc52\ud835\udc66) call returns false, and get(\ud835\udc58\ud835\udc52\ud835\udc66) returns an error. After a pair is inserted contains(\ud835\udc58\ud835\udc52\ud835\udc66) calls returns true, and get(\ud835\udc58\ud835\udc52\ud835\udc66) return the value. An authority maintains the following persistent stores:  The order lock map Lock\ud835\udc63ObjRef TxSignOption re- cords the first valid transaction Tx seen and signed by the authority for an owned object version ObjRef, or None if the object version exists but no valid transaction using as an input it has been seen. It may also record the first certificate seen with this object as an input.",
      "This table, and its update rules, represents the state of the distributed locks on objects across Sui authorities, and ensures safety under concurrent processing of transactions. The certificate map Ct\ud835\udc63TxDigest (TxCert, EffSign) records all full certificates TxCert, which also includes Tx, processed by the authority within their validity epoch, along with their signed effects EffSign. They are indexed by transaction digest TxDigest  The object map Obj\ud835\udc63ObjRef Obj records all objects Obj created by transactions included in certificates within Ct\ud835\udc63indexed by ObjRef. This store can be completely de- rived by re-executing all certificates in Ct\ud835\udc63. A secondary in- dex is maintained that maps ObjID to the latest object with this ID. This is the only information necessary to process new transactions, and older versions are only maintained to facilitate reads and audit.",
      "The synchronization map Sync\ud835\udc63ObjRef TxDigest in- dexes all certificates within Ct\ud835\udc63by the objects they create, mutate or delete as tuples ObjRef. This structure can be fully re-created by processing all certificates in Ct\ud835\udc63, and is used to help client synchronize transactions affecting objects they care about. Authorities maintain all four structures, and also provide access to local checkpoints of their certificate map to allow other authori- ties and replicas to download their full set of processed certificates. A replica does not process transactions but only certificates, and re-executes them to update the other tables as authorities do. It also maintains an order lock map to audit non-equivocation. 5Note that if the signature algorithm permits it, authority signatures can be compressed, but always using accountable signature aggregation, because tracking who signed is important for gas profit distribution and other network health measurements.",
      "An authority may be designed as a full replica maintaining all four stores (and checkpoints) to facilitate reads and synchroniza- tion, combined with a minimal authority core that only maintains object locks and objects for the latest version of objects used to pro- cess new transactions and certificates. This minimizes the Trusted Computing Base relied upon for safety. Only the order lock map requires strong key self-consistency, namely a read on a key should always return whether a value or None is present for a key that exists, and such a check should be atomic with an update that sets a lock to a non-None value. This is a weaker property than strong consistency across keys, and allows for efficient sharding of the store for scaling. The other stores may be eventually consistent without affecting safety. Authority Base Operation Process Transaction. Upon receiving a transaction Tx an authority performs a number of checks: (1) It ensures epoch(Tx) is the current epoch.",
      "(2) It ensures all object references inputs(Tx) and the gas object reference in payment(Tx) exist within Obj\ud835\udc63and loads them into Obj. For owned objects the exact reference should be available; for read-only or shared objects the object ID should exist. (3) Ensures sufficient gas can be made available in the gas object to cover the cost of executing the transaction. (4) It checks valid(Tx, Obj) is true. This step ensures the au- thentication information in the transaction allows access to the owned objects. (5) It checks that Lock\ud835\udc63ObjRef for all owned inputs(Tx) ob- jects exist, and it is either None or set to the same Tx, and atomically sets it to TxSign. (We call these the locks checks). If any of the checks fail processing ends, and an error is returned. However, it is safe for a partial update of Lock\ud835\udc63to persist (although our current implementation does not do partial updates, but atomic updates of all locks).",
      "If all checks are successful then the authority returns a signature on the transaction, ie. a partial certificate TxSign. Processing an order is idempotent upon success, and returns a partial certificate (TxSign), or a full certificate (TxCert) if one is available. Any party may collate a transaction and signatures (TxSign) for a set of authorities forming a quorum for epoch \ud835\udc52, to form a transaction certificate TxCert. Process Certificate. Upon receiving a certificate an authority checks all validity conditions for the transaction, except those re- lating to locks (the so-called locks checks). Instead it performs the following checks: for each owned input object in inputs(Tx) it checks that the lock exists, and that it is either None, set to any TxSign, or set to a certificate for the same transaction as the cur- rent certificate.",
      "If this modified locks check fails, the authority has detected an unrecoverable Byzantine failure, halts normal opera- tions, and starts a disaster recovery process. For shared objects (see Sect. 4.4) authorities check that the locks have been set through the certificate being sequenced in a consensus, to determine the and The MystenLabs Team version of the share object to use. If so, the transaction may be executed; otherwise it needs to wait for such sequencing first. If the check succeeds, the authority adds the certificate to its certificate map, along with the effects resulting from its execution, ie. Ct\ud835\udc63TxDigest (TxCert, EffSign); it updates the locks map to record the certificate Lock\ud835\udc63ObjRef TxCert for all owned input objects that have locks not set to a certificate. As soon as all objects in Input(Tx) is inserted in Obj\ud835\udc63, then all effects in EffSign are also materialized by adding their ObjRef and contents to Obj\ud835\udc63.",
      "Finally for all created or mutated in EffSign the synchronization map is updated to map them to Tx. Remarks. The logic for handling transactions and certificates leads to a number of important properties:  Causality  parallelism. The processing conditions for both transactions and certificates ensure causal execution: an authority only votes by signing a transaction if it has processed all certificates creating the objects the transaction depends upon, both owned, shared and read-only. Similarly, an authority only processes a certificate if all input objects upon which it depends exist in its local objects map. This imposes a causal execution order, but also enables transac- tions not causally dependent on each other to be executed in parallel on different cores or machines. Sign once, and safety. All owned input objects locks in Lock\ud835\udc63 are set to the first transaction Tx that passes the checks using them, and then the first certificate that uses the object as an input.",
      "We call this locking the object to this transaction, and there is no unlocking within an epoch. As a result an authority only signs a single transaction per lock, which is an essential component of consistent broadcast 6, and thus the safety of Sui. Disaster recovery. An authority detecting two contradic- tory certificates for the same lock, has proof of irrecover- able Byzantine behaviour  namely proof that the quorum honest authority assumption does not hold. The two contra- dictory certificates are a fraud proof 1, that may be shared with all authorities and replicas to trigger disaster recovery processes. Authorities may also get other forms of proof of unrecoverable byzantine behaviour such as 13 signatures on effects (EffSign) that represent an incorrect execution of a certificate. Or a certificate with input objects that do not represent the correct outputs of previously processed certificates.",
      "These also can be packaged as a fraud proof and shared with all authorities and replicas. Note these are distinct from proofs that a tolerable minority of authorities (13 by stake) or object owners (any number) is byzan- tine or equivocating, which can be tolerated without any service interruption. Finality. Authorities return a certificate (TxCert) and the signed effects (EffSign) for any read requests for an index in Lock\ud835\udc63, Ct\ud835\udc63and Obj\ud835\udc63, Sync\ud835\udc63. A transaction is considered final if over a quorum of authorities reports Tx as included in their Ct\ud835\udc63store. This means that an effects certificate (EffCert) is a transferable proof of finality. However, a cer- tificate using an object is also proof that all dependent certificates in its causal path are also final. Providing a cer- tificate to any party, that may then submit it to a super majority of authorities for processing also ensures finality for the effects of the certificate.",
      "Note that finality is later than fastpay 3 to ensure safety under re-configuration. However, an authority can apply the effect of a transaction upon seeing a certificate rather than waiting for a commit. Owners, Authorization, and Shared Objects Transaction validity (see Sect. 4.3) ensures a transaction is autho- rized to include all specified input objects in a transaction. This check depends on the nature of the object, as well as the owner field. Read-only objects cannot be mutated or deleted, and can be used in transactions concurrently and by all users. Move modules for example are read-only. Such objects do have an owner that might be used as part of the smart contract, but that does not affect au- thorization to use them. They can be included in any transaction. Owned objects have an owner field. The owner can be set to an address representing a public key. In that case, a transaction is authorized to use the object, and mutate it, if it is signed by that address.",
      "A transaction is signed by a single address, and therefore can use one or more objects owned by that address. However, a single transaction cannot use objects owned by more than one address. The owner of an object, called a child object, can be set to the ObjID of another object, called the parent object. In that case the child object may only be used if the parent object is included in the transaction, and the transaction is authorized to use the object. This facility may be used by contracts to construct efficient collections and other complex data structures. Shared objects are mutable, but do not have a specific owner. They can instead be included in transactions by different parties, and do not require any authorization. Instead they perform their own authorization logic. Such objects, by virtue of having to support multiple writers while ensuring safety and liveness, do require a full agreement protocol to be used safely. Therefore they require additional logic before execution.",
      "Authorities process transactions as specified in Sect. 4.3 for owned objects and read-only objects to manage their locks. However, authorities do not rely on con- sistent broadcast to manage the locks of shared objects. Instead, the creators of transactions that involve shared objects insert the certificate on the transaction into a high-throughput consensus system, e.g. 9. All authorities observe a consistent sequence of such certificates, and assign the version of shared objects used by each transaction according to this sequence. Then execution can proceed and is guaranteed to be consistent across all authorities. Au- thorities include the version of shared objects used in a transaction execution within the Effects certificate. The above rules ensure that execution for transactions involving read-only and owned objects requires only consistent broadcast and a single certificate to proceed; and Byzantine agreement is only required for transactions involving shared objects.",
      "Smart contract authors can therefore design their types and their operations to optimize transfers and other operations on objects of a single user to have lower latency, while enjoying the flexibility of using shared The Sui Smart Contracts Platform objects to implement logic that needs to be accessed by multiple users. Clients Full Clients  Replicas. Replicas, also sometimes called full clients, do not validate new transactions, but maintain a consistent copy of the valid state of the system for the purposes of audit, as well as to construct transactions or operate services incl. read infrastructures for light client queries. Light Clients. Both object references and transactions contain information that allows the authentication of the full causal chain of transactions that leading up to their creation or execution.",
      "Specif- ically, an object reference (ObjRef) contains an ObjDigest that is an authenticator for the full state of the object, including the facility to get parent(Obj), namely the TxDigest that created the object. Simi- larly, a TxDigest authenticates a transaction, including the facility to extract through inputs(Tx) the object references of the input objects. Therefore the set of objects and certificates form a bipartite graph that is self-authenticating. Furthermore, effects structures are also signed, and may be collated into effects certificates that directly certify the results of transaction executions. These facilities may be used to support light clients that can per- form high-integrity reads into the state of Sui, without maintaining a full replica node.",
      "Specifically an authority or full node may pro- vide a succinct bundle of evidence, comprising a certificate TxCert on a transaction Tx and the input objects Obj corresponding to inputs(Tx) to convince a light client that a transition can take place within Sui. A light client may then submit this certificate, or check whether it has been seen by a quorum or sample of authorities to ensure finality. Or it may craft a transaction using the objects resulting from the execution, and observe whether it is successful. More directly, a service may provide an effects certificate to a client to convince them of the existence and finality of a transi- tion within Sui, with no further action or interaction within the system. If a checkpoint of finalized certificates is available, at an epoch boundary or otherwise, a bundle of evidence including the input objects and certificate, alongside a proof of inclusion of the certificate in the checkpoint is also a proof of finality.",
      "Authorities may use a periodic checkpointing mechanism to create collective checkpoints of finalized transactions, as well as the state of Sui over time. A certificate with a quorum of stake over a checkpoint can be used by light clients to efficiently validate the recent state of objects and emitted events. A check pointing mechanism is necessary for committee reconfiguration between epochs. More frequent checkpoints are useful to light clients, and may also be used by authorities to compress their internal data structures as well as synchronize their state with other authorities more efficiently. Bridges Native support for light clients and shared objects managed by Byzantine agreement allows Sui to support two-way bridges to other blockchains 13. The trust assumption of such bridges reflect the trust assumptions of Sui and the other blockchain, and do not have to rely on trusted oracles or hardware if the other blockchain also supports light clients 7.",
      "Bridges are used to import an asset issued on another blockchain, to represent it and use it as a wrapped asset within the Sui system. Eventually, the wrapped asset can be unlocked and transferred back to a user on the native blockchain. Bridges can also allow assets issued on Sui to be locked, and used as wrapped assets on other blockchains. Eventually, the wrapped object on the other system can be destroyed, and the object on Sui updated to reflect any changes of state or ownership, and unlocked. The semantics of bridged assets are of some importance to en- sure wrapped assets are useful. Fungible assets bridged across blockchains can provide a richer wrapped representation that al- lows them to be divisible and transferable when wrapped. Non- fungible assets are not divisible, but only transferable.",
      "They may also support other operations that mutates their state in a con- trolled manner when wrapped, which may necessitate custom smart contract logic to be executed when they are bridged back and unwrapped. Sui is flexible and allows smart contract authors to define such experiences, since bridges are just smart contracts implemented in Move rather than native Sui concepts  and there- fore can be extended using the composability and safety guarantees Move provides. Committee Reconfiguration Reconfiguration occurs between epochs when a committee \ud835\udc36\ud835\udc52is replaced by a committee \ud835\udc36\ud835\udc52, where \ud835\udc52  \ud835\udc52 1. Reconfiguration safety ensures that if a transaction Tx was committed at \ud835\udc52or before, no conflicting transaction can be committed after \ud835\udc52. Liveness en- sures that if Tx was committed at or before \ud835\udc52, then it must also be committed after \ud835\udc52. We leverage the Sui smart contract system to perform a lot of the work necessary for reconfiguration.",
      "Within Sui a system smart contract allows users to lock and delegate stake to candidate authorities. During an epoch, owners of coins are free to delegate by locking tokens, undelegate by unlocking tokens or change their delegation to one or more authorities. Once a quorum of stake for epoch \ud835\udc52vote to end the epoch, author- ities exchange information to commit to a checkpoint, determine the next committee, and change the epoch. First, authorities run a check pointing protocol, with the help of an agreement proto- col 9, to agree on a certified checkpoint for the end of epoch \ud835\udc52. The checkpoint contains the union of all transactions, and poten- tially resulting objects, that have been processed by a quorum of authorities.",
      "As a result if a transaction has been processed by a quorum of authorities, then at least one honest authorities that processed it will have its processed transactions included in the end-of-epoch checkpoint, ensuring the transaction and its effects are durable across epochs. Furthermore, such a certified checkpoint guarantees that all transactions are available to honest authorities of epoch \ud835\udc52. The stake delegation at the end-of-epoch checkpoint is then used to determine the new set of authorities for epoch \ud835\udc52 1. Both a quorum of the old authorities stake and a quorum of the new authority stake signs the new committee \ud835\udc36\ud835\udc52, and checkpoint at which the new epoch commences. Once both set of signatures are and The MystenLabs Team available the new set of authorities start processing transactions for the new epoch, and old authorities may delete their epoch signing keys. Recovery.",
      "It is possible due to client error or client equivocation for an owned object to become locked within an epoch, preventing any transaction concerning it from being certified (or finalized). For example, a client signing two different transactions using the same owned object version, with half of authorities signing each, would be unable to form a certificate requiring a quorum of signatures on any of the two certificates. Recovery ensures that once epochs change such objects are again in a state that allows them to be used in transactions. Since, no certificate can be formed, the original object is available at the start of the next epoch to be operated on. Since transactions contain an epoch number, the old equivocating transactions will not lock the object again, giving its owner a chance to use it. Rewards  cryptoeconomics. Sui has a native token SUI, with a fixed supply. SUI is used to pay for gas, and is also be used as delegated stake on authorities within an epoch.",
      "The voting power of authorities within this epoch is a function of this delegated stake. At the end of the epoch fees collected through all transactions pro- cessed are distributed to authorities according to their contribution to the operation of Sui, and in turn they share some of the fees as rewards to addresses that delegated stake to them. We postpone a full description of the token economics of Sui to a dedicated paper. Authority  Replica Updating Client-driven. Due to client failures or non-byzantine authority failures, some authorities may not have processed all certificates. As a result causally related transactions depending on missing objects generated by these certificates would be rejected. However, a client can always update an honest authority to the point where it is able to process a correct transaction. It may do this using its own store of past certificates, or using one or more other honest authorities as a source for past certificates.",
      "Given a certificate \ud835\udc50and a \ud835\udc36\ud835\udc61\ud835\udc63store that includes \ud835\udc50and its causal history, a client can update an honest authority \ud835\udc63 to the point where \ud835\udc50would also be applied. This involves finding the smallest set of certificates not in \ud835\udc63 such that when applied the Objects in \ud835\udc63 include all inputs of \ud835\udc50. Updating a lagging authority \ud835\udc35using a store \ud835\udc36\ud835\udc61\ud835\udc63including the certificate TxCert involves:  The client maintains a list of certificates to sync, initially set to contain just TxCert. The client considers the last TxCert requiring sync. It ex- tracts the Tx within the TxCert and derives all its input objects (using Input(Tx)). For each input object it checks whether the Tx that gener- ated or mutated last (using the Sync\ud835\udc63index on \ud835\udc36\ud835\udc61\ud835\udc63) has a certificate within \ud835\udc35, otherwise its certificate is read from \ud835\udc36\ud835\udc61\ud835\udc63and added to the list of certificates to sync.",
      "If no more certificates can be added to the list (because no more inputs are missing from \ud835\udc35) the certificate list is sorted in a causal order and submitted to \ud835\udc35. The algorithm above also applies to updating an object to a specific version to enable a new transaction. In this case the certificate for the Tx that generated the object version, found in Sync\ud835\udc63ObjRef, is submitted to the lagging authority. Once it is executed on \ud835\udc35the object at the correct version will become available to use. A client performing this operation is called a relayer. There can be multiple relayers operating independently and concurrently. They are untrusted in terms of integrity, and their operation is keyless. Besides clients, authorities can run the relayer logic to update each other, and replicas operating services can also act as relayers to update lagging authorities. Bulk. Authorities provide facilities for a follower to receive updates when they process a certificate.",
      "This allows replicas to maintain an up-to-date view of an authortys state. Furthermore, authori- ties may use a push-pull gossip network to update each other of the latest processed transaction in the short term and to reduce the need for relayers to perform this function. In the longer term lagging authorities may use periodic state commitments, at epoch boundaries or more frequently, to ensure they have processed a complete set of certificates up to certain check points. SCALING AND LATENCY The Sui system allows scaling though authorities devoting more resources, namely CPUs, memory, network and storage within a ma- chine or over multiple machines, to the processing of transactions. More resources lead to an increased ability to process transactions, leading to increased fees income to fund these resources. More resources also results in lower latency, as operations are performed without waiting for necessary resources to become available. Throughput.",
      "To ensure that more resources result in increased capacity quasi-linearly, the Sui design aggressively reduces bottle- necks and points of synchronization requiring global locks within authorities. Processing transactions is cleanly separated into two phases, namely (1) ensuring the transaction has exclusive access to the owned or shared objects at a specific version, and (2) then subsequently executing the transaction and committing its effects. Phase (1) requires a transaction acquiring distributed locks at the granularity of objects. For owned objects this is performed trough a reliable broadcast primitive, that requires no global synchronization within the authority, and therefore can be scaled through sharding the management of locks across multiple machines by ObjID. For transactions involving shared objects sequencing is required using a consensus protocol, which does impose a global order on these transactions and has the potential to be a bottleneck.",
      "However, recent advances on engineering high-throughput consensus pro- tocols 9 demonstrate that sequential execution is the bottleneck in state machine replication, not sequencing. In Sui, sequencing is only used to determine a version for the input shared object, namely incrementing an object version number and associating it with the transaction digest, rather than performing sequential execution. Phase (2) takes place when the version of all input objects is known to an authority (and safely agreed across authorities) and involves execution of the Move transaction and commitment of its effects. Once the version of input objects is known, execution can The Sui Smart Contracts Platform take place completely in parallel. Move virtual machines on mul- tiple cores or physical machines read the versioned input objects, execute, and write the resulting objects from and to stores.",
      "The consistency requirements on stores for objects and transactions (besides the order lock map) are very loose, allowing scalable dis- tributed key-value stores to be used internally by each authority. Execution is idempotent, making even crashes or hardware failures on components handling execution easy to recover from. As a result, execution for transactions that are not causally re- lated to each other can proceed in parallel. Smart contract design- ers may therefore design the data model of objects and operations within their contracts to take advantage of this parallelism. Check-pointing and state commitments are computed off the critical transaction processing path to not block the handling of fresh transactions. These involve read operations on committed data rather than requiring computation and agreement before a transaction reaches finality.",
      "Therefore they do not affect the latency or throughput of processing new transactions, and can themselves be distributed across available resources. Reads can benefit from very aggressive, and scalable caching. Authorities sign and make available all data that light clients require for reads, which may be served by distributed stores as static data. Certificates act as roots of trust for their full causal history of transactions and objects. State commitments further allow for the whole system to have regular global roots of trust for all state and transactions processed, at least every epoch or more frequently. Latency. Smart contract designers are given the flexibility to con- trol the latency of operations they define, depending on whether they involve owned or shared objects. Owned objects rely on a reli- able broadcast before execution and commit, which requires two round trips to a quorum of authorities to reach finality.",
      "Operations involving shared objects, on the other hand, require a a consistent broadcast to create a certificate, and then be processed within a consensus protocol, leading to increased latency (4 to 8 round trips to quorums as of 9). 1 Mustafa Al-Bassam, Alberto Sonnino, Vitalik Buterin, and Ismail Khoffi. 2021. Fraud and Data Availability Proofs: Detecting Invalid Blocks in Light Clients. In Financial Cryptography and Data Security - 25th International Conference, FC 2021, Virtual Event, March 1-5, 2021, Revised Selected Papers, Part II (Lecture Notes in Computer Science, Vol. 12675), Nikita Borisov and Claudia Diaz (Eds.). Springer, 279298. 2 Shehar Bano, Alberto Sonnino, Mustafa Al-Bassam, Sarah Azouvi, Patrick Mc- Corry, Sarah Meiklejohn, and George Danezis. 2019. SoK: Consensus in the Age of Blockchains. In Proceedings of the 1st ACM Conference on Advances in Financial Technologies, AFT 2019, Zurich, Switzerland, October 21-23, 2019. ACM, 183198.",
      "3 Mathieu Baudet, George Danezis, and Alberto Sonnino. 2020. FastPay: High- Performance Byzantine Fault Tolerant Settlement. In AFT 20: 2nd ACM Confer- ence on Advances in Financial Technologies, New York, NY, USA, October 21-23, 2020. ACM, 163177. 4 Sam Blackshear, Evan Cheng, David L. Dill, Victor Gao, Ben Maurer, Todd Nowacki, Alistair Pott, Shaz Qadeer, Ra in, Dario Russi, Stephane Sezer, Tim Za- kian, and Runtian Zhou. 2019. Move: A Language With Programmable Resources. 5 Sam Blackshear, David L. Dill, Shaz Qadeer, Clark W. Barrett, John C. Mitchell, Oded Padon, and Yoni Zohar. 2020. Resources: A Safe Language Abstraction for Money. CoRR abs2004.05106 (2020). arXiv:2004.05106 https:arxiv.orgabs 2004.05106 6 Christian Cachin, Rachid Guerraoui, and Lu\u00eds Rodrigues. 2011. Introduction to reliable and secure distributed programming. Springer Science  Business Media. 7 Panagiotis Chatzigiannis, Foteini Baldimtsi, and Konstantinos Chalkias. 2021. SoK: Blockchain Light Clients.",
      "IACR Cryptol. ePrint Arch. (2021), 1657. 8 Daniel Collins, Rachid Guerraoui, Jovan Komatovic, Petr Kuznetsov, Matteo Monti, Matej Pavlovic, Yvonne-Anne Pignolet, Dragos-Adrian Seredinschi, An- drei Tonkikh, and Athanasios Xygkis. 2020. Online Payments by Merely Broad- casting Messages. In 50th Annual IEEEIFIP International Conference on Dependable Systems and Networks, DSN 2020, Valencia, Spain, June 29 - July 2, 2020. IEEE, 2638. 9 George Danezis, Eleftherios Kokoris-Kogias, Alberto Sonnino, and Alexander Spiegelman. 2021. Narwhal and Tusk: A DAG-based Mempool and Efficient BFT Consensus. CoRR abs2105.11827 (2021). 10 David L. Dill, Wolfgang Grieskamp, Junkil Park, Shaz Qadeer, Meng Xu, and Jingyi Emma Zhong. 2021. Fast and Reliable Formal Verification of Smart Con- tracts with the Move Prover. CoRR abs2110.08362 (2021). arXiv:2110.08362 11 Rachid Guerraoui, Petr Kuznetsov, Matteo Monti, Matej Pavlovic, and Dragos- Adrian Seredinschi. 2018. AT2: Asynchronous Trustworthy Transfers.",
      "CoRR abs1812.10844 (2018). 12 Rachid Guerraoui, Petr Kuznetsov, Matteo Monti, Matej Pavlovic, and Dragos- Adrian Seredinschi. 2019. The Consensus Number of a Cryptocurrency. In Proceedings of the 2019 ACM Symposium on Principles of Distributed Computing, PODC 2019, Toronto, ON, Canada, July 29 - August 2, 2019, Peter Robinson and Faith Ellen (Eds.). ACM, 307316. 13 Patrick McCorry, Chris Buckland, Bennet Yee, and Dawn Song. 2021. SoK: Validating Bridges as a Scaling Solution for Blockchains. IACR Cryptol. ePrint Arch. (2021), 1589. 14 Marco Patrignani and Sam Blackshear. 2021. Robust Safety for Move. CoRR abs2110.05043 (2021). arXiv:2110.05043 https:arxiv.orgabs2110.05043 15 Jerome H Saltzer and Michael D Schroeder. 1975. The protection of information in computer systems. Proc. IEEE 63, 9 (1975), 12781308. 16 Jingyi Emma Zhong, Kevin Cheang, Shaz Qadeer, Wolfgang Grieskamp, Sam Blackshear, Junkil Park, Yoni Zohar, Clark W. Barrett, and David L. Dill. 2020. The Move Prover.",
      "In Computer Aided Verification - 32nd International Conference, CAV 2020, Los Angeles, CA, USA, July 21-24, 2020, Proceedings, Part I (Lecture Notes in Computer Science, Vol. 12224), Shuvendu K. Lahiri and Chao Wang (Eds.). Springer, 137150. https:doi.org10.1007978-3-030-53288-8_7"
    ],
    "word_count": 10296,
    "page_count": 11
  },
  "SUSHI": {
    "chunks": [
      "Sushi  Are you an LLM? Read llms.txt for a summary of the docs, or llms-full.txt for the full context. Skip to content Search... Sushi  Docs Examples 6.4.11 Sushi  Docs Examples 6.4.11 Docs Chevron Down Sushi  cookinx27; npm pnpm yarn npm install sushi Unleashing the Power of DeFi, One Roll at a Time! Get started GitHub GitHub Discord X"
    ],
    "word_count": 58,
    "page_count": 1
  },
  "UNI": {
    "chunks": [
      "Uniswap v2 Core Hayden Adams haydenuniswap.org Noah Zinsmeister noahuniswap.org Dan Robinson danparadigm.xyz March 2020 Abstract This technical whitepaper explains some of the design decisions behind the Uniswap v2 core contracts. It covers the contracts new featuresincluding arbitrary pairs between ERC20s, a hardened price oracle that allows other contracts to estimate the time-weighted average price over a given interval, \ufb02ash swaps that allow traders to receive assets and use them elsewhere before paying for them later in the transaction, and a protocol fee that can be turned on in the future. It also re-architects the contracts to reduce their attack surface. This whitepaper describes the mechanics of Uniswap v2s core contracts including the pair contract that stores liquidity providers fundsand the factory contract used to instantiate pair contracts.",
      "Introduction Uniswap v1 is an on-chain system of smart contracts on the Ethereum blockchain, imple- menting an automated liquidity protocol based on a constant product formula 1. Each Uniswap v1 pair stores pooled reserves of two assets, and provides liquidity for those two assets, maintaining the invariant that the product of the reserves cannot decrease. Traders pay a 30-basis-point fee on trades, which goes to liquidity providers. The contracts are non-upgradeable. Uniswap v2 is a new implementation based on the same formula, with several new highly- desirable features. Most signi\ufb01cantly, it enables the creation of arbitrary ERC20ERC20 pairs, rather than supporting only pairs between ERC20 and ETH. It also provides a hard- ened price oracle that accumulates the relative price of the two assets at the beginning of each block. This allows other contracts on Ethereum to estimate the time-weighted average price for the two assets over arbitrary intervals.",
      "Finally, it enables \ufb02ash swaps where users can receive assets freely and use them elsewhere on the chain, only paying for (or returning) those assets at the end of the transaction. While the contract is not generally upgradeable, there is a private key that has the ability to update a variable on the factory contract to turn on an on-chain 5-basis-point fee on trades. This fee will initially be turned o\ufb00, but could be turned on in the future, after which liquidity providers would earn 25 basis points on every trade, rather than 30 basis points. As discussed in section 3, Uniswap v2 also \ufb01xes some minor issues with Uniswap v1, as well as rearchitecting the implementation, reducing Uniswaps attack surface and making the system more easily upgradeable by minimizing the logic in the core contract that holds liquidity providers funds. This paper describes the mechanics of that core contract, as well as the factory contract used to instantiate those contracts.",
      "Actually using Uniswap v2 will require calling the pair contract through a router contract that computes the trade or deposit amount and transfers funds to the pair contract. New features ERC-20 pairs Uniswap v1 used ETH as a bridge currency. Every pair included ETH as one of its assets. This makes routing simplerevery trade between ABC and XYZ goes through the ETHABC pair and the ETHXYZ pairand reduces fragmentation of liquidity. However, this rule imposes signi\ufb01cant costs on liquidity providers. All liquidity providers have exposure to ETH, and su\ufb00er impermanent loss based on changes in the prices of other assets relative to ETH. When two assets ABC and XYZ are correlatedfor example, if they are both USD stablecoinsliquidity providers on a Uniswap pair ABCXYZ would generally be subject to less impermanent loss than the ABCETH or XYZETH pairs. Using ETH as a mandatory bridge currency also imposes costs on traders.",
      "Traders have to pay twice as much in fees as they would on a direct ABCXYZ pair, and they su\ufb00er slippage twice. Uniswap v2 allows liquidity providers to create pair contracts for any two ERC-20s. A proliferation of pairs between arbitrary ERC-20s could make it somewhat more di\ufb03cult to \ufb01nd the best path to trade a particular pair, but routing can be handled at a higher layer (either o\ufb00-chain or through an on-chain router or aggregator). Price oracle The marginal price o\ufb00ered by Uniswap (not including fees) at time t can be computed by dividing the reserves of asset a by the reserves of asset b. pt  ra Since arbitrageurs will trade with Uniswap if this price is incorrect (by a su\ufb03cient amount to make up for the fee), the price o\ufb00ered by Uniswap tends to track the relative market price of the assets, as shown by Angeris et al 2. This means it can be used as an approximate price oracle.",
      "However, Uniswap v1 is not safe to use as an on-chain price oracle, because it is very easy to manipulate. Suppose some other contract uses the current ETH-DAI price to settle a derivative. An attacker who wishes to manipulate the measured price can buy ETH from the ETH-DAI pair, trigger settlement on the derivative contract (causing it to settle based on the in\ufb02ated price), and then sell ETH back to the pair to trade it back to the true price.1 This might even be done as an atomic transaction, or by a miner who controls the ordering of transactions within a block. Uniswap v2 improves this oracle functionality by measuring and recording the price before the \ufb01rst trade of each block (or equivalently, after the last trade of the previous 1For a real-world example of how using Uniswap v1 as an oracle can make a contract vulnerable to such an attack, see 3. block). This price is more di\ufb03cult to manipulate than prices during a block.",
      "If the attacker submits a transaction that attempts to manipulate the price at the end of a block, some other arbitrageur may be able to submit another transaction to trade back immediately afterward in the same block. A miner (or an attacker who uses enough gas to \ufb01ll an entire block) could manipulate the price at the end of a block, but unless they mine the next block as well, they may not have a particular advantage in arbitraging the trade back. Speci\ufb01cally, Uniswap v2 accumulates this price, by keeping track of the cumulative sum of prices at the beginning of each block in which someone interacts with the contract. Each price is weighted by the amount of time that has passed since the last block in which it was updated, according to the block timestamp.2 This means that the accumulator value at any given time (after being updated) should be the sum of the spot price at each second in the history of the contract.",
      "at  To estimate the time-weighted average price from time t1 to t2, an external caller can checkpoint the accumulators value at t1 and then again at t2, subtract the \ufb01rst value from the second, and divide by the number of seconds elapsed. (Note that the contract itself does not store historical values for this accumulatorthe caller has to call the contract at the beginning of the period to read and store this value.) pt1,t2  it1 pi t2 t1 i1 pi Pt1 i1 pi t2 t1  at2 at1 t2 t1 Users of the oracle can choose when to start and end this period. Choosing a longer period makes it more expensive for an attacker to manipulate the TWAP, although it results in a less up-to-date price. One complication: should we measure the price of asset A in terms of asset B, or the price of asset B in terms of asset A?",
      "While the spot price of A in terms of B is always the reciprocal of the spot price of B in terms of A, the mean price of asset A in terms of asset B over a particular period of time is not equal to the reciprocal of the mean price of asset B in terms of asset A.3 For example, if the USDETH price is 100 in block 1 and 300 in block 2, the average USDETH price will be 200 USDETH, but the average ETHUSD price will be 1150 ETHUSD. Since the contract cannot know which of the two assets users would want to use as the unit of account, Uniswap v2 tracks both prices. Another complication is that it is possible for someone to send assets to the pair con- tractand thus change its balances and marginal pricewithout interacting with it, and thus without triggering an oracle update.",
      "If the contract simply checked its own balances and updated the oracle based on the current price, an attacker could manipulate the oracle by sending an asset to the contract immediately before calling it for the \ufb01rst time in a block. If the last trade was in a block whose timestamp was X seconds ago, the contract would incorrectly multiply the new price by X before accumulating it, even though nobody 2Since miners have some freedom to set the block timestamp, users of the oracle should be aware that these values may not correspond precisely to real-world times. 3The arithmetic mean price of asset A in terms of asset B over a given period is equal to the reciprocal of the harmonic mean price of asset B in terms of asset A over that period. If the contract measured the geometric mean price, then the prices would be the reciprocals of each other. However, the geometric mean TWAP is less commonly used, and is di\ufb03cult to compute on Ethereum. has had an opportunity to trade at that price.",
      "To prevent this, the core contract caches its reserves after each interaction, and updates the oracle using the price derived from the cached reserves rather than the current reserves. In addition to protecting the oracle from manipulation, this change enables the contract re-architecture described below in section 3.2. 2.2.1 Precision Because Solidity does not have \ufb01rst-class support for non-integer numeric data types, the Uniswap v2 uses a simple binary \ufb01xed point format to encode and manipulate prices. Speci\ufb01cally, prices at a given moment are stored as UQ112.112 numbers, meaning that 112 fractional bits of precision are speci\ufb01ed on either side of the decimal point, with no sign. These numbers have a range of 0, 2112 14 and a precision of 2112 . The UQ112.112 format was chosen for a pragmatic reason  because these numbers can be stored in a uint224, this leaves 32 bits of a 256 bit storage slot free.",
      "It also happens that the reserves, each stored in a uint112, also leave 32 bits free in a (packed) 256 bit storage slot. These free spaces are used for the accumulation process described above. Speci\ufb01cally, the reserves are stored alongside the timestamp of the most recent block with at least one trade, modded with 232 so that it \ufb01ts into 32 bits. Additionally, although the price at any given moment (stored as a UQ112.112 number) is guaranteed to \ufb01t in 224 bits, the accumulation of this price over an interval is not. The extra 32 bits on the end of the storage slots for the accumulated price of AB and BA are used to store over\ufb02ow bits resulting from repeated summations of prices. This design means that the price oracle only adds an additional three SSTORE operations (a current cost of about 15,000 gas) to the \ufb01rst trade in each block. The primary downside is that 32 bits isnt quite enough to store timestamp values that will reasonably never over\ufb02ow.",
      "In fact, the date when the Unix timestamp over\ufb02ows a uint32 is 02072106. To ensure that this system continues to function properly after this date, and every multiple of 232 1 seconds thereafter, oracles are simply required to check prices at least once per interval (approximately 136 years). This is because the core method of accumulation (and modding of timestamp), is actually over\ufb02ow-safe, meaning that trades across over\ufb02ow intervals can be appropriately accounted for given that oracles are using the proper (simple) over\ufb02ow arithmetic to compute deltas. Flash Swaps In Uniswap v1, a user purchasing ABC with XYZ needs to send the XYZ to the contract before they could receive the ABC. This is inconvenient if that user needs the ABC they are buying in order to obtain the XYZ they are paying with.",
      "For example, the user might be using that ABC to purchase XYZ in some other contract in order to arbitrage a price di\ufb00erence from Uniswap, or they could be unwinding a position on Maker or Compound by selling the collateral to repay Uniswap. Uniswap v2 adds a new feature that allows a user to receive and use an asset before paying for it, as long as they make the payment within the same atomic transaction. The swap function makes a call to an optional user-speci\ufb01ed callback contract in between transferring out the tokens requested by the user and enforcing the invariant. Once the callback is complete, the contract checks the new balances and con\ufb01rms that the invariant is satis\ufb01ed 4The theoretical upper bound of 2112 ( 2112 ) does not apply in this setting, as UQ112.112 numbers in Uniswap are always generated from the ratio of two uint112s. The largest such ratio is 21121  2112 1. (after adjusting for fees on the amounts paid in).",
      "If the contract does not have su\ufb03cient funds, it reverts the entire transaction. A user can also repay the Uniswap pool using the same token, rather than completing the swap. This is e\ufb00ectively the same as letting anyone \ufb02ash-borrow any of assets stored in a Uniswap pool (for the same 0.30 fee as Uniswap charges for trading).5 Protocol fee Uniswap v2 includes a 0.05 protocol fee that can be turned on and o\ufb00. If turned on, this fee would be sent to a feeTo address speci\ufb01ed in the factory contract. Initially, feeTo is not set, and no fee is collected. A pre-speci\ufb01ed addressfeeToSettercan call the setFeeTo function on the Uniswap v2 factory contract, setting feeTo to a di\ufb00erent value. feeToSetter can also call the setFeeToSetter to change the feeToSetter address itself. If the feeTo address is set, the protocol will begin charging a 5-basis-point fee, which is taken as a 1 6 cut of the 30-basis-point fees earned by liquidity providers.",
      "That is, traders will continue to pay a 0.30 fee on all trades; 83.3 of that fee (0.25 of the amount traded) will go to liquidity providers, and 16.6 of that fee (0.05 of the amount traded) will go to the feeTo address. Collecting this 0.05 fee at the time of the trade would impose an additional gas cost on every trade. To avoid this, accumulated fees are collected only when liquidity is deposited or withdrawn. The contract computes the accumulated fees, and mints new liquidity tokens to the fee bene\ufb01ciary, immediately before any tokens are minted or burned. The total collected fees can be computed by measuring the growth in k (that is, x  y) since the last time fees were collected.6 This formula gives you the accumulated fees between t1 and t2 as a percentage of the liquidity in the pool at t2: f1,2  1  If the fee was activated before t1, the feeTo address should capture 1 6 of fees that were accumulated between t1 and t2.",
      "Therefore, we want to mint new liquidity tokens to the feeTo address that represent \u03c6  f1,2 of the pool, where \u03c6 is 1 That is, we want to choose sm to satisfy the following relationship, where s1 is the total quantity of outstanding shares at time t1: sm  s1  \u03c6  f1,2 After some manipulation, including substituting 1  k2 for f1,2 and solving for sm, we can rewrite this as: sm  k2 k1 \u03c6 1)  k2  k1  s1 Setting \u03c6 to 1 6 gives us the following formula: 5Because Uniswap charges fees on input amounts, the fee relative to the withdrawn amount is actually slightly higher: 10.003 1  997 0.3009203. 6We can use this invariant, which does not account for liquidity tokens that were minted or burned, because we know that fees are collected every time liquidity is deposited or withdrawn. sm  k2 k1 5  k2  k1  s1 Suppose the initial depositor puts 100 DAI and 1 ETH into a pair, receiving 10 shares.",
      "Some time later (without any other depositor having participated in that pair), they attempt to withdraw it, at a time when the pair has 96 DAI and 1.5 ETH. Plugging those values into the above formula gives us the following: sm  1.5  96  1  100 1.5  96  1  100  10 0.0286 Meta transactions for pool shares Pool shares minted by Uniswap v2 pairs natively support meta transactions. This means users can authorize a transfer of their pool shares with a signature7, rather than an on-chain transaction from their address. Anyone can submit this signature on the users behalf by calling the permit function, paying gas fees and possibly performing other actions in the same transaction. Other changes Solidity Uniswap v1 is implemented in Vyper, a Python-like smart contract language.",
      "Uniswap v2 is implemented in the more widely-used Solidity, since it requires some capabilities that were not yet available in Vyper (such as the ability to interpret the return values of non-standard ERC-20 tokens, as well as access to new opcodes such as chainid via inline assembly) at the time it was being developed. Contract re-architecture One design priority for Uniswap v2 is to minimize the surface area and complexity of the core pair contractthe contract that stores liquidity providers assets. Any bugs in this contract could be disastrous, since millions of dollars of liquidity might be stolen or frozen. When evaluating the security of this core contract, the most important question is whether it protects liquidity providers from having their assets stolen or locked. Any feature that is meant to support or protect tradersother than the basic functionality of allowing one asset in the pool to be swapped for anothercan be handled in a router contract.",
      "In fact, even part of the swap functionality can be pulled out into the router contract. As mentioned above, Uniswap v2 stores the last recorded balance of each asset (in order to prevent a particular manipulative exploit of the oracle mechanism). The new architecture takes advantage of this to further simplify the Uniswap v1 contract. In Uniswap v2, the seller sends the asset to the core contract before calling the swap function. Then, the contract measures how much of the asset it has received, by comparing the last recorded balance to its current balance. This means the core contract is agnostic 7The signed message conforms to the EIP-712 standard, the same one used by meta transactions for tokens like CHAI and DAI. to the way in which the trader transfers the asset. Instead of transferFrom, it could be a meta transaction, or any other future mechanism for authorizing the transfer of ERC-20s.",
      "3.2.1 Adjustment for fee Uniswap v1s trading fee is applied by reducing the amount paid into the contract by 0.3 before enforcing the constant-product invariant. The contract implicitly enforces the following formula: (x1 0.003  xin))  y1  x0  y0 With \ufb02ash swaps, Uniswap v2 introduces the possibility that xin and yin might both be non-zero (when a user wants to pay the pair back using the same asset, rather than swapping). To handle such cases while properly applying fees, the contract is written to enforce the following invariant:8 (x1 0.003  xin)  (y1 0.003  yin)  x0  y0 (10) To simplify this calculation on-chain, we can multiply each side of the inequality by 1,000,000: (1000  x1 3  xin)  (1000  y1 3  yin)  1000000  x0  y0 (11) 3.2.2 sync() and skim() To protect against bespoke token implementations that can update the pair contracts balance, and to more gracefully handle tokens whose total supply can be greater than 2112, Uniswap v2 has two bail-out functions: sync()and skim().",
      "sync() functions as a recovery mechanism in the case that a token asynchronously de\ufb02ates the balance of a pair. In this case, trades will receive sub-optimal rates, and if no liquidity provider is willing to rectify the situation, the pair is stuck. sync() exists to set the reserves of the contract to the current balances, providing a somewhat graceful recovery from this situation. skim() functions as a recovery mechanism in case enough tokens are sent to an pair to over\ufb02ow the two uint112 storage slots for reserves, which could otherwise cause trades to fail. skim() allows a user to withdraw the di\ufb00erence between the current balance of the pair and 2112 1 to the caller, if that di\ufb00erence is greater than 0. Handling non-standard and unusual tokens The ERC-20 standard requires that transfer() and transferFrom() return a boolean in- dicating the success or failure of the call 4.",
      "The implementations of one or both of these functions on some tokensincluding popular ones like Tether (USDT) and Binance Coin (BNB)instead have no return value. Uniswap v1 interprets the missing return value of these improperly de\ufb01ned functions as falsethat is, as an indication that the transfer was not successfuland reverts the transaction, causing the attempted transfer to fail. 8Note that using the new architecture, xin is not provided by the user; instead, it is calculated by measuring the contracts balance after the callback, x1, and subtracting (x0 - xout) from it. This logic does not distinguish between assets sent into the contract before it is called and assets sent into the contract during the callback. yin is computed in the same way, based on y0, y1, and yout. Uniswap v2 handles non-standard implementations di\ufb00erently. Speci\ufb01cally, if a transfer() call9 has no return value, Uniswap v2 interprets it as a success rather than as a failure.",
      "This change should not a\ufb00ect any ERC-20 tokens that conform to the standard (because in those tokens, transfer() always has a return value). Uniswap v1 also makes the assumption that calls to transfer() and transferFrom() cannot trigger a reentrant call to the Uniswap pair contract. This assumption is violated by certain ERC-20 tokens, including ones that support ERC-777s hooks 5. To fully support such tokens, Uniswap v2 includes a lock that directly prevents reentrancy to all public state- changing functions. This also protects against reentrancy from the user-speci\ufb01ed callback in a \ufb02ash swap, as described in section 2.3. Initialization of liquidity token supply When a new liquidity provider deposits tokens into an existing Uniswap pair, the number of liquidity tokens minted is computed based on the existing quantity of tokens: sminted  xdeposited xstarting  sstarting (12) But what if they are the \ufb01rst depositor? In that case, xstarting is 0, so this formula will not work.",
      "Uniswap v1 sets the initial share supply to be equal to the amount of ETH deposited (in wei). This was a somewhat reasonable value, because if the initial liquidity was deposited at the correct price, then 1 liquidity pool share (which, like ETH, is an 18-decimal token) would be worth approximately 2 ETH. However, this meant that the value of a liquidity pool share was dependent on the ratio at which liquidity was initially deposited, which was fairly arbitrary, especially since there was no guarantee that that ratio re\ufb02ected the true price. Additionally, Uniswap v2 supports arbitrary pairs, so many pairs will not include ETH at all. Instead, Uniswap v2 initially mints shares equal to the geometric mean of the amounts deposited: sminted  xdeposited  ydeposited (13) This formula ensures that the value of a liquidity pool share at any time is essentially independent of the ratio at which liquidity was initially deposited. For example, suppose that the price of 1 ABC is currently 100 XYZ.",
      "If the initial deposit had been 2 ABC and 200 XYZ (a ratio of 1:100), the depositor would have received 2  200  20 shares. Those shares should now still be worth 2 ABC and 200 XYZ, plus accumulated fees. If the initial deposit had been 2 ABC and 800 XYZ (a ratio of 1:400), the depositor would have received 2  800  40 pool shares.10 The above formula ensures that a liquidity pool share will never be worth less than the geometric mean of the reserves in that pool. However, it is possible for the value of 9As described above in section 3.2, Uniswap v2 core does not use transferFrom().",
      "10This also reduces the likelihood of rounding errors, since the number of bits in the quantity of shares will be approximately the mean of the number of bits in the quantity of asset X in the reserves, and the number of bits in the quantity of asset Y in the reserves: log2 x  y  log2 x  log2 y (14) a liquidity pool share to grow over time, either by accumulating trading fees or through donations to the liquidity pool. In theory, this could result in a situation where the value of the minimum quantity of liquidity pool shares (1e-18 pool shares) is worth so much that it becomes infeasible for small liquidity providers to provide any liquidity. To mitigate this, Uniswap v2 burns the \ufb01rst 1e-15 (0.000000000000001) pool shares that are minted (1000 times the minimum quantity of pool shares), sending them to the zero address instead of to the minter. This should be a negligible cost for almost any token pair.11 But it dramatically increases the cost of the above attack.",
      "In order to raise the value of a liquidity pool share to 100, the attacker would need to donate 100,000 to the pool, which would be permanently locked up as liquidity. Wrapping ETH The interface for transacting with Ethereums native asset, ETH, is di\ufb00erent from the standard interface for interacting with ERC-20 tokens. As a result, many other protocols on Ethereum do not support ETH, instead using a canonical wrapped ETH token, WETH 6. Uniswap v1 is an exception. Since every Uniswap v1 pair included ETH as one asset, it made sense to handle ETH directly, which was slightly more gas-e\ufb03cient. Since Uniswap v2 supports arbitrary ERC-20 pairs, it now no longer makes sense to support unwrapped ETH. Adding such support would double the size of the core codebase, and risks fragmentation of liquidity between ETH and WETH pairs12. Native ETH needs to be wrapped into WETH before it can be traded on Uniswap v2.",
      "Deterministic pair addresses As in Uniswap v1, all Uniswap v2 pair contracts are instantiated by a single factory contract. In Uniswap v1, these pair contracts were created using the CREATE opcode, which meant that the address of such a contract depended on the order in which that pair was created. Uniswap v2 uses Ethereums new CREATE2 opcode 8 to generate a pair contract with a deterministic address. This means that it is possible to calculate a pairs address (if it exists) o\ufb00-chain, without having to look at the chain state. Maximum token balance In order to e\ufb03ciently implement the oracle mechanism, Uniswap v2 only support reserve balances of up to 2112 1. This number is high enough to support 18-decimal-place tokens with a totalSupply over 1 quadrillion. If either reserve balance does go above 2112 1, any call to the swap function will begin to fail (due to a check in the _update() function).",
      "To recover from this situation, any user can call the skim() function to remove excess assets from the liquidity pool. 11In theory, there are some cases where this burn could be non-negligible, such as pairs between high-value zero-decimal tokens. However, these pairs are a poor \ufb01t for Uniswap anyway, since rounding errors would make trading infeasible. 12As of this writing, one of the highest-liquidity pairs on Uniswap v1 is the pair between ETH and WETH 7. Hayden Adams. 2018. url: https:hackmd.io477aQ9OrQTCbVR3fq1QzxgHJ9jLsfTz? typeview. Guillermo Angeris et al. An analysis of Uniswap markets. 2019. arXiv: 1911.03380 q-fin.TR. samczsun. Taking undercollateralized loans for fun and for pro\ufb01t. Sept. 2019. url: for-profit. Fabian Vogelsteller and Vitalik Buterin. Nov. 2015. url: https:eips.ethereum. orgEIPSeip-20. Jordi Baylina Jacques Da\ufb04on and Thomas Shababi. EIP 777: ERC777 Token Standard. Nov. 2017. url: https:eips.ethereum.orgEIPSeip-777. Radar. WTF is WETH? url: https:weth.io.",
      "Uniswap.info. Wrapped Ether (WETH). url: https:uniswap.infotoken0xc02aaa39b223fe8d0a0e5c4f27e Vitalik Buterin. EIP 1014: Skinny CREATE2. Apr. 2018. url: https:eips.ethereum. orgEIPSeip-1014. This paper is for general information purposes only. It does not constitute investment advice or a recommendation or solicitation to buy or sell any investment and should not be used in the evaluation of the merits of making any investment decision. It should not be relied upon for accounting, legal or tax advice or investment recommendations. This paper re\ufb02ects current opinions of the authors and is not made on behalf of Paradigm or its a\ufb03liates and does not necessarily re\ufb02ect the opinions of Paradigm, its a\ufb03liates or individuals associated with Paradigm. The opinions re\ufb02ected herein are subject to change without being updated."
    ],
    "word_count": 4747,
    "page_count": 10
  },
  "XMR": {
    "chunks": [
      "CryptoNote v 2.0 Nicolas van Saberhagen October 17, 2013 Introduction Bitcoin 1 has been a successful implementation of the concept of p2p electronic cash. Both professionals and the general public have come to appreciate the convenient combination of public transactions and proof-of-work as a trust model. Today, the user base of electronic cash is growing at a steady pace; customers are attracted to low fees and the anonymity provided by electronic cash and merchants value its predicted and decentralized emission. Bitcoin has e\ufb00ectively proved that electronic cash can be as simple as paper money and as convenient as credit cards. Unfortunately, Bitcoin su\ufb00ers from several de\ufb01ciencies. For example, the systems distributed nature is in\ufb02exible, preventing the implementation of new features until almost all of the net- work users update their clients. Some critical \ufb02aws that cannot be \ufb01xed rapidly deter Bitcoins widespread propagation.",
      "In such in\ufb02exible models, it is more e\ufb03cient to roll-out a new project rather than perpetually \ufb01x the original project. In this paper, we study and propose solutions to the main de\ufb01ciencies of Bitcoin. We believe that a system taking into account the solutions we propose will lead to a healthy competition among di\ufb00erent electronic cash systems. We also propose our own electronic cash, CryptoNote, a name emphasizing the next breakthrough in electronic cash. Bitcoin drawbacks and some possible solutions Traceability of transactions Privacy and anonymity are the most important aspects of electronic cash. Peer-to-peer payments seek to be concealed from third partys view, a distinct di\ufb00erence when compared with traditional banking. In particular, T. Okamoto and K. Ohta described six criteria of ideal electronic cash, which included privacy: relationship between the user and his purchases must be untraceable by anyone 30.",
      "From their description, we derived two properties which a fully anonymous electronic cash model must satisfy in order to comply with the requirements outlined by Okamoto and Ohta: Untraceability: for each incoming transaction all possible senders are equiprobable. Unlinkability: for any two outgoing transactions it is impossible to prove they were sent to the same person. Unfortunately, Bitcoin does not satisfy the untraceability requirement. Since all the trans- actions that take place between the networks participants are public, any transaction can be unambiguously traced to a unique origin and \ufb01nal recipient. Even if two participants exchange funds in an indirect way, a properly engineered path-\ufb01nding method will reveal the origin and \ufb01nal recipient. It is also suspected that Bitcoin does not satisfy the second property. Some researchers stated (33, 35, 29, 31) that a careful blockchain analysis may reveal a connection between the users of the Bitcoin network and their transactions.",
      "Although a number of methods are disputed 25, it is suspected that a lot of hidden personal information can be extracted from the public database. Bitcoins failure to satisfy the two properties outlined above leads us to conclude that it is not an anonymous but a pseudo-anonymous electronic cash system. Users were quick to develop solutions to circumvent this shortcoming. Two direct solutions were laundering services 2 and the development of distributed methods 3, 4. Both solutions are based on the idea of mixing several public transactions and sending them through some intermediary address; which in turn su\ufb00ers the drawback of requiring a trusted third party. Recently, a more creative scheme was proposed by I. Miers et al. 28: Zerocoin. Zerocoin utilizes a cryptographic one-way accumulators and zero-knoweldge proofs which permit users to convert bitcoins to zerocoins and spend them using anonymous proof of ownership instead of explicit public-key based digital signatures.",
      "However, such knowledge proofs have a constant but inconvenient size - about 30kb (based on todays Bitcoin limits), which makes the proposal impractical. Authors admit that the protocol is unlikely to ever be accepted by the majority of Bitcoin users 5. The proof-of-work function Bitcoin creator Satoshi Nakamoto described the majority decision making algorithm as one- CPU-one-vote and used a CPU-bound pricing function (double SHA-256) for his proof-of-work scheme. Since users vote for the single history of transactions order 1, the reasonableness and consistency of this process are critical conditions for the whole system. The security of this model su\ufb00ers from two drawbacks. First, it requires 51 of the networks mining power to be under the control of honest users.",
      "Secondly, the systems progress (bug \ufb01xes, security \ufb01xes, etc...) require the overwhelming majority of users to support and agree to the changes (this occurs when the users update their wallet software) 6.Finally this same voting mechanism is also used for collective polls about implementation of some features 7. This permits us to conjecture the properties that must be satis\ufb01ed by the proof-of-work pricing function. Such function must not enable a network participant to have a signi\ufb01cant advantage over another participant; it requires a parity between common hardware and high cost of custom devices. From recent examples 8, we can see that the SHA-256 function used in the Bitcoin architecture does not posses this property as mining becomes more e\ufb03cient on GPUs and ASIC devices when compared to high-end CPUs.",
      "Therefore, Bitcoin creates favourable conditions for a large gap between the voting power of participants as it violates the one-CPU-one-vote principle since GPU and ASIC owners posses a much larger voting power when compared with CPU owners. It is a classical example of the Pareto principle where 20 of a systems participants control more than 80 of the votes. One could argue that such inequality is not relevant to the networks security since it is not the small number of participants controlling the majority of the votes but the honesty of these participants that matters. However, such argument is somewhat \ufb02awed since it is rather the possibility of cheap specialized hardware appearing rather than the participants honesty which poses a threat. To demonstrate this, let us take the following example. Suppose a malevolent individual gains signi\ufb01cant mining power by creating his own mining farm through the cheap hardware described previously.",
      "Suppose that the global hashrate decreases signi\ufb01cantly, even for a moment, he can now use his mining power to fork the chain and double-spend. As we shall see later in this article, it is not unlikely for the previously described event to take place. Irregular emission Bitcoin has a predetermined emission rate: each solved block produces a \ufb01xed amount of coins. Approximately every four years this reward is halved. The original intention was to create a limited smooth emission with exponential decay, but in fact we have a piecewise linear emission function whose breakpoints may cause problems to the Bitcoin infrastructure. When the breakpoint occurs, miners start to receive only half of the value of their previous reward. The absolute di\ufb00erence between 12.5 and 6.25 BTC (projected for the year 2020) may seem tolerable. However, when examining the 50 to 25 BTC drop that took place on November 28 2012, felt inappropriate for a signi\ufb01cant number of members of the mining community.",
      "Figure 1 shows a dramatic decrease in the networks hashrate in the end of November, exactly when the halving took place. This event could have been the perfect moment for the malevolent individual described in the proof-of-work function section to carry-out a double spending attack 36. Fig. 1. Bitcoin hashrate chart (source: http:bitcoin.sipa.be) Hardcoded constants Bitcoin has many hard-coded limits, where some are natural elements of the original design (e.g. block frequency, maximum amount of money supply, number of con\ufb01rmations) whereas other seem to be arti\ufb01cial constraints. It is not so much the limits, as the inability of quickly changing them if necessary that causes the main drawbacks. Unfortunately, it is hard to predict when the constants may need to be changed and replacing them may lead to terrible consequences. A good example of a hardcoded limit change leading to disastrous consequences is the block size limit set to 250kb1.",
      "This limit was su\ufb03cient to hold about 10000 standard transactions. In early 2013, this limit had almost been reached and an agreement was reached to increase the limit. The change was implemented in wallet version 0.8 and ended with a 24-blocks chain split and a successful double-spend attack 9. While the bug was not in the Bitcoin protocol, but rather in the database engine it could have been easily caught by a simple stress test if there was no arti\ufb01cially introduced block size limit. Constants also act as a form of centralization point. Despite the peer-to-peer nature of Bitcoin, an overwhelming majority of nodes use the o\ufb03cial reference client 10 developed by a small group of people. This group makes the decision to implement changes to the protocol and most people accept these changes irrespective of their correctness. Some decisions caused heated discussions and even calls for boycott 11, which indicates that the community and the developers may disagree on some important points.",
      "It therefore seems logical to have a protocol with user-con\ufb01gurable and self-adjusting variables as a possible way to avoid these problems. Bulky scripts The scripting system in Bitcoin is a heavy and complex feature. It potentially allows one to create sophisticated transactions 12, but some of its features are disabled due to security concerns and some have never even been used 13. The script (including both senders and receivers parts) for the most popular transaction in Bitcoin looks like this: sig pubKey OP DUP OP HASH160 pubKeyHash OP EQUALVERIFY OP CHECKSIG. The script is 164 bytes long whereas its only purpose is to check if the receiver possess the secret key required to verify his signature. The CryptoNote Technology Now that we have covered the limitations of the Bitcoin technology, we will concentrate on presenting the features of CryptoNote.",
      "Untraceable Transactions In this section we propose a scheme of fully anonymous transactions satisfying both untraceability and unlinkability conditions. An important feature of our solution is its autonomy: the sender is not required to cooperate with other users or a trusted third party to make his transactions; hence each participant produces a cover tra\ufb03c independently. Literature review Our scheme relies on the cryptographic primitive called a group signature. First presented by D. Chaum and E. van Heyst 19, it allows a user to sign his message on behalf of the group. After signing the message the user provides (for veri\ufb01cation purposes) not his own single public 1This is so-called soft limit  the reference client restriction for creating new blocks. Hard maximum of possible blocksize was 1 MB key, but the keys of all the users of his group. A veri\ufb01er is convinced that the real signer is a member of the group, but cannot exclusively identify the signer.",
      "The original protocol required a trusted third party (called the Group Manager), and he was the only one who could trace the signer. The next version called a ring signature, introduced by Rivest et al. in 34, was an autonomous scheme without Group Manager and anonymity revocation. Various modi\ufb01cations of this scheme appeared later: linkable ring signature 26, 27, 17 allowed to determine if two signatures were produced by the same group member, traceable ring signature 24, 23 limited excessive anonymity by providing possibility to trace the signer of two messages with respect to the same metainformation (or tag in terms of 24). A similar cryptographic construction is also known as a ad-hoc group signature 16, 38. It emphasizes the arbitrary group formation, whereas groupring signature schemes rather imply a \ufb01xed set of members. For the most part, our solution is based on the work Traceable ring signature by E. Fujisaki and K. Suzuki 24.",
      "In order to distinguish the original algorithm and our modi\ufb01cation we will call the latter a one-time ring signature, stressing the users capability to produce only one valid signature under his private key. We weakened the traceability property and kept the linkability only to provide one-timeness: the public key may appear in many foreign verifying sets and the private key can be used for generating a unique anonymous signature. In case of a double spend attempt these two signatures will be linked together, but revealing the signer is not necessary for our purposes. De\ufb01nitions 4.2.1 Elliptic curve parameters As our base signature algorithm we chose to use the fast scheme EdDSA, which is developed and implemented by D.J. Bernstein et al. 18. Like Bitcoins ECDSA it is based on the elliptic curve discrete logarithm problem, so our scheme could also be applied to Bitcoin in future.",
      "Common parameters are: q: a prime number; q  2255 19; d: an element of Fq; d  121665121666; E: an elliptic curve equation; x2  y2  1  dx2y2; G: a base point; G  (x, 45); l: a prime order of the base point; l  2252  27742317777372353535851937790883648493; Hs: a cryptographic hash function 0, 1Fq; Hp: a deterministic hash function E(Fq) E(Fq). 4.2.2 Terminology Enhanced privacy requires a new terminology which should not be confused with Bitcoin entities.",
      "private ec-key is a standard elliptic curve private key: a number a 1, l 1; public ec-key is a standard elliptic curve public key: a point A  aG; one-time keypair is a pair of private and public ec-keys; private user key is a pair (a, b) of two di\ufb00erent private ec-keys; tracking key is a pair (a, B) of private and public ec-key (where B  bG and a  b); public user key is a pair (A, B) of two public ec-keys derived from (a, b); standard address is a representation of a public user key given into human friendly string with error correction; truncated address is a representation of the second half (point B) of a public user key given into human friendly string with error correction. The transaction structure remains similar to the structure in Bitcoin: every user can choose several independent incoming payments (transactions outputs), sign them with the corresponding private keys and send them to di\ufb00erent destinations.",
      "Contrary to Bitcoins model, where a user possesses unique private and public key, in the proposed model a sender generates a one-time public key based on the recipients address and some random data. In this sense, an incoming transaction for the same recipient is sent to a one-time public key (not directly to a unique address) and only the recipient can recover the corresponding private part to redeem his funds (using his unique private key). The recipient can spend the funds using a ring signature, keeping his ownership and actual spending anonymous. The details of the protocol are explained in the next subsections. Unlinkable payments Classic Bitcoin addresses, once being published, become unambiguous identi\ufb01er for incoming payments, linking them together and tying to the recipients pseudonyms. If someone wants to receive an untied transaction, he should convey his address to the sender by a private channel.",
      "If he wants to receive di\ufb00erent transactions which cannot be proven to belong to the same owner he should generate all the di\ufb00erent addresses and never publish them in his own pseudonym. Public Private Alice Carol Bobs addr 1 Bobs addr 2 Bobs key 1 Bobs key 2 Fig. 2. Traditional Bitcoin keystransactions model. We propose a solution allowing a user to publish a single address and receive unconditional unlinkable payments. The destination of each CryptoNote output (by default) is a public key, derived from recipients address and senders random data. The main advantage against Bitcoin is that every destination key is unique by default (unless the sender uses the same data for each of his transactions to the same recipient). Hence, there is no such issue as address reuse by design and no observer can determine if any transactions were sent to a speci\ufb01c address or link two addresses together. Public Private Alice Carol One-time key One-time key One-time key Bobs Key Bobs Address Fig. 3.",
      "CryptoNote keystransactions model. First, the sender performs a Di\ufb03e-Hellman exchange to get a shared secret from his data and half of the recipients address. Then he computes a one-time destination key, using the shared secret and the second half of the address. Two di\ufb00erent ec-keys are required from the recipient for these two steps, so a standard CryptoNote address is nearly twice as large as a Bitcoin wallet address. The receiver also performs a Di\ufb03e-Hellman exchange to recover the corresponding secret key. A standard transaction sequence goes as follows: 1. Alice wants to send a payment to Bob, who has published his standard address. unpacks the address and gets Bobs public key (A, B). 2. Alice generates a random r 1, l1 and computes a one-time public key P  Hs(rA)G 3. Alice uses P as a destination key for the output and also packs value R  rG (as a part of the Di\ufb03e-Hellman exchange) somewhere into the transaction.",
      "Note that she can create other outputs with unique public keys: di\ufb00erent recipients keys (Ai, Bi) imply di\ufb00erent Pi even with the same r. Transaction Tx public key Tx output Amount Destination key R  rG P  Hs(rA)G  B Receivers public key Senders ran- dom data (A, B) Fig. 4. Standard transaction structure. 4. Alice sends the transaction. 5. Bob checks every passing transaction with his private key (a, b), and computes P   Hs(aR)G  B. If Alices transaction for with Bob as the recipient was among them, then aR  arG  rA and P   P. 6. Bob can recover the corresponding one-time private key: x  Hs(aR)  b, so as P  xG. He can spend this output at any time by signing a transaction with x. Transaction Tx public key Tx output Amount Destination key P   Hs(aR)G  bG one-time public key x  Hs(aR)  b one-time private key Receivers private key (a, b) ? P Fig. 5. Incoming transaction check. As a result Bob gets incoming payments, associated with one-time public keys which are unlinkable for a spectator.",
      "Some additional notes:  When Bob recognizes his transactions (see step 5) he practically uses only half of his private information: (a, B). This pair, also known as the tracking key, can be passed to a third party (Carol). Bob can delegate her the processing of new transactions. Bob doesnt need to explicitly trust Carol, because she cant recover the one-time secret key p without Bobs full private key (a, b). This approach is useful when Bob lacks bandwidth or computation power (smartphones, hardware wallets etc.). In case Alice wants to prove she sent a transaction to Bobs address she can either disclose r or use any kind of zero-knowledge protocol to prove she knows r (for example by signing the transaction with r). If Bob wants to have an audit compatible address where all incoming transaction are linkable, he can either publish his tracking key or use a truncated address.",
      "That address represent only one public ec-key B, and the remaining part required by the protocol is derived from it as follows: a  Hs(B) and A  Hs(B)G. In both cases every person is able to recognize all of Bobs incoming transaction, but, of course, none can spend the funds enclosed within them without the secret key b. One-time ring signatures A protocol based on one-time ring signatures allows users to achieve unconditional unlinkability. Unfortunately, ordinary types of cryptographic signatures permit to trace transactions to their respective senders and receivers. Our solution to this de\ufb01ciency lies in using a di\ufb00erent signature type than those currently used in electronic cash systems. We will \ufb01rst provide a general description of our algorithm with no explicit reference to electronic cash. A one-time ring signature contains four algorithms: (GEN, SIG, VER, LNK): GEN: takes public parameters and outputs an ec-pair (P, x) and a public key I.",
      "SIG: takes a message m, a set S of public keys Piis, a pair (Ps, xs) and outputs a signature \u03c3 and a set S  S Ps. VER: takes a message m, a set S, a signature \u03c3 and outputs true or false. LNK: takes a set I  Ii, a signature \u03c3 and outputs linked or indep. The idea behind the protocol is fairly simple: a user produces a signature which can be checked by a set of public keys rather than a unique public key. The identity of the signer is indistinguishable from the other users whose public keys are in the set until the owner produces a second signature using the same keypair. Private keys       Public keys       Ring Signature sign verify Fig. 6. Ring signature anonymity. GEN: The signer picks a random secret key x 1, l 1 and computes the corresponding public key P  xG. Additionally he computes another public key I  xHp(P) which we will call the key image. SIG: The signer generates a one-time ring signature with a non-interactive zero-knowledge proof using the techniques from 21.",
      "He selects a random subset S of n from the other users public keys Pi, his own keypair (x, P) and key image I. Let 0 s n be signers secret index in S (so that his public key is Ps). He picks a random qi  i  0 . . . n and wi  i  0 . . . n, i  s from (1 . . . l) and applies the following transformations: Li  qiG, if i  s qiG  wiPi, if i  s Ri  qiHp(Pi), if i  s qiHp(Pi)  wiI, if i  s The next step is getting the non-interactive challenge: c  Hs(m, L1, . . . , Ln, R1, . . . , Rn) Finally the signer computes the response: ci  if i  s mod l, if i  s ri  if i  s qs csx mod l, if i  s The resulting signature is \u03c3  (I, c1, . . . , cn, r1, . . . , rn). VER: The veri\ufb01er checks the signature by applying the inverse transformations: i  riG  ciPi i  riHp(Pi)  ciI Finally, the veri\ufb01er checks if ? Hs(m, L 0, . . . , L n, R 0, . . . , R n) mod l If this equality is correct, the veri\ufb01er runs the algorithm LNK. Otherwise the veri\ufb01er rejects the signature.",
      "LNK: The veri\ufb01er checks if I has been used in past signatures (these values are stored in the set I). Multiple uses imply that two signatures were produced under the same secret key. The meaning of the protocol: by applying L-transformations the signer proves that he knows such x that at least one Pi  xG. To make this proof non-repeatable we introduce the key image as I  xHp(P). The signer uses the same coe\ufb03cients (ri, ci) to prove almost the same statement: he knows such x that at least one Hp(Pi)  I  x1. If the mapping x I is an injection: 1. Nobody can recover the public key from the key image and identify the signer; 2. The signer cannot make two signatures with di\ufb00erent Is and the same x. A full security analysis is provided in Appendix A. Standard CryptoNote transaction By combining both methods (unlinkable public keys and untraceable ring signature) Bob achieves new level of privacy in comparison with the original Bitcoin scheme.",
      "It requires him to store only one private key (a, b) and publish (A, B) to start receiving and sending anonymous transactions. While validating each transaction Bob additionally performs only two elliptic curve multi- plications and one addition per output to check if a transaction belongs to him. For his every output Bob recovers a one-time keypair (pi, Pi) and stores it in his wallet. Any inputs can be circumstantially proved to have the same owner only if they appear in a single transaction. In fact this relationship is much harder to establish due to the one-time ring signature. With a ring signature Bob can e\ufb00ectively hide every input among somebody elses; all possible spenders will be equiprobable, even the previous owner (Alice) has no more information than any observer. When signing his transaction Bob speci\ufb01es n foreign outputs with the same amount as his output, mixing all of them without the participation of other users.",
      "Bob himself (as well as anybody else) does not know if any of these payments have been spent: an output can be used in thousands of signatures as an ambiguity factor and never as a target of hiding. The double spend check occurs in the LNK phase when checking against the used key images set. Bob can choose the ambiguity degree on his own: n  1 means that the probability he has spent the output is 50 probability, n  99 gives 1. The size of the resulting signature increases linearly as O(n1), so the improved anonymity costs to Bob extra transaction fees. He also can set n  0 and make his ring signature to consist of only one element, however this will instantly reveal him as a spender. Transaction Tx input Output0 . . . Outputi . . . Outputn Key image Signatures Ring Signature Destination key Output1 Destination key Outputn Foreign transactions Senders output Destination key One-time keypair One-time private key I  xHp(P) P, x Fig. 7. Ring signature generation in a standard transaction.",
      "Egalitarian Proof-of-work In this section we propose and ground the new proof-of-work algorithm. Our primary goal is to close the gap between CPU (majority) and GPUFPGAASIC (minority) miners. It is appropriate that some users can have a certain advantage over others, but their investments should grow at least linearly with the power. More generally, producing special-purpose devices has to be as less pro\ufb01table as possible. Related works The original Bitcoin proof-of-work protocol uses the CPU-intensive pricing function SHA-256. It mainly consists of basic logical operators and relies solely on the computational speed of processor, therefore is perfectly suitable for multicoreconveyer implementation. However, modern computers are not limited by the number of operations per second alone, but also by memory size. While some processors can be substantially faster than others 8, memory sizes are less likely to vary between machines.",
      "Memory-bound price functions were \ufb01rst introduced by Abadi et al and were de\ufb01ned as functions whose computation time is dominated by the time spent accessing memory 15. The main idea is to construct an algorithm allocating a large block of data (scratchpad) within memory that can be accessed relatively slowly (for example, RAM) and accessing an unpredictable sequence of locations within it. A block should be large enough to make preserving the data more advantageous than recomputing it for each access. The algorithm also should prevent internal parallelism, hence N simultaneous threads should require N times more memory at once. Dwork et al 22 investigated and formalized this approach leading them to suggest another variant of the pricing function: Mbound. One more work belongs to F. Coelho 20, who proposed the most e\ufb00ective solution: Hokkaido. To our knowledge the last work based on the idea of pseudo-random searches in a big array is the algorithm known as scrypt by C. Percival 32.",
      "Unlike the previous functions it focuses on key derivation, and not proof-of-work systems. Despite this fact scrypt can serve our purpose: it works well as a pricing function in the partial hash conversion problem such as SHA-256 in Bitcoin. By now scrypt has already been applied in Litecoin 14 and some other Bitcoin forks. How- ever, its implementation is not really memory-bound: the ratio memory access time  overall time is not large enough because each instance uses only 128 KB. This permits GPU miners to be roughly 10 times more e\ufb00ective and continues to leave the possibility of creating relatively cheap but highly-e\ufb03cient mining devices. Moreover, the scrypt construction itself allows a linear trade-o\ufb00between memory size and CPU speed due to the fact that every block in the scratchpad is derived only from the previous. For example, you can store every second block and recalculate the others in a lazy way, i.e. only when it becomes necessary.",
      "The pseudo-random indexes are assumed to be uniformly distributed, hence the expected value of the additional blocks recalculations is 1 2 N, where N is the number of iterations. The overall computation time increases less than by half because there are also time independent (constant time) operations such as preparing the scratchpad and hashing on every iteration. Saving 23 of the memory costs 1 3  N  1 3  2  N  N additional recalculations; 910 results in 10  N  . . . 1 10  9  N  4.5N. It is easy to show that storing only 1 s of all blocks increases the time less than by a factor of s1 2 . This in turn implies that a machine with a CPU 200 times faster than the modern chips can store only 320 bytes of the scratchpad. The proposed algorithm We propose a new memory-bound algorithm for the proof-of-work pricing function. It relies on random access to a slow memory and emphasizes latency dependence.",
      "As opposed to scrypt every new block (64 bytes in length) depends on all the previous blocks. As a result a hypothetical memory-saver should increase his calculation speed exponentially. Our algorithm requires about 2 Mb per instance for the following reasons: 1. It \ufb01ts in the L3 cache (per core) of modern processors, which should become mainstream in a few years; 2. A megabyte of internal memory is an almost unacceptable size for a modern ASIC pipeline; 3. GPUs may run hundreds of concurrent instances, but they are limited in other ways: GDDR5 memory is slower than the CPU L3 cache and remarkable for its bandwidth, not random access speed. 4. Signi\ufb01cant expansion of the scratchpad would require an increase in iterations, which in turn implies an overall time increase. Heavy calls in a trust-less p2p network may lead to serious vulnerabilities, because nodes are obliged to check every new blocks proof-of-work.",
      "If a node spends a considerable amount of time on each hash evaluation, it can be easily DDoSed by a \ufb02ood of fake objects with arbitrary work data (nonce values). Further advantages Smooth emission The upper bound for the overall amount of CryptoNote digital coins is: MSupply  264 1 atomic units. This is a natural restriction based only on implementation limits, not on intuition such as N coins ought to be enough for anybody. To ensure the smoothness of the emission process we use the following formula for block rewards: BaseReward  (MSupply A) 18, where A is amount of previously generated coins. Adjustable parameters 6.2.1 Di\ufb03culty CryptoNote contains a targeting algorithm which changes the di\ufb03culty of every block. This decreases the systems reaction time when the network hashrate is intensely growing or shrinking, preserving a constant block rate.",
      "The original Bitcoin method calculates the relation of actual and target time-span between the last 2016 blocks and uses it as the multiplier for the current di\ufb03culty. Obviously this is unsuitable for rapid recalculations (because of large inertia) and results in oscillations. The general idea behind our algorithm is to sum all the work completed by the nodes and divide it by the time they have spent. The measure of work is the corresponding di\ufb03culty values in each block. But due to inaccurate and untrusted timestamps we cannot determine the exact time interval between blocks. A user can shift his timestamp into the future and the next time intervals might be improbably small or even negative. Presumably there will be few incidents of this kind, so we can just sort the timestamps and cut-o\ufb00the outliers (i.e. 20). The range of the rest values is the time which was spent for 80 of the corresponding blocks.",
      "6.2.2 Size limits Users pay for storing the blockchain and shall be entitled to vote for its size. Every miner deals with the trade-o\ufb00between balancing the costs and pro\ufb01t from the fees and sets his own soft-limit for creating blocks. Also the core rule for the maximum block size is necessary for preventing the blockchain from being \ufb02ooded with bogus transaction, however this value should not be hard-coded. Let MN be the median value of the last N blocks sizes. Then the hard-limit for the size of accepting blocks is 2  MN. It averts the blockchain from bloating but still allows the limit to slowly grow with time if necessary. Transaction size does not need to be limited explicitly. It is bounded by the size of a block; and if somebody wants to create a huge transaction with hundreds of inputsoutputs (or with the high ambiguity degree in ring signatures), he can do so by paying su\ufb03cient fee.",
      "6.2.3 Excess size penalty A miner still has the ability to stu\ufb00a block full of his own zero-fee transactions up to its maximum size 2  Mb. Even though only the majority of miners can shift the median value, there is still a possibility to bloat the blockchain and produce an additional load on the nodes. To discourage malevolent participants from creating large blocks we introduce a penalty function: NewReward  BaseReward  BlkSize This rule is applied only when BlkSize is greater than minimal free block size which should be close to max(10kb, MN 110). Miners are permitted to create blocks of usual size and even exceed it with pro\ufb01t when the overall fees surpass the penalty. But fees are unlikely to grow quadratically unlike the penalty value so there will be an equilibrium. Transaction scripts CryptoNote has a very minimalistic scripting subsystem. A sender speci\ufb01es an expression \u03a6  f (x1, x2, . . . , xn), where n is the number of destination public keys Pin i1.",
      "Only \ufb01ve binary operators are supported: min, max, sum, mul and cmp. When the receiver spends this payment, he produces 0 k n signatures and passes them to transaction input. The veri\ufb01cation process simply evaluates \u03a6 with xi  1 to check for a valid signature for the public key Pi, and xi  0. A veri\ufb01er accepts the proof i\ufb00\u03a6  0. Despite its simplicity this approach covers every possible case:  Multi-Threshold signature. For the Bitcoin-style M-out-of-N multi-signature (i.e. the receiver should provide at least 0 M N valid signatures) \u03a6  x1x2. . .xN M (for clarity we are using common algebraic notation). The weighted threshold signature (some keys can be more important than other) could be expressed as \u03a6  w1  x1  w2  x2  . . . wN  xN wM. And scenario where the master-key corresponds to \u03a6  max(M  x, x1  x2  . . . xN) M. It is easy to show that any sophisticated case can be expressed with these operators, i.e. they form basis. Password protection.",
      "Possession of a secret password s is equivalent to the knowledge of a private key, deterministically derived from the password: k  KDF(s). Hence, a receiver can prove that he knows the password by providing another signature under the key k. The sender simply adds the corresponding public key to his own output. Note that this method is much more secure than the transaction puzzle used in Bitcoin 13, where the password is explicitly passed in the inputs. Degenerate cases. \u03a6  1 means that anybody can spend the money; \u03a6  0 marks the output as not spendable forever. In the case when the output script combined with public keys is too large for a sender, he can use special output type, which indicates that the recipient will put this data in his input while the sender provides only a hash of it. This approach is similar to Bitcoins pay-to-hash feature, but instead of adding new script commands we handle this case at the data structure level.",
      "Conclusion We have investigated the major \ufb02aws in Bitcoin and proposed some possible solutions. These ad- vantageous features and our ongoing development make new electronic cash system CryptoNote a serious rival to Bitcoin, outclassing all its forks. Nobel prize laureate Friedrich Hayek in his famous work proves that the existence of con- current independent currencies has a huge positive e\ufb00ect. Each currency issuer (or developer in our case) is trying to attract users by improving his product. Currency is like a commodity: it can have unique bene\ufb01ts and shortcomings and the most convenient and trusted currency has the greatest demand. Suppose we had a currency excelling Bitcoin: it means that Bitcoin would develop faster and become better. The biggest support as an open source project would come from its own users, who are interested in it. We do not consider CryptoNote as a full replacement to Bitcoin.",
      "On the contrary, having two (or more) strong and convenient currencies is better than having just one. Running two and more di\ufb00erent projects in parallel is the natural \ufb02ow of electronic cash economics. Nicolas van Saberhagen Digitally signed by Nicolas van Saberhagen DN: cnNicolas van Saberhagen, emailnvsaberhagengmail.co Date: 2013.10.17 14:44:50 02'00' Security We shall give a proof for our one-time ring signature scheme. At some point it coincides with the parts of the proof in 24, but we decided to rewrite them with a reference rather than to force a reader to rush about from one paper to another. These are the properties to be established:  Linkability. Given all the secret keys xin i1 for a set S it is impossible to produce n  1 valid signatures \u03c31, \u03c32, . . . , \u03c3n1, such that all of them pass the LNK phase (i.e. with n  1 di\ufb00erent key images Ii). This property implies the double spending protection in the context of CryptoNote. Exculpability.",
      "Given set S, at most n1 corresponding private keys xi (excluding i  j) and the image Ij of the keys xj it is impossible to produce a valid signature \u03c3 with Ij. This property implies theft protection in the context of CryptoNote. Unforgeability. Given only a public keys set S it is impossible to produce a valid signature  Anonymity. Given a signature \u03c3 and the corresponding set S it is impossible to determine the secret index j of the signer with a probability p  1 Linkability Theorem 1. Our one-time ring signature scheme is linkable under the random oracle model. Proof. Suppose an adversary can produce n  1 valid signatures \u03c3i with key images Ii  Ij for any i, j 1 . . . n. Since S  n, at least one Ii  xiHp(Pi) for every i. Consider the corresponding signature \u03c3  (I, c1, . . . , cn, r1, . . . , rn). VER(\u03c3)  true, this means that i  riG  ciPi i  riHp(Pi)  ciI ci  Hs(m, L 1, . . . , L n, R 1, . . .",
      ", R mod l The \ufb01rst two equalities imply logG L i  ri  cixi logHp(Pi) R i  ri  ci logHp(Pi) I where logA B informally denotes the discrete logarithm of B to the base A. As in 24 we note that i : xi  logHp(Pi) I implies that all cis are uniquely determined. The third equality forces the adversary to \ufb01nd a pre-image of Hs to succeed in the attack, an event whose probability is considered to be negligible. Exculpability Theorem 2. Our one-time ring signature scheme is exculpable under the discrete logarithm assumption in the random oracle model. Proof. Suppose an adversary can produce a valid signature \u03c3  (I, c1, . . . , cn, r1, . . . , rn) with I  xjHP (Pj) with given xi  i  1, . . . , j1, j1, . . . , n. Then, we can construct an algorithm A which solves the discrete logarithm problem in E(Fq). Suppose inst  (G, P) E(Fq) is a given instance of the DLP and the goal is to get s, such that P  sG.",
      "Using the standard technique (as in 24), A simulates the random and signing oracles and makes the adversary produce two valid signatures with Pj  P in the set S: \u03c3  (I, c1, . . . , cn, r1, . . . , rn) and \u03c3  (I, c 1, . . . , c n, r 1, . . . , r Since I  xjHp(Pj) in both signatures we compute xj  logHp(Pj) I  rjr jcj mod l A outputs xj because Lj  rjG  cjPj  r jG  c jPj and Pj  P. Unforgeability It has been shown in 24 that unforgeability is just an implication of both linkability and excul- pability. Theorem 3. If a one-time ring signature scheme is linkable and exculpable, then it is unforgeable. Proof. Suppose an adversary can forge a signature for a given set S: \u03c30  (I0, . . .). Consider all valid signatures (produced by the honest signers) for the same message m and the set S: \u03c31, \u03c32, . . . , \u03c3n. There are two possible cases: 1. I0 Iin i1. Which contradicts exculpability. 2. I0 Iin i1. Which contradicts linkability. Anonymity Theorem 4.",
      "Our one-time ring signature scheme is anonymous under the decisional Di\ufb03e- Hellman assumption in the random oracle model. Proof. Suppose an adversary can determine the secret index j of the Signer with a probability n  \u03f5. Then, we can construct algorithm A which solves the decisional Di\ufb03e-Hellman problem in E(Fq) with the probability 1 2  \u03f5 Let inst  (G1, G2, Q1, Q2) E(Fq) be the instance of DDH and the goal to determine if logG1 Q1  logG2 Q2. A feeds the adversary with valid signature \u03c30  (I, . . .), where Pj  xjG1  Q1 and I  Q2 and simulates oracle Hp, returning G2 for query Hp(Pj). The adversary returns k as his guess for the index i: I  xiHP (Pi). If k  j, then A returns 1 (for yes) otherwise a random r 1, 0.",
      "The probability of the right choice is com- puted as in 24: 1 2 Pr (1  inst DDH)Pr (1  inst DDH)  1 2 Pr (k  j  inst DDH) Pr (k  j  inst DDH)Pr (r  1)Pr (k  j  inst DDH)Pr (k  j  inst DDH)Pr (r  0)  2  1 n  \u03f5  ( n1 \u03f5)  1 2 1 n n1 2  1 2  \u03f5 In fact, the result should be reduced by the probability of collision in Hs, but this value is considered to be negligible. Notes on the hash function Hp We de\ufb01ned Hp as deterministic hash function E(Fq) E(Fq). None of the proofs demands Hp to be an ideal cryptographic hash function. Its main purpose is to get a pseudo-random base for image key I  xHp(xG) in some determined way. With \ufb01xed base (I  xG2) the following scenario is possible: 1. Alice sends two standard transactions to Bob, generating one-time tx-keys: P2  Hs(r1A)G B and P1  Hs(r2A)G  B. 2. Bob recovers corresponding one-time private tx-keys x1 and x2 and spends the outputs with valid signatures and images keys I1  x1G2 and I2  x2G2. 3.",
      "Now Alice can link these signatures, checking the equality I1I2 ? (Hs(r1A)Hs(r2A))G2. The problem is that Alice knows the linear correlation between public keys P1 and P2 and in case of \ufb01xed base G2 she also gets the same correlation between key images I1 and I2. Replacing G2 with Hp(xG2), which does not preserve linearity, \ufb01xes that \ufb02aw. For constructing deterministic Hp we use algorithm presented in 37. 1 http:bitcoin.org. 2 https:en.bitcoin.itwikiCategory:Mixing Services. 3 http:blog.ezyang.com201207secure-multiparty-bitcoin-anonymization. 4 https:bitcointalk.orgindex.php?topic279249.0. 5 http:msrvideo.vo.msecnd.netrmcvideos192058dl192058.pdf. 6 https:github.combitcoinbipsblobmasterbip-0034.mediawikiSpeci\ufb01cation. 7 https:github.combitcoinbipsblobmasterbip-0016.mediawikiBackwards Compatibility. 8 https:en.bitcoin.itwikiMining hardware comparison. 9 https:github.combitcoinbipsblobmasterbip-0050.mediawiki. 10 http:luke.dashjr.orgprogramsbitcoin\ufb01leschartsbranches.html.",
      "11 https:bitcointalk.orgindex.php?topic196259.0. 12 https:en.bitcoin.itwikiContracts. 13 https:en.bitcoin.itwikiScript. 14 http:litecoin.org. 15 Mart\u0131n Abadi, Michael Burrows, and Ted Wobber. Moderately hard, memory-bound func- tions. In NDSS, 2003. 16 Ben Adida, Susan Hohenberger, and Ronald L. Rivest. Ad-hoc-group signatures from hi- jacked keypairs. In in DIMACS Workshop on Theft in E-Commerce, 2005. 17 Man Ho Au, Sherman S. M. Chow, Willy Susilo, and Patrick P. Tsang. Short linkable ring signatures revisited. In EuroPKI, pages 101115, 2006. 18 Daniel J. Bernstein, Niels Duif, Tanja Lange, Peter Schwabe, and Bo-Yin Yang. High-speed high-security signatures. J. Cryptographic Engineering, 2(2):7789, 2012. 19 David Chaum and Eugene van Heyst. Group signatures. In EUROCRYPT, pages 257265, 1991. 20 Fabien Coelho. Exponential memory-bound functions for proof of work protocols. IACR Cryptology ePrint Archive, 2005:356, 2005. 21 Ronald Cramer, Ivan Damgard, and Berry Schoenmakers.",
      "Proofs of partial knowledge and simpli\ufb01ed design of witness hiding protocols. In CRYPTO, pages 174187, 1994. 22 Cynthia Dwork, Andrew Goldberg, and Moni Naor. On memory-bound functions for \ufb01ghting spam. In CRYPTO, pages 426444, 2003. 23 Eiichiro Fujisaki. Sub-linear size traceable ring signatures without random oracles. In CT- RSA, pages 393415, 2011. 24 Eiichiro Fujisaki and Koutarou Suzuki. Traceable ring signature. In Public Key Cryptogra- phy, pages 181200, 2007. 25 Jezz Garzik. Peer review of quantitative analysis of the full bitcoin transaction graph. 26 Joseph K. Liu, Victor K. Wei, and Duncan S. Wong. Linkable spontaneous anonymous group signature for ad hoc groups (extended abstract). In ACISP, pages 325335, 2004. 27 Joseph K. Liu and Duncan S. Wong. Linkable ring signatures: Security models and new schemes. In ICCSA (2), pages 614623, 2005. 28 Ian Miers, Christina Garman, Matthew Green, and Aviel D. Rubin. Zerocoin: Anonymous distributed e-cash from bitcoin.",
      "In IEEE Symposium on Security and Privacy, pages 397 411, 2013. 29 Micha Ober, Stefan Katzenbeisser, and Kay Hamacher. Structure and anonymity of the bitcoin transaction graph. Future internet, 5(2):237250, 2013. 30 Tatsuaki Okamoto and Kazuo Ohta. Universal electronic cash. In CRYPTO, pages 324337, 1991. 31 Marc Santamaria Ortega. The bitcoin transaction graph  anonymity. Masters thesis, Universitat Oberta de Catalunya, June 2013. 32 Colin Percival. Stronger key derivation via sequential memory-hard functions. Presented at BSDCan09, May 2009. 33 Fergal Reid and Martin Harrigan. An analysis of anonymity in the bitcoin system. CoRR, abs1107.4524, 2011. 34 Ronald L. Rivest, Adi Shamir, and Yael Tauman. How to leak a secret. In ASIACRYPT, pages 552565, 2001. 35 Dorit Ron and Adi Shamir. Quantitative analysis of the full bitcoin transaction graph. IACR Cryptology ePrint Archive, 2012:584, 2012. 36 Meni Rosenfeld. Analysis of hashrate-based double-spending. 2012. 37 Maciej Ulas.",
      "Rational points on certain hyperelliptic curves over \ufb01nite \ufb01elds. Bulletin of the Polish Academy of Sciences. Mathematics, 55(2):97104, 2007. 38 Qianhong Wu, Willy Susilo, Yi Mu, and Fangguo Zhang. Ad hoc group signatures. In IWSEC, pages 120135, 2006."
    ],
    "word_count": 7592,
    "page_count": 20
  },
  "YFI": {
    "chunks": [
      "Yearn Docs Skip to main content User Docs Dev Docs DAO Docs Blog Community Discord Twitter Telegram Medium Governance Forum Snapshot Voting Start your Yearn journey. Everything you need to use Yearn User Docs Build the future of finance. Everything you need to build on Yearn Dev Docs Tame the decentralized beast. Everything you need to govern Yearn DAO Docs"
    ],
    "word_count": 60,
    "page_count": 1
  },
  "ZEC": {
    "chunks": [
      "Zcash Protocol Speci\ufb01cation Version 2025.6.3-12-ge6b7f9 NU6.1 Daira-Emma Hopwood Sean Bowe  Taylor Hornby  Nathan Wilcox January 14, 2026 Abstract. Zcash is an implementation of the Decentralized Anonymous Payment scheme Zerocash, with security \ufb01xes and improvements to performance and functionality. It bridges the existing transparent payment scheme used by Bitcoin with a shielded payment scheme secured by zero-knowledge succinct non-interactive arguments of knowledge (zk-SNARKs). It attempted to address the problem of mining centralization by use of the Equihash memory-hard proof-of-work algorithm. This speci\ufb01cation de\ufb01nes the Zcash consensus protocol at launch, and after each of the upgrades codenamed Overwinter, Sapling, Blossom, Heartwood, Canopy, NU55, NU66, and NU6.6.1. It is a work in progress. Protocol differences from Zerocash and Bitcoin are also explained.",
      "Keywords: anonymity, applications, cryptographic protocols, electronic commerce and payment, \ufb01nancial privacy, proof of work, zero knowledge. Contents Introduction Caution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . High-level Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Notation  Electric Coin Company 1 Jubjub bird image credit: Peter Newell 1902; Daira-Emma Hopwood 2018. Concepts Payment Addresses and Keys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.1 Note Plaintexts and Memo Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.2 Note Commitments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.3 Nulli\ufb01ers . . . . . . . .",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . The Block Chain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Transactions and Treestates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . JoinSplit Transfers and Descriptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Spend Transfers, Output Transfers, and their Descriptions . . . . . . . . . . . . . . . . . . . . . . . . . Action Transfers and their Descriptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Note Commitment Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Nulli\ufb01er Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.10 Block Subsidy, Funding Streams, and Founders Reward . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "3.11 Coinbase Transactions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.12 Mainnet and Testnet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Abstract Protocol Abstract Cryptographic Schemes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.1 Hash Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.2 Pseudo Random Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.3 Pseudo Random Permutations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.4 Symmetric Encryption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.5 Key Agreement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.6 Key Derivation . . . . . . . . . . . . . . . . . . . . . . .",
      ". . . . . . . . . . . . . . . . . . . . . . . . . 4.1.7 Signature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.7.1 Signature with Re-Randomizable Keys . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.7.2 Signature with Signing Key to Validating Key Monomorphism . . . . . . . . . . . . . 4.1.8 Commitment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.9 Represented Group . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.10 Coordinate Extractor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.11 Group Hash . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.12 Represented Pairing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.13 Zero-Knowledge Proving System . . . . . . . . . . . . . . . . .",
      ". . . . . . . . . . . . . . . . . . . Key Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.1 Sprout Key Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.2 Sapling Key Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.3 Orchard Key Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . JoinSplit Descriptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Spend Descriptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Output Descriptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Action Descriptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Sending Notes . . . . . . . . . . . . . . . . .",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.7.1 Sending Notes (Sprout) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.7.2 Sending Notes (Sapling) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.7.3 Sending Notes (Orchard) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Dummy Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.8.1 Dummy Notes (Sprout) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.8.2 Dummy Notes (Sapling) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.8.3 Dummy Notes (Orchard) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Merkle Path Validity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "4.10 SIGHASH Transaction Hashing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.11 Non-malleability (Sprout) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.12 Balance (Sprout) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.13 Balance and Binding Signature (Sapling) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.14 Balance and Binding Signature (Orchard) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.15 Spend Authorization Signature (Sapling and Orchard) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.16 Computing \u03c1 values and Nulli\ufb01ers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.17 Chain Value Pool Balances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.18 Zk-SNARK Statements . . . . . . . . . . . . . .",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.18.1 JoinSplit Statement (Sprout) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.18.2 Spend Statement (Sapling) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.18.3 Output Statement (Sapling) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.18.4 Action Statement (Orchard) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.19 In-band secret distribution (Sprout) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.19.1 Encryption (Sprout) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.19.2 Decryption (Sprout) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.20 In-band secret distribution (Sapling and Orchard) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "4.20.1 Encryption (Sapling and Orchard) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.20.2 Decryption using an Incoming Viewing Key (Sapling and Orchard) . . . . . . . . . . . . . . . 4.20.3 Decryption using an Outgoing Viewing Key (Sapling and Orchard) . . . . . . . . . . . . . . . . 4.21 Block Chain Scanning (Sprout) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.22 Block Chain Scanning (Sapling and Orchard) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Concrete Protocol Integers, Bit Sequences, and Endianness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Bit layout diagrams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Constants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Concrete Cryptographic Schemes . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      ". . . . . . . . . . . 5.4.1 Hash Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.1.1 SHA-256, SHA-256d, SHA256Compress, and SHA-512 Hash Functions . . . . . . . . . . 5.4.1.2 BLAKE2 Hash Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.1.3 Merkle Tree Hash Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.1.4 hSig Hash Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.1.5 CRHivk Hash Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.1.6 DiversifyHashSapling and DiversifyHashOrchard Hash Functions . . . . . . . . . . . . . . . . 5.4.1.7 Pedersen Hash Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.1.8 Mixing Pedersen Hash Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.1.9 Sinsemilla Hash Function . . . . . . . . . . . . .",
      ". . . . . . . . . . . . . . . . . . . . . . 5.4.1.10 PoseidonHash Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.1.11 Equihash Generator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.2 Pseudo Random Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.3 Symmetric Encryption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.4 Pseudo Random Permutations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.5 Key Agreement And Derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.5.1 Sprout Key Agreement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.5.2 Sprout Key Derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.5.3 Sapling Key Agreement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      ". . 5.4.5.4 Sapling Key Derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.5.5 Orchard Key Agreement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.5.6 Orchard Key Derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.6 Ed25519 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.7 RedDSA, RedJubjub, and RedPallas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.7.1 Spend Authorization Signature (Sapling and Orchard) . . . . . . . . . . . . . . . . . 5.4.7.2 Binding Signature (Sapling and Orchard) . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.8 Commitment schemes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.8.1 Sprout Note Commitments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.8.2 Windowed Pedersen commitments . . . . . . . . .",
      ". . . . . . . . . . . . . . . . . . . . 5.4.8.3 Homomorphic Pedersen commitments (Sapling and Orchard) . . . . . . . . . . . . 5.4.8.4 Sinsemilla commitments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.9 Represented Groups and Pairings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.9.1 BN-254 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.9.2 BLS12-381 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.9.3 Jubjub . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.9.4 Coordinate Extractor for Jubjub . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.9.5 Group Hash into Jubjub . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.9.6 Pallas and Vesta . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "5.4.9.7 Coordinate Extractor for Pallas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.9.8 Group Hash into Pallas and Vesta . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.10 Zero-Knowledge Proving Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.10.1 BCTV14 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.10.2 Groth16 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.10.3 Halo 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Encodings of Note Plaintexts and Memo Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Encodings of Addresses and Keys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.6.1 Transparent Encodings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "5.6.1.1 Transparent Addresses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.6.1.2 Transparent Private Keys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.6.2 Sprout Encodings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.6.2.1 Sprout Payment Addresses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.6.2.2 Sprout Incoming Viewing Keys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.6.2.3 Sprout Spending Keys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.6.3 Sapling Encodings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.6.3.1 Sapling Payment Addresses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.6.3.2 Sapling Incoming Viewing Keys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.6.3.3 Sapling Full Viewing Keys . . . . . . . . . . . .",
      ". . . . . . . . . . . . . . . . . . . . . . . 5.6.3.4 Sapling Spending Keys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.6.4 Uni\ufb01ed and Orchard Encodings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.6.4.1 Uni\ufb01ed Payment Addresses and Viewing Keys . . . . . . . . . . . . . . . . . . . . . . 5.6.4.2 Orchard Raw Payment Addresses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.6.4.3 Orchard Raw Incoming Viewing Keys . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.6.4.4 Orchard Raw Full Viewing Keys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.6.4.5 Orchard Spending Keys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . BCTV14 zk-SNARK Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Groth16 zk-SNARK Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Randomness Beacon . . . . .",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Network Upgrades Consensus Changes from Bitcoin Transaction Encoding and Consensus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.1.1 Transaction Identi\ufb01ers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.1.2 Transaction Consensus Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . JoinSplit Description Encoding and Consensus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Spend Description Encoding and Consensus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Output Description Encoding and Consensus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Action Description Encoding and Consensus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Block Header Encoding and Consensus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      ". . . . . Proof of Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.7.1 Equihash . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.7.2 Dif\ufb01culty \ufb01lter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.7.3 Dif\ufb01culty adjustment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.7.4 nBits conversion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.7.5 De\ufb01nition of Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Calculating Block Subsidy, Funding Streams, Lockbox Disbursement, and Founders Reward . . . . Payment of Founders Reward . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.10 Payment of Funding Streams, Deferred Lockbox, and Lockbox Disbursement . . . . . . .",
      ". . . . . . 7.10.1 ZIP 214 Funding Streams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.11 Changes to the Script System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.12 Bitcoin Improvement Proposals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Differences from the Zerocash paper Transaction Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Memo Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Uni\ufb01cation of Mints and Pours . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Faerie Gold attack and \ufb01x . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Internal hash collision attack and \ufb01x . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "Changes to PRF inputs and truncation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . In-band secret distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Omission in Zerocash security proof . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Miscellaneous . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 Change History 11 References Appendices Circuit Design Quadratic Constraint Programs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Elliptic curve background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Circuit Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.1 Operations on individual bits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "A.3.1.1 Boolean constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.1.2 Conditional equality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.1.3 Selection constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.1.4 Nonzero constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.1.5 Exclusive-or constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.2 Operations on multiple bits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.2.1 Unpacking modulo \ud835\udc5fS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.2.2 Range check . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.3 Elliptic curve operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "A.3.3.1 Checking that Af\ufb01ne-ctEdwards coordinates are on the curve . . . . . . . . . . . . . A.3.3.2 ctEdwards decompression and validation . . . . . . . . . . . . . . . . . . . . . . . . A.3.3.3 ctEdwards Montgomery conversion . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.3.4 Af\ufb01ne-Montgomery arithmetic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.3.5 Af\ufb01ne-ctEdwards arithmetic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.3.6 Af\ufb01ne-ctEdwards nonsmall-order check . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.3.7 Fixed-base Af\ufb01ne-ctEdwards scalar multiplication . . . . . . . . . . . . . . . . . . . . A.3.3.8 Variable-base Af\ufb01ne-ctEdwards scalar multiplication . . . . . . . . . . . . . . . . . . A.3.3.9 Pedersen hash . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.3.10 Mixing Pedersen hash . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "A.3.4 Merkle path check . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.5 Windowed Pedersen Commitment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.6 Homomorphic Pedersen Commitment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.7 BLAKE2s hashes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . The Sapling Spend circuit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . The Sapling Output circuit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Batching Optimizations RedDSA batch validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Groth16 batch veri\ufb01cation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Ed25519 batch validation . . . . . . . . . . . . . . . . . . . . . . . . . .",
      ". . . . . . . . . . . . . . . . . . . . List of Theorems and Lemmata Index Introduction Zcash is an implementation of the Decentralized Anonymous Payment scheme Zerocash BCGGMTV2014, with security \ufb01xes and improvements to performance and functionality. It bridges the existing transparent payment scheme used by Bitcoin Nakamoto2008 with a shielded payment scheme secured by zero-knowledge succinct non-interactive arguments of knowledge (zk-SNARKs). In this document, technical terms for concepts that play an important r\u00f4le in Zcash are written in slanted text, which links to an index entry. Italics are used for emphasis and for references between sections of the document. The symbol  precedes section numbers in cross-references. The keywords MUST, MUSTNOT, SHOULD, SHOULD NOT, RECOMMENDED, MAY, and OPTIONALin this document are to be interpreted as described in RFC-2119 when they appear in ALL CAPS.",
      "These words may also appear in this document in lower case as plain English words, absent their normative meanings. The most signi\ufb01cant changes from the original Zerocash are explained in section8 Differences from the Zerocash paper on page 142. Changes speci\ufb01c to the Overwinter upgrade are highlighted in bright blue. Changes speci\ufb01c to the Sapling upgrade following Overwinter are highlighted in green. Changes speci\ufb01c to the Blossom upgrade following Sapling are highlighted in red. Changes speci\ufb01c to the Heartwood upgrade following Blossom are highlighted in orange. Changes speci\ufb01c to the Canopy upgrade following Heartwood are highlighted in purple. Changes speci\ufb01c to the NU55 upgrade following Canopy are highlighted in slate blue. Changes speci\ufb01c to the NU66 upgrade following NU55 are highlighted in pink. Changes speci\ufb01c to the NU6.6.1 upgrade following NU66 are highlighted in burgundy. All of these are also changes from Zerocash.",
      "The name Sprout is used for the shielded protocol de\ufb01ned prior to the Sapling upgrade. This speci\ufb01cation is structured as follows:  Notation  de\ufb01nitions of notation used throughout the document;  Concepts  the principal abstractions needed to understand the protocol;  Abstract Protocol  a high-level description of the protocol in terms of ideal cryptographic components;  Concrete Protocol  how the functions and encodings of the abstract protocol are instantiated;  Network Upgrades  the strategy for upgrading the Zcash protocol. Consensus Changes from Bitcoin  how Zcash differs from Bitcoin at the consensus layer, including the Proof of Work;  Differences from the Zerocash protocol  a summary of changes from the protocol in BCGGMTV2014. Appendix: Circuit Design  details of how the Sapling circuits are de\ufb01ned as quadratic constraint programs. Appendix: Batching Optimizations  improvements to the ef\ufb01ciency of validating multiple signatures and verifying multiple proofs.",
      "Caution Zcash security depends on consensus. Should a program interacting with the Zcash network diverge from con- sensus, its security will be weakened or destroyed. The cause of the divergence doesnt matter: it could be a bug in your program, it could be an error in this documentation which you implemented as described, or it could be that you do everything right but other software on the network behaves unexpectedly. The speci\ufb01c cause will not matter to the users of your software whose wealth is lost. Having said that, a speci\ufb01cation of intended behaviour is essential for security analysis, understanding of the protocol, and maintenance of Zcash and related software. If you \ufb01nd any mistake in this speci\ufb01cation, please \ufb01le an issue at https:github.comzcashzipsissues or contact securityz.cash.",
      "High-level Overview The following overview is intended to give a concise summary of the ideas behind the protocol, for an audience already familiar with block chain-based cryptocurrencies such as Bitcoin. It is imprecise in some aspects and is not part of the normative protocol speci\ufb01cation. This overview applies to Sprout, Sapling, and Orchard, differences in the cryptographic constructions used notwithstanding. All value in Zcash belongs to some chain value pool. There is a single transparent chain value pool, a chain value pool for each shielded protocol (Sprout or Sapling or Orchard), and a deferred development fund chain value pool. Transfers of transparent value work essentially as in Bitcoin and have the same privacy properties. Value in a shielded chain value pool is carried by notes 2, which specify an amount and (indirectly) a shielded payment address, which is a destination to which notes can be sent.",
      "As in Bitcoin, this is associated with a private key that can be used to spend notes sent to the address; in Zcash this is called a spending key. To each note there is cryptographically associated a note commitment. Once the transaction creating a note has been mined, the note is associated with a \ufb01xed note position in a tree of note commitments, and with a nulli\ufb01er2 unique to that note. Computing the nulli\ufb01er requires the associated private spending key (or the nulli\ufb01er deriving key for Sapling or Orchard notes). It is infeasible to correlate the note commitment or note position with the corresponding nulli\ufb01er without knowledge of at least this key. An unspent valid note, at a given point on the block chain, is one for which the note commitment has been publically revealed on the block chain prior to that point, but the nulli\ufb01er has not. A transaction can contain transparent inputs, outputs, and scripts, which all work as in Bitcoin Bitcoin-Protocol.",
      "It also can include JoinSplit descriptions, Spend descriptions, Output descriptions and Action descriptions. Together these describe shielded transfers which take in shielded input notes, andor produce shielded output notes. (For Sprout, each JoinSplit description handles up to two shielded inputs and up to two shielded outputs. For Sapling, each shielded input or shielded output has its own description. For Orchard, each Action description handles up to one shielded input and up to one shielded output.) It is also possible for value to be transferred between chain value pools, either transparent or shielded; this always reveals the amount transferred. In each shielded transfer, the nulli\ufb01ers of the input notes are revealed (preventing them from being spent again) and the commitments of the output notes are revealed (allowing them to be spent in future).",
      "A transaction also includes computationally sound zk-SNARK proofs and signatures, which prove that all of the following hold except with insigni\ufb01cant probability: For each shielded input,  Sapling onward there is a revealed value commitment to the same value as the input note;3  if the value is nonzero, some revealed note commitment exists for this note;  the prover knew the proof authorizing key of the note; 2 In Zerocash BCGGMTV2014, notes were called coins, and nulli\ufb01ers were called serial numbers. 3 For Orchard, each Action reveals a single value commitment to the net value spent by the Action, rather than one value commitment for the input note and one for the output note. the nulli\ufb01er and note commitment are computed correctly.",
      "and for each shielded output,  Sapling onward there is a revealed value commitment to the same value as the output note;3  the note commitment is computed correctly;  it is infeasible to cause the nulli\ufb01er of the output note to collide with the nulli\ufb01er of any other note. For Sprout, the JoinSplit statement also includes an explicit balance check. For Sapling and Orchard, the value commitments corresponding to the inputs and outputs are checked to balance (together with any net transparent input or output) outside the zk-SNARK. In addition, various measures (differing between Sprout and Sapling or Orchard) are used to ensure that the transaction cannot be modi\ufb01ed by a party not authorized to do so. Outside the zk-SNARK, it is checked that the nulli\ufb01ers for the input notes had not already been revealed (i.e. they had not already been spent). A shielded payment address includes a transmission key for a key-private asymmetric encryption scheme.",
      "Key-private means that ciphertexts do not reveal information about which key they were encrypted to, except to a holder of the corresponding private key, which in this context is called the receiving key. This facility is used to communicate encrypted output notes on the block chain to their intended recipient, who can use the receiving key to scan the block chain for notes addressed to them and then decrypt those notes. In Sapling and Orchard, for each spending key there is a full viewing key that allows recognizing both incoming and outgoing notes without having spending authority. This is implemented by an additional ciphertext in each Output description or Action description. The basis of the privacy properties of Zcash is that when a note is spent, the spender only proves that some commitment for it had been revealed, without revealing which one. This implies that a spent note cannot be linked to the transaction in which it was created.",
      "That is, from an adversarys point of view the set of possibilities for a given note input to a transaction its note traceability set includes all previous notes that the adversary does not control or know to have been spent.4 This contrasts with other proposals for private payment systems, such as CoinJoin Bitcoin-CoinJoin or CryptoNote vanSaberh2014, that are based on mixing of a limited number of transactions and that therefore have smaller note traceability sets. The nulli\ufb01ers are necessary to prevent double-spending: each note on the block chain only has one valid nulli\ufb01er, and so attempting to spend a note twice would reveal the nulli\ufb01er twice, which would cause the second transaction to be rejected. 4 We make this claim only for fully shielded transactions.",
      "It does not exclude the possibility that an adversary may use data present in the cleartext of a transaction such as the number of inputs and outputs, or metadata-based heuristics such as timing, to make proba- bilistic inferences about transaction linkage. For consequences of this in the case of partially shielded transactions, see Peterson2017, Quesnelle2017, and KYMM2018. Notation B means the type of bit values, i.e. 0, 1. BY means the type of byte values, i.e. 0 .. 255. N means the type of nonnegative integers. N means the type of positive integers. Z means the type of integers. Q means the type of rationals. \ud835\udc47is used to specify that \ud835\udc65has type \ud835\udc47. A cartesian product type is denoted by \ud835\udc46 \ud835\udc47, and a function type by \ud835\udc46\ud835\udc47. An argument to a function can determine other argument or result types. The type of a randomized algorithm is denoted by \ud835\udc46 R \ud835\udc47. The domain of a randomized algorithm may be (), indicating that it requires no arguments.",
      "Given \ud835\udc53 R \ud835\udc47and \ud835\udc60 \ud835\udc46, sampling a variable \ud835\udc65 \ud835\udc47from the output of \ud835\udc53 applied to \ud835\udc60is denoted by \ud835\udc65 R \ud835\udc53(\ud835\udc60). Initial arguments to a function or randomized algorithm may be written as subscripts, e.g. if \ud835\udc65 \ud835\udc4b, \ud835\udc66 \ud835\udc4c, and \ud835\udc4b \ud835\udc4c\ud835\udc4d, then an invocation of \ud835\udc53(\ud835\udc65, \ud835\udc66) can also be written \ud835\udc53\ud835\udc65(\ud835\udc66). \ud835\udc47 \ud835\udc5d\ud835\udc65 means the subset of \ud835\udc65from \ud835\udc47for which \ud835\udc5d\ud835\udc65(a boolean expression depending on \ud835\udc65) holds. \ud835\udc47\ud835\udc48indicates that \ud835\udc47is an inclusive subset or subtype of \ud835\udc48. \ud835\udc46\ud835\udc47means the set union of \ud835\udc46and \ud835\udc47. \ud835\udc46\ud835\udc47means the set intersection of \ud835\udc46and \ud835\udc47, i.e. \ud835\udc65 \ud835\udc46 \ud835\udc65\ud835\udc47. \ud835\udc46\ud835\udc47means the set difference obtained by removing elements in \ud835\udc47from \ud835\udc46, i.e. \ud835\udc65 \ud835\udc46 \ud835\udc65\ud835\udc47. \ud835\udc47\ud835\udc52\ud835\udc65 \ud835\udc48means the function of type \ud835\udc47\ud835\udc48mapping formal parameter \ud835\udc65to \ud835\udc52\ud835\udc65(an expression depending on \ud835\udc65). The types \ud835\udc47and \ud835\udc48are always explicit. \ud835\udc47\ud835\udc49\ud835\udc52\ud835\udc65 \ud835\udc48means \ud835\udc65 \ud835\udc47\ud835\udc52\ud835\udc65 \ud835\udc48\ud835\udc49restricted to the domain \ud835\udc65 \ud835\udc47 \ud835\udc52\ud835\udc65\ud835\udc49 and range \ud835\udc48. means the powerset of \ud835\udc47. is a distinguished value used to indicate unavailable information, a failed decryption or validity check, or an exceptional case.",
      "\ud835\udc47\u2113, where \ud835\udc47is a type and \u2113is an integer, means the type of sequences of length \u2113with elements in \ud835\udc47. For example, B\u2113 means the set of sequences of \u2113bits, and BY\ud835\udc58 means the set of sequences of \ud835\udc58bytes. BYN means the type of byte sequences of arbitrary length. length(\ud835\udc46) means the length of (number of elements in) \ud835\udc46. truncate\ud835\udc58(\ud835\udc46) means the sequence formed from the \ufb01rst \ud835\udc58elements of \ud835\udc46. 0x followed by a string of monospace hexadecimal digits means the corresponding integer converted from hexadec- imal. 0x00\u2113means the sequence of \u2113zero bytes. ... means the given string represented as a sequence of bytes in US-ASCII. For example, abc represents the byte sequence  0x61, 0x62, 0x63 . 0\u2113means the sequence of \u2113zero bits. 1\u2113means the sequence of \u2113one bits. \ud835\udc4e..\ud835\udc4f, used as a subscript, means the sequence of values with indices \ud835\udc4ethrough \ud835\udc4finclusive. For example, anew pk,1..Nnew means the sequence anew pk,1, anew pk,2, ... anew pk,Nnew.",
      "(For consistency with the notation in BCGGMTV2014 and in BK2016, this speci\ufb01cation uses 1-based indexing and inclusive ranges, notwithstanding the compelling arguments to the contrary made in EWD-831.) \ud835\udc4e.. \ud835\udc4f means the set or type of integers from \ud835\udc4ethrough \ud835\udc4finclusive. \ud835\udc53(\ud835\udc65) for \ud835\udc65from \ud835\udc4eup to \ud835\udc4f means the sequence formed by evaluating \ud835\udc53on each integer from \ud835\udc4eto \ud835\udc4finclusive, in ascending order. Similarly,  \ud835\udc53(\ud835\udc65) for \ud835\udc65from \ud835\udc4edown to \ud835\udc4f means the sequence formed by evaluating \ud835\udc53on each integer from \ud835\udc4eto \ud835\udc4finclusive, in descending order. \ud835\udc4e \ud835\udc4fmeans the concatenation of sequences \ud835\udc4ethen \ud835\udc4f. concatB(\ud835\udc46) means the sequence of bits obtained by concatenating the elements of \ud835\udc46as bit sequences. sorted(\ud835\udc46) means the sequence formed by sorting the elements of \ud835\udc46. F\ud835\udc5bmeans the \ufb01nite \ufb01eld with \ud835\udc5belements, and F \ud835\udc5bmeans its group under multiplication (which excludes 0). Where there is a need to make the distinction, we denote the unique representative of \ud835\udc4e F\ud835\udc5bin the range 0 ..",
      "\ud835\udc5b1 (or the unique representative of \ud835\udc4e \ud835\udc5bin the range 1 .. \ud835\udc5b1) as \ud835\udc4emod \ud835\udc5b. Conversely, we denote the element of F\ud835\udc5b corresponding to an integer \ud835\udc58 Z as \ud835\udc58(mod \ud835\udc5b). We also use the latter notation in the context of an equality \ud835\udc58 \ud835\udc58 (mod \ud835\udc5b) as shorthand for \ud835\udc58mod \ud835\udc5b \ud835\udc58 mod \ud835\udc5b, and similarly \ud835\udc58 \ud835\udc58 (mod \ud835\udc5b) as shorthand for \ud835\udc58mod \ud835\udc5b \ud835\udc58 mod \ud835\udc5b. (When referring to constants such as 0 and 1 it is usually not necessary to make the distinction between \ufb01eld elements and their representatives, since the meaning is normally clear from context.) F\ud835\udc5b\ud835\udc67 means the ring of polynomials over \ud835\udc67with coef\ufb01cients in F\ud835\udc5b. \ud835\udc4e \ud835\udc4fmeans the sum of \ud835\udc4eand \ud835\udc4f. This may refer to addition of integers, rationals, \ufb01nite \ufb01eld elements, or group elements (see section4.1.9 Represented Group on page 32) according to context. \ud835\udc4emeans the value of the appropriate integer, rational, \ufb01nite \ufb01eld, or group type such that (\ud835\udc4e)  \ud835\udc4e 0 (or when \ud835\udc4e is an element of a group G, (\ud835\udc4e)  \ud835\udc4e \ud835\udcaaG), and \ud835\udc4e\ud835\udc4fmeans \ud835\udc4e (\ud835\udc4f). \ud835\udc4e \ud835\udc4fmeans the product of multiplying \ud835\udc4eand \ud835\udc4f.",
      "This may refer to multiplication of integers, rationals, or \ufb01nite \ufb01eld elements according to context (this notation is not used for group elements). \ud835\udc4e\ud835\udc4f, also written \ud835\udc4e \ud835\udc4f, means the value of the appropriate integer, rational, or \ufb01nite \ufb01eld type such that (\ud835\udc4e\ud835\udc4f)  \ud835\udc4f \ud835\udc4e. \ud835\udc4emod \ud835\udc5e, for \ud835\udc4e N and \ud835\udc5e N, means the remainder on dividing \ud835\udc4eby \ud835\udc5e. (This usage does not con\ufb02ict with the notation above for the unique representative of a \ufb01eld element.) \ud835\udc4e\ud835\udc4fmeans the bitwise-exclusive-or of \ud835\udc4eand \ud835\udc4f, and \ud835\udc4e\u00ee \ud835\udc4fmeans the bitwise-and of \ud835\udc4eand \ud835\udc4f. These are de\ufb01ned on integers (which include bits and bytes), or elementwise on equal-length sequences of integers, according to context. \ud835\udc4e\ud835\udc56means the sum of \ud835\udc4e1..N. \ud835\udc4e\ud835\udc56means the product of \ud835\udc4e1..N. \ud835\udc4e\ud835\udc56means the bitwise-exclusive-or of \ud835\udc4e1..N. When \ud835\udc41 0 these yield the appropriate neutral element, i.e. 0 \ud835\udc561\ud835\udc4e\ud835\udc56 0, 0 \ud835\udc561\ud835\udc4e\ud835\udc56 1, and 0 \ud835\udc561\ud835\udc4e\ud835\udc56 0 or the all-zero bit sequence of length given by the type of \ud835\udc4e. \ud835\udc4e, where \ud835\udc4e F\ud835\udc5e, means the positive square root of \ud835\udc4ein F\ud835\udc5e, i.e. in the range 0 .. \ud835\udc5e1 .",
      "It is only used in cases where the square root must exist. \ud835\udc4e, where \ud835\udc4e F\ud835\udc5e, means an arbitrary square root of \ud835\udc4ein F\ud835\udc5e, or if no such square root exists. \ud835\udc4f? \ud835\udc65: \ud835\udc66means \ud835\udc65when \ud835\udc4f 1, or \ud835\udc66when \ud835\udc4f 0. \ud835\udc4e\ud835\udc4f, for \ud835\udc4ean integer or \ufb01nite \ufb01eld element and \ud835\udc4f Z, means the result of raising \ud835\udc4eto the exponent \ud835\udc4f, i.e. \ud835\udc4e\ud835\udc4f: \ud835\udc561 \ud835\udc4e, if \ud835\udc4f0 \ud835\udc4f \ud835\udc4e, otherwise. The \ud835\udc58 \ud835\udc43notation for scalar multiplication in a group is de\ufb01ned in section4.1.9 Represented Group on page 32. The convention of af\ufb01xing to a variable name is used for variables that denote bit sequence representations of group elements. The binary relations , , , , and  have their conventional meanings on integers and rationals, and are de\ufb01ned lexicographically on sequences of integers. floor(\ud835\udc65) means the largest integer \ud835\udc65. ceiling(\ud835\udc65) means the smallest integer \ud835\udc65. bitlength(\ud835\udc65), for \ud835\udc65 N, means the smallest integer \u2113such that 2\u2113 \ud835\udc65.",
      "The following integer constants will be instantiated in section5.3 Constants on page 74: MerkleDepthSprout, MerkleDepthSapling, MerkleDepthOrchard, \u2113Sprout Merkle, \u2113Sapling Merkle , \u2113Orchard Merkle , Nold, Nnew, \u2113value, \u2113hSig, \u2113Sprout PRF , \u2113PRFexpand, \u2113PRFnfSapling, \u2113Sprout , \u2113Seed, \u2113ask, \u2113Sprout , \u2113sk, \u2113d, \u2113dk, \u2113Sapling , \u2113ovk, \u2113Sapling scalar , \u2113Orchard scalar , \u2113Orchard base , MAX_MONEY, BlossomActivationHeight, CanopyActivationHeight, ZIP212GracePeriod, NUFiveActivationHeight, SlowStartInterval, PreBlossomHalvingInterval, MaxBlockSubsidy, NumFounderAddresses, PoWLimit, PoWAveragingWindow, PoWMedianBlockSpan, PoWDampingFactor, PreBlossomPoWTargetSpacing, and PostBlossomPoWTargetSpacing. The rational constants FoundersFraction, PoWMaxAdjustDown, and PoWMaxAdjustUp; the bit-sequence constants UncommittedSprout B\u2113Sprout Merkle and UncommittedSapling B\u2113Sapling Merkle ; and the constant UncommittedOrchard 0 .. \ud835\udc5eP 1 will also be de\ufb01ned in that section.",
      "We use the abbreviation ctEdwards to refer to complete twisted Edwards elliptic curves and coordinates (see section5.4.9.3 Jubjub on page 102). Concepts Payment Addresses and Keys Users who wish to receive shielded payments in the Zcash protocol must have a shielded payment address, which is generated from a spending key. The following diagrams depict the relations between key components in Sprout and Sapling and Orchard. Arrows point from a component to any other component(s) that can be derived from it. Double lines indicate that the same component is used in multiple abstractions. Image description: three diagrams showing the derivation of Sprout, Sapling, and Orchard key components. The key components are shown by their mathematical abbreviations. They are collected into rounded boxes for each named abstraction, which are colour-coded from purple to green in the direction of derivation, or from more secret to less secret values.",
      "The information in the diagram is otherwise mostly redundant with the textual description of the key components and their derivation that follows it. Sprout A Sprout receiving key skenc, incoming viewing key ivk  (apk, skenc), and shielded payment address addrpk  (apk, pkenc) are derived from the spending key ask, as described in section4.2.1 Sprout Key Components on page 36. Sapling onward A Sapling expanded spending key is composed of a Spend authorizing key ask, a nulli\ufb01er private key nsk, and an outgoing viewing key ovk. From these components we can derive a proof authorizing key (ak, nsk), a full viewing key (ak, nk, ovk), an incoming viewing key ivk, and a set of diversi\ufb01ed payment addresses addrd  (d, pkd), as described in section4.2.2 Sapling Key Components on page 36. The consensus protocol does not depend on how an expanded spending key is constructed. Two methods of doing so are de\ufb01ned: 1.",
      "Generate a spending key sk at random and derive the expanded spending key (ask, nsk, ovk) from it, as shown in the diagram above and described in section4.2.2 Sapling Key Components on page 36. 2. Obtain an extended spending key as speci\ufb01ed in ZIP-32; this includes a superset of the components of an expanded spending key. This method is used in the context of a Hierarchical Deterministic Wallet. NU55 onward An Orchard spending key sk is used to derive a Spend authorizing key ask, and a full viewing key (ak, nk, rivk). From the full viewing key we can also derive an incoming viewing key (composed of a diversi\ufb01er key dk and a KAOrchard private key ivk), an outgoing viewing key ovk, and a set of diversi\ufb01ed payment addresses addrd  (d, pkd), as described in section4.2.3 Orchard Key Components on page 38. The derivations of ask and rivk shown are not the only possibility. For further detail see section4.2.3 on page 38.",
      "Non-normative note: Most Zcash wallets derive Sapling and Orchard keys and addresses according to ZIP-32. The composition of shielded payment addresses, incoming viewing keys, full viewing keys, and spending keys is a cryptographic protocol detail that should not normally be exposed to users. However, user-visible operations should be provided to obtain a shielded payment address, incoming viewing key, or full viewing key from a spending key or extended spending key. Users can accept payment from multiple parties with a single shielded payment address and the fact that these payments are destined to the same payee is not revealed on the block chain, even to the paying parties. However if two parties collude to compare a shielded payment address they can trivially determine they are the same. In the case that a payee wishes to prevent this they should create a distinct shielded payment address for each payer.",
      "Sapling onward Sapling and Orchard provide a mechanism to allow the ef\ufb01cient creation of diversi\ufb01ed payment addresses with the same spending authority. A group of such addresses shares the same full viewing key and incoming viewing key, and so creating as many unlinkable addresses as needed does not increase the cost of scanning the block chain for relevant transactions. Note: It is conventional in cryptography to call the key used to encrypt a message in an asymmetric encryption scheme a public key. However, the public key used as the transmission key component of an address (pkenc or pkd) need not be publically distributed; it has the same distribution as the shielded payment address itself. As mentioned above, limiting the distribution of the shielded payment address is important for some use cases.",
      "This also helps to reduce reliance of the overall protocol on the security of the cryptosystem used for note encryption (see section4.19 In-band secret distribution (Sprout) on page 65 and section4.20 In-band secret distribution (Sapling and Orchard) on page 67), since an adversary would have to know pkenc or some pkd in order to exploit a hypothetical weakness in that cryptosystem. Notes A note (denoted n) can be a Sprout note or a Sapling note or an Orchard note. In each case it represents that a value v is spendable by the recipient who holds the spending key corresponding to a given shielded payment address. Let MAX_MONEY, \u2113Sprout PRF , \u2113PRFnfSapling, \u2113d, and \u2113value be as de\ufb01ned in section5.3 Constants on page 74. Let NoteCommitSprout be as de\ufb01ned in section5.4.8.1 Sprout Note Commitments on page 95. Let NoteCommitSapling be as de\ufb01ned in section5.4.8.2 Windowed Pedersen commitments on page 96. Let KASapling be as de\ufb01ned in section5.4.5.3 Sapling Key Agreement on page 89.",
      "Let DiversifyHashSaplingbe as de\ufb01ned in section5.4.1.6 DiversifyHashSapling and DiversifyHashOrchard Hash Functions on page 78. Let NoteCommitOrchard be as de\ufb01ned in section5.4.8.4 Sinsemilla commitments on page 98. Let KAOrchard be as de\ufb01ned in section5.4.5.5 Orchard Key Agreement on page 90. Let DiversifyHashOrchardbe as de\ufb01ned in section5.4.1.6 DiversifyHashSapling and DiversifyHashOrchard Hash Functions on page 78. Let PRFnfOrchard be as de\ufb01ned in section5.4.2 Pseudo Random Functions on page 86. Let \ud835\udc5eP be as de\ufb01ned in section5.4.9.6 Pallas and Vesta on page 105. A Sprout note is a tuple (apk, v, \u03c1, rcm), where:  apk B\u2113Sprout PRF  is the paying key of the recipients shielded payment address; 0 ..",
      "MAX_MONEY is an integer representing the value of the note in zatoshi (1 ZEC  10 to the 8 zatoshi); B\u2113Sprout PRF  is used as input to PRFnfSprout to derive the nulli\ufb01er of the note;  rcm NoteCommitSprout.Trapdoor is a random commitment trapdoor as de\ufb01ned in section4.1.8 Commitment on page 31. Let NoteSprout be the type of a Sprout note, i.e. NoteSprout : B\u2113Sprout PRF   0 .. MAX_MONEY  B\u2113Sprout PRF   NoteCommitSprout.Trapdoor. A Sapling note is a tuple (d, pkd, v, rcm), where: B\u2113d is the diversi\ufb01er of the recipients shielded payment address;  pkd KASapling.PublicPrimeOrder is the diversi\ufb01ed transmission key of the recipients shielded payment address; 0 .. MAX_MONEY is an integer representing the value of the note in zatoshi;  rcm NoteCommitSapling.Trapdoor is a random commitment trapdoor as de\ufb01ned in section4.1.8 Commitment on page 31. Let NoteSapling be the type of a Sapling note, i.e. NoteSapling : B\u2113d  KASapling.PublicPrimeOrder  0 .. MAX_MONEY  NoteCommitSapling.Trapdoor.",
      "An Orchard note is a tuple (d, pkd, v, \u03c1, \u03c8, rcm), where: B\u2113d is the diversi\ufb01er of the recipients shielded payment address;  pkd KAOrchard.PublicPrimeOrder is the diversi\ufb01ed transmission key of the recipients shielded payment address; 0 .. 2\u2113value1 is an integer representing the value of the note in zatoshi; F\ud835\udc5eP is used as input to PRFnfOrchard as part of deriving the nulli\ufb01er of the note; F\ud835\udc5eP is additional randomness used in deriving the nulli\ufb01er;  rcm NoteCommitOrchard.Trapdoor is a random commitment trapdoor as de\ufb01ned in section4.1.8 Commitment on page 31. Let NoteOrchard be the type of an Orchard note, i.e. NoteOrchard : B\u2113d  KAOrchard.PublicPrimeOrder  0 .. 2\u2113value1  F\ud835\udc5eP  F\ud835\udc5eP  NoteCommitOrchard.Trapdoor. Creation of new notes is described in section4.7 Sending Notes on page 43.",
      "3.2.1 Note Plaintexts and Memo Fields Transmitted notes are stored on the block chain in encrypted form, together with a representation of the note commitment cm described in section3.2.2 Note Commitments on page 16. A note plaintext also includes a 512-byte memo \ufb01eld associated with this note. The usage of the memo \ufb01eld is by agreement between the sender and recipient of the note. RECOMMENDED non-consensus constraints on the memo \ufb01eld contents are speci\ufb01ed in ZIP-302. For Sprout, the note plaintexts in each JoinSplit description are encrypted to the respective transmission keys pknew enc,1..Nnew, as speci\ufb01ed in section4.7.1 Sending Notes (Sprout) on page 43. Each Sprout note plaintext (denoted np) consists of (leadByte BY, v 0 .. 2\u2113value1, \u03c1 B\u2113Sprout PRF , rcm NoteCommitSprout.Trapdoor, memo BY512). The \ufb01eld leadByte is always 0x00 for Sprout. The \ufb01elds v, \u03c1, and rcm are as de\ufb01ned in section3.2 Notes on page 14.",
      "Sapling onward For Sapling and Orchard, the note plaintext in each Output description or Action description is encrypted to the diversi\ufb01ed payment address (d, pkd), as speci\ufb01ed in section4.7.2 Sending Notes (Sapling) on page 44 or section4.7.3 Sending Notes (Orchard) on page 45. Each Sapling or Orchard note plaintext (denoted np) consists of (leadByte BY, d B\u2113d, v 0 .. 2\u2113value1, rseed BY32, memo BY512) The \ufb01eld leadByte indicates the version of the encoding of a Sapling or Orchard note plaintext. Let the constants CanopyActivationHeight and ZIP212GracePeriod be as de\ufb01ned in section5.3 Constants on page 74. Let protocol Sapling, Orchard be the shielded protocol of the note. Let height be the block height of the block containing the transaction having the encrypted note plaintext as an output, and let txVersion be the transaction version number.",
      "De\ufb01ne allowedLeadBytesprotocol(height, txVersion) : 0x01, if height  CanopyActivationHeight 0x01, 0x02, if CanopyActivationHeight height  CanopyActivationHeight  ZIP212GracePeriod 0x02, otherwise. The leadByte of a Sapling or Orchard note MUST satisfy leadByte allowedLeadBytesprotocol(height, txVersion). Senders SHOULD choose the highest note plaintext lead byte allowed under this condition. Non-normative notes:  NU55 onward Since Orchard was introduced after the end of the ZIP-212 grace period, note plaintexts for Orchard notes MUST have leadByte 0x02. It is intentional that the de\ufb01nition of allowedLeadBytes does not currently depend on protocol or txVersion. It might do so in future. The \ufb01elds d and v are as de\ufb01ned in section3.2 Notes on page 14. The use of the \ufb01eld rseed is described in ZIP-212. Encodings are given in section5.5 Encodings of Note Plaintexts and Memo Fields on page 112. The result of encryption forms part of a transmitted note(s) ciphertext.",
      "For further details, see section4.19 In-band secret distribution (Sprout) on page 65 and section4.20 In-band secret distribution (Sapling and Orchard) on page 67. 3.2.2 Note Commitments When a note is created as an output of a transaction, only a commitment (see section4.1.8 Commitment on page 31) to the note contents is disclosed publically in the associated JoinSplit description or Output description or Action description. If the transaction is entered into the block chain, each such note commitment is appended to the note commitment tree of the associated treestate. This allows the value and recipient to be kept private, while the commitment is used by the zk-SNARK proof when the note is spent, to check that it exists on the block chain. Treestates are described in section3.4 Transactions and Treestates on page 18, and note commitment trees are described in section3.8 Note Commitment Trees on page 21.",
      "A Sprout note commitment on a note n  (apk, v, \u03c1, rcm) is computed as NoteCommitmentSprout(n)  NoteCommitSprout (apk, v, \u03c1), where NoteCommitSprout is instantiated in section5.4.8.1 Sprout Note Commitments on page 95. A Sapling note commitment on a note n  (d, pkd, v, rcm) is computed as gd : DiversifyHashSapling(d) NoteCommitmentSapling(n) : if gd   NoteCommitSapling (reprJ(gd), reprJ(pkd), v), otherwise. where NoteCommitSapling is instantiated in section5.4.8.2 Windowed Pedersen commitments on page 96. Notice that the above de\ufb01nition of a Sapling note does not have a \u03c1 component. There is in fact a \u03c1 value associated with each Sapling note, but this can only be computed once its position in the note commitment tree (see section3.4 Transactions and Treestates on page 18) is known. We refer to the combination of a note and its note position pos, as a positioned note.",
      "For a positioned note, we can compute the value \u03c1 as described in section4.16 Computing \u03c1 values and Nulli\ufb01ers on page 57. A Sapling note commitment is represented in an Output description by the \ud835\udc62-coordinate of a Jubjub curve point, as speci\ufb01ed in section4.5 Output Descriptions on page 41. An Orchard note commitment on a note n  (d, pkd, v, \u03c1, \u03c8, rcm) is computed as gd : DiversifyHashOrchard(d) NoteCommitmentOrchard(n) : NoteCommitOrchard (reprP(gd), reprP(pkd), v, \u03c1, \u03c8) where NoteCommitOrchard is instantiated in section5.4.8.4 Sinsemilla commitments on page 98. If NoteCommitOrchard returns (which happens with insigni\ufb01cant probability), the note is invalid and should be recreated with a different rseed. Unlike in Sapling, the de\ufb01nition of an Orchard note includes the \u03c1 component; the notes position in the note commitment tree does not need to be known in order to compute this value.",
      "An Orchard note commitment is represented in an Action description by the \ud835\udc65-coordinate of a Pallas curve point, as speci\ufb01ed in section4.6 Action Descriptions on page 42. 3.2.3 Nulli\ufb01ers The nulli\ufb01er for a note, denoted nf, is a value unique to the note that is used to prevent double-spends. When a transaction that contains one or more JoinSplit descriptions or Spend descriptions or Action descriptions is entered into the block chain, all of the nulli\ufb01ers for notes spent by that transaction are added to the nulli\ufb01er set of the associated treestate. A transaction is not valid if it would have added a nulli\ufb01er to the nulli\ufb01er set that already exists in the set. Treestates are described in section3.4 Transactions and Treestates on page 18, and nulli\ufb01er sets are described in section3.9 Nulli\ufb01er Sets on page 22.",
      "In more detail, when a note is spent, the spender creates a zero-knowledge proof that it knows (\u03c1, ask) or (\u03c1, ak, nsk) or (\u03c1, ak, nk), consistent with the publically disclosed nulli\ufb01er and some previously committed note commitment. Because each note can have only a single nulli\ufb01er, and the same nulli\ufb01er value cannot appear more than once in a valid block chain, double-spending is prevented. The nulli\ufb01er for a Sprout note is derived from the \u03c1 value and the recipients spending key ask. The nulli\ufb01er for a Sapling note is derived from the \u03c1 value and the recipients nulli\ufb01er deriving key nk. The nulli\ufb01er for an Orchard note is derived from the \u03c1 and \u03c8 values, the recipients nulli\ufb01er deriving key nk, and the note commitment. The nulli\ufb01er computation uses a Pseudo Random Function (see section4.1.2 Pseudo Random Functions on page 25), as described in section4.16 Computing \u03c1 values and Nulli\ufb01ers on page 57.",
      "The Block Chain At a given point in time, each full validator is aware of a set of candidate blocks. These form a tree rooted at the genesis block, where each node in the tree refers to its parent via the hashPrevBlock block header \ufb01eld (see section7.6 Block Header Encoding and Consensus on page 131). A path from the root toward the leaves of the tree consisting of a sequence of one or more valid blocks consistent with consensus rules, is called a valid block chain. Each block in a block chain has a block height. The block height of the genesis block is 0, and the block height of each subsequent block in the block chain increments by 1. Implementations MUST support block heights up to and in- cluding 2311. As of NU55, there is a consensus rule that all coinbase transactions (see section3.11 Coinbase Transactions on page 22) MUST have the nExpiryHeight \ufb01eld set to the block height, and this limits the maximum block height to 232 1, absent future consensus changes.",
      "In order to choose the best valid block chain in its view of the overall block tree, a node sums the work, as de\ufb01ned in section7.7.5 De\ufb01nition of Work on page 136, of all blocks in each valid block chain, and considers the valid block chain with greatest total work to be best. To break ties between leaf blocks, a node will prefer the block that it received \ufb01rst. The consensus protocol is designed to ensure that for any given block height, the vast majority of well-connected nodes should eventually agree on their best valid block chain up to that height. A full validator5 SHOULD attempt to obtain candidate blocks from multiple sources in order to increase the likelihood that it will \ufb01nd a valid block chain that re\ufb02ects a recent consensus state. A network upgrade is settled on a given network when there is a social consensus that it has activated with a given activation block hash.",
      "A full validator that potentially risks Mainnet funds or displays Mainnet transaction information to a user MUST do so only for a block chain that includes the activation block of the most recent settled network upgrade, with the corresponding activation block hash. Currently, there is social consensus that NU6.6.1 has activated on the Zcash Mainnet and Testnet with the activation block hashes given in section3.12 Mainnet and Testnet on page 22. A full validator MAY impose a limit on the number of blocks it will roll back when switching from one best valid block chain to another that is not a descendent. For zcashd and zebra this limit is 100 blocks. Transactions and Treestates Each block contains one or more transactions. Each transaction has a transaction ID.",
      "Transaction IDs are used to refer to transactions in tx_out \ufb01elds, in leaf nodes of a blocks transaction tree rooted at hashMerkleRoot, and in other parts of the ecosystem; for example they are shown in block chain explorers and can be used in higher-level protocols. Version 5 transactions also have a wtxid, which is used instead of the transaction ID when gossiping transactions in the peer-to-peer protocol ZIP-239. The computation of transaction IDs and wtxids is described in section7.1.1 Transaction Identi\ufb01ers on page 124. For more detail on the distinction between these two identi\ufb01ers and when to use each of them, see ZIP-239 and ZIP-244. Transparent inputs to a transaction insert value into a transparent transaction value pool associated with the transaction, and transparent outputs remove value from this pool.",
      "The effect of Sprout JoinSplit transfers, Sapling Spend transfers and Output transfers, and Orchard Action transfers on the transparent transaction value pool are speci\ufb01ed in section4.18.1 JoinSplit Statement (Sprout) on page 60, section4.13 Balance and Binding Signature (Sapling) on page 52, and section4.14 Balance and Binding Signature (Orchard) on page 54 respectively. As in Bitcoin, the remaining value in the transparent transaction value pool of a non-coinbase transaction is available to miners as a fee. That is, the sum of those values for non-coinbase transactions in each block is treated as an implicit input to the transparent transaction value pool balance of the blocks coinbase transaction (in addition to the implicit input created by issuance). 5 There is reason to follow the requirements in this section also for non-full validators, but those are outside the scope of this protocol speci\ufb01cation.",
      "The remaining value in the transparent transaction value pool of coinbase transactions in blocks prior to NU66 is destroyed. From NU66, this remaining value is required to be zero; that is, all of the available balance MUST be consumed by outputs of the coinbase transaction. Consensus rule: The remaining value in the transparent transaction value pool MUST be nonnegative. To each transaction there are associated initial treestates for Sprout and for Sapling and for Orchard. Each treestate consists of:  a note commitment tree (section3.8 Note Commitment Trees on page 21);  a nulli\ufb01er set (section3.9 Nulli\ufb01er Sets on page 22). Validation state associated with transparent inputs and outputs, such as the UTXO (unspent transaction output) set, is not described in this document; it is used in essentially the same way as in Bitcoin. An anchor is a Merkle tree root of a note commitment tree (either the Sprout tree or the Sapling tree or the Orchard tree).",
      "It uniquely identi\ufb01es a note commitment tree state given the assumed security properties of the Merkle trees hash function. Since the nulli\ufb01er set is always updated together with the note commitment tree, this also identi\ufb01es a particular state of the associated nulli\ufb01er set. In a given block chain, for each of Sprout and Sapling and Orchard, treestates are chained as follows:  The input treestate of the \ufb01rst block is the empty treestate. The input treestate of the \ufb01rst transaction of a block is the \ufb01nal treestate of the immediately preceding block. The input treestate of each subsequent transaction in a block is the output treestate of the immediately preceding transaction. The \ufb01nal treestate of a block is the output treestate of its last transaction. JoinSplit descriptions also have interstitial input and output treestates for Sprout, explained in the following section. There is no equivalent of interstitial treestates for Sapling or for Orchard.",
      "JoinSplit Transfers and Descriptions A JoinSplit description is data included in a transaction that describes a JoinSplit transfer, i.e. a shielded value transfer. In Sprout, this kind of value transfer was the primary Zcash-speci\ufb01c operation performed by transactions. A JoinSplit transfer spends Nold notes nold 1..Nold and transparent input vold pub, and creates Nnew notes nnew 1..Nnew and transparent output vnew pub. It is associated with a JoinSplit statement instance (section4.18.1 JoinSplit Statement (Sprout) on page 60), for which it provides a zk-SNARK proof . Each transaction has a sequence of JoinSplit descriptions. The total vnew pub value adds to, and the total vold pub value subtracts from the transparent transaction value pool of the containing transaction. The anchor of each JoinSplit description in a transaction refers to a Sprout treestate. For each of the Nold shielded inputs, a nulli\ufb01er is revealed.",
      "This allows detection of double-spends as described in section3.9 Nulli\ufb01er Sets on page 22. For each JoinSplit description in a transaction, an interstitial output treestate is constructed which adds the note commitments and nulli\ufb01ers speci\ufb01ed in that JoinSplit description to the input treestate referred to by its anchor. This interstitial output treestate is available for use as the anchor of subsequent JoinSplit descriptions in the same transaction. In general, therefore, the set of interstitial treestates associated with a transaction forms a tree in which the parent of each node is determined by its anchor. Interstitial treestates are necessary because when a transaction is constructed, it is not known where it will eventually appear in a mined block. Therefore the anchors that it uses must be independent of its eventual position. The input and output values of each JoinSplit transfer MUST balance exactly.",
      "This is not a consensus rule since it cannot be checked directly; it is enforced by the Balance rule of the JoinSplit statement. Consensus rules:  For the \ufb01rst JoinSplit description of a transaction, the anchor MUST be the output Sprout treestate of a previous block. The anchor of each JoinSplit description in a transaction MUST refer to either some earlier blocks \ufb01nal Sprout treestate, or to the interstitial output treestate of any prior JoinSplit description in the same transaction. Spend Transfers, Output Transfers, and their Descriptions JoinSplit transfers are not used for Sapling notes. Instead, there is a separate Spend transfer for each shielded input, and a separate Output transfer for each shielded output. Spend descriptions and Output descriptions are data included in a transaction that describe Spend transfers and Output transfers, respectively. A Spend transfer spends a note nold. Its Spend description includes a Pedersen value commitment to the value of the note.",
      "It is associated with an instance of a Spend statement (section4.18.2 Spend Statement (Sapling) on page 61) for which it provides a zk-SNARK proof . An Output transfer creates a note nnew. Similarly, its Output description includes a Pedersen value commitment to the note value. It is associated with an instance of an Output statement (section4.18.3 Output Statement (Sapling) on page 62) for which it provides a zk-SNARK proof . Each transaction has a sequence of Spend descriptions and a sequence of Output descriptions. To ensure balance, we use a homomorphic property of Pedersen commitments that allows them to be added and subtracted, as elliptic curve points (section5.4.8.3 Homomorphic Pedersen commitments (Sapling and Orchard) on page 97). The result of adding two Pedersen value commitments, committing to values v1 and v2, is a new Pedersen value commitment that commits to v1  v2. Subtraction works similarly.",
      "Therefore, balance can be enforced by adding all of the value commitments for shielded inputs, subtracting all of the value commitments for shielded outputs, and proving by use of a Sapling binding signature (as described in section4.13 Balance and Binding Signature (Sapling) on page 52) that the result commits to a value consistent with the net transparent value change. This approach allows all of the zk-SNARK statements to be independent of each other, potentially increasing opportunities for precomputation. A Spend description speci\ufb01es an anchor, which refers to the output Sapling treestate of a previous block. It also reveals a nulli\ufb01er, which allows detection of double-spends as described in section3.2.3 Nulli\ufb01ers on page 17. Non-normative note: Interstitial treestates are not necessary for Sapling, because a Spend transfer in a given transaction cannot spend any of the shielded outputs of the same transaction.",
      "This is not an onerous restriction because, unlike Sprout where each JoinSplit transfer must balance individually, in Sapling it is only necessary for the whole transaction to balance. Consensus rules:  The Spend transfers and Action transfers of a transaction MUST be consistent with its vbalanceSapling value as speci\ufb01ed in section4.13 Balance and Binding Signature (Sapling) on page 52. The anchor of each Spend description MUST refer to some earlier blocks \ufb01nal Sapling treestate. The anchor is encoded separately in each Spend description for v4 transactions, or encoded once and shared between all Spend descriptions in a v5 transaction. Action Transfers and their Descriptions Orchard introduces Action transfers, each of which can optionally perform a spend, and optionally perform an output. Action descriptions are data included in a transaction that describe Action transfers. An Action transfer spends a note nold, and creates a note nnew.",
      "Its Action description includes a Pedersen value commitment to the net value, i.e. the value of the spent note minus the value of the created note. It is associated with an instance of an Action statement (section4.18.4 Action Statement (Orchard) on page 63) for which it provides a zk-SNARK proof . Each version 5 transaction has a sequence of Action descriptions. Version 4 transactions cannot contain Action descriptions. As in Sapling, we use the homomorphic property of Pedersen commitments to enforce balance: we add all of the value commitments and prove by use of an Orchard binding signature that the result commits to a value consistent with the net transparent value change (as described in section4.14 Balance and Binding Signature (Orchard) on page 54). This approach allows all of the zk-SNARK statements to be independent of each other, potentially increasing opportunities for precomputation.",
      "The \ufb01elds of an Action description are essentially a merger of the \ufb01elds of a Spend description and an Output description, but with only a single value commitment. Also, the zk-SNARK proof is encoded outside the Action description, in order to more easily take advantage of space and performance optimizations in the Halo 2 proof system (section5.4.10.3 Halo 2 on page 112) that apply when multiple proofs are aggregated. Each Action description does not encode a separate anchor \ufb01eld, because that is encoded once in the anchorOrchard \ufb01eld of the transaction. Non-normative note: As with Sapling, interstitial treestates are not necessary for Orchard, because an Action transfer in a given transaction cannot spend any of the shielded outputs of the same transaction. Consensus rules:  The Action transfers of a transaction MUST be consistent with its vbalanceOrchard value as speci\ufb01ed in section4.14 Balance and Binding Signature (Orchard) on page 54.",
      "The anchorOrchard \ufb01eld of the transaction, whenever it exists (i.e. when there are any Action descriptions), MUST refer to some earlier blocks \ufb01nal Orchard treestate. Note Commitment Trees Let \u2113Sprout Merkle, MerkleDepthSprout, \u2113Sapling Merkle , MerkleDepthSapling, \u2113Orchard Merkle , and MerkleDepthOrchardbe as de\ufb01ned in section5.3 Constants on page 74. A note commitment tree is an incremental Merkle tree of \ufb01xed depth used to store note commitments that JoinSplit transfers or Spend transfers or Action transfers produce. Just as the UTXO (unspent transaction output) set used in Bitcoin, it is used to express the existence of value and the capability to spend it. However, unlike the UTXO set, it is not the job of this tree to protect against double-spending, as it is append-only. A root of a note commitment tree is associated with each treestate (section3.4 Transactions and Treestates on page 18).",
      "Each node in the incremental Merkle tree is associated with a hash value of size \u2113Sprout Merkle or \u2113Sapling Merkle or \u2113Orchard Merkle bits. The layer numbered \u210e, counting from layer 0 at the root, has 2\u210enodes with indices 0 to 2\u210e1 inclusive. The hash value associated with the node at index \ud835\udc56in layer \u210eis denoted Mh The index of a notes commitment at the leafmost layer (MerkleDepthSprout or MerkleDepthSapling or MerkleDepthOrchard) is called its note position. Consensus rules:  A block MUST NOT add Sprout note commitments that would result in the Sprout note commitment tree exceeding its capacity of 2MerkleDepthSprout leaf nodes. Sapling onward A block MUST NOT add Sapling note commitments that would result in the Sapling note commitment tree exceeding its capacity of 2MerkleDepthSapling leaf nodes. NU55 onward A block MUST NOT add Orchard note commitments that would result in the Orchard note commitment tree exceeding its capacity of 2MerkleDepthOrchard leaf nodes.",
      "Nulli\ufb01er Sets Each full validator maintains a nulli\ufb01er set logically associated with each treestate. As valid transactions contain- ing JoinSplit transfers or Spend transfers or Action transfers are processed, the nulli\ufb01ers revealed in JoinSplit descriptions and Spend descriptions and Action descriptions are inserted into the nulli\ufb01er set associated with the new treestate. Nulli\ufb01ers are enforced to be unique within a valid block chain, in order to prevent double-spends. Consensus rule: A nulli\ufb01er MUST NOT repeat either within a transaction, or across transactions in a valid block chain. Sprout and Sapling and Orchard nulli\ufb01ers are considered disjoint, even if they have the same bit pattern. 3.10 Block Subsidy, Funding Streams, and Founders Reward Like Bitcoin, Zcash creates currency when blocks are mined. The value created on mining a block is called the block subsidy. Pre-Canopy The block subsidy is composed of a miner subsidy and a Founders Reward.",
      "Canopy onward The block subsidy is composed of a miner subsidy and a series of funding streams. As in Bitcoin, the miner of a block also receives transaction fees. The calculations of the block subsidy, miner subsidy, Founders Reward, and funding streams depend on the block height, as de\ufb01ned in section3.3 The Block Chain on page 18. section7.8 Calculating Block Subsidy, Funding Streams, Lockbox Disbursement, and Founders Reward on page 136 describes these calculations. 3.11 Coinbase Transactions A transaction that has a single transparent input with a null prevout \ufb01eld, is called a coinbase transaction. Every block has a single coinbase transaction as the \ufb01rst transaction in the block. The purpose of this coinbase transaction is to collect and spend any miner subsidy, and transaction fees paid by other transactions included in the block. Pre-Canopy section7.9 Payment of Founders Reward on page 137 speci\ufb01es that the coinbase transaction MUST also pay the Founders Reward.",
      "Canopy onward section7.10 Payment of Funding Streams, Deferred Lockbox, and Lockbox Disbursement on page 139 speci\ufb01es that the coinbase transaction MUST also pay the funding streams. 3.12 Mainnet and Testnet The production Zcash network, which supports the ZEC token, is called Mainnet. Governance of its protocol is by social consensus on which full validator implementations are considered to be faithful implementations of the intended Mainnet consensus rules (currently, zebra maintained by the Zcash Foundation, and zcashd maintained by the Electric Coin Company), and on how those implementations should be modi\ufb01ed. Subject to errors and omissions, each version of this document intends to describe some version (or planned version) of the Zcash protocol. All block hashes given in this section are in RPC byte order (that is, byte-reversed relative to the normal order for a SHA-256 hash).",
      "Mainnet genesis block: 00040fe8ec8471911baa1db1266ea15dd06b4a8a5c453883c000b031973dce08 Mainnet NU6.6.1 activation block: 0000000000b98a7d8f390793fa113bf6755935f0c14ea817af07d2c16f2c3ef4 There is also a public test network called Testnet. It supports a TAZ token which is intended to have no monetary value. By convention, Testnet activates network upgrades (as described in section6 Network Upgrades on page 120) before Mainnet, in order to allow for errors or ambiguities in their speci\ufb01cation and implementation to be discovered. The Testnet block chain is subject to being rolled back to a prior block at any time. Testnet genesis block: 05a60a92d99d85997cce3b87616c089f6124d7342af37106edc76126334a2c38 Testnet NU6.6.1 activation block: 01b947c7556b23040dc6840e9d3e4c6d9478c67a87b9737a83be848729d6e0af We call the smallest units of currency (on either network) zatoshi.6 On Mainnet, 1 ZEC  10 to the 8 zatoshi. On Testnet, 1 TAZ  10 to the 8 zatoshi.",
      "Other networks using variants of the Zcash protocol may exist, but are not described by this speci\ufb01cation. Abstract Protocol We all know that the only mental tool by means of which a very \ufb01nite piece of reasoning can cover a myriad cases is called abstraction; as a result the effective exploitation of their powers of abstraction must be regarded as one of the most vital activities of a competent programmer. In this connection it might be worth-while to point out that the purpose of abstracting is not to be vague, but to create a new semantic level in which one can be absolutely precise. Edsger Dijkstra, The Humble Programmer EWD-340 Abstraction is an incredibly important idea in the design of any complex system. Without abstraction, we would not be able to design anything as ambitious as a computer, or a cryptographic protocol. Were we to attempt it, the computer would be hopelessly unreliable or the protocol would be insecure, if they could be completed at all.",
      "The aim of abstraction is primarily to limit how much a human working on a piece of a system has to keep in mind at one time, in order to apprehend the connections of that piece to the remainder. The work could be to extend or maintain the system, to understand its security or other properties, or to explain it to others. In this speci\ufb01cation, we make use wherever possible of abstractions that have been developed by the cryptography community to model cryptographic primitives: Pseudo Random Functions, commitment schemes, signature schemes, etc. Each abstract primitive has associated syntax (its interface as used by the rest of the system) and security properties, as documented in this part. Their instantiations are documented in part section5 Concrete Protocol on page 73. In some cases this syntax or these security requirements have been extended to meet the needs of the Zcash protocol.",
      "For example, some of the PRFs used in Zcash need to be collision-resistant, which is not part of the usual security requirement for a PRF; some signature schemes need to support additional functionality and security properties; and so on. Also, security requirements are sometimes intentionally stronger than what is known to be needed, because the stronger property is simpler or less error-prone to work with, andor because it has been studied in the cryptographic literature in more depth. 6 tazoshi may be used for the smallest units of currency on Testnet, but it is usually more convenient to use a network-independent term. We explicitly do not claim, however, that all of these instantiations satisfying their documented syntax and security requirements would be suf\ufb01cient for security or correctness of the overall Zcash protocol, or that it is always necessary.",
      "The claim is only that it helps to understand the protocol; that is, that analysis or extension is simpli\ufb01ed by making use of the abstraction. In other words, a good way to understand the use of that primitive in the protocol is to model it as an instance of the given abstraction. And furthermore, if the instantiated primitive does not in fact satisfy the requirements of the abstraction, then this is an error that should be corrected whether or not it leads to a vulnerability since that would compromise the facility to understand its use in terms of the abstraction. In this respect the abstractions play a similar r\u00f4le to that of a type system (which we also use): they add a form of redundancy to the speci\ufb01cation that helps to express the intent. Each property is a claim that may be incorrect (or that may be insuf\ufb01ciently precisely stated to determine whether it is correct).",
      "An example of an incorrect security claim occurs in the Zerocash protocol BCGGMTV2014: the instantiation of the note commitment scheme used in Zerocash failed to be binding at the intended security level (see section8.5 Internal hash collision attack and \ufb01x on page 145). Anotherhazard that we should be aware of is that abstractions can be leaky: an instantiation mayimpose conditions on its correct or secure use that are not captured by the abstractions interface and semantics. Ideally, the abstraction would be changed to explicitly document these conditions, or the protocol changed to rely only on the original abstraction. An abstraction can also be incomplete (not quite the same thing as being leaky): it intentionally usually for simplicity does not model an aspect of behaviour that is important to security or correctness. An example would be resistance to side-channel attacks; this speci\ufb01cation says little about side-channel defence, among many other implementation concerns.",
      "Abstract Cryptographic Schemes 4.1.1 Hash Functions Let MerkleDepthSprout, \u2113Sprout Merkle, MerkleDepthSapling, \u2113Sapling Merkle , MerkleDepthOrchard, \u2113Orchard Merkle , \u2113Sapling , \u2113d, \u2113Seed, \u2113Sprout PRF , \u2113hSig, and Nold be as de\ufb01ned in section5.3 Constants on page 74. Let J, J(\ud835\udc5f), J(\ud835\udc5f), \ud835\udc5fJ, and \u2113J be as de\ufb01ned in section5.4.9.3 Jubjub on page 102. Let P be as de\ufb01ned in section5.4.9.6 Pallas and Vesta on page 105. The following hash functions are used in section4.9 Merkle Path Validity on page 49: MerkleCRHSprout 0 .. MerkleDepthSprout 1  B\u2113Sprout Merkle  B\u2113Sprout Merkle B\u2113Sprout Merkle MerkleCRHSapling 0 .. MerkleDepthSapling 1  B\u2113Sapling Merkle   B\u2113Sapling Merkle  B\u2113Sapling Merkle  MerkleCRHOrchard 0 .. MerkleDepthOrchard 1  0 .. \ud835\udc5eP 1  0 .. \ud835\udc5eP 1 0 .. \ud835\udc5eP 1. MerkleCRHSproutis collision-resistant except on its \ufb01rst argument. MerkleCRHSaplingand MerkleCRHOrchardare collision- resistant on all their arguments.",
      "These functions are instantiated in section5.4.1.3 Merkle Tree Hash Function on page 76. hSigCRH B\u2113Seed  B\u2113Sprout PRF Nold  JoinSplitSig.Public B\u2113hSig is a collision-resistant hash function used in section4.3 JoinSplit Descriptions on page 39. It is instantiated in section5.4.1.4 hSig Hash Function on page 77. EquihashGen N)  N  BYN  N B\ud835\udc5b is another hash function, used in section7.7.1 Equihash on page 133 to generate input to the Equihash solver. The \ufb01rst two arguments, representing the Equihash parameters \ud835\udc5band \ud835\udc58, are written subscripted. It is instantiated in section5.4.1.11 Equihash Generator on page 85. CRHivk B\u2113JB\u2113J 1 .. 2\u2113Sapling 1 is a collision-resistant hash function used in section4.2.2 Sapling Key Components on page 36 to derive an incoming viewing key for a Sapling shielded payment address. It is also used in the Spend statement (section4.18.2 Spend Statement (Sapling) on page 61) to con\ufb01rm use of the correct keys for the note being spent.",
      "It is instantiated in section5.4.1.5 CRHivk Hash Function on page 77. MixingPedersenHash J  0 .. \ud835\udc5fJ 1 J is a hash function used in section4.16 Computing \u03c1 values and Nulli\ufb01ers on page 57 to derive the unique \u03c1 value for a Sapling note. It is also used in the Spend statement to con\ufb01rm use of the correct \u03c1 value as an input to nulli\ufb01er derivation. It is instantiated in section5.4.1.8 Mixing Pedersen Hash Function on page 81. DiversifyHashSapling  B\u2113d J(\ud835\udc5f)  and DiversifyHashOrchard  B\u2113d P are hash functions instantiated in section5.4.1.6 DiversifyHashSapling and DiversifyHashOrchard Hash Functions on page 78, satisfying the Unlinkability security property described in that section. They are used to derive a diversi\ufb01ed base from a diversi\ufb01er, which is speci\ufb01ed in section4.2.2 Sapling Key Components on page 36 and in section4.2.3 Orchard Key Components on page 38. 4.1.2 Pseudo Random Functions PRF\ud835\udc65denotes a Pseudo Random Function keyed by \ud835\udc65.",
      "Let \u2113ask, \u2113hSig, \u2113Sprout PRF , \u2113Sprout , \u2113sk, \u2113ovk, \u2113PRFexpand, \u2113PRFnfSapling, Nold, and Nnew be as de\ufb01ned in section5.3 Constants on page 74. Let Sym be as de\ufb01ned in section5.4.3 Symmetric Encryption on page 88. Let \u2113J and J (\ud835\udc5f) be as de\ufb01ned in section5.4.9.3 Jubjub on page 102. Let \u2113P and \ud835\udc5eP be as de\ufb01ned in section5.4.9.6 Pallas and Vesta on page 105. For Sprout, four independent PRF\ud835\udc65are needed: PRFaddr B\u2113ask  BY B\u2113Sprout PRF  PRFpk B\u2113ask  1..Nold  B\u2113hSig B\u2113Sprout PRF  PRF\u03c1 B\u2113Sprout   1..Nnew  B\u2113hSig B\u2113Sprout PRF  PRFnfSprout B\u2113ask  B\u2113Sprout PRF  B\u2113Sprout PRF  These are used in section4.18.1 JoinSplit Statement (Sprout) on page 60; PRFaddr is also used to derive a shielded payment address from a spending key in section4.2.1 Sprout Key Components on page 36.",
      "For Sapling, three additional PRF\ud835\udc65are needed: PRFexpand B\u2113sk  BYN BY\u2113PRFexpand8 PRFockSapling BY\u2113ovk8  BY\u2113J8  BY\u2113J8  BY\u2113J8 Sym.K PRFnfSapling  B\u2113J BY\u2113PRFnfSapling8 For Orchard, we need PRFexpand, and also: PRFockOrchard BY\u2113ovk8  BY\u2113P8  BY\u2113P8  BY\u2113P8 Sym.K PRFnfOrchard F\ud835\udc5eP  F\ud835\udc5eP F\ud835\udc5eP PRFexpand is used in the following places:  section4.2.2 Sapling Key Components on page 36, with inputs 0x00, 0x01, 0x02, and 0x03, \ud835\udc56 BY;  NU55 onward in section4.2.3 Orchard Key Components on page 38, with inputs 0x06, 0x07, 0x08, and with \ufb01rst byte 0x82 (the last of these is also speci\ufb01ed in ZIP-32);  in the processes of sending (section4.7.2 Sending Notes (Sapling) on page 44 and section4.7.3 Sending Notes (Orchard) on page 45) and of receiving (section4.20 In-band secret distribution (Sapling and Orchard) on page 67) notes, for Sapling with inputs 0x04 and 0x05, and for Orchard \ud835\udc61  \u03c1 with \ud835\udc610x05, 0x04, 0x09;  in ZIP-32, with inputs 0x00, 0x01, 0x02 (intentionally matching section4.2.2 on page 36), 0x10, 0x13, 0x14, and with \ufb01rst byte in 0x11, 0x12, 0x15, 0x16, 0x17, 0x18, 0x80, 0x81, 0x82, 0x83;  in ZIP-316, with \ufb01rst byte 0xD0.",
      "PRFockSapling and PRFockOrchard are used in section4.20 In-band secret distribution (Sapling and Orchard) on page 67. PRFnfSapling is used in section4.18.2 Spend Statement (Sapling) on page 61. PRFnfOrchard is used in section4.18.4 Action Statement (Orchard) on page 63. All of these Pseudo Random Functions are instantiated in section5.4.2 Pseudo Random Functions on page 86. Security requirements:  Security de\ufb01nitions for Pseudo Random Functions are given in BDJR2000, section 4. In addition to being Pseudo Random Functions, it is required that PRFaddr , PRF\u03c1 \ud835\udc65, PRFnfSprout , PRFnfSapling PRFnfOrchard be collision-resistant across all \ud835\udc65 i.e. \ufb01nding (\ud835\udc65, \ud835\udc66)  (\ud835\udc65, \ud835\udc66) such that PRFaddr (\ud835\udc66)  PRFaddr \ud835\udc65 (\ud835\udc66) should not be feasible, and similarly for PRF\u03c1, PRFnfSprout, PRFnfSapling, and PRFnfOrchard. See the note in section4.2.3 Orchard Key Components on page 38 for a security caveat about the use of PRFexpand.",
      "Non-normative note: PRFnfSprout was called PRFsn in Zerocash BCGGMTV2014, and just PRFnf in some previous versions of this speci\ufb01cation. 4.1.3 Pseudo Random Permutations PRP\ud835\udc65denotes a Pseudo Random Permutation keyed by \ud835\udc65. Let \u2113dk and \u2113d be as de\ufb01ned in section5.3 Constants on page 74. One Pseudo Random Permutation is used for Orchard, to generate diversi\ufb01ers from a diversi\ufb01er key and index (an identical construction is also used for Sapling in ZIP-32): PRPd BY\u2113dk8  B\u2113d B\u2113d. It is instantiated in section5.4.4 Pseudo Random Permutations on page 88. Security requirement: PRPd is a keyed Pseudo Random Permutation as de\ufb01ned in BKR2001. 4.1.4 Symmetric Encryption Let Sym be an authenticated one-time symmetric encryption scheme with keyspace Sym.K, encrypting plaintexts in Sym.P to produce ciphertexts in Sym.C. Sym.Encrypt Sym.K  Sym.P Sym.C is the encryption algorithm.",
      "Sym.Decrypt Sym.K  Sym.C Sym.P  is the decryption algorithm, such that for any K Sym.K and P Sym.P, Sym.DecryptK(Sym.EncryptK(P))  P. is used to represent the decryption of an invalid ciphertext. Security requirement: Sym must be one-time (INT-CTXT IND-CPA)-secure BN2007. One-time here means that an honest protocol participant will almost surely encrypt only one message with a given key; however, the adversary may make many adaptive chosen ciphertext queries for a given key. 4.1.5 Key Agreement A key agreement scheme is a cryptographic protocol in which two parties agree a shared secret, each using their private key and the other partys public key. A key agreement scheme KA de\ufb01nes a type of public keys KA.Public, a type of private keys KA.Private, and a type of shared secrets KA.SharedSecret. Optionally, it also de\ufb01nes a type KA.PublicPrimeOrder KA.Public. Optional: Let KA.FormatPrivate B\u2113Sprout PRF  KA.Private be a function to convert a bit string of length \u2113Sprout to a KA private key.",
      "Let KA.DerivePublic KA.PrivateKA.Public KA.Public be a function that derives the KA public key corresponding to a given KA private key and base point. Let KA.Agree KA.Private  KA.Public KA.SharedSecret be the agreement function. Optional: Let KA.Base KA.Public be a public base point. Note: The range of KA.DerivePublic may be a strict subset of KA.Public. Security requirements:  KA.FormatPrivate must preserve suf\ufb01cient entropy from its input to be used as a secure KA private key. The key agreement and the KDF de\ufb01ned in the next section must together satisfy a suitable adaptive security assumption along the lines of Bernstein2006, section 3 or ABR1999, De\ufb01nition 3. More precise formalization of these requirements is beyond the scope of this speci\ufb01cation.",
      "4.1.6 Key Derivation A Key Derivation Function is de\ufb01ned for a particular key agreement scheme and authenticated one-time symmetric encryption scheme; it takes the shared secret produced by the key agreement and additional arguments, and derives a key suitable for the encryption scheme. The inputs to the Key Derivation Function differ between the Sprout and Sapling and Orchard KDFs: KDFSprout takes as input an output index in 1..Nnew, the value hSig, the shared Dif\ufb01eHellman secret sharedSecret, the ephemeral public key epk, and the recipients public transmission key pkenc. It is suitable for use with KASprout and derives keys for Sym.Encrypt. KDFSprout  1..Nnew  B\u2113hSig  KASprout.SharedSecret  KASprout.Public  KASprout.Public Sym.K KDFSapling takes as input the shared Dif\ufb01eHellman secret sharedSecret and the ephemeral public key epk. (It does not have inputs taking the place of the output index, hSig, or pkenc.) It is suitable for use with KASapling and derives keys for Sym.Encrypt.",
      "KDFSapling  KASapling.SharedSecret  BY\u2113J8 Sym.K As in Sapling, KDFOrchard takes as input the shared Dif\ufb01eHellman secret sharedSecret and the ephemeral public key epk. It is suitable for use with KAOrchard and derives keys for Sym.Encrypt. KDFOrchard  KAOrchard.SharedSecret  BY\u2113P8 Sym.K Security requirements:  The asymmetric encryption scheme in section4.19 In-band secret distribution (Sprout) on page 65, constructed from KASprout, KDFSprout and Sym, is required to be IND-CCA2-secure and key-private. The asymmetric encryption scheme in section4.20 In-band secret distribution (Sapling and Orchard) on page 67, constructed from KASapling, KDFSapling and Sym or from KAOrchard, KDFOrchard and Sym, is required to be IND- CCA2-secure and key-private. Key privacy is de\ufb01ned in BBDP2001.",
      "4.1.7 Signature A signature scheme Sig de\ufb01nes:  a type of signing keys Sig.Private;  a type of validating keys Sig.Public;  a type of messages Sig.Message;  a type of signatures Sig.Signature;  a randomized signing key generation algorithm Sig.GenPrivate ()  R Sig.Private;  an injective validating key derivation algorithm Sig.DerivePublic Sig.Private Sig.Public;  a randomized signing algorithm Sig.Sign Sig.Private  Sig.Message  R Sig.Signature;  a validating algorithm Sig.Validate Sig.Public  Sig.Message  Sig.Signature B; such that for any signing key sk  R Sig.GenPrivate() and corresponding validating key vk  Sig.DerivePublic(sk), and any \ud835\udc5a Sig.Message and \ud835\udc60 Sig.Signature  R Sig.Signsk(\ud835\udc5a), Sig.Validatevk(\ud835\udc5a, \ud835\udc60)  1.",
      "Zcash uses four signature schemes:  one used for signatures that can be validated by script operations such as OP_CHECKSIG and OP_CHECKMULTISIG as in Bitcoin;  one called JoinSplitSig which is used to sign transactions that contain at least one JoinSplit description (instantiated in section5.4.6 Ed25519 on page 90);  Sapling onward one called SpendAuthSig which is used to sign authorizations of Spend transfers (instantiated in section5.4.7.1 Spend Authorization Signature (Sapling and Orchard) on page 95);  Sapling onward one called BindingSig. A Sapling binding signature is used to enforce balance of Spend transfers and Output transfers, and to prevent their replay across transactions. Similarly, an Orchard binding signature is used to enforce balance of Action transfers and to prevent their replay. BindingSig is instantiated for both Sapling and Orchard in section5.4.7.2 Binding Signature (Sapling and Orchard) on page 95.",
      "The signature scheme used in script operations is instantiated by ECDSA on the secp256k1 curve. JoinSplitSig is instantiated by Ed25519. SpendAuthSig and BindingSig are instantiated by RedDSA; on the Jubjub curve in Sapling, and on the Pallas curve in Orchard. The following security property is needed for JoinSplitSig and BindingSig. Security requirements for SpendAuthSig are de\ufb01ned in the next section, section4.1.7.1 Signature with Re-Randomizable Keys on page 29. An additional requirement for BindingSig is de\ufb01ned in section4.1.7.2 Signature with Signing Key to Validating Key Monomorphism on page 30.",
      "Security requirement: JoinSplitSig and each instantiation of BindingSig must be Strongly Unforgeable under (non-adaptive) Chosen Message Attack (SU-CMA), as de\ufb01ned for example in BDEHR2011, De\ufb01nition 6.7 This allows an adversary to obtain signatures on chosen messages, and then requires it to be infeasible for the adversary to forge a previously unseen valid (message, signature) pair without access to the signing key. Non-normative notes:  We need separate signing key generation and validating key derivation algorithms, rather than the more conventional combined key pair generation algorithm Sig.Gen ()  R Sig.PrivateSig.Public, to support the key derivation in section4.2.2 Sapling Key Components on page 36 and in section4.2.3 Orchard Key Components on page 38. The de\ufb01nitions of schemes with additional features in section4.1.7.1 Signature with Re-Randomizable Keys on page 29 and in section4.1.7.2 Signature with Signing Key to Validating Key Monomorphism on page 30 also become simpler.",
      "7 The scheme de\ufb01ned in that paper was attacked in LM2017, but this has no impact on the applicability of the de\ufb01nition. A fresh signature key pair is generated for each transaction containing a JoinSplit description. Since each key pair is only used for one signature (see section4.11 Non-malleability (Sprout) on page 51), a one-time signature scheme would suf\ufb01ce for JoinSplitSig. This is also the reason why only security against non-adaptive chosen message attack is needed. In fact the instantiation of JoinSplitSig uses a scheme designed for security under adaptive attack even when multiple signatures are signed under the same key. Sapling onward The same remarks as above apply to BindingSig, except that the key is derived from the randomness of value commitments. This results in the same distribution as of freshly generated key pairs, for each transaction containing Spend descriptions or Output descriptions or Action descriptions.",
      "SU-CMA security requires it to be infeasible for the adversary, not knowing the private key, to forge a distinct signature on a previously seen message. That is, JoinSplit signatures and Sapling binding signatures and Orchard binding signatures are intended to be nonmalleable in the sense of BIP-62. The terminology used in this speci\ufb01cation is that we validate signatures, and verify zk-SNARK proofs.",
      "4.1.7.1 Signature with Re-Randomizable Keys A signature scheme with re-randomizable keys Sig is a signature scheme that additionally de\ufb01nes:  a type of randomizers Sig.Random;  a randomizer generator Sig.GenRandom ()  R Sig.Random;  a signing key randomization algorithm Sig.RandomizePrivate Sig.Random  Sig.Private Sig.Private;  a validating key randomization algorithm Sig.RandomizePublic Sig.Random  Sig.Public Sig.Public;  a distinguished identity randomizer \ud835\udcaaSig.Random Sig.Random such that:  for any \ud835\udefc Sig.Random, Sig.RandomizePrivate\ud835\udefc Sig.Private Sig.Private is injective and easily invertible;  Sig.RandomizePrivate\ud835\udcaaSig.Random is the identity function on Sig.Private. for any sk Sig.Private, Sig.RandomizePrivate(\ud835\udefc, sk) : \ud835\udefc R Sig.GenRandom() is identically distributed to Sig.GenPrivate(). for any sk Sig.Private and \ud835\udefc Sig.Random, Sig.RandomizePublic(\ud835\udefc, Sig.DerivePublic(sk))  Sig.DerivePublic(Sig.RandomizePrivate(\ud835\udefc, sk)).",
      "The following security requirement for such signature schemes is based on that given in FKMSSS2016, section 3. Note that we require Strong Unforgeability with Re-randomized Keys, not Existential Unforgeability with Re- randomized Keys (the latter is called Unforgeability under Re-randomized Keys in FKMSSS2016, De\ufb01nition 8). Unlike the case for JoinSplitSig, we require security under adaptive chosen message attack with multiple messages signed using a given key.",
      "(Although each note uses a different re-randomized key pair, the same original key pair can be re-randomized for multiple notes, and also it can happen that multiple transactions spending the same note are revealed to an adversary.) Security requirement: Strong Unforgeability with Re-randomized Keys under adaptive Chosen Message Attack (SURK-CMA) For any sk Sig.Private, let Sig.Message  Sig.Random Sig.Signature be a signing oracle with state \ud835\udc44 Sig.Message  Sig.Signature initialized to  that records queried messages and corresponding signatures. Osk : let mutable \ud835\udc44 in (\ud835\udc5a Sig.Message, \ud835\udefc Sig.Random)  let \ud835\udf0e Sig.SignSig.RandomizePrivate(\ud835\udefc,sk)(\ud835\udc5a) set \ud835\udc44\ud835\udc44(\ud835\udc5a, \ud835\udf0e) return \ud835\udf0e Sig.Signature. For random sk  R Sig.GenPrivate() and vk  Sig.DerivePublic(sk), it must be infeasible for an adversary given vk and a new instance of Osk to \ufb01nd (\ud835\udc5a, \ud835\udf0e, \ud835\udefc) such that Sig.ValidateSig.RandomizePublic(\ud835\udefc,vk)(\ud835\udc5a, \ud835\udf0e)  1 and (\ud835\udc5a, \ud835\udf0e) Osk.\ud835\udc44.",
      "Non-normative notes:  The randomizer and key arguments to Sig.RandomizePrivate and Sig.RandomizePublic are swapped relative to FKMSSS2016, section 3. The requirement for the identity randomizer \ud835\udcaaSig.Random simpli\ufb01es the de\ufb01nition of SURK-CMA by removing the need for two oracles (because the oracle for original keys, called O1 in FKMSSS2016, is a special case of the oracle for randomized keys). Since Sig.RandomizePrivate(\ud835\udefc, sk) : \ud835\udefc R Sig.Random has an identical distribution to Sig.GenPrivate(), and since Sig.DerivePublic is a deterministic function, the combination of a re-randomized validating key and signature(s) under that key do not reveal the key from which it was re-randomized. Since Sig.RandomizePrivate\ud835\udefcis injective and easily invertible, knowledge of Sig.RandomizePrivate(\ud835\udefc, sk) and \ud835\udefc implies knowledge of sk.",
      "4.1.7.2 Signature with Signing Key to Validating Key Monomorphism A signature scheme with key monomorphism Sig is a signature scheme that additionally de\ufb01nes:  an abelian group on signing keys, with operation Sig.Private  Sig.Private Sig.Private and identity \ud835\udcaa;  an abelian group on validating keys, with operation Sig.Public  Sig.Public Sig.Public and identity \ud835\udcaa. such that for any sk1..2 Sig.Private, Sig.DerivePublic(sk1 sk2)  Sig.DerivePublic(sk1) Sig.DerivePublic(sk2). In other words, Sig.DerivePublic is a monomorphism (that is, an injective homomorphism) from the signing key group to the validating key group. For N N, \ud835\udc561sk\ud835\udc56means sk1    skN; \ud835\udc561vk\ud835\udc56means vk1    vkN. When N  0 these yield the appropriate group identity, i.e. \ud835\udc561sk\ud835\udc56 \ud835\udcaa \ud835\udc561vk\ud835\udc56 \ud835\udcaa. sk means the signing key such that ( sk  \ud835\udcaa, and sk1 sk2 means sk1 sk2). vk means the validating key such that ( vk  \ud835\udcaa, and vk1 vk2 means vk1 vk2).",
      "With a change of notation from \ud835\udf07to Sig.DerivePublic,  to , and  to , this is similar to the de\ufb01nition of a Signature with Secret Key to Public Key Homomorphism in DS2016, De\ufb01nition 13, except for an additional requirement for the homomorphism to be injective. Security requirement: For any sk1 Sig.Private, and an unknown sk2  R Sig.GenPrivate() chosen independently of sk1, the distribution of sk1 sk2 is computationally indistinguishable from that of Sig.GenPrivate(). (Since is an abelian group operation, this implies that for \ud835\udc5b N, \ud835\udc561sk\ud835\udc56is computationally indistinguishable from Sig.GenPrivate() when at least one of sk1..\ud835\udc5bis unknown.) 4.1.8 Commitment A commitment scheme is a function that, given a commitment trapdoor generated at random and an input, can be used to commit to the input in such a way that:  no information is revealed about it without the trapdoor (hiding); and  given the trapdoor and input, the commitment can be veri\ufb01ed to open to that input and no other (binding).",
      "A commitment scheme COMM de\ufb01nes a type of inputs COMM.Input, a type of commitments COMM.Output, a type of commitment trapdoors COMM.Trapdoor, and a trapdoor generator COMM.GenTrapdoor ()  R COMM.Trapdoor. Let COMM COMM.Trapdoor  COMM.Input COMM.Output be a function satisfying the following security requirements. Security requirements:  Computational hiding: For all \ud835\udc65, \ud835\udc65 COMM.Input, the distributions  COMM\ud835\udc5f(\ud835\udc65)  \ud835\udc5f R COMM.GenTrapdoor()  and  COMM\ud835\udc5f(\ud835\udc65)  \ud835\udc5f R COMM.GenTrapdoor()  are computationally indistinguishable. Computational binding: It is infeasible to \ufb01nd \ud835\udc65, \ud835\udc65 COMM.Input and \ud835\udc5f, \ud835\udc5f COMM.Trapdoor such that \ud835\udc65 \ud835\udc65 and COMM\ud835\udc5f(\ud835\udc65)  COMM\ud835\udc5f(\ud835\udc65). Notes:  COMM.GenTrapdoor need not produce the uniform distribution on COMM.Trapdoor. In that case, it is incorrect to choose a trapdoor from the latter distribution. If it were only feasible to \ufb01nd \ud835\udc65 COMM.Input and \ud835\udc5f, \ud835\udc5f COMM.Trapdoor such that \ud835\udc5f \ud835\udc5f and COMM\ud835\udc5f(\ud835\udc65)  COMM\ud835\udc5f(\ud835\udc65), this would not contradict the computational binding security requirement.",
      "(In fact, this is feasible for NoteCommitSapling and ValueCommitSapling because trapdoors are equivalent modulo \ud835\udc5fJ, and the range of a trapdoor for those algorithms is 0 .. 2\u2113Sapling scalar 1 where 2\u2113Sapling scalar  \ud835\udc5fJ.) Let \u2113Sprout , \u2113Sprout Merkle, \u2113Sprout PRF , and \u2113value be as de\ufb01ned in section5.3 Constants on page 74. De\ufb01ne NoteCommitSprout.Trapdoor : B\u2113Sprout  and NoteCommitSprout.Output : B\u2113Sprout Merkle. Sprout uses a note commitment scheme NoteCommitSprout NoteCommitSprout.Trapdoor  B\u2113Sprout PRF   0 .. 2\u2113value1  B\u2113Sprout PRF  NoteCommitSprout.Output, instantiated in section5.4.8.1 Sprout Note Commitments on page 95. Let \u2113Sapling scalar be as de\ufb01ned in section5.3 Constants on page 74. Let J(\ud835\udc5f), \u2113J, and \ud835\udc5fJ be as de\ufb01ned in section5.4.9.3 Jubjub on page 102. De\ufb01ne: NoteCommitSapling.Trapdoor : 0 .. 2\u2113Sapling scalar 1 and NoteCommitSapling.Output : J; ValueCommitSapling.Trapdoor : 0 .. 2\u2113Sapling scalar 1 and ValueCommitSapling.Output : J.",
      "Sapling uses two additional commitment schemes: NoteCommitSapling NoteCommitSapling.Trapdoor  B\u2113J  B\u2113J  0 .. 2\u2113value1 NoteCommitSapling.Output ValueCommitSapling ValueCommitSapling.Trapdoor  \ud835\udc5fJ1 .. \ud835\udc5fJ1 ValueCommitSapling.Output NoteCommitSapling is instantiated in section5.4.8.2 Windowed Pedersen commitments on page 96, and ValueCommitSapling is instantiated in section5.4.8.3 Homomorphic Pedersen commitments (Sapling and Orchard) on page 97. Non-normative note: NoteCommitSapling and ValueCommitSapling always return points in the subgroup J(\ud835\udc5f). However, we declare the type of these commitment outputs to be J because they are not directly checked to be in the subgroup when ValueCommitSapling outputs appear in Spend descriptions and Output descriptions, or when the cmu \ufb01eld derived from a NoteCommitSapling appears in an Output description. Let \u2113Orchard scalar be as de\ufb01ned in section5.3 Constants on page 74. Let P, \u2113P, \ud835\udc5eP, and \ud835\udc5fP be as de\ufb01ned in section5.4.9.6 Pallas and Vesta on page 105.",
      "De\ufb01ne: NoteCommitOrchard.Trapdoor : 0 .. 2\u2113Orchard scalar 1 and NoteCommitOrchard.Output : P ; ValueCommitOrchard.Trapdoor : 0 .. 2\u2113Orchard scalar 1 and ValueCommitOrchard.Output : P. Commitivk.Trapdoor : 0 .. 2\u2113Orchard scalar 1 and Commitivk.Output : 0 .. \ud835\udc5eP 1 . Orchard uses three additional commitment schemes: NoteCommitOrchard NoteCommitOrchard.Trapdoor  B\u2113P  B\u2113P  0 .. 2\u2113value1  F\ud835\udc5eP  F\ud835\udc5eP NoteCommitOrchard.Output ValueCommitOrchard ValueCommitOrchard.Trapdoor  \ud835\udc5fP1 .. \ud835\udc5fP1 ValueCommitOrchard.Output Commitivk Commitivk.Trapdoor  0 .. \ud835\udc5eP 1  F\ud835\udc5eP Commitivk.Output Notes:  NoteCommitOrchard and Commitivk can return (with insigni\ufb01cant probability). Commitivk can return 0 (with insigni\ufb01cant probability) even though that is not a valid KAOrchard private key. The use of Commitivk to obtain an Orchard incoming viewing key in section4.2.3 Orchard Key Components on page 38 explicitly accounts for the 0 and cases. Use of Commitivk in the Action circuit does not require special handling of the 0 case.",
      "NoteCommitOrchard and Commitivk are instantiated in section5.4.8.4 Sinsemilla commitments on page 98. ValueCommitOrchard is instantiated in section5.4.8.3 Homomorphic Pedersen commitments (Sapling and Orchard) on page 97. 4.1.9 Represented Group A represented group G consists of:  a subgroup order parameter \ud835\udc5fG N, which must be prime;  a cofactor parameter \u210eG N;  a group G of order \u210eG  \ud835\udc5fG, written additively with operation  G  G G, and additive identity \ud835\udcaaG;  a bit-length parameter \u2113G  a representation function reprG G B\u2113G and an abstraction function abstG B\u2113G G , such that abstG is a left inverse of reprG, i.e. for all \ud835\udc43G, abstG reprG(\ud835\udc43)  \ud835\udc43. Note: Ideally, we would also have that for all \ud835\udc46not in the image of reprG, abstG(\ud835\udc46)  . This may not be true in all cases, i.e. there can be non-canonical encodings \ud835\udc43such that reprG abstG(\ud835\udc43)  \ud835\udc43. De\ufb01ne G(\ud835\udc5f) as the order-\ud835\udc5fG subgroup of G, which is called a represented subgroup. Note that this includes \ud835\udcaaG.",
      "For the set of points of order \ud835\udc5fG (which excludes \ud835\udcaaG), we write G(\ud835\udc5f). De\ufb01ne G (\ud835\udc5f) : reprG(\ud835\udc43) B\u2113G  \ud835\udc43G(\ud835\udc5f). (This intentionally excludes non-canonical encodings if there are any.) For \ud835\udc3a G we write \ud835\udc3afor the negation of \ud835\udc3a, such that (\ud835\udc3a)  \ud835\udc3a \ud835\udcaaG. We write \ud835\udc3a\ud835\udc3bfor \ud835\udc3a (\ud835\udc3b). We also extend the notation to addition on group elements. For \ud835\udc3a G and \ud835\udc58 Z we write \ud835\udc58 \ud835\udc3afor scalar multiplication on the group, i.e. \ud835\udc58 \ud835\udc3a: \ud835\udc561\ud835\udc3a, if \ud835\udc580 \ud835\udc58 \ud835\udc561(\ud835\udc3a), otherwise. For \ud835\udc3a G and \ud835\udc4e F\ud835\udc5fG, we may also write \ud835\udc4e \ud835\udc3ameaning \ud835\udc4emod \ud835\udc5fG \ud835\udc3aas de\ufb01ned above. (This variant is not de\ufb01ned for \ufb01elds other than F\ud835\udc5fG.) 4.1.10 Coordinate Extractor A coordinate extractor for a represented group G is a function ExtractG(\ud835\udc5f) G(\ud835\udc5f) \ud835\udc47for some type \ud835\udc47. Note: Unlike the representation function reprG, ExtractG(\ud835\udc5f) need not have an ef\ufb01ciently computable left inverse.",
      "4.1.11 Group Hash Given a represented subgroup G(\ud835\udc5f), a family of group hashes into the subgroup, denoted GroupHashG(\ud835\udc5f) , consists of:  a type GroupHashG(\ud835\udc5f) .URSType of Uniform Random Strings;  a type GroupHashG(\ud835\udc5f) .Input of inputs;  a function GroupHashG(\ud835\udc5f) GroupHashG(\ud835\udc5f) .URSType  GroupHashG(\ud835\udc5f) .Input G(\ud835\udc5f). In section5.4.9.5 Group Hash into Jubjub on page 104, we instantiate a family of group hashes into the Jubjub curve de\ufb01ned by section5.4.9.3 Jubjub on page 102. Security requirement: For a randomly selected URS GroupHashG(\ud835\udc5f) .URSType, it must be reasonable to model GroupHashG(\ud835\udc5f) URS (restricted to inputs for which it does not return ) as a random oracle. In section5.4.9.8 Group Hash into Pallas and Vesta on page 107, we instantiate group hashes into the Pallas and Vesta curves. These are not strictly speaking families of group hashes, because they have a trivial URS, and so the above security de\ufb01nition does not apply.",
      "Nevertheless, they can be heuristically modelled as random oracles. Non-normative notes:  GroupHashJ(\ud835\udc5f) is used to obtain generators of the Jubjub curve for various purposes: the bases \ud835\udca2Sapling and \u210bSapling used in Sapling key generation, the Pedersen hash de\ufb01ned in section5.4.1.7 Pedersen Hash Function on page 79, and the commitment schemes de\ufb01ned in section5.4.8.2 Windowed Pedersen commitments on page 96 and in section5.4.8.3 Homomorphic Pedersen commitments (Sapling and Orchard) on page 97. The security property needed for these uses can alternatively be de\ufb01ned in the standard model as follows: Discrete Logarithm Independence: Fora randomlyselected memberGroupHashG(\ud835\udc5f) URS of the family, it is infeasible to \ufb01nd a sequence of distinct inputs \ud835\udc5a1..\ud835\udc5b GroupHashG(\ud835\udc5f) .Input\ud835\udc5b and a sequence of nonzero \ud835\udc651..\ud835\udc5b such that \ud835\udc5b \ud835\udc65\ud835\udc56 GroupHashG(\ud835\udc5f) URS(\ud835\udc5a\ud835\udc56)  \ud835\udcaaG. Under the Discrete Logarithm assumption on G(\ud835\udc5f), a random oracle almost surely satis\ufb01es Discrete Logarithm Independence.",
      "Discrete Logarithm Independence implies collision resistance, since a collision (\ud835\udc5a1, \ud835\udc5a2) for GroupHashG(\ud835\udc5f) URS trivially gives a discrete logarithm relation with \ud835\udc651  1 and \ud835\udc652  1. GroupHashJ(\ud835\udc5f) is used in section5.4.1.6 DiversifyHashSapling and DiversifyHashOrchard Hash Functions on page 78 to in- stantiate DiversifyHashSapling. We do not know how to prove the Unlinkability property de\ufb01ned in that section in the standard model, but in a model where GroupHashJ(\ud835\udc5f) (restricted to inputs for which it does not return ) is taken as a random oracle, it is implied by the Decisional Dif\ufb01eHellman assumption on J(\ud835\udc5f), and similarly for GroupHashP. URS is a Uniform Random String; we chose it veri\ufb01ably at random (see section5.9 Randomness Beacon on page 120), after \ufb01xing the concrete group hash algorithm to be used. This mitigates the possibility that the group hash algorithm could have been backdoored.",
      "For Orchard, we considered a URS to be unnecessary, because we follow ID-hashtocurve which does not use one. 4.1.12 Represented Pairing A represented pairing PAIR consists of:  a group order parameter \ud835\udc5fPAIR N which must be prime;  two represented subgroups PAIR(\ud835\udc5f) 1,2, both of order \ud835\udc5fPAIR;  a group PAIR(\ud835\udc5f) \ud835\udc47of order \ud835\udc5fPAIR, written multiplicatively with operation  PAIR(\ud835\udc5f) \ud835\udc47 PAIR(\ud835\udc5f) \ud835\udc47PAIR(\ud835\udc5f) \ud835\udc47and group identity 1PAIR;  three generators \ud835\udcabPAIR1,2,\ud835\udc47of PAIR(\ud835\udc5f) 1,2,\ud835\udc47respectively;  a pairing function \ud835\udc52PAIR PAIR(\ud835\udc5f) 1  PAIR(\ud835\udc5f) 2 PAIR(\ud835\udc5f) \ud835\udc47satisfying:  (Bilinearity) for all \ud835\udc4e, \ud835\udc4f \ud835\udc5f, \ud835\udc43 PAIR(\ud835\udc5f) 1 , and \ud835\udc44 PAIR(\ud835\udc5f) 2 , \ud835\udc52PAIR(\ud835\udc4e \ud835\udc43, \ud835\udc4f \ud835\udc44) \ud835\udc52PAIR(\ud835\udc43, \ud835\udc44)\ud835\udc4e\ud835\udc4f; and  (Nondegeneracy) there does not exist \ud835\udc43 PAIR(\ud835\udc5f) such that for all \ud835\udc44 PAIR(\ud835\udc5f) 2 , \ud835\udc52PAIR(\ud835\udc43, \ud835\udc44) 1PAIR.",
      "4.1.13 Zero-Knowledge Proving System A zero-knowledge proving system is a cryptographic protocol that allows proving a particular statement, dependent on primary and auxiliary inputs, in zero knowledge  that is, without revealing information about the auxiliary inputs other than that implied by the statement. The type of zero-knowledge proving system needed by Zcash is a preprocessing zk-SNARK BCCGLRT2014.",
      "A preprocessing zk-SNARK instance ZK de\ufb01nes:  a type of zero-knowledge proving keys, ZK.ProvingKey;  a type of zero-knowledge verifying keys, ZK.VerifyingKey;  a type of primary inputs ZK.PrimaryInput;  a type of auxiliary inputs ZK.AuxiliaryInput;  a type of zk-SNARK proofs ZK.Proof;  a type ZK.SatisfyingInputs ZK.PrimaryInput  ZK.AuxiliaryInput of inputs satisfying the statement;  a randomized key pair generation algorithm ZK.Gen ()  R ZK.ProvingKey  ZK.VerifyingKey;  a proving algorithm ZK.Prove ZK.ProvingKey  ZK.SatisfyingInputs ZK.Proof;  a verifying algorithm ZK.Verify ZK.VerifyingKey  ZK.PrimaryInput  ZK.Proof B; The security requirements below are supposed to hold with overwhelming probability for (pk, vk)  R ZK.Gen(). Security requirements:  Completeness: An honestly generated proof will convince a veri\ufb01er: for any (\ud835\udc65, \ud835\udc64) ZK.SatisfyingInputs, if ZK.Provepk(\ud835\udc65, \ud835\udc64) outputs \ud835\udf0b, then ZK.Verifyvk(\ud835\udc65, \ud835\udf0b)  1.",
      "Knowledge Soundness: For any adversary \ud835\udc9cable to \ufb01nd an \ud835\udc65 ZK.PrimaryInput and proof \ud835\udf0b ZK.Proof such that ZK.Verifyvk(\ud835\udc65, \ud835\udf0b)  1, there is an ef\ufb01cient extractor \u2130\ud835\udc9csuch that if \u2130\ud835\udc9c(vk, pk) returns \ud835\udc64, then the probability that (\ud835\udc65, \ud835\udc64) ZK.SatisfyingInputs is insigni\ufb01cant. Statistical Zero Knowledge: An honestly generated proof is statistical zero knowledge. That is, there is a feasible stateful simulator \ud835\udcaesuch that, for all stateful distinguishers \ud835\udc9f, the following two probabilities are not signi\ufb01cantly different: (\ud835\udc65, \ud835\udc64) ZK.SatisfyingInputs \ud835\udc9f(\ud835\udf0b)  1  (pk, vk)  R ZK.Gen() (\ud835\udc65, \ud835\udc64)  R \ud835\udc9f(pk, vk) R ZK.Provepk(\ud835\udc65, \ud835\udc64) and Pr (\ud835\udc65, \ud835\udc64) ZK.SatisfyingInputs \ud835\udc9f(\ud835\udf0b)  1  (pk, vk)  R \ud835\udcae() (\ud835\udc65, \ud835\udc64)  R \ud835\udc9f(pk, vk) R \ud835\udcae(\ud835\udc65) These de\ufb01nitions are derived from those in BCTV2014b, Appendix C, adapted to state concrete security for a \ufb01xed circuit, rather than asymptotic security for arbitrary circuits.",
      "(ZK.Prove corresponds to \ud835\udc43, ZK.Verify corresponds to \ud835\udc49, and ZK.SatisfyingInputs corresponds to \u211b\ud835\udc36in the notation of that appendix.) The Knowledge Soundness de\ufb01nition is a way to formalize the property that it is infeasible to \ufb01nd a new proof \ud835\udf0bwhere ZK.Verifyvk(\ud835\udc65, \ud835\udf0b)  1 without knowing an auxiliary input \ud835\udc64such that (\ud835\udc65, \ud835\udc64) ZK.SatisfyingInputs. Note that Knowledge Soundness implies Soundness  i.e. the property that it is infeasible to \ufb01nd a new proof \ud835\udf0bwhere ZK.Verifyvk(\ud835\udc65, \ud835\udf0b)  1 without there existing an auxiliary input \ud835\udc64such that (\ud835\udc65, \ud835\udc64) ZK.SatisfyingInputs. Non-normative notes:  The above properties do not include nonmalleability DSDCOPS2001, and the design of the protocol using the zero-knowledge proving system must take this into account. The terminology used in this speci\ufb01cation is that we validate signatures, and verify zk-SNARK proofs.",
      "Zcash uses three proving systems:  BCTV14 (section5.4.10.1 BCTV14 on page 110) is used with the BN-254 pairing (section5.4.9.1 BN-254 on page 99), to prove and verify the Sprout JoinSplit statement (section4.18.1 JoinSplit Statement (Sprout) on page 60) before Sapling activation. Groth16 (section5.4.10.2 Groth16 on page 111) is used with the BLS12-381 pairing (section5.4.9.2 BLS12-381 on page 101), to prove and verify the Sapling Spend statement (section4.18.2 Spend Statement (Sapling) on page 61) and Output statement (section4.18.3 Output Statement (Sapling) on page 62). It is also used to prove and verify the JoinSplit statement after Sapling activation. NU55 onward Halo 2 (section5.4.10.3 Halo 2 on page 112) is used with the Vesta curve (section5.4.9.6 Pallas and Vesta on page 105) to prove and verify the Orchard Action statement (section4.18.4 Action Statement (Orchard) on page 63).",
      "These specializations are:  ZKJoinSplit for the Sprout JoinSplit statement (with BCTV14 and BN-254, or Groth16 and BLS12-381);  ZKSpend for the Sapling Spend statement and ZKOutput for the Sapling Output statement;  NU55 onward ZKAction for the Orchard Action statement. We omit key subscripts on ZKJoinSplit.Prove and ZKJoinSplit.Verify, taking them to be either the BCTV14 proving key and verifying key de\ufb01ned in section5.7 BCTV14 zk-SNARK Parameters on page 119, or the sprout-groth16.params Groth16 proving key and verifying key de\ufb01ned in section5.8 Groth16 zk-SNARK Parameters on page 119, according to whether the proof appears in a block before or after Sapling activation. We omit subscripts on ZKSpend.Prove, ZKSpend.Verify, ZKOutput.Prove, and ZKOutput.Verify, taking them to be the relevant Groth16 proving keys and verifying keys de\ufb01ned in section5.8 Groth16 zk-SNARK Parameters on page 119. We also omit subscripts on ZKAction.Prove and ZKAction.Verify.",
      "For Halo 2, parameters for a given circuit imple- mentation are generated on the \ufb02y by the halo2 library, and do not require parameter \ufb01les. Key Components 4.2.1 Sprout Key Components Let \u2113ask be as de\ufb01ned in section5.3 Constants on page 74. Let PRFaddr be a Pseudo Random Function, instantiated in section5.4.2 Pseudo Random Functions on page 86. Let KASprout be a key agreement scheme, instantiated in section5.4.5.1 Sprout Key Agreement on page 88. A new Sprout spending key ask is generated by choosing a bit sequence uniformly at random from B\u2113ask. apk, skenc and pkenc are derived from ask as follows: apk : PRFaddr ask (0) skenc : KASprout.FormatPrivate(PRFaddr ask (1)) pkenc : KASprout.DerivePublic(skenc, KASprout.Base). 4.2.2 Sapling Key Components Let \u2113PRFexpand, \u2113sk, \u2113Sapling , \u2113ovk, and \u2113d be as de\ufb01ned in section5.3 Constants on page 74.",
      "Let J(\ud835\udc5f), J(\ud835\udc5f), J (\ud835\udc5f), reprJ, and \ud835\udc5fJ be as de\ufb01ned in section5.4.9.3 Jubjub on page 102, and let FindGroupHashJ(\ud835\udc5f) be as de\ufb01ned in section5.4.9.5 Group Hash into Jubjub on page 104. Let PRFexpand and PRFockSapling, instantiated in section5.4.2 Pseudo Random Functions on page 86, be Pseudo Random Functions. Let KASapling, instantiated in section5.4.5.3 Sapling Key Agreement on page 89, be a key agreement scheme. Let CRHivk, instantiated in section5.4.1.5 CRHivk Hash Function on page 77, be a hash function. Let DiversifyHashSapling, instantiated in section5.4.1.6 DiversifyHashSapling and DiversifyHashOrchard Hash Functions on page 78, be a hash function. Let SpendAuthSigSapling, instantiated in section5.4.7.1 Spend Authorization Signature (Sapling and Orchard) on page 95, be a signature scheme with re-randomizable keys. Let LEBS2OSP N)  B\u2113 BYceiling(\u21138) and LEOS2IP N  \u2113mod 8  0)  BY\u21138 0 .. 2\u21131 be as de\ufb01ned in section5.1 Integers, Bit Sequences, and Endianness on page 73.",
      "De\ufb01ne \u210bSapling : FindGroupHashJ(\ud835\udc5f) (Zcash_H_, ). De\ufb01ne ToScalarSapling(\ud835\udc65 BY\u2113PRFexpand8) : LEOS2IP\u2113PRFexpand(\ud835\udc65) (mod \ud835\udc5fJ). A new Sapling spending key sk is generated by choosing a bit sequence uniformly at random from B\u2113sk. From this spending key, the Spend authorizing key ask \ud835\udc5fJ, the proof authorizing key nsk F\ud835\udc5fJ, and the outgoing viewing key ovk BY\u2113ovk8 are derived as follows: ask : ToScalarSapling( PRFexpand (0) nsk : ToScalarSapling( PRFexpand (1) ovk : truncate(\u2113ovk8) PRFexpand (2) If ask  0, discard this key and repeat with a new sk. J(\ud835\udc5f), nk J(\ud835\udc5f), and the incoming viewing key ivk 1 .. 2\u2113Sapling 1 are then derived as: ak : SpendAuthSigSapling.DerivePublic(ask) nk : nsk \u210bSapling ivk : CRHivk( reprJ(ak), reprJ(nk) If ivk  0, discard this key and repeat with a new sk. As explained in section3.1 Payment Addresses and Keys on page 13, Sapling allows the ef\ufb01cient creation of multiple diversi\ufb01ed payment addresses with the same spending authority.",
      "A group of such addresses shares the same full viewing key and incoming viewing key. To create a new diversi\ufb01ed payment address given an incoming viewing key ivk, repeatedly pick a diversi\ufb01er d uniformly at random from B\u2113d until the diversi\ufb01ed base gd  DiversifyHashSapling(d) is not . Then calculate the diversi\ufb01ed transmission key pkd: pkd : KASapling.DerivePublic(ivk, gd). The resulting diversi\ufb01ed payment address is (d B\u2113d, pkd KASapling.PublicPrimeOrder). For each spending key, there is also a default diversi\ufb01ed payment address with a random-looking diversi\ufb01er. This allows an implementation that does not expose diversi\ufb01ed addresses as a user-visible feature, to use a default address that cannot be distinguished (without knowledge of the spending key) from one with a random diversi\ufb01er as above. Note however that the zcashd wallet picks diversi\ufb01ers as in ZIP-32, rather than using this procedure. Let first (BY \ud835\udc47) \ud835\udc47 be as de\ufb01ned in section5.4.9.5 Group Hash into Jubjub on page 104.",
      "De\ufb01ne: CheckDiversifier(d B\u2113d) : , if DiversifyHashSapling(d)   d, otherwise DefaultDiversifier(sk B\u2113sk) : first BY CheckDiversifier(truncate(\u2113d8)(PRFexpand (3, \ud835\udc56))) J(\ud835\udc5f)  For a random spending key, DefaultDiversifier returns with probability approximately 2256; if this happens, discard the key and repeat with a different sk. Notes:  The protocol does not prevent using the diversi\ufb01er d to produce vanity addresses that start with a meaningful string when encoded in Bech32 (see section5.6.3.1 Sapling Payment Addresses on page 115). Users and writers of software that generates addresses should be aware that this provides weaker privacy properties than a randomly chosen diversi\ufb01er, since a vanity address can obviously be distinguished, and might leak more information than intended as to who created it. Similarly, address generators MAY encode information in the diversi\ufb01er that can be recovered by the recipient of a payment to determine which diversi\ufb01ed payment address was used.",
      "It is RECOMMENDED, instead of directly encoding information in the diversi\ufb01er, to encode it in the diversi\ufb01er index speci\ufb01ed in ZIP-32. This ensures that the information is only accessible to a holder of the diversi\ufb01er key dk. Non-normative notes:  Assume that PRFexpand is a PRF with output range BY\u2113PRFexpand8, where 2\u2113PRFexpand is large compared to \ud835\udc5fJ. De\ufb01ne \ud835\udc53 B\u2113sk  BYN F\ud835\udc5fJ by \ud835\udc53sk(\ud835\udc61) : ToScalarSapling( PRFexpand \ud835\udc53is also a PRF since LEOS2IP\u2113PRFexpand BY\u2113PRFexpand8 0 .. 2\u2113PRFexpand1 is injective; the bias introduced by reduc- tion modulo \ud835\udc5fJ is small because section5.3 Constants on page 74 de\ufb01nes \u2113PRFexpand as 512, while \ud835\udc5fJ has length 252 bits. It follows that the distribution of ask, i.e. PRFexpand (0) : sk  R B\u2113sk, is computationally indistinguishable from SpendAuthSigSapling.GenPrivate() de\ufb01ned in section5.4.7.1 Spend Authorization Signature (Sapling and Orchard) on page 95. The distribution of nsk, i.e.",
      "ToScalarSapling( PRFexpand (1) : sk  R B\u2113sk, is computationally indistinguishable from the uniform distribution on F\ud835\udc5fJ. Since nsk F\ud835\udc5fJ reprJ nsk \u210bSapling (\ud835\udc5f)) is bijective, the distribution of reprJ(nk)will be computationally indistinguishable from uniform on J (\ud835\udc5f) (the keyspace of PRFnfSapling). 4.2.3 Orchard Key Components Let \u2113PRFexpand, \u2113sk, \u2113ovk, \u2113d, and \u2113dk be as de\ufb01ned in section5.3 Constants on page 74. Let P, reprP, \u2113P, \ud835\udc5eP, and \ud835\udc5fP be as de\ufb01ned in section5.4.9.6 Pallas and Vesta on page 105. Let ExtractP be as de\ufb01ned in section5.4.9.7 Coordinate Extractor for Pallas on page 106. Let GroupHashP be as de\ufb01ned in section5.4.9.8 Group Hash into Pallas and Vesta on page 107. Let PRFexpand and PRFockOrchard be as de\ufb01ned in section5.4.2 Pseudo Random Functions on page 86. Let DeriveInternalFVKOrchard be as de\ufb01ned in ZIP-32, Orchard internal key derivation. Let PRPd BY\u2113dk8  B\u2113d B\u2113d be as de\ufb01ned in section5.4.4 Pseudo Random Permutations on page 88.",
      "Let KAOrchard, instantiated in section5.4.5.5 Orchard Key Agreement on page 90, be a key agreement scheme. Let Commitivk, instantiated in section5.4.8.4 Sinsemilla commitments on page 98, be a commitment scheme. Let DiversifyHashOrchardbe as de\ufb01ned in section5.4.1.6 DiversifyHashSapling and DiversifyHashOrchard Hash Functions on page 78. Let SpendAuthSigOrchard instantiated in section5.4.7.1 Spend Authorization Signature (Sapling and Orchard) on page 95 be a signature scheme with re-randomizable keys. Let I2LEBSP, I2LEOSP, and LEOS2IP be as de\ufb01ned in section5.1 Integers, Bit Sequences, and Endianness on page 73. De\ufb01ne ToBaseOrchard(\ud835\udc65 BY\u2113PRFexpand8) : LEOS2IP\u2113PRFexpand(\ud835\udc65) (mod \ud835\udc5eP). De\ufb01ne ToScalarOrchard(\ud835\udc65 BY\u2113PRFexpand8) : LEOS2IP\u2113PRFexpand(\ud835\udc65) (mod \ud835\udc5fP). A new Orchard spending key sk is generated by choosing a bit sequence uniformly at random from B\u2113sk. From this spending key, the Spend authorizing key ask \ud835\udc5fP, the Spend validating key ak 0 ..",
      "\ud835\udc5eP 1, the nulli\ufb01er deriving key nk F\ud835\udc5eP, the Commitivk randomness rivk F\ud835\udc5fP, the diversi\ufb01er key dk BY\u2113dk8, the KAOrchard private key 1 .. \ud835\udc5eP 1, the outgoing viewing key ovk BY\u2113ovk8, and corresponding internal keys are derived as follows: let mutable ask ToScalarOrchard( PRFexpand (6) let nk  ToBaseOrchard( PRFexpand (7) let rivk  ToScalarOrchard( PRFexpand (8) if ask  0, discard this key and repeat with a new sk. let akP  SpendAuthSigOrchard.DerivePublic(ask) if the last bit (that is, the \ud835\udc66bit) of reprP(akP) is 1: set ask ask let ak  ExtractP(akP) let ivk  Commitivk rivk ak, nk if ivk 0, , discard this key and repeat with a new sk. let \ud835\udc3e I2LEBSP\u2113sk(rivk) let \ud835\udc45 PRFexpand 0x82  I2LEOSP256(ak)  I2LEOSP256(nk) let dk be the \ufb01rst \u2113dk8 bytes of \ud835\udc45and let ovk be the remaining \u2113ovk8 bytes of \ud835\udc45.",
      "let (akinternal, nkinternal, rivkinternal)  DeriveInternalFVKOrchard(ak, nk, rivk) let ivkinternal  Commitivk rivkinternal akinternal, nkinternal if ivkinternal 0, , discard this key and repeat with a new sk. let \ud835\udc3einternal  I2LEBSP\u2113sk(rivkinternal) let \ud835\udc45internal  PRFexpand \ud835\udc3einternal 0x82  I2LEOSP256(akinternal)  I2LEOSP256(nkinternal) let dkinternal be the \ufb01rst \u2113dk8 bytes of \ud835\udc45internal and let ovkinternal be the remaining \u2113ovk8 bytes of \ud835\udc45internal. Note: akinternal  ak and nkinternal  nk. As explained in section3.1 Payment Addresses and Keys on page 13, Orchard allows the ef\ufb01cient creation of multiple diversi\ufb01ed payment addresses with the same spending authority. A group of such addresses shares the same full viewing key, incoming viewing key, and outgoing viewing key. To create a new diversi\ufb01ed payment address given an incoming viewing key (dk, ivk), pick a diversi\ufb01er index index uniquely from B\u2113d.",
      "Then calculate the diversi\ufb01er d and the diversi\ufb01ed transmission key pkd: d : PRPd dk(index) gd : DiversifyHashOrchard(d) pkd : KAOrchard.DerivePublic(ivk, gd). The resulting diversi\ufb01ed payment address is (d B\u2113d, pkd KAOrchard.PublicPrimeOrder). The diversi\ufb01ed payment address with diversi\ufb01er index 0 is called the default diversi\ufb01ed payment address. Notes:  Diversi\ufb01er indices SHOULD NOT be chosen at random. ZIP-32 speci\ufb01es their usage in the context of hierarchical deterministic wallets. Address generators MAY encode information in the diversi\ufb01er index that can be recovered by the recipient of a payment, given the diversi\ufb01er key. rivk is used both as a randomizer for Commitivk, and as a key for PRFexpand to derive dk and ovk. If dk and ovk are known to an adversary, then this reuse prevents proving that the use of Commitivk in this context is perfectly hiding. It is also not suf\ufb01cient to model PRFexpand only as a PRF.",
      "In practice, we believe it would be extremely surprising if there were an exploitable interaction between scalar multiplication used in Commitivk, and BLAKE2b used to instantiate PRFexpand. It is possible, albeit somewhat inelegantly, to model this usage by a joint assumption on Pallas scalar multiplication and PRFexpand. Non-normative notes:  The uses of ToScalarOrchard and ToBaseOrchard produce output that is uniform on F\ud835\udc5fP and F\ud835\udc5eP respectively when applied to random input, by a similar argument to that used in section4.2.2 Sapling Key Components on page 36. The output of Commitivk is the af\ufb01ne-short-Weierstrass \ud835\udc65-coordinate of a Pallas curve point, which we then use as a KAOrchard private key ivk for note encryption. The fact that ivk is non-uniform on F\ud835\udc5fP (since it can only take on roughly half of the possible values) is not expected to cause any security issue.",
      "JoinSplit Descriptions A JoinSplit transfer, as speci\ufb01ed in section3.5 JoinSplit Transfers and Descriptions on page 19, is encoded in transactions as a JoinSplit description. Each transaction includes a sequence of zero or more JoinSplit descriptions. When this sequence is non-empty, the transaction also includes encodings of a JoinSplitSig public validating key and signature. Let \u2113Sprout Merkle, \u2113Sprout PRF , \u2113Seed, Nold, Nnew, and MAX_MONEY be as de\ufb01ned in section5.3 Constants on page 74. Let hSigCRH be as de\ufb01ned in section4.1.1 Hash Functions on page 24. Let NoteCommitSprout be as de\ufb01ned in section4.1.8 Commitment on page 31. Let KASprout be as de\ufb01ned in section4.1.5 Key Agreement on page 26. Let Sym be as de\ufb01ned in section4.1.4 Symmetric Encryption on page 26. Let ZKJoinSplit be as de\ufb01ned in section4.1.13 Zero-Knowledge Proving System on page 34.",
      "A JoinSplit description comprises (vold pub, vnew pub, rtSprout, nfold 1..Nold, cmnew 1..Nnew, epk, randomSeed, h1..Nold, \ud835\udf0bZKJoinSplit, Cenc 1..Nnew) where  vold 0 .. MAX_MONEY is the value that the JoinSplit transfer removes from the transparent transaction value pool;  vnew 0 .. MAX_MONEY is the value that the JoinSplit transfer inserts into the transparent transaction value pool;  rtSprout B\u2113Sprout Merkle is an anchor, as de\ufb01ned in section3.4 Transactions and Treestates on page 18, for the output treestate of either a previous block, or a previous JoinSplit transfer in this transaction.",
      "nfold 1..Nold B\u2113Sprout PRF Nold is the sequence of nulli\ufb01ers for the input notes;  cmnew 1..Nnew NoteCommitSprout.OutputNnew is the sequence of note commitments for the output notes;  epk KASprout.Public is a key agreement public key, used to derive the key for encryption of the transmitted notes ciphertext (section4.19 In-band secret distribution (Sprout) on page 65);  randomSeed B\u2113Seed is a seed that must be chosen independently at random for each JoinSplit description;  h1..Nold B\u2113Sprout PRF Nold is a sequence of tags that bind hSig to each ask of the input notes;  \ud835\udf0bZKJoinSplit ZKJoinSplit.Proof is a zk proof with primary input (rtSprout, nfold 1..Nold, cmnew 1..Nnew, vold pub, vnew pub, hSig, h1..Nold) for the JoinSplit statement de\ufb01ned in section4.18.1 JoinSplit Statement (Sprout) on page 60 (this is a BCTV14 proof before Sapling activation, and a Groth16 proof after Sapling activation);  Cenc 1..Nnew Sym.CNnew is a sequence of ciphertext components for the encrypted output notes.",
      "The ephemeralKey and encCiphertexts \ufb01elds together form the transmitted notes ciphertext. The value hSig is also computed from randomSeed, nfold 1..Nold, and the joinSplitPubKey of the containing transaction: hSig : hSigCRH(randomSeed, nfold 1..Nold, joinSplitPubKey). Consensus rules:  Elements of a JoinSplit description MUST have the types given above (for example: 0 vold pub MAX_MONEY and 0 vnew pub MAX_MONEY). The proof \ud835\udf0bZKJoinSplit MUST be valid given a primary input formed from the relevant other \ufb01elds and hSig  i.e. ZKJoinSplit.Verify (rtSprout, nfold 1..Nold, cmnew 1..Nnew, vold pub, vnew pub, hSig, h1..Nold), \ud835\udf0bZKJoinSplit  1. Either vold pub or vnew pub MUST be zero. Canopy onward vold pub MUST be zero. Spend Descriptions A Spend transfer, as speci\ufb01ed in section3.6 Spend Transfers, Output Transfers, and their Descriptions on page 20, is encoded in transactions as a Spend description. Each transaction includes a sequence of zero or more Spend descriptions.",
      "Each Spend description is authorized by a signature, called the spend authorization signature. Let \u2113Sapling Merkle and \u2113PRFnfSapling be as de\ufb01ned in section5.3 Constants on page 74. Let \ud835\udcaaJ, abstJ, reprJ, and \u210eJ be as de\ufb01ned in section5.4.9.3 Jubjub on page 102. Let ValueCommitSapling.Output be as de\ufb01ned in section4.1.8 Commitment on page 31. Let SpendAuthSigSapling be as de\ufb01ned in section4.15 Spend Authorization Signature (Sapling and Orchard) on page 56. Let ZKSpend be as de\ufb01ned in section4.1.13 Zero-Knowledge Proving System on page 34.",
      "A Spend description comprises (cv, rtSapling, nf, rk, \ud835\udf0bZKSpend, spendAuthSig) where  cv ValueCommitSapling.Output is the value commitment to the value of the input note;  rtSapling B\u2113Sapling Merkle  is an anchor, as de\ufb01ned in section3.4 Transactions and Treestates on page 18, for the output treestate of a previous block;  nf BY\u2113PRFnfSapling8 is the nulli\ufb01er for the input note;  rk SpendAuthSigSapling.Public is a randomized validating key that should be used to validate spendAuthSig;  \ud835\udf0bZKSpend ZKSpend.Proof is a zk-SNARK proof with primary input (cv, rtSapling, nf, rk) for the Spend statement de\ufb01ned in section4.18.2 Spend Statement (Sapling) on page 61;  spendAuthSig SpendAuthSigSapling.Signature is a spend authorization signature, validated as speci\ufb01ed in section4.15 Spend Authorization Signature (Sapling and Orchard) on page 56. Consensus rules:  Elements of a Spend description MUST be valid encodings of the types given above. cv and rk MUST NOT be of small order, i.e.",
      "\u210eJ cv MUST NOT be \ud835\udcaaJ and \u210eJ rk MUST NOT be \ud835\udcaaJ. The proof \ud835\udf0bZKSpend MUST be valid given a primary input formed from the other \ufb01elds except spendAuthSig  i.e. ZKSpend.Verify (cv, rtSapling, nf, rk), \ud835\udf0bZKSpend  1. Let SigHash be the SIGHASH transaction hash of this transaction, not associated with an input, as de\ufb01ned in section4.10 SIGHASH Transaction Hashing on page 50 using SIGHASH_ALL. The spend authorization signature MUST be a valid SpendAuthSigSapling signature over SigHash using rk as the validating key i.e. SpendAuthSigSapling.Validaterk(SigHash, spendAuthSig)  1. NU55 onward As speci\ufb01ed in section5.4.7 RedDSA, RedJubjub, and RedPallas on page 92, the validation of the \ud835\udc45 component of the signature changes to prohibit non-canonical encodings. This change is also retrospectively valid on Mainnet and Testnet before NU55.",
      "Non-normative notes:  As stated in section5.4.8.3 on page 97, an implementation of HomomorphicPedersenCommitSapling MAY resample the commitment trapdoor until the resulting commitment is not \ud835\udcaaJ. The rule that cv and rk MUST not be small-order has the effect of also preventing non-canonical encodings of these \ufb01elds, as required by ZIP-216. That is, it is necessarily the case that reprJ abstJ(cv)  cv and reprJ abstJ(rk)  rk. Output Descriptions An Output transfer, as speci\ufb01ed in section3.6 Spend Transfers, Output Transfers, and their Descriptions on page 20, is encoded in transactions as an Output description. Each transaction includes a sequence of zero or more Output descriptions. There are no signatures associated with Output descriptions. Let \u2113Sapling Merkle be as de\ufb01ned in section5.3 Constants on page 74. Let \ud835\udcaaJ, abstJ, reprJ, and \u210eJ be as de\ufb01ned in section5.4.9.3 Jubjub on page 102. Let ValueCommitSapling.Output be as de\ufb01ned in section4.1.8 Commitment on page 31.",
      "Let KASapling be as de\ufb01ned in section4.1.5 Key Agreement on page 26. Let Sym be as de\ufb01ned in section4.1.4 Symmetric Encryption on page 26. Let ZKOutput be as de\ufb01ned in section4.1.13 Zero-Knowledge Proving System on page 34.",
      "An Output description comprises (cv, cm\ud835\udc62, epk, Cenc, Cout, \ud835\udf0bZKOutput) where  cv ValueCommitSapling.Output is the value commitment to the value of the output note;  cm\ud835\udc62 B\u2113Sapling Merkle  is the result of applying ExtractJ(\ud835\udc5f) (de\ufb01ned in section5.4.9.4 Coordinate Extractor for Jubjub on page 104) to the note commitment for the output note;  epk KASapling.Public is a key agreement public key, used to derive the key for encryption of the transmitted note ciphertext (section4.20 In-band secret distribution (Sapling and Orchard) on page 67);  Cenc Sym.C is a ciphertext component for the encrypted output note;  Cout Sym.C is a ciphertext component that allows the holder of the outgoing cipher key (which can be derived from a full viewing key) to recover the recipient diversi\ufb01ed transmission key pkd and the ephemeral private key esk, hence the entire note plaintext;  \ud835\udf0bZKOutput ZKOutput.Proof is a zk-SNARK proof with primary input (cv, cm\ud835\udc62, epk) for the Output statement de\ufb01ned in section4.18.3 Output Statement (Sapling) on page 62.",
      "Consensus rules:  Elements of an Output description MUST be valid encodings of the types given above. cv and epk MUST NOT be of small order, i.e. \u210eJ cv MUST NOT be \ud835\udcaaJ and \u210eJ epk MUST NOT be \ud835\udcaaJ. The proof \ud835\udf0bZKOutput MUST be valid given a primary input formed from the other \ufb01elds except Cenc and Cout  i.e. ZKOutput.Verify (cv, cm\ud835\udc62, epk), \ud835\udf0bZKOutput  1. Non-normative notes:  As stated in section5.4.8.3 on page 97, an implementation of HomomorphicPedersenCommitSapling MAY resample the commitment trapdoor until the resulting commitment is not \ud835\udcaaJ. The rule that cv and epk MUST not be small-order has the effect of also preventing non-canonical encodings of these \ufb01elds, as required by ZIP-216. That is, it is necessarily the case that reprJ abstJ(cv)  cv and reprJ abstJ(epk)  rk. Action Descriptions An Action transfer, as speci\ufb01ed in section3.7 Action Transfers and their Descriptions on page 20, is encoded in trans- actions as an Action description.",
      "Each version 5 transaction includes a sequence of zero or more Action descriptions. (Version 4 transactions cannot contain Action descriptions.) Each Action description is authorized by a signature, called the spend authorization signature. Let \u2113Orchard Merkle be as de\ufb01ned in section5.3 Constants on page 74. Let \ud835\udc5eP be as de\ufb01ned in section5.4.9.6 Pallas and Vesta on page 105. Let ExtractP be as de\ufb01ned in section5.4.9.7 Coordinate Extractor for Pallas on page 106. Let ValueCommitOrchard.Output be as de\ufb01ned in section4.1.8 Commitment on page 31. Let SpendAuthSigOrchard be as de\ufb01ned in section4.15 Spend Authorization Signature (Sapling and Orchard) on page 56. Let KAOrchard be as de\ufb01ned in section4.1.5 Key Agreement on page 26. Let Sym be as de\ufb01ned in section4.1.4 Symmetric Encryption on page 26. Let ZKAction be as de\ufb01ned in section4.1.13 Zero-Knowledge Proving System on page 34.",
      "An Action description comprises (cvnet, rtOrchard, nf, rk, spendAuthSig, cm\ud835\udc65, epk, Cenc, Cout, enableSpends, enableOutputs, \ud835\udf0b) where  cvnet ValueCommitOrchard.Output is the value commitment to the value of the input note minus the value of the output note;  rtOrchard 0 .. \ud835\udc5eP 1 is an anchor, as de\ufb01ned in section3.4 Transactions and Treestates on page 18, for the output treestate of a previous block;  nf 0 .. \ud835\udc5eP 1 is the nulli\ufb01er for the input note;  rk SpendAuthSigOrchard.Public is a randomized validating key that should be used to validate spendAuthSig;  spendAuthSig SpendAuthSigOrchard.Signature is a spend authorization signature, validated as speci\ufb01ed in section4.15 Spend Authorization Signature (Sapling and Orchard) on page 56;  cm\ud835\udc65 0 ..",
      "\ud835\udc5eP 1 is the result of applying ExtractP to the note commitment for the output note;  epk KAOrchard.Public is a key agreement public key, used to derive the key for encryption of the transmitted note ciphertext (section4.20 In-band secret distribution (Sapling and Orchard) on page 67);  Cenc Sym.C is a ciphertext component for the encrypted output note;  Cout Sym.C is a ciphertext component that allows the holder of the outgoing cipher key (which can be derived from a full viewing key) to recover the recipient diversi\ufb01ed transmission key pkd and the ephemeral private key esk, hence the entire note plaintext;  enableSpends B is a \ufb02ag that is set in order to enable non-zero-valued spends in this Action;  enableOutputs B is a \ufb02ag that is set in order to enable non-zero-valued outputs in this Action; ZKAction.Proof is a zk-SNARK proof with primaryinput (cv, rtOrchard, nf, rk, cm\ud835\udc65, enableSpends, enableOutputs) for the Action statement de\ufb01ned in section4.18.4 Action Statement (Orchard) on page 63.",
      "Note: The rtOrchard, enableSpends, and enableOutputs components are the same for all Action transfers in a trans- action. They are encoded once in the transaction body (see section7.1 Transaction Encoding and Consensus on page 122), not in the ActionDescription structure. \ud835\udf0bis aggregated with other Action proofs and encoded in the proofsOrchard \ufb01eld of a transaction. Consensus rules:  Elements of an Action description MUST be canonical encodings of the types given above. Let SigHash be the SIGHASH transaction hash of this transaction, not associated with an input, as de\ufb01ned in section4.10 SIGHASH Transaction Hashing on page 50 using SIGHASH_ALL. The spend authorization signature MUST be a valid SpendAuthSigOrchard signature over SigHash using rk as the validating key i.e. SpendAuthSigOrchard.Validaterk(SigHash, spendAuthSig)  1. As speci\ufb01ed in section5.4.7 RedDSA, RedJubjub, and RedPallas on page 92, validation of the \ud835\udc45component of the signature prohibits non- canonical encodings.",
      "The proof \ud835\udf0bMUST be valid given a primary input (cv, rtOrchard, nf, rk, cm\ud835\udc65, enableSpends, enableOutputs)  i.e. ZKAction.Verify (cv, rtOrchard, nf, rk, cm\ud835\udc65, enableSpends, enableOutputs), \ud835\udf0b  1. Non-normative notes:  cv and rk can be the zero point \ud835\udcaaP. epk cannot be \ud835\udcaaP. nf and cm\ud835\udc65are not checked to be valid af\ufb01ne-short-Weierstrass \ud835\udc65-coordinates on the Pallas curve; they are only checked to encode integers in 0 .. \ud835\udc5eP 1. Sending Notes 4.7.1 Sending Notes (Sprout) In order to send Sprout shielded value, the sender constructs a transaction containing one or more JoinSplit descriptions. Let JoinSplitSig be as speci\ufb01ed in section4.1.7 Signature on page 28. Let NoteCommitSprout be as speci\ufb01ed in section4.1.8 Commitment on page 31. Let \u2113Seed and \u2113Sprout be as speci\ufb01ed in section5.3 Constants on page 74.",
      "Sending a transaction containing JoinSplit descriptions involves \ufb01rst generating a new JoinSplitSig key pair: joinSplitPrivKey  R JoinSplitSig.GenPrivate() joinSplitPubKey : JoinSplitSig.DerivePublic(joinSplitPrivKey). For each JoinSplit description, the sender chooses randomSeed uniformly at random on B\u2113Seed, and selects the input notes. At this point there is suf\ufb01cient information to compute hSig, as described in the previous section. The sender also chooses \u03d5 uniformly at random on B\u2113Sprout . Then it creates each output note with index \ud835\udc56 1..Nnew:  Choose uniformly random rcm\ud835\udc56 R NoteCommitSprout.GenTrapdoor(). Compute \u03c1\ud835\udc56 PRF\u03c1 \u03d5(\ud835\udc56, hSig). Compute cm\ud835\udc56 NoteCommitSprout rcm\ud835\udc56(apk,\ud835\udc56, v\ud835\udc56, \u03c1\ud835\udc56). Let np\ud835\udc56 (0x00, v\ud835\udc56, \u03c1\ud835\udc56, rcm\ud835\udc56, memo\ud835\udc56). np1..Nnew are then encrypted to the recipient transmission keys pkenc,1..Nnew, giving the transmitted notes ciphertext (epk, Cenc 1..Nnew), as described in section4.19 In-band secret distribution (Sprout) on page 65.",
      "In order to minimize information leakage, the sender SHOULD randomize the order of the input notes and of the output notes. Other considerations relating to information leakage from the structure of transactions are beyond the scope of this speci\ufb01cation. After generating all of the JoinSplit descriptions, the sender obtains dataToBeSigned BYN as described in section4.11 Non-malleability (Sprout) on page 51, and signs it with the private JoinSplit signing key: joinSplitSig  R JoinSplitSig.SignjoinSplitPrivKey(dataToBeSigned) Then the encoded transaction including joinSplitSig is submitted to the peer-to-peer network. Canopy onward Note: ZIP-211 speci\ufb01es that nodes and wallets MUST disable any facilities to send to Sprout addresses. This SHOULD be made clear in user interfaces and API documentation. The facility to send to Sprout addresses is in any case OPTIONAL for a particular node or wallet implementation.",
      "4.7.2 Sending Notes (Sapling) In order to send Sapling shielded value, the sender constructs a transaction with one or more Output descriptions. Let ValueCommitSapling and NoteCommitSapling be as speci\ufb01ed in section4.1.8 Commitment on page 31. Let KASapling be as speci\ufb01ed in section4.1.5 Key Agreement on page 26. Let DiversifyHashSapling be as speci\ufb01ed in section4.1.1 Hash Functions on page 24. Let ToScalarSapling be as speci\ufb01ed in section4.2.2 Sapling Key Components on page 36. Let reprJ and \ud835\udc5fJ be as de\ufb01ned in section5.4.9.3 Jubjub on page 102. Let ovk be a Sapling outgoing viewing key that is intended to be able to decrypt this payment. This may be one of:  the outgoing viewing key for the address (or one of the addresses) from which the payment was sent;  the outgoing viewing key for all payments associated with an account, to be de\ufb01ned in ZIP-32;  , if the sender should not be able to decrypt the payment once it has deleted its own copy.",
      "Note: Choosing ovk  is useful if the sender prefers to obtain forward secrecy of the payment information with respect to compromise of its own secrets. Let leadByte be the note plaintext lead byte, chosen according to section3.2.1 Note Plaintexts and Memo Fields on page 15 with protocol  Sapling. For each Output description, the sender selects a value v 0 .. MAX_MONEY and a destination Sapling shielded payment address (d, pkd), and then performs the following steps: Check that pkd is of type KASapling.PublicPrimeOrder, i.e. it MUST be a valid ctEdwards curve point on the Jubjub curve (as de\ufb01ned in section5.4.9.3 Jubjub on page 102), \ud835\udc5fJ pkd  \ud835\udcaaJ, and pkd  \ud835\udcaaJ. Calculate gd  DiversifyHashSapling(d) and check that gd  . Choose a uniformly random commitment trapdoor rcv  R ValueCommitSapling.GenTrapdoor(). If leadByte  0x01: Choose a uniformly random ephemeral private key esk  R KASapling.Private 0. Choose a uniformly random commitment trapdoor rcm  R NoteCommit.GenTrapdoor().",
      "Set rseed : I2LEOSP256(rcm). else: Choose uniformly random rseed  R BY32. Derive rcm  ToScalarSapling( PRFexpand rseed (4) Derive esk  ToScalarSapling( PRFexpand rseed (5) Let cv  ValueCommitSapling (v). Let cm  NoteCommitSapling (reprJ(gd), reprJ(pkd), v). Let np  (leadByte, d, v, rseed, memo). Encrypt np to the recipient diversi\ufb01ed transmission key pkd with diversi\ufb01ed base gd, and to the outgoing viewing key ovk, giving the transmitted note ciphertext (epk, Cenc, Cout). This procedure is described in section4.20.1 Encryption (Sapling and Orchard) on page 67; it also uses cv and cmu to derive ock, and takes esk as input. Generate a proof \ud835\udf0bZKOutput for the Output statement in section4.18.3 Output Statement (Sapling) on page 62. Return (cv, cm, epk, Cenc, Cout, \ud835\udf0bZKOutput). In order to minimize information leakage, the sender SHOULD randomize the order of Output descriptions in a transaction.",
      "Other considerations relating to information leakage from the structure of transactions are beyond the scope of this speci\ufb01cation. The encoded transaction is submitted to the peer-to-peer network. 4.7.3 Sending Notes (Orchard) In order to send Orchard shielded value, the sender constructs a transaction with one or more Action descriptions. This section describes how to produce the output-related \ufb01elds of an Action description. Let ValueCommitOrchard and NoteCommitOrchard be as speci\ufb01ed in section4.1.8 Commitment on page 31. Let PRFexpand be as speci\ufb01ed in section4.1.2 Pseudo Random Functions on page 25. Let KAOrchard be as speci\ufb01ed in section4.1.5 Key Agreement on page 26. Let DiversifyHashOrchard be as speci\ufb01ed in section4.1.1 Hash Functions on page 24. Let ToScalarOrchard and ToBaseOrchard be as speci\ufb01ed in section4.2.3 Orchard Key Components on page 38. Let reprP, \ud835\udc5fP, and the Pallas curve be as de\ufb01ned in section5.4.9.6 Pallas and Vesta on page 105.",
      "Let Extract P be as de\ufb01ned in section5.4.9.7 Coordinate Extractor for Pallas on page 106. Let I2LEOSP be as de\ufb01ned in section5.1 Integers, Bit Sequences, and Endianness on page 73. Let ovk be an Orchard outgoing viewing key that is intended to be able to decrypt this payment. The considerations for choosing outgoing viewing keys are as described for Sapling in section4.7.2 Sending Notes (Sapling) on page 44. Let leadByte be the note plaintext lead byte, chosen according to section3.2.1 Note Plaintexts and Memo Fields on page 15 with protocol  Orchard. For each Action description, the sender selects a value v 0 .. MAX_MONEY and a destination Orchard shielded payment address (d, pkd), and performs the following steps: Check that pkd is of type KAOrchard.PublicPrimeOrder. Calculate gd  DiversifyHashOrchard(d). Choose a uniformly random commitment trapdoor rcv  R ValueCommitOrchard.GenTrapdoor(). Choose uniformly random rseed  R BY32.",
      "Let \u03c1  nfold from the same Action description, and let \u03c1  I2LEOSP256(\u03c1). Derive esk  ToScalarOrchard( PRFexpand rseed (4  \u03c1) If esk  0 (mod \ud835\udc5fP), repeat the above steps using a different rseed. Derive rcm  ToScalarOrchard( PRFexpand rseed (5  \u03c1) Derive \u03c8  ToBaseOrchard( PRFexpand rseed (9  \u03c1) Let cvnet be the value commitment to the value of the input note minus the value v of the output note for this Action transfer, using rcv, as described in section4.14 Balance and Binding Signature (Orchard) on page 54. Let cm\ud835\udc65 Extract NoteCommitOrchard (reprP(gd), reprP(pkd), v, \u03c1, \u03c8) If cm\ud835\udc65 , repeat the above steps using a different rseed. Let np  (leadByte, d, v, rseed, memo). Encrypt np to the recipient diversi\ufb01ed transmission key pkd with diversi\ufb01ed base gd, and to the outgoing viewing key ovk, giving the transmitted note ciphertext (epk, Cenc, Cout).",
      "This procedure is described in section4.20.1 Encryption (Sapling and Orchard) on page 67; it uses cvnet and cm\ud835\udc65to derive ock, and takes esk as input. Fill in the spending side of the Action transfer (section4.15 Spend Authorization Signature (Sapling and Orchard) on page 56), and generate a proof \ud835\udf0bfor the Action statement in section4.18.4 Action Statement (Orchard) on page 63. Return (cv, cm\ud835\udc65, epk, Cenc, Cout, \ud835\udf0b). If no real Orchard note is being spent in the same Action transfer, the sender SHOULD create a dummy note to spend as described in section4.8.3 Dummy Notes (Orchard) on page 48, and use that dummy notes nulli\ufb01er as the \u03c1 value. In order to minimize information leakage, the sender SHOULD randomize the order of Action descriptions in a transaction. Other considerations relating to information leakage from the structure of transactions are beyond the scope of this speci\ufb01cation. The encoded transaction is submitted to the peer-to-peer network.",
      "Note: The domain separators 4 and 5 used in the input to PRFexpand rseed are swapped for Orchard relative to Sapling. This was due to an oversight and there is no good reason for it. Dummy Notes 4.8.1 Dummy Notes (Sprout) The \ufb01elds in a JoinSplit description allow for Nold input notes, and Nnew output notes. In practice, we may wish to encode a JoinSplit transfer with fewer input or output notes. This is achieved using dummy notes. Let \u2113ask and \u2113Sprout be as de\ufb01ned in section5.3 Constants on page 74. Let PRFnfSprout be as de\ufb01ned in section4.1.2 Pseudo Random Functions on page 25. Let NoteCommitSprout be as de\ufb01ned in section4.1.8 Commitment on page 31. A dummy Sprout input note, with index \ud835\udc56in the JoinSplit description, is constructed as follows:  Generate a new uniformly random spending key aold sk,\ud835\udc56 R B\u2113ask and derive its paying key aold pk,\ud835\udc56. Set vold  0. Choose uniformly random \u03c1old R B\u2113Sprout PRF  and rcmold R NoteCommitSprout.GenTrapdoor().",
      "Compute nfold  PRFnfSprout aold sk,\ud835\udc56 (\u03c1old  Let path\ud835\udc56be a dummy Merkle path for the auxiliary input to the JoinSplit statement (this will not be checked). When generating the JoinSplit proof, set enforceMerklePath\ud835\udc56to 0. A dummy Sprout output note is constructed as normal but with zero value, and sent to a random shielded payment address. 4.8.2 Dummy Notes (Sapling) In Sapling there is no need to use dummy notes simply in order to \ufb01ll otherwise unused inputs as in the case of a JoinSplit description; nevertheless it may be useful for privacy to obscure the number of real shielded inputs from Sapling notes. Let \u2113sk be as de\ufb01ned in section5.3 Constants on page 74. Let ValueCommitSapling and NoteCommitSapling be as de\ufb01ned in section4.1.8 Commitment on page 31. Let DiversifyHashSapling be as speci\ufb01ed in section4.1.1 Hash Functions on page 24. Let ToScalarSapling be as speci\ufb01ed in section4.2.2 Sapling Key Components on page 36.",
      "Let reprJ and \ud835\udc5fJ be as de\ufb01ned in section5.4.9.3 Jubjub on page 102. Let PRFnfSapling be as de\ufb01ned in section4.1.2 Pseudo Random Functions on page 25. Let NoteCommitSapling be as de\ufb01ned in section4.1.8 Commitment on page 31. A Spend description for a dummy Sapling input note with note plaintext lead byte 0x02 is constructed as follows:  Choose uniformly random sk  R B\u2113sk. Generate the ak and nk components of a full viewing key and a diversi\ufb01ed payment address (d, pkd) for sk, as described in section4.2.2 Sapling Key Components on page 36. Let v  0 and pos  0. Choose uniformly random rcv  R ValueCommitSapling.GenTrapdoor(). Choose uniformly random rseed  R BY32. Derive rcm  ToScalarSapling( PRFexpand rseed (4)  Let cv  ValueCommitSapling (v). Let cm  NoteCommitSapling reprJ(gd), reprJ(pkd), v  Let \u03c1 reprJ MixingPedersenHash(cm, pos)  Let nk reprJ(nk). Let nf  PRFnfSapling (\u03c1).",
      "Construct a dummy Merkle path path for use in the auxiliary input to the Spend statement (this will not be checked, because v  0). As in Sprout, a dummy Sapling output note is constructed as normal but with zero value, and sent to a random shielded payment address. 4.8.3 Dummy Notes (Orchard) As for Sapling, it may be useful for privacy to obscure the number of real shielded inputs from Orchard notes. Let \u2113sk be as de\ufb01ned in section5.3 Constants on page 74. Let ValueCommitOrchard and NoteCommitOrchard be as de\ufb01ned in section4.1.8 Commitment on page 31. Let DiversifyHashOrchard be as speci\ufb01ed in section4.1.1 Hash Functions on page 24. Let ToScalarOrchard and ToBaseOrchard be as speci\ufb01ed in section4.2.3 Orchard Key Components on page 38. Let reprP and \ud835\udc5fP be as de\ufb01ned in section5.4.9.6 Pallas and Vesta on page 105. Let DeriveNullifier be as de\ufb01ned in section4.16 Computing \u03c1 values and Nulli\ufb01ers on page 57. Let NoteCommitOrchard be as de\ufb01ned in section4.1.8 Commitment on page 31.",
      "Let I2LEOSP be as de\ufb01ned in section5.1 Integers, Bit Sequences, and Endianness on page 73. Let leadByte be the note plaintext lead byte, chosen according to section3.2.1 Note Plaintexts and Memo Fields on page 15 with protocol  Orchard. The spend-related \ufb01elds of an Action description for a dummy Orchard input note are constructed as follows:  Choose uniformly random sk  R B\u2113sk. Generate a full viewing key (ak, nk, rivk) and a diversi\ufb01ed payment address (d, pkd) for sk as described in section4.2.3 Orchard Key Components on page 38. Let v  0. Choose uniformly random rseed  R BY32. Choose uniformly random \u03c1P  R P. Let \u03c1  ExtractP(\u03c1P) and \u03c1  I2LEOSP256(\u03c1). Derive rcm  ToScalarOrchard( PRFexpand rseed (5  \u03c1)  Derive \u03c8  ToBaseOrchard( PRFexpand rseed (9  \u03c1)  Let cm  NoteCommitOrchard reprP(gd), reprP(pkd), v, \u03c1, \u03c8  If cm  , repeat the above steps using a different rseed. Let nf  DeriveNullifiernk(\u03c1, \u03c8, cm).",
      "Construct a dummy Merkle path path for use in the auxiliary input to the Action statement (this will not be checked, because v  0). As in Sprout and Sapling, a dummy Orchard output note is constructed as normal but with zero value, and sent to a random shielded payment address. Note: The domain separators 4 and 5 used in the input to PRFexpand rseed are swapped for Orchard relative to Sapling. This was due to an oversight and there is no good reason for it. Merkle Path Validity Let MerkleDepth be MerkleDepthSprout for the Sprout note commitment tree, or MerkleDepthSapling for the Sapling note commitment tree, or MerkleDepthOrchard for the Orchard note commitment tree. These constants are de\ufb01ned in section5.3 Constants on page 74. Similarly, let MerkleCRH be MerkleCRHSprout for Sprout, or MerkleCRHSapling for Sapling, or MerkleCRHOrchard for Or- chard. The following discussion applies independently to the Sprout and Sapling and Orchard note commitment trees.",
      "Each node in the incremental Merkle tree is associated with a hash value, which is a bit sequence. The layer numbered \u210e, counting from layer 0 at the root, has 2\u210enodes with indices 0 to 2\u210e1 inclusive. Let Mh \ud835\udc56be the hash value associated with the node at index \ud835\udc56in layer \u210e. The nodes at layer MerkleDepth are called leaf nodes. When a note commitment is added to the tree, it occupies the leaf node hash value MMerkleDepth for the next available \ud835\udc56. As-yet unused leaf nodes are associated with a distinguished hash value UncommittedSprout or UncommittedSapling or UncommittedOrchard. It is assumed to be infeasible to \ufb01nd a preimage note n such that NoteCommitmentSprout(n)  UncommittedSprout.",
      "(No similar assumption is needed for Sapling or Orchard because we use a representation for UncommittedSapling that cannot occur as an output of NoteCommitmentSapling, and similarly for Orchard.) The nodes at layers 0 to MerkleDepth 1 inclusive are called internal nodes, and are associated with MerkleCRH outputs. Internal nodes are computed from their children in the next layer as follows: for 0 \u210e MerkleDepth and 0 \ud835\udc56 2\u210e, \ud835\udc56: MerkleCRH(\u210e, Mh1 2\ud835\udc56, Mh1 2\ud835\udc561).",
      "A Merkle path from leaf node MMerkleDepth in the incremental Merkle tree is the sequence  Mh sibling(\u210e,\ud835\udc56) for \u210efrom MerkleDepth down to 1 , where sibling(\u210e, \ud835\udc56) : floor 2MerkleDepth\u210e Given such a Merkle path, it is possible to verify that leaf node MMerkleDepth is in a tree with a given root rt  M0 Notes:  For Sapling, Merkle hash values are speci\ufb01ed to be encoded as bit sequences, but the root rtSapling is encoded for the primary input of a Spend proof as an element of F\ud835\udc5eJ, as speci\ufb01ed in sectionA.4 The Sapling Spend circuit on page 217. The Spend circuit allows inputs to MerkleCRHSapling at each node to be non-canonically encoded, as speci\ufb01ed in sectionA.3.4 Merkle path check on page 213. For Orchard, Merkle hash values have type 0 .. \ud835\udc5eP 1 as de\ufb01ned in section5.4.9.7 Coordinate Extractor for Pallas on page 106. Similarly to Sapling, the Action circuit allows inputs to MerkleCRHOrchard at each node to be non- canonically encoded.",
      "The Action circuit is permitted to be implemented in such a way that the Merkle path validity check can pass if any hash value on the path, including the root, is 0. This can only happen if SinsemillaHash returned for that hash, because 0 is not the af\ufb01ne-short-Weierstrass \ud835\udc65-coordinate of any point on the Pallas curve (as shown in a note at section5.4.9.7 Coordinate Extractor for Pallas on page 106), and SinsemillaHashToPoint cannot return \ud835\udcaaP. Allowing the validity check to pass in that case models the fact that incomplete addition is used to implement Sinsemilla in the circuit. As proven in Theorem 5.4.4 on page 84, a output from SinsemillaHash yields a nontrivial discrete logarithm relation. Since we assume \ufb01nding such a relation to be infeasible, we can argue that it is safe to allow an adversary to create a proof that passes the Merkle validity check in such a case.",
      "4.10 SIGHASH Transaction Hashing Bitcoin and Zcash use signatures andor non-interactive proofs associated with transaction inputs to authorize spending. Because these signatures or proofs could otherwise be replayed in a different transaction, it is necessary to bind them to the transaction for which they are intended. This is done by hashing information about the transaction and (where applicable) the speci\ufb01c input, to give a SIGHASH transaction hash which is then used for the Spend authorization. The means of authorization differs between transparent inputs, inputs to Sprout JoinSplit transfers, and Sapling Spend transfers or Orchard Action transfers, but for a given transaction version the same SIGHASH transaction hash algorithm is used.",
      "In the case of Zcash, the BCTV14 and Groth16 and Halo 2 proving systems used are malleable, meaning that there is the potential for an adversary who does not know all of the auxiliary inputs to a proof, to malleate it in order to create a new proof involving related auxiliary inputs DSDCOPS2001. This can be understood as similar to a malleability attack on an encryption scheme, in which an adversary can malleate a ciphertext in order to create an encryption of a related plaintext, without knowing the original plaintext. Zcash has been designed to mitigate malleability attacks, as described in section4.11 Non-malleability (Sprout) on page 51, section4.13 Balance and Binding Signature (Sapling) on page 52, and section4.15 Spend Authorization Signature (Sapling and Orchard) on page 56. To provide additional \ufb02exibility when combining spend authorizations from different sources, Bitcoin de\ufb01nes sev- eral SIGHASH types that cover various parts of a transaction Bitcoin-SigHash.",
      "One of these types is SIGHASH_ALL, which is used for Zcash-speci\ufb01c signatures, i.e. JoinSplit signatures, spend authorization signatures, Sapling binding signatures, and Orchard binding signatures. In these cases the SIGHASH transaction hash is not associated with a transparent input, and so the input to hashing excludes all of the scriptSig \ufb01elds in the non-Zcash-speci\ufb01c parts of the transaction. In Zcash, all SIGHASH types are extended to cover the Zcash-speci\ufb01c \ufb01elds nJoinSplit, vJoinSplit, and if present joinSplitPubKey. These \ufb01elds are described in section7.1 Transaction Encoding and Consensus on page 122. The hash does not cover the \ufb01eld joinSplitSig. After Overwinter activation, all SIGHASH types are also extended to cover transaction \ufb01elds introduced in that upgrade, and similarly after Sapling activation and after NU55 activation.",
      "The original SIGHASH algorithm de\ufb01ned by Bitcoin suffered from some de\ufb01ciencies as described in ZIP-143; in Zcash these were addressed by changing this algorithm as part of the Overwinter upgrade. Orchard and the NU55 network upgrade introduce transaction version 5, which MUST be used if any Action transfers are present. This version also provides nonmalleable transaction identi\ufb01ers, and MAY be used for that reason whether or not Action transfers are present. Consensus rules:  NU55 onward Any SIGHASH type encoding used in a version 5 transaction MUST be the canonical encoding of one of the de\ufb01ned SIGHASH types, i.e. one of 0x01, 0x02, 0x03, 0x81, 0x82, or 0x83. (Previously, unde\ufb01ned bits of a SIGHASH type encoding were ignored.)  Pre-Overwinter  The SIGHASH algorithm used prior to Overwinter activation, i.e. for version 1 and 2 transactions, will be de\ufb01ned in ZIP-76 (to be written).",
      "Overwinter only, pre-Sapling The SIGHASH algorithm used after Overwinter activation and before Sapling activation, i.e. for version 3 transactions, is de\ufb01ned in ZIP-143. Overwinter only, pre-Sapling All transactions MUST use the Overwinter consensus branch ID 0x5BA81B19 as de\ufb01ned in ZIP-201. Sapling onward The SIGHASH algorithm used after Sapling activation, i.e. for version 4 transactions, is de\ufb01ned in ZIP-243. Sapling only, pre-Blossom All transactions MUST use the Sapling consensus branch ID 0x76B809BB as de\ufb01ned in ZIP-205. Blossom only, pre-Heartwood All transactions MUST use the Blossom consensus branch ID 0x2BB40E60 as de\ufb01ned in ZIP-206. Heartwood only, pre-Canopy All transactions MUST use the Heartwood consensus branch ID 0xF5B9230B as de\ufb01ned in ZIP-250. Canopy only, pre-NU55 All transactions MUST use the Canopy consensus branch ID 0xE9FF75A6 as de\ufb01ned in ZIP-251.",
      "NU55 onward The SIGHASH algorithm used for version 5 transactions introduced by the NU55 network upgrade is de\ufb01ned in ZIP-244. Version 4 transactions continue to use the SIGHASH algorithm de\ufb01ned in ZIP-243. NU55 only, pre-NU66 All transactions MUST use the NU55 consensus branch ID 0xF919A198 as de\ufb01ned in ZIP-252. NU66 only, pre-NU6.6.1 All transactions MUST use the NU66 consensus branch ID 0xC8E71055 as de\ufb01ned in ZIP-253. NU6.6.1 only All transactions MUST use the NU6.6.1 consensus branch ID 0x4DEC4DF0 as de\ufb01ned in ZIP-255. 4.11 Non-malleability (Sprout) Let dataToBeSigned be the hash of the transaction, not associated with an input, using the SIGHASH_ALL SIGHASH type.",
      "In order to ensure that a JoinSplit description is cryptographically bound to the transparent inputs and outputs corresponding to vnew pub and vold pub, and to the other JoinSplit descriptions in the same transaction, an ephemeral JoinSplitSig key pair is generated for each transaction, and the dataToBeSigned is signed with the private signing key of this key pair. The corresponding public validating key is included in the transaction encoding as joinSplitPubKey. JoinSplitSig is instantiated in section5.4.6 Ed25519 on page 90. If nJoinSplit is zero, the joinSplitPubKey and joinSplitSig \ufb01elds are omitted. Otherwise, a transaction has a correct JoinSplit signature if and only if JoinSplitSig.ValidatejoinSplitPubKey(dataToBeSigned, joinSplitSig)  1. Let hSig be computed as speci\ufb01ed in section4.3 JoinSplit Descriptions on page 39. Let PRFpk be as de\ufb01ned in section4.1.2 Pseudo Random Functions on page 25.",
      "For each \ud835\udc561..Nold, the creator of a JoinSplit description calculates h\ud835\udc56 PRFpk aold sk,\ud835\udc56(\ud835\udc56, hSig). The correctness of h1..Nold is enforced by the JoinSplit statement given in section4.18.1 JoinSplit Statement (Sprout) on page 60. This ensures that a holder of all of the aold sk,1..Nold for every JoinSplit description in the transaction has authorized the use of the private signing key corresponding to joinSplitPubKey to sign this transaction. 4.12 Balance (Sprout) In Bitcoin, all inputs to and outputs from a transaction are transparent. The total value of transparent outputs must not exceed the total value of transparent inputs. The net value of transparent inputs minus transparent outputs is transferred to the miner of the block containing the transaction; it is added to the miner subsidy in the coinbase transaction of the block. Zcash Sprout extends this by adding JoinSplit transfers.",
      "Each JoinSplit transfer can be seen, from the perspective of the transparent transaction value pool, as an input and an output simultaneously. vold pub takes value from the transparent transaction value pool and vnew pub adds value to the transparent transaction value pool. As a result, vold pub is treated like an output value, whereas vnew pub is treated like an input value. Unlike original Zerocash BCGGMTV2014, Zcash does not have a distinction between Mint and Pour operations. The addition of vold pub to a JoinSplit description subsumes the functionality of both Mint and Pour. Also, a difference in the number of real input notes does not by itself cause two JoinSplit descriptions to be distinguishable. As stated in section4.3 JoinSplit Descriptions on page 39, either vold pub or vnew pub MUST be zero.",
      "No generality is lost because, if a transaction in which both vold pub and vnew pub were nonzero were allowed, it could be replaced by an equivalent one in which min(vold pub, vnew pub) is subtracted from both of these values. This restriction helps to avoid unnecessary distinctions between transactions according to client implementation. 4.13 Balance and Binding Signature (Sapling) Sapling adds Spend transfers and Output transfers to the transparent and JoinSplit transfers present in Sprout. The net value of Spend transfers minus Output transfers in a transaction is called the Sapling balancing value, measured in zatoshi as a signed integer vbalanceSapling. vbalanceSapling is encoded in a transaction as the \ufb01eld valueBalanceSapling. For a v4 transaction, vbalanceSapling is always explicitly encoded. For a v5 transaction, vbalanceSapling is implicitly zero if the transaction has no Spend descriptions or Output descriptions.",
      "Transaction \ufb01elds are described in section7.1 Transaction Encoding and Consensus on page 122. A positive Sapling balancing value takes value from the Sapling transaction value pool and adds it to the transparent transaction value pool. A negative Sapling balancing value does the reverse. As a result, positive vbalanceSapling is treated like an input to the transparent transaction value pool, whereas negative vbalanceSapling is treated like an output from that pool. Consistency of vbalanceSapling with the value commitments in Spend descriptions and Output descriptions is enforced by the Sapling binding signature.",
      "This signature has a dual r\u00f4le in the Sapling protocol:  To prove that the total value spent by Spend transfers, minus that produced by Output transfers, is consistent with the vbalanceSapling \ufb01eld of the transaction;  To prove that the signer knew the randomness used for the Spend and Output value commitments, in order to prevent Output descriptions from being replayed by an adversary in a different transaction. (A Spend description already cannot be replayed due to its spend authorization signature.) Instead of generating a key pair at random, we generate it as a function of the value commitments in the Spend descriptions and Output descriptions of the transaction, and the Sapling balancing value. Let J(\ud835\udc5f), J(\ud835\udc5f), and \ud835\udc5fJ be as de\ufb01ned in section5.4.9.3 Jubjub on page 102. section5.4.8.3 Homomorphic Pedersen commitments (Sapling and Orchard) on page 97 instantiates: ValueCommitSapling  ValueCommitSapling.Trapdoor  \ud835\udc5fJ1 ..",
      "\ud835\udc5fJ1 ValueCommitSapling.Output; \ud835\udcb1Sapling  J(\ud835\udc5f), the value base in ValueCommitSapling; \u211bSapling  J(\ud835\udc5f), the randomness base in ValueCommitSapling. BindingSigSapling, , and are instantiated in section5.4.7.2 Binding Signature (Sapling and Orchard) on page 95. section4.1.7.2 Signature with Signing Key to Validating Key Monomorphism on page 30 speci\ufb01es these operations and the derived notation \ud835\udc561, , and \ud835\udc561, which in this section are to be interpreted as operating on the prime-order subgroup of points on the Jubjub curve and on its scalar \ufb01eld. Suppose that the transaction has:  \ud835\udc5bSpend descriptions with value commitments cvold 1..\ud835\udc5b, committing to values vold 1..\ud835\udc5bwith randomness rcvold 1..\ud835\udc5b;  \ud835\udc5aOutput descriptions with value commitments cvnew 1..\ud835\udc5a, committing to values vnew 1..\ud835\udc5awith randomness rcvnew 1..\ud835\udc5a;  Sapling balancing value vbalanceSapling.",
      "In a correctly constructed transaction, vbalanceSapling  \ud835\udc5b \ud835\udc561vold \ud835\udc5a \ud835\udc571vnew , but validators cannot check this directly because the values are hidden by the commitments. Instead, validators calculate the transaction binding validating key as: bvkSapling : cvold cvnew ValueCommitSapling vbalanceSapling) (This key is not encoded explicitly in the transaction and must be recalculated.) The signer knows rcvold 1..\ud835\udc5band rcvnew 1..\ud835\udc5a, and so can calculate the corresponding signing key as: bskSapling : rcvold rcvnew In order to check for implementation faults, the signer SHOULD also check that bvkSapling  BindingSigSapling.DerivePublic(bskSapling). Let SigHash be the SIGHASH transaction hash as de\ufb01ned in ZIP-243 for a version 4 transaction or ZIP-244 for a version 5 transaction, not associated with an input, using the SIGHASH type SIGHASH_ALL. A validator checks balance by validating that BindingSigSapling.ValidatebvkSapling(SigHash, bindingSigSapling)  1. We now explain why this works.",
      "A Sapling binding signature proves knowledge of the discrete logarithm bskSapling of bvkSapling with respect to \u211bSapling. That is, bvkSapling  bskSapling \u211bSapling. So the value 0 and randomness bskSapling is an opening of the Pedersen commitment bvkSapling  ValueCommitSapling bskSapling(0). By the binding property of the Pedersen commitment, it is infeasible to \ufb01nd another opening of this commitment to a different value. Similarly, the binding property of the value commitments in the Spend descriptions and Output descriptions ensures that an adversary cannot \ufb01nd an opening to more than one value for any of those commitments, i.e. we may assume that vold 1..\ud835\udc5bare determined by cvold 1..\ud835\udc5b, and that vnew 1..\ud835\udc5aare determined by cvnew 1..\ud835\udc5a. We may also assume, from Knowledge Soundness of Groth16, that the Spend proofs could not have been generated without knowing rcvold 1..\ud835\udc5b (mod \ud835\udc5fJ), and the Output proofs could not have been generated without knowing rcvnew 1..\ud835\udc5a(mod \ud835\udc5fJ).",
      "Using the fact that ValueCommitSapling (v)  v \ud835\udcb1Sapling rcv \u211bSapling, the expression for bvkSapling above is equivalent bvkSapling  ( vold vnew vbalanceSapling \ud835\udcb1Sapling ( rcvold rcvnew ) \u211bSapling  ValueCommitSapling bskSapling vold vnew vbalanceSapling Let v  vold vnew vbalanceSapling. Suppose that v  vbad  0 (mod \ud835\udc5fJ). Then bvkSapling  ValueCommitSapling bskSapling(vbad). If the adversary were able to \ufb01nd the discrete logarithm of this bvkSapling with respect to \u211bSapling, say bsk (as needed to create a valid Sapling binding signature), then (vbad, bskSapling) and (0, bsk) would be distinct openings of bvkSapling to different values, breaking the binding property of the value commitment scheme. The preceding argument shows only that v  0 (mod \ud835\udc5fJ); in order to show that v  0, we will also demonstrate that it does not over\ufb02ow \ud835\udc5fJ1 .. \ud835\udc5fJ1 The Spend statements (section4.18.2 Spend Statement (Sapling) on page 61) prove that all of vold 1..\ud835\udc5bare in 0 .. 2\u2113value1.",
      "Sim- ilarly the Output statements (section4.18.3 Output Statement (Sapling) on page 62) prove all of vnew 1..\ud835\udc5aare in 0 .. 2\u2113value1. vbalanceSapling is encoded in the transaction as a signed twos complement 64-bit integer in the range 263 .. 263 1. \u2113value is de\ufb01ned as 64, so v is in the range \ud835\udc5a (264 1) 263  1 .. \ud835\udc5b (264 1)  263. The maximum transaction size is 2 MB, and the minimum contributions of a Spend description and an Output description to transaction size are (in a v5 transaction) 352 bytes and 948 bytes respectively, limiting \ud835\udc5bto at most floor (2000000  5681 and \ud835\udc5ato at most floor (2000000  2109. This ensures that v 38913406623490299131842 .. 104805176454780817500623, a subrange of \ud835\udc5fJ1 .. \ud835\udc5fJ1 Thus checking the Sapling binding signature ensures that the Spend transfers and Output transfers in the transaction balance, without their individual values being revealed.",
      "In addition this proves that the signer, knowing the -sum of the Sapling value commitment randomnesses, authorized a transaction with the given SIGHASH transaction hash by signing SigHash. Note: The spender MAY reveal any strict subset of the Sapling value commitment randomnesses to other parties that are cooperating to create the transaction. If all of the value commitment randomnesses are revealed, that could allow replaying the Output descriptions of the transaction. Non-normative note: The technique of checking signatures using a validating key derived from a sum of Pedersen commitments is also used in the Mimblewimble protocol Jedusor2016. The private key bskSapling acts as a synthetic blinding factor, in the sense that it is synthesized from the other blinding factors (trapdoors) rcvold 1..\ud835\udc5band rcvnew 1..\ud835\udc5a; this technique is also used in Bulletproofs Dalek-notes.",
      "4.14 Balance and Binding Signature (Orchard) Orchard introduces Action transfers, each of which can optionally perform a spend, and optionally perform an output. Similarly to Sapling, the net value of Orchard spends minus outputs in a transaction is called the Orchard balancing value, measured in zatoshi as a signed integer vbalanceOrchard. vbalanceOrchard is encoded in a transaction as the \ufb01eld valueBalanceOrchard. If a transaction has no Action descriptions, vbalanceOrchard is implicitly zero. Transaction \ufb01elds are described in section7.1 Transaction Encoding and Consensus on page 122. Apositive Orchard balancing value takes value from the Orchard transaction value pool and adds it to the transparent transaction value pool. A negative Orchard balancing value does the reverse. As a result, positive vbalanceOrchard is treated like an input to the transparent transaction value pool, whereas negative vbalanceOrchard is treated like an output from that pool.",
      "Consistency of vbalanceOrchard with the value commitments in Action descriptions is enforced by the Orchard binding signature. The r\u00f4le of this signature in the Orchard protocol is to prove that the net value spent (i.e. the total value spent minus the total value produced) by Action transfers is consistent with the vbalanceOrchard \ufb01eld of the transaction. Non-normative note: The other r\u00f4le of Sapling binding signatures, to prove that the signer knew the randomness used for commitments in order to prevent them from being replayed, is less important in Orchard because all Action descriptions have a spend authorization signature. Still, an Orchard binding signature does prove that the signer knew this commitment randomness; this provides defence in depth and reduces the differences of Orchard from Sapling, which may simplify security analysis.",
      "Instead of generating a key pair at random, we generate it as a function of the value commitments in the Action descriptions of the transaction, and the Orchard balancing value. Let P, P, and \ud835\udc5fP be as de\ufb01ned in section5.4.9.6 Pallas and Vesta on page 105. section5.4.8.3 Homomorphic Pedersen commitments (Sapling and Orchard) on page 97 instantiates: ValueCommitOrchard  ValueCommitOrchard.Trapdoor  \ud835\udc5fP1 .. \ud835\udc5fP1 ValueCommitOrchard.Output; \ud835\udcb1Orchard  P, the value base in ValueCommitOrchard; \u211bOrchard  P, the randomness base in ValueCommitOrchard. BindingSigOrchard, , and are instantiated in section5.4.7.2 Binding Signature (Sapling and Orchard) on page 95. section4.1.7.2 Signature with Signing Key to Validating Key Monomorphism on page 30 speci\ufb01es these operations and the derived notation \ud835\udc561, , and \ud835\udc561, which in this section are to be interpreted as operating on the Pallas curve and its scalar \ufb01eld.",
      "Suppose that the transaction has:  \ud835\udc5bAction descriptions with value commitments cvnet 1..\ud835\udc5b, committing to values vnet 1..\ud835\udc5bwith randomness rcvnet 1..\ud835\udc5b;  Orchard balancing value vbalanceOrchard. In a correctly constructed transaction, vbalanceOrchard  \ud835\udc5b \ud835\udc561vnet \ud835\udc56, but validators cannot check this directly because the values are hidden by the commitments. Instead, validators calculate the transaction binding validating key as: bvkOrchard : cvnet ValueCommitOrchard vbalanceOrchard) (This key is not encoded explicitly in the transaction and must be recalculated.) The signer knows rcvnet 1..\ud835\udc5b, and so can calculate the corresponding signing key as: bskOrchard : rcvnet In order to check for implementation faults, the signer SHOULD also check that bvkOrchard  BindingSigOrchard.DerivePublic(bskOrchard). A transaction containing Action descriptions is necessarily a version 5 transaction.",
      "Let SigHash be the SIGHASH transaction hash for a version 5 transaction as de\ufb01ned in ZIP-244 as modi\ufb01ed by ZIP-225, not associated with an input, using the SIGHASH type SIGHASH_ALL. A validator checks balance by validating that BindingSigOrchard.ValidatebvkOrchard(SigHash, bindingSigOrchard)  1. The security argument is very similar to that for Sapling binding signatures, but for completeness we spell it out, since there are minor differences due to the net value commitments, and a different bound on the net value sum v. An Orchard binding signature proves knowledge of the discrete logarithm bskOrchard of bvkOrchard with respect to \u211bOrchard. That is, bvkOrchard  bskOrchard \u211bOrchard. So the value 0 and randomness bskOrchard is an opening of the Pedersen commitment bvkOrchard  ValueCommitOrchard bskOrchard(0). By the binding property of the Pedersen commitment, it is infeasible to \ufb01nd another opening of this commitment to a different value.",
      "Similarly, the binding property of the value commitments in the Action descriptions ensures that an adversary cannot \ufb01nd an opening to more than one value for any of those commitments, i.e. we may assume that vnet 1..\ud835\udc5bare determined by cvnet 1..\ud835\udc5b. We may also assume, from Knowledge Soundness of Halo 2, that the Action proofs could not have been generated without knowing rcvnet 1..\ud835\udc5b(mod \ud835\udc5fP). Using the fact ValueCommitOrchard (v)  v \ud835\udcb1Orchard rcv \u211bOrchard, the expression for bvkOrchard above is equivalent bvkOrchard  ( vnet vbalanceOrchard \ud835\udcb1Orchard rcvnet \u211bOrchard  ValueCommitOrchard bskOrchard vnet vbalanceOrchard Let v  vnet vbalanceOrchard. Suppose that v  vbad  0 (mod \ud835\udc5fJ). Then bvkOrchard  ValueCommitOrchard bskOrchard(vbad).",
      "If the adversary were able to \ufb01nd the discrete logarithm of this bvkOrchard with respect to \u211bOrchard, say bsk (as needed to create a valid Orchard binding signature), then (vbad, bskOrchard) and (0, bsk) would be distinct openings of bvkOrchard to different values, breaking the binding property of the value commitment scheme. The preceding argument shows only that v  0 (mod \ud835\udc5fP); in order to show that v  0, we will also demonstrate that it does not over\ufb02ow \ud835\udc5fP1 .. \ud835\udc5fP1 The Action statements (section4.18.4 Action Statement (Orchard) on page 63) prove that all vnet 1..\ud835\udc5bare in 264  1 .. 264 1. vbalanceOrchard is encoded in the transaction as a signed twos complement 64-bit integer in the range 263 .. 263 1. Therefore, v is in the range \ud835\udc5b (264 1) 263  1 .. \ud835\udc5b (264 1)  263. \ud835\udc5bis limited by consensus rule to at most 216 1 (this rule is technically redundant due to the 2 MB transaction size limit, but it suf\ufb01ces here). This ensures that v 1208916596242592319864832 ..",
      "1208916596242592319864833, a subrange of \ud835\udc5fP1 .. \ud835\udc5fP1 Thus checking the Orchard binding signature ensures that the Action transfers in the transaction balance, without their individual net values being revealed. In addition this proves that the signer, knowing the -sum of the Orchard value commitment randomnesses, authorized a transaction with the given SIGHASH transaction hash by signing SigHash. Note: The spender MAY reveal any strict subset of the Orchard value commitment randomnesses to other parties that are cooperating to create the transaction. 4.15 Spend Authorization Signature (Sapling and Orchard) SpendAuthSig is used in Sapling and Orchard to prove knowledge of the spending key authorizing spending of an input note. It is instantiated in section5.4.7.1 Spend Authorization Signature (Sapling and Orchard) on page 95. We use SpendAuthSigSapling to refer to the spend authorization signature scheme for Sapling, which is instantiated on the Jubjub curve.",
      "We use SpendAuthSigOrchard to refer to the spend authorization signature scheme for Orchard, which is instantiated on the Pallas curve. The following discussion applies to both. Knowledge of the spending key could have been proven directly in the Spend statement or Action statement, similar to the check in section4.18.1 JoinSplit Statement (Sprout) on page 60 that is part of the JoinSplit statement. The motivation for a separate signature is to allow devices that are limited in memory and computational capacity, such as hardware wallets, to authorize a Sapling or Orchard Spend. Typically such devices cannot create, and may not be able to verify, zk-SNARK proofs for a statement of the size needed using the Groth16 or Halo 2 proving systems. The validating key of the signature must be revealed in the Spend description so that the signature can be checked by validators.",
      "To ensure that the validating key cannot be linked to the shielded payment address or spending key from which the note was spent, we use a signature scheme with re-randomizable keys. The Spend statement or Action statement proves that this validating key is a re-randomization of the spend authorization address key ak with a randomizer known to the signer. The spend authorization signature is over the SIGHASH transaction hash, so that it cannot be replayed in other transactions. Let SigHash be the SIGHASH transaction hash as de\ufb01ned in ZIP-243 or as de\ufb01ned in ZIP-244 modi\ufb01ed by ZIP-225, not associated with an input, using the SIGHASH type SIGHASH_ALL. Let ask be the spend authorization private key as de\ufb01ned in section4.2.2 Sapling Key Components on page 36 or in section4.2.3 Orchard Key Components on page 38. Let SpendAuthSig be SpendAuthSigSapling or SpendAuthSigOrchard as applicable.",
      "For each Spend description or Action description, the signer chooses a fresh spend authorization randomizer \ud835\udefc: 1. Choose \ud835\udefc R SpendAuthSig.GenRandom(). 2. Let rsk  SpendAuthSig.RandomizePrivate(\ud835\udefc, ask). 3. Let rk  SpendAuthSig.DerivePublic(rsk). 4. Generate a proof \ud835\udf0bof the Spend statement (section4.18.2 Spend Statement (Sapling) on page 61) or Action statement (section4.18.4 Action Statement (Orchard) on page 63), with \ud835\udefcin the auxiliary input and rk in the primary input. 5. Let spendAuthSig  SpendAuthSig.Signrsk(SigHash). The resulting spendAuthSig and \ud835\udf0bare included in the Spend description, or in the vSpendAuthSigsSapling or vSpendAuthSigsOrchard \ufb01eld of a version 5 transaction. Note: If the spender is computationally or memory-limited, step 4 (and only step 4) MAY be delegated to a different party that is capable of performing the zk-SNARK proof .",
      "In this case privacy will be lost to that party since it needs ak and the proof authorizing key nsk; this allows also deriving the nk component of the full viewing key. (In Orchard, that party needs the nk directly to make the zk-SNARK proof .) Together ak and nk are suf\ufb01cient to recognize spent notes and to recognize and decrypt incoming notes. However, the other party will not obtain spending authority for other transactions, since it is not able to create a spend authorization signature by itself. 4.16 Computing \u03c1 values and Nulli\ufb01ers In Sprout and Orchard, each note has a \u03c1 component, de\ufb01ned as part of the note. In Sapling, each positioned note (as de\ufb01ned in section3.2.2 Note Commitments on page 16) has an associated \u03c1 value, which is computed from its note commitment cm and note position pos as follows: \u03c1 : MixingPedersenHash(cm, pos). MixingPedersenHash is de\ufb01ned in section5.4.1.8 Mixing Pedersen Hash Function on page 81.",
      "Let PRFnfSprout and PRFnfSapling and PRFnfOrchard be as instantiated in section5.4.2 Pseudo Random Functions on page 86. For a Sprout note, the nulli\ufb01er (see section3.2.3 Nulli\ufb01ers on page 17) is derived as PRFnfSprout (\u03c1), where ask is the spending key associated with the note. For a Sapling note, the nulli\ufb01er is derived as PRFnfSapling (\u03c1), where nkis a representation of the nulli\ufb01er deriving key associated with the note and \u03c1 reprJ(\u03c1). The derivation of nulli\ufb01ers for Orchard notes is a little more complicated. Let P and \ud835\udc5eP be as de\ufb01ned in section5.4.9.6 Pallas and Vesta on page 105. Let ExtractP be as de\ufb01ned in section5.4.9.7 Coordinate Extractor for Pallas on page 106. Let GroupHashP be as de\ufb01ned in section5.4.9.8 Group Hash into Pallas and Vesta on page 107. De\ufb01ne \ud835\udca6Orchard : GroupHashP(z.cash:Orchard, K).",
      "To avoid repetition, we de\ufb01ne a function DeriveNullifier F\ud835\udc5eP  F\ud835\udc5eP  F\ud835\udc5eP  P F\ud835\udc5eP as follows: DeriveNullifiernk(\u03c1, \u03c8, cm)  ExtractP ( (PRFnfOrchard (\u03c1)  \u03c8) mod \ud835\udc5eP \ud835\udca6Orchard  cm where nk is the nulli\ufb01er deriving key associated with the note; \u03c1 and \u03c8 are part of the note; and cm is the note commitment. Notes:  The addition of PRFnfOrchard (\u03c1) and \u03c8 is intentionally done modulo \ud835\udc5eP, even though the scalar multiplication is on the Pallas curve which has scalar \ufb01eld F\ud835\udc5fP. For correctness, the \u03c1 and \u03c8 inputs must always be consistent with the note committed to by cm. Security requirement: For each shielded protocol, the requirements on nulli\ufb01er derivation are as follows:  The derived nulli\ufb01er must be determined completely by the \ufb01elds of the note, and possibly its position, in a way that can be checked in the corresponding statement that controls spends (i.e. the JoinSplit statement, Spend statement, or Action statement).",
      "Under the assumption that \u03c1 values are unique, it must not be possible to generate two notes with distinct note commitments but the same nulli\ufb01er. (See section8.4 Faerie Gold attack and \ufb01x on page 143 for further discussion.)  Given a set of nulli\ufb01ers of a priori unknown notes, they must not be linkable to those notes with probability greater than expected by chance, even to an adversary with the corresponding incoming viewing keys (but not full viewing keys), and even if the adversary may have created the notes. 4.17 Chain Value Pool Balances The transparent chain value pool balance for a given block chain is the sum of the values of all UTXOs in the UTXO (unspent transaction output) set for that chain. It is denoted by ChainValuePoolBalanceTransparent(height).",
      "As de\ufb01ned in ZIP-209, the Sprout chain value pool balance for a given block chain is the sum of all vold pub \ufb01eld values for transactions in the block chain, minus the sum of all vnew pub \ufb01eld values for transactions in the block chain. It is denoted by ChainValuePoolBalanceSprout(height). Consensus rule: If the Sprout chain value pool balance would become negative in the block chain created as a result of accepting a block, then all nodes MUST reject the block as invalid. As de\ufb01ned in ZIP-209, the Sapling chain value pool balance for a given block chain is the negation of the sum of all valueBalanceSapling values for transactions in the block chain. It is denoted by ChainValuePoolBalanceSapling(height). Consensus rule: If the Sapling chain value pool balance would become negative in the block chain created as a result of accepting a block, then all nodes MUST reject the block as invalid.",
      "Similarly to the Sapling chain value pool balance de\ufb01ned in ZIP-209, the Orchard chain value pool balance for a given block chain is the negation of the sum of all valueBalanceOrchard \ufb01eld values for transactions in the block chain. It is denoted by ChainValuePoolBalanceOrchard(height). Consensus rule: If the Orchard chain value pool balance would become negative in the block chain created as a result of accepting a block, then all nodes MUST reject the block as invalid. De\ufb01ne totalDeferredOutput and totalDeferredInput as in section7.8 on page 136.",
      "Then, consistent with ZIP-207, the deferred development fund chain value pool balance for a block chain up to and including height height is given by: ChainValuePoolBalanceDeferred(height) : height totalDeferredOutput(h) totalDeferredInput(h) Consensus rule: If the deferred development fund chain value pool balance would become negative in the block chain created as a result of accepting a block, then all nodes MUST reject the block as invalid. Non-normative notes:  totalDeferredOutput(h) is necessarily zero for heights h prior to NU66 activation. totalDeferredInput(h) is necessarily zero for heights h prior to NU6.6.1 activation.",
      "The total issued supply of a block chain at block height height is given by the function: IssuedSupply(height) : ChainValuePoolBalanceTransparent(height)  ChainValuePoolBalanceSprout(height)  ChainValuePoolBalanceSapling(height)  ChainValuePoolBalanceOrchard(height)  ChainValuePoolBalanceDeferred(height) 4.18 Zk-SNARK Statements 4.18.1 JoinSplit Statement (Sprout) Let \u2113Sprout Merkle, \u2113Sprout PRF , MerkleDepthSprout, \u2113value, \u2113ask, \u2113Sprout , \u2113hSig, Nold, Nnew be as de\ufb01ned in section5.3 Constants on page 74. Let PRFaddr, PRFnfSprout, PRFpk, and PRF\u03c1 be as de\ufb01ned in section4.1.2 Pseudo Random Functions on page 25. Let NoteCommitSprout be as de\ufb01ned in section4.1.8 Commitment on page 31, and let NoteSprout and NoteCommitmentSprout be as de\ufb01ned in section3.2 Notes on page 14. A valid instance of a JoinSplit statement, \ud835\udf0bZKJoinSplit, assures that given a primary input: rtSprout B\u2113Sprout Merkle, nfold 1..Nold B\u2113Sprout PRF Nold, cmnew 1..Nnew NoteCommitSprout.OutputNnew, vold 0 ..",
      "2\u2113value1, vnew 0 .. 2\u2113value1, hSig B\u2113hSig, h1..Nold B\u2113Sprout PRF Nold) the prover knows an auxiliary input: path1..Nold B\u2113Sprout MerkleMerkleDepthSproutNold, pos1..Nold 0 .. 2MerkleDepthSprout 1Nold, nold 1..Nold NoteSproutNold, aold sk,1..Nold B\u2113askNold, nnew 1..Nnew NoteSproutNnew, B\u2113Sprout enforceMerklePath1..Nold BNold) where: for each \ud835\udc561..Nold: nold  (aold pk,\ud835\udc56, vold \ud835\udc56, \u03c1old \ud835\udc56, rcmold for each \ud835\udc561..Nnew: nnew  (anew pk,\ud835\udc56, vnew , \u03c1new , rcmnew such that the following conditions hold: Merkle path validity for each \ud835\udc561..Nold  enforceMerklePath\ud835\udc56 1: (path\ud835\udc56, pos\ud835\udc56) is a valid Merkle path (see section4.9 Merkle Path Validity on page 49) of depth MerkleDepthSprout from NoteCommitmentSprout(nold \ud835\udc56) to the anchor rtSprout. Note: Merkle path validity covers conditions 1. (a) and 1. (d) of the NP statement in BCGGMTV2014, section 4.2. Merkle path enforcement for each \ud835\udc561..Nold, if vold  0 then enforceMerklePath\ud835\udc56 1. Balance vold pub  Nold \ud835\udc561vold  vnew pub  Nnew \ud835\udc561 vnew 0 .. 2\u2113value1.",
      "Nulli\ufb01er integrity for each \ud835\udc561..Nold: nfold  PRFnfSprout aold sk,\ud835\udc56 (\u03c1old Spend authority for each \ud835\udc561..Nold: aold pk,\ud835\udc56 PRFaddr aold sk,\ud835\udc56(0). Non-malleability for each \ud835\udc561..Nold: h\ud835\udc56 PRFpk aold sk,\ud835\udc56(\ud835\udc56, hSig). Uniqueness of \u03c1new for each \ud835\udc561..Nnew: \u03c1new  PRF\u03c1 \u03d5(\ud835\udc56, hSig). Note commitment integrity for each \ud835\udc561..Nnew: cmnew  NoteCommitmentSprout(nnew For details of the form and encoding of proofs, see section5.4.10.1 BCTV14 on page 110. 4.18.2 Spend Statement (Sapling) Let \u2113Sapling Merkle , \u2113PRFnfSapling, \u2113Sapling scalar , and MerkleDepthSapling be as de\ufb01ned in section5.3 Constants on page 74. Let ValueCommitSapling and NoteCommitSapling be as speci\ufb01ed in section4.1.8 Commitment on page 31. Let SpendAuthSigSapling be as de\ufb01ned in section5.4.7.1 Spend Authorization Signature (Sapling and Orchard) on page 95. Let J, J(\ud835\udc5f), reprJ, \ud835\udc5eJ, \ud835\udc5fJ, and \u210eJ be as de\ufb01ned in section5.4.9.3 Jubjub on page 102.",
      "Let ExtractJ(\ud835\udc5f) J(\ud835\udc5f) B\u2113Sapling Merkle  be as de\ufb01ned in section5.4.9.4 Coordinate Extractor for Jubjub on page 104. Let \u210bSapling be as de\ufb01ned in section4.2.2 Sapling Key Components on page 36. A valid instance of a Spend statement, \ud835\udf0bZKSpend, assures that given a primary input: rtSapling B\u2113Sapling Merkle , cvold ValueCommitSapling.Output, nfold BY\u2113PRFnfSapling8, SpendAuthSigSapling.Public the prover knows an auxiliary input: path B\u2113Sapling Merkle MerkleDepthSapling, 0 .. 2MerkleDepthSapling 1, vold 0 .. 2\u2113value1, rcvold 0 .. 2\u2113Sapling scalar 1, cmold rcmold 0 .. 2\u2113Sapling scalar 1, 0 .. 2\u2113Sapling scalar 1, SpendAuthSigSapling.Public, 0 .. 2\u2113Sapling scalar 1 such that the following conditions hold: Note commitment integrity cmold  NoteCommitSapling rcmold (reprJ(gd), reprJ(pkd), vold).",
      "Merkle path validity Either vold  0; or (path, pos) is a valid Merkle path of depth MerkleDepthSapling, as de\ufb01ned in section4.9 Merkle Path Validity on page 49, from cm\ud835\udc62 ExtractJ(\ud835\udc5f)(cmold) to the anchor rtSapling. Value commitment integrity cvold  ValueCommitSapling rcvold (vold). Small order checks gd and ak are not of small order, i.e. \u210eJ gd  \ud835\udcaaJ and \u210eJ ak  \ud835\udcaaJ. Nulli\ufb01er integrity nfold  PRFnfSapling (\u03c1) where nk reprJ nsk \u210bSapling) \u03c1 reprJ MixingPedersenHash(cmold, pos) Spend authority rk  SpendAuthSigSapling.RandomizePublic(\ud835\udefc, ak). Diversi\ufb01ed address integrity pkd  ivk gd where ivk  CRHivk(ak, nk) ak reprJ(ak). For details of the form and encoding of Spend statement proofs, see section5.4.10.2 Groth16 on page 111. Notes:  Primary and auxiliary inputs MUST be constrained to have the types speci\ufb01ed. In particular, see sectionA.3.3.2 ctEdwards decompression and validation on page 205, for required validity checks on compressed repre- sentations of Jubjub curve points.",
      "The ValueCommitSapling.Output and SpendAuthSigSapling.Public types also represent points, i.e. J. In the Merkle path validity check, each layer does not check that its input bit sequence is a canonical encoding (in 0 .. \ud835\udc5eJ 1) of the integer from the previous layer. It is not checked in the Spend statement that rk is not of small order. However, this is checked outside the Spend statement, as speci\ufb01ed in section4.4 Spend Descriptions on page 40. It is not checked that rcvold  \ud835\udc5fJ or that rcmold  \ud835\udc5fJ. SpendAuthSigSapling.RandomizePublic(\ud835\udefc, ak)  ak  \ud835\udefc \ud835\udca2Sapling. (\ud835\udca2Sapling is as de\ufb01ned in section5.4.7.1 Spend Authorization Signature (Sapling and Orchard) on page 95.) 4.18.3 Output Statement (Sapling) Let \u2113Sapling Merkle and \u2113Sapling scalar be as de\ufb01ned in section5.3 Constants on page 74. Let ValueCommitSapling and NoteCommitSapling be as speci\ufb01ed in section4.1.8 Commitment on page 31. Let J, reprJ, and \u210eJ be as de\ufb01ned in section5.4.9.3 Jubjub on page 102.",
      "Let ExtractJ(\ud835\udc5f) J(\ud835\udc5f) B\u2113Sapling Merkle  be as de\ufb01ned in section5.4.9.4 Coordinate Extractor for Jubjub on page 104. A valid instance of an Output statement, \ud835\udf0bZKOutput, assures that given a primary input: cvnew ValueCommitSapling.Output, B\u2113Sapling Merkle , the prover knows an auxiliary input: pkd B\u2113J, vnew 0 .. 2\u2113value1, rcvnew 0 .. 2\u2113Sapling scalar 1, rcmnew 0 .. 2\u2113Sapling scalar 1, 0 .. 2\u2113Sapling scalar 1) such that the following conditions hold: Note commitment integrity cm\ud835\udc62 ExtractJ(\ud835\udc5f) NoteCommitSapling rcmnew (gd, pkd, vnew) , where gd  reprJ(gd). Value commitment integrity cvnew  ValueCommitSapling rcvnew (vnew). Small order check gd is not of small order, i.e. \u210eJ gd  \ud835\udcaaJ. Ephemeral public key integrity epk  esk gd. For details of the form and encoding of Output statement proofs, see section5.4.10.2 Groth16 on page 111. Notes:  Primary and auxiliary inputs MUST be constrained to have the types speci\ufb01ed.",
      "In particular, see sectionA.3.3.2 ctEdwards decompression and validation on page 205, for required validity checks on compressed repre- sentations of Jubjub curve points. The ValueCommitSapling.Output type also represents points, i.e. J. The validity of pkd is not checked in this circuit (which is the reason why it is typed as a bit sequence rather than as a point). It is not checked that rcvold  \ud835\udc5fJ or that rcmold  \ud835\udc5fJ. 4.18.4 Action Statement (Orchard) Let \u2113Orchard Merkle , \u2113Orchard scalar , and MerkleDepthOrchard be as de\ufb01ned in section5.3 Constants on page 74. Let ValueCommitOrchard, NoteCommitOrchard, and Commitivk be as speci\ufb01ed in section4.1.8 Commitment on page 31. Let SpendAuthSigOrchardbe as de\ufb01ned in section5.4.7.1 Spend Authorization Signature (Sapling and Orchard) on page 95. Let P, P, reprP, \ud835\udc5eP, and \ud835\udc5fP be as de\ufb01ned in section5.4.9.6 Pallas and Vesta on page 105. Let \ud835\udc65, \ud835\udc66, ExtractP, and Extract P be as de\ufb01ned in section5.4.9.7 Coordinate Extractor for Pallas on page 106.",
      "Let DeriveNullifier be as de\ufb01ned in section4.16 Computing \u03c1 values and Nulli\ufb01ers on page 57. A valid instance of an Action statement, \ud835\udf0b, assures that given a primary input: rtOrchard 0 .. \ud835\udc5eP 1, cvnet ValueCommitOrchard.Output, nfold 0 .. \ud835\udc5eP 1, SpendAuthSigOrchard.Public, 0 .. \ud835\udc5eP 1, enableSpends enableOutputs the prover knows an auxiliary input: path 0 .. \ud835\udc5eP 1MerkleDepthOrchard, 0 .. 2MerkleDepthOrchard 1, gold P, pkold P, vold 0 .. 2\u2113value1, \u03c1old F\ud835\udc5eP, \u03c8old F\ud835\udc5eP, rcmold 0 .. 2\u2113Orchard scalar 1, cmold 0 .. 2\u2113Orchard scalar 1, P, F\ud835\udc5eP, rivk Commitivk.Trapdoor, gnew P, pknew P, vnew 0 .. 2\u2113value1, \u03c8new F\ud835\udc5eP, rcmnew 0 .. 2\u2113Orchard scalar 1, 0 .. 2\u2113Orchard scalar 1 such that the following conditions hold: Old note commitment integrity NoteCommitOrchard rcmold (reprP(gold d ), reprP(pkold d ), vold, \u03c1old, \u03c8old) cmold, .",
      "Merkle path validity Either vold  0; or (path, pos) is a valid Merkle path of depth MerkleDepthOrchard, as de\ufb01ned in section4.9 Merkle Path Validity on page 49, from ExtractP(cmold) to the anchor rtOrchard. Value commitment integrity cvnet  ValueCommitOrchard (vold vnew). Nulli\ufb01er integrity nfold  DeriveNullifiernk(\u03c1old, \u03c8old, cmold). Spend authority rk  SpendAuthSigOrchard.RandomizePublic(\ud835\udefc, akP). Diversi\ufb01ed address integrity ivk  or pkold  ivk gold where ivk  Commitivk rivk ExtractP(akP), nk Newnote commitment integrity Extract NoteCommitOrchard rcmnew (reprP(gnew ), reprP(pknew ), vnew, \u03c1new, \u03c8new) cm\ud835\udc65, , where \u03c1new  nfold (mod \ud835\udc5eP). Enable spend \ufb02ag vold  0 or enableSpends  1. Enable output \ufb02ag vnew  0 or enableOutputs  1. For details of the form and encoding of Action statement proofs, see section5.4.10.3 Halo 2 on page 112.",
      "Notes:  The primary inputs are encoded as the following sequence of type F\ud835\udc5eP 9: rtOrchard (mod \ud835\udc5eP),\ud835\udc65 cvnet) cvnet) , nfold (mod \ud835\udc5eP),\ud835\udc65(rk),\ud835\udc66(rk), cm\ud835\udc65(mod \ud835\udc5eP), enableSpends (mod \ud835\udc5eP), enableOutputs (mod \ud835\udc5eP) (Recall from section2 Notation on page 10 that (mod \ud835\udc5eP) interprets an integer as an F\ud835\udc5eP element.)  Primary and auxiliary inputs MUST be constrained to have the types speci\ufb01ed. In particular, gold d , pkold d , gnew pknew , and akP cannot be \ud835\udcaaP. The ValueCommitOrchard.Output and SpendAuthSigOrchard.Public types represent Pallas curve points, i.e. P. The scalar multiplication used in ValueCommitOrchard must operate correctly on the range 264  1 .. 264 1, which is different to the range 263 .. 263 1 of vbalanceOrchard. In the Merkle path validity check, each layer does not check that its input bit sequence is a canonical encoding (in 0 .. \ud835\udc5eP 1) of the integer from the previous layer.",
      "As speci\ufb01ed in section4.9 Merkle Path Validity on page 49, the validity check is permitted to be implemented in such a way that it can pass if any MerkleCRHOrchard hash on the Merkle path outputs 0. This allows nondeterministic, incomplete addition to be used in the circuit for SinsemillaHash. It is not checked that rcv  \ud835\udc5fP or that rcmold  \ud835\udc5fP or that rcmnew  \ud835\udc5fP. SpendAuthSigOrchard.RandomizePublic(\ud835\udefc, akP)  akP  \ud835\udefc \ud835\udca2Orchard. (\ud835\udca2Orchard is as de\ufb01ned in section5.4.7.1 Spend Authorization Signature (Sapling and Orchard) on page 95.)  The validity of gd and pkd are not checked in this circuit. Also, rtOrchard and cm\ud835\udc65are not checked to be Pallas af\ufb01ne-short-Weierstrass \ud835\udc65-coordinates or 0. When a value given as a \ufb01eld element in the Action circuit is used as a scalar for scalar multiplication, it involves witnessing the scalar as a sequence of bits or window indices, typically for a total of 255 bits (except in the case of multiplying by the value difference vold vnew).",
      "This raises the possibility that the witnessed 255-bit representation may match the original \ufb01eld element modulo \ud835\udc5eP, but not modulo \ud835\udc5fP. Unless it can be proven to result in an equivalent statement, the decomposition of each scalar value MUST be canonical. The cases in which not checking canonicity results in an equivalent statement are those where the state- ment only requires to prove knowledge of the scalar, without using it elsewhere  i.e. the multiplica- tions by rcmold or rcmnew in NoteCommitOrchard, by rcv in ValueCommitOrchard, by rivk in Commitivk, and by \ud835\udefcin SpendAuthSigOrchard.RandomizePublic.",
      "In particular, the representation of (PRFnfOrchard (\u03c1)  \u03c8) mod \ud835\udc5eP that is used for the scalar multiplication in DeriveNullifier MUST be checked to be canonical in order to avoid a potential double-spend vulnerability, and similarly for the representation of ivk in ivk gold Non-normative notes:  The procedure in section4.2.3 Orchard Key Components on page 38 will always produce a spend authorization address key that effectively has the compressed \ud835\udc66-coordinate, \ud835\udc66, set to 0. The Action statement, on the other hand, allows the prover to witness akP with \ud835\udc66set to 0 or 1. This is harmless because if the prover and signer(s) of the spend authorization signature collectively know rsk and \ud835\udefc, we can conclude that they collectively know ask up to sign, which is suf\ufb01cient for spend authorization. There is intentionally no equivalent to the Ephemeral public key integrity check from the Sapling Output statement.",
      "It is unnecessary for the sender of an Orchard note to prove knowledge of esk, because the potential attack this originally addressed for Sapling is prevented by checks added at Canopy activation in ZIP-212. These checks are required after the end of the ZIP 212 grace period, which precedes NU55 activation. If NoteCommitOrchard returns for the old or new note, then the corresponding note commitment integrity check is satis\ufb01ed. Similarly, if Commitivk returns , then the diversi\ufb01ed address integrity check is satis\ufb01ed. This models the fact that the implemented circuit uses incomplete point addition to compute SinsemillaHashToPoint. If an exceptional case were to occur, the prover could arbitrarily choose the intermediate \ud835\udf06value in an addition, which must be assumed to allow them to control the output.",
      "(The formal output of SinsemillaHashToPoint is in such a case, while the output computed by the circuit would be nondeterministic.) But as proven in Theorem 5.4.4 on page 84, these exceptional cases allow immediately \ufb01nding a nontrivial discrete logarithm relation. If the Discrete Logarithm Problem is hard on the Pallas curve, then \ufb01nding such a case is infeasible. 4.19 In-band secret distribution (Sprout) In Sprout, the secrets that need to be transmitted to a recipient of funds in order for them to later spend, are v, \u03c1, and rcm. A memo \ufb01eld (section3.2.1 Note Plaintexts and Memo Fields on page 15) is also transmitted. To transmit these secrets securely to a recipient without requiring an out-of-band communication channel, the transmission key pkenc is used to encrypt them. The recipients possession of the associated incoming viewing key ivk is used to reconstruct the original note and memo \ufb01eld.",
      "A single ephemeral public key is shared between encryptions of the Nnew shielded outputs in a JoinSplit description. All of the resulting ciphertexts are combined to form a transmitted notes ciphertext. For both encryption and decryption,  let Sym be the scheme instantiated in section5.4.3 Symmetric Encryption on page 88;  let KDFSprout be the Key Derivation Function instantiated in section5.4.5.2 Sprout Key Derivation on page 89;  let KASprout be the key agreement scheme instantiated in section5.4.5.1 Sprout Key Agreement on page 88;  let hSig be the value computed for this JoinSplit description in section4.3 JoinSplit Descriptions on page 39. 4.19.1 Encryption (Sprout) Let KASprout be the key agreement scheme instantiated in section5.4.5.1 Sprout Key Agreement on page 88. Let pkenc,1..Nnew be the transmission keys for the intended recipient addresses of each new note. Let np1..Nnew be Sprout note plaintexts de\ufb01ned in section3.2.1 Note Plaintexts and Memo Fields on page 15.",
      "Then to encrypt:  Generate a new KASprout (public, private) key pair (epk, esk). For \ud835\udc561..Nnew,  Let Penc BY585 be the raw encoding of np\ud835\udc56. Let sharedSecret\ud835\udc56 KASprout.Agree(esk, pkenc,\ud835\udc56). Let Kenc  KDFSprout(\ud835\udc56, hSig, sharedSecret\ud835\udc56, epk, pkenc,\ud835\udc56). Let Cenc  Sym.EncryptKenc \ud835\udc56(Penc The resulting transmitted notes ciphertext is (epk, Cenc 1..Nnew). Note: It is technically possible to replace Cenc for a given note with a random (and undecryptable) dummy ciphertext, relying instead on out-of-band transmission of the note to the recipient. In this case the ephemeral key MUST still be generated as a random public key (rather than a random bit sequence) to ensure indistinguishability from other JoinSplit descriptions. This mode of operation raises further security considerations, for example of how to validate a Sprout note received out-of-band, which are not addressed in this document.",
      "4.19.2 Decryption (Sprout) Let ivk  (apk, skenc) be the recipients incoming viewing key, and let pkenc be the corresponding transmission key derived from skenc as speci\ufb01ed in section4.2.1 Sprout Key Components on page 36. Let cm1..Nnew be the note commitments of each output coin. Then for each \ud835\udc561..Nnew, the recipient will attempt to decrypt that ciphertext component (epk, Cenc ) as follows: let sharedSecret\ud835\udc56 KASprout.Agree(skenc, epk) let Kenc  KDFSprout(\ud835\udc56, hSig, sharedSecret\ud835\udc56, epk, pkenc) let Penc  Sym.DecryptKenc \ud835\udc56(Cenc if Penc  , return  return ExtractNoteSprout(Penc , cm\ud835\udc56, apk). ExtractNoteSprout(Penc BY585, cm\ud835\udc56 BY32, apk B\u2113Sprout PRF ) is de\ufb01ned as follows: extract np\ud835\udc56 (leadByte\ud835\udc56 BY, v\ud835\udc56 0 .. 2\u2113value1, \u03c1\ud835\udc56 B\u2113Sprout PRF , rcm\ud835\udc56 NoteCommitSprout.Trapdoor, memo\ud835\udc56 BY512) from Penc let n\ud835\udc56 (apk, v\ud835\udc56, \u03c1\ud835\udc56, rcm\ud835\udc56) if leadByte\ud835\udc56 0x00 or NoteCommitmentSprout(n\ud835\udc56)  cm\ud835\udc56, return  return (n\ud835\udc56, memo\ud835\udc56). Notes:  The decryption algorithm corresponds to step 3 (b) i. and ii.",
      "(\ufb01rst bullet point) of the Receive algorithm shown in BCGGMTV2014, Figure 2. To test whether a note is unspent in a particular block chain also requires the spending key ask; the coin is unspent if and only if nf  PRFnfSprout (\u03c1) is not in the nulli\ufb01er set for that block chain. A note can change from being unspent to spent as a nodes view of the best valid block chain is extended by new transactions. Also, block chain reorganizations can cause a node to switch to a different best valid block chain that does not contain the transaction in which a note was output. See section8.7 In-band secret distribution on page 147 for further discussion of the security and engineering rationale behind this encryption scheme. 4.20 In-band secret distribution (Sapling and Orchard) In Sapling and Orchard, the secrets that need to be transmitted to a recipient of a note so that they can later spend it, are d, v, and rcm or rseed.",
      "A memo \ufb01eld (section3.2.1 Note Plaintexts and Memo Fields on page 15) is also transmitted. To transmit these secrets securely to a recipient without requiring an out-of-band communication channel, the diversi\ufb01ed transmission key pkd is used to encrypt them. The recipients possession of the associated KASapling or KAOrchard private key ivk is used to reconstruct the original note and memo \ufb01eld. Unlike in Sprout, each Sapling or Orchard shielded output is encrypted by a fresh ephemeral public key.",
      "For both encryption and decryption,  let \u2113ovk be as de\ufb01ned in section5.3 Constants on page 74;  let Sym be the encryption scheme instantiated in section5.4.3 Symmetric Encryption on page 88;  let KA be the key agreement scheme KASapling or KAOrchard instantiated in section5.4.5.3 Sapling Key Agreement on page 89 or section5.4.5.5 Orchard Key Agreement on page 90;  let KDFbe the KeyDerivation Function KDFSaplingorKDFOrchardinstantiated in section5.4.5.4 Sapling Key Derivation on page 89 or section5.4.5.6 Orchard Key Derivation on page 90;  let G, \u2113G, and reprG be instantiated as J, \u2113J, and reprJ de\ufb01ned in section5.4.9.3 Jubjub on page 102, or P, \u2113P, and reprP de\ufb01ned in section5.4.9.6 Pallas and Vesta on page 105;  let ExtractG(\ud835\udc5f) be ExtractJ(\ud835\udc5f) as de\ufb01ned in section5.4.9.4 Coordinate Extractor for Jubjub on page 104 or ExtractP as de\ufb01ned in section5.4.9.7 Coordinate Extractor for Pallas on page 106;  let PRFock be PRFockSapling or PRFockOrchard instantiated in section5.4.2 Pseudo Random Functions on page 86;  let DiversifyHash be DiversifyHashSapling in section5.4.1.6 DiversifyHashSapling and DiversifyHashOrchard Hash Functions on page 78, or DiversifyHashOrchard in the same section;  let NoteCommitmentbe NoteCommitmentSaplingorNoteCommitmentOrchardde\ufb01ned in section3.2.2 Note Commitments on page 16;  let ToScalar be ToScalarSapling de\ufb01ned in section4.2.2 Sapling Key Components on page 36 or ToScalarOrchard de\ufb01ned in section4.2.3 Orchard Key Components on page 38;  LEBS2OSP, LEOS2IP, I2LEBSP, and I2LEOSP are de\ufb01ned in section5.1 Integers, Bit Sequences, and Endianness on page 73.",
      "4.20.1 Encryption (Sapling and Orchard) Let pkd KA.PublicPrimeOrder be the diversi\ufb01ed transmission key for the intended recipient address of a new Sapling or Orchard note, and let gd KA.PublicPrimeOrder be the corresponding diversi\ufb01ed base computed as DiversifyHash(d). Since Sapling note encryption is used only in the context of section4.7.2 Sending Notes (Sapling) on page 44, and similarly Orchard note encryption is used only in the context of section4.7.3 Sending Notes (Orchard) on page 45, we may assume that gd has already been calculated and is not . Also, the ephemeral private key esk has been chosen. Let ovk BY\u2113ovk8  be as described in section4.7.2 on page 44 or section4.7.3 on page 45, i.e. the outgoing viewing key of the shielded payment address from which the note is being spent, or an outgoing viewing key associated with a ZIP-32 account, or . Let np  (leadByte, d, v, rseed, memo) be the Sapling or Orchard note plaintext.",
      "np is encoded as de\ufb01ned in section5.5 Encodings of Note Plaintexts and Memo Fields on page 112. Let cv be the value commitment for the Output description or Action description (for Orchard, this also depends on the value of the note being spent), and let cm be the note commitment. These are needed to derive the outgoing cipher key ock in order to produce the outgoing ciphertext Cout. Then to encrypt: let Penc be the raw encoding of np let epk  KA.DerivePublic(esk, gd) let ephemeralKey  LEBS2OSP\u2113G reprG(epk) let sharedSecret  KA.Agree(esk, pkd) let Kenc  KDF(sharedSecret, ephemeralKey) let Cenc  Sym.EncryptKenc(Penc) if ovk  : choose random ock  R Sym.K and op  R BY(\u2113G256)8 else: let cv  LEBS2OSP\u2113G reprG(cv) let cm  LEBS2OSP256 ExtractG(\ud835\udc5f)(cm) let ock  PRFock ovk(cv, cm, ephemeralKey) let op  LEBS2OSP\u2113G256 reprG(pkd)  I2LEBSP256(esk) let Cout  Sym.Encryptock(op) The resulting transmitted note ciphertext is (ephemeralKey, Cenc, Cout).",
      "Note: It is technically possible to replace Cenc for a given note with a random (and undecryptable) dummy ciphertext, relying instead on out-of-band transmission of the note to the recipient. In this case the ephemeral key MUST still be generated as a random public key (rather than a random bit sequence) to ensure indistinguishability from other Output descriptions. This mode of operation raises further security considerations, for example of how to validate a Sapling or Orchard note received out-of-band, which are not addressed in this document. 4.20.2 Decryption using an Incoming Viewing Key (Sapling and Orchard) Let ivk 1 .. 2\u2113Sapling 1 (in Sapling) or 1 .. \ud835\udc5eP 1 (in Orchard) be the recipients KASapling or KAOrchard private key, as speci\ufb01ed in section4.2.2 Sapling Key Components on page 36 or in section4.2.3 Orchard Key Components on page 38. Let (ephemeralKey, Cenc, Cout) be the transmitted note ciphertext from the Output description.",
      "Let cm be the cmu or cmx \ufb01eld of the Output description or Action description respectively. (This encodes the af\ufb01ne-ctEdwards \ud835\udc62-coordinate or af\ufb01ne-short-Weierstrass \ud835\udc65-coordinate of the note commitment, i.e. ExtractG(\ud835\udc5f)(cm).) Let protocol and allowedLeadBytes be as de\ufb01ned in section3.2.1 Note Plaintexts and Memo Fields on page 15. Let height be the block height of the block containing this transaction, and let txVersion be the transaction version number. The recipient will attempt to decrypt the ephemeralKey and Cenc components of the transmitted note ciphertext: let epk  abstG(ephemeralKey). if epk  , return  let sharedSecret  KA.Agree(ivk, epk) let Kenc  KDF(sharedSecret, ephemeralKey) let Penc  Sym.DecryptKenc(Cenc). if Penc  , return  extract np  (leadByte BY, d B\u2113d, v 0 ..",
      "2\u2113value1, rseed BY32, memo BY512) from Penc if leadByte allowedLeadBytesprotocol(height, txVersion), return  for Sapling, let pre_rcm  4 and pre_esk  5 for Orchard, let \u03c1  I2LEOSP256 nfold from the same Action description , pre_rcm  5  \u03c1, and pre_esk  4  \u03c1 let rcm  LEOS2IP256(rseed), if leadByte  0x01 ToScalar PRFexpand rseed (pre_rcm) , otherwise if rcm \ud835\udc5fG, return  let gd  DiversifyHash(d). if (for Sapling) gd  , return  Canopy onward if leadByte  0x01: esk  ToScalar PRFexpand rseed (pre_esk) if reprG KA.DerivePublic(esk, gd)  ephemeralKey, return  let pkd  KA.DerivePublic(ivk, gd) for Sapling, let n  (d, pkd, v, rcm) for Orchard, let n  (d, pkd, v, \u03c1, \u03c8, rcm) where \u03c8  ToBaseOrchard( PRFexpand rseed (9  \u03c1) let cm   NoteCommitment(n). if (for Orchard) cm   , return  if I2LEOSP256 ExtractG(\ud835\udc5f)(cm  cm, return  return (n, memo). Notes:  gd has already been computed when applying NoteCommitment, and need not be computed again.",
      "For Sapling, as explained in the note in section5.4.9.3 Jubjub on page 102, abstJ accepts non-canonical compressed encodings of Jubjub curve points. Therefore, an implementation MUST use the original ephemeralKey \ufb01eld as encoded in the transaction as input to KDFSapling, and (if Canopy is active and leadByte  0x01) in the comparison against reprG KA.DerivePublic(esk, gd) . For consistency this is also what is speci\ufb01ed for Orchard. Normally only transmitted note ciphertexts of transactions in blocks need to be decrypted. In that case, any received Sapling note is necessarily a positioned note, so its \u03c1 value can immediately be calculated per section4.16 Computing \u03c1 values and Nulli\ufb01ers on page 57. To test whether a Sapling or Orchard note is unspent in a particular block chain also requires the nulli\ufb01er deriving key nk; the coin is unspent if and only if the nulli\ufb01er computed as in section4.16 on page 57 is not in the nulli\ufb01er set for that block chain.",
      "A note can change from being unspent to spent as a nodes view of the best valid block chain is extended by new transactions. Also, block chain reorganizations can cause a node to switch to a different best valid block chain that does not contain the transaction in which a note was output. A client MAY attempt to decrypt a transmitted note ciphertext of a transaction in the mempool, using the next block height for height. However, in that case it MUST NOT assume that the transaction will be mined and MUST treat the decrypted information as provisional, and private. NU55 onward It is a consensus rule (in section4.6 Action Descriptions on page 42) that each Action description \ufb01eld MUST be a valid encoding of its declared type, which in the case of ephemeralKey is KAOrchard.Public (i.e. P), and therefore epk cannot be \ud835\udcaaP. NU55 onward The domain separators 4 and 5 used in the input to PRFexpand rseed are swapped for Orchard relative to Sapling.",
      "This was due to an oversight and there is no good reason for it. 4.20.3 Decryption using an Outgoing Viewing Key (Sapling and Orchard) Let ovk BY\u2113ovk8 be the outgoing viewing key, as speci\ufb01ed in section4.2.2 Sapling Key Components on page 36 or section4.2.3 Orchard Key Components on page 38, that is to be used for decryption. (If ovk  was used for encryption, the payment is not decryptable by this method.) Let the constants CanopyActivationHeight and ZIP212GracePeriod be as de\ufb01ned in section5.3 Constants on page 74. Let height be the block height of the block containing this transaction, and let txVersion be the transaction version number. Let (ephemeralKey, Cenc, Cout) be the transmitted note ciphertext. For a Sapling transmitted note ciphertext, let cv and cm be the cv and cmu \ufb01elds of the Output description. For an Orchard transmitted note ciphertext, let cv and cm be the cv and cmx \ufb01elds of the Action description.",
      "The outgoing viewing key holder will attempt to decrypt the transmitted note ciphertext as follows: let ock  PRFock ovk(cv, cm, ephemeralKey) let op  Sym.Decryptock(Cout) . if op  , return  extract (pkd B\u2113G, esk BY32) from op let esk  LEOS2IP256 and pkd  abstG(pkd) if esk \ud835\udc5fG or pkd  , return  if reprP  pkd, return  let sharedSecret  KA.Agree(esk, pkd) let Kenc  KDF(sharedSecret, ephemeralKey) let Penc  Sym.DecryptKenc(Cenc). if Penc  , return  extract np  (leadByte BY, d B\u2113d, v 0 .. 2\u2113value1, rseed BY32, memo BY512) from Penc if leadByte allowedLeadBytesprotocol(height, txVersion), return  for Sapling, let pre_rcm  4 and pre_esk  5 for Orchard, let \u03c1  I2LEOSP256 nfold from the same Action description , pre_rcm  5  \u03c1, and pre_esk  4  \u03c1 Canopy onward if leadByte  0x01 and ToScalar PRFexpand rseed (pre_esk)  esk, return  let rcm  LEOS2IP256(rseed), if leadByte  0x01 ToScalar PRFexpand rseed (pre_rcm) , otherwise if rcm \ud835\udc5fG, return  let gd  DiversifyHash(d).",
      "if (for Sapling) gd  or pkd J(\ud835\udc5f) (see note below), return  for Sapling, let n  (d, pkd, v, rcm) for Orchard, let n  (d, pkd, v, \u03c1, \u03c8, rcm) where \u03c8  ToBaseOrchard( PRFexpand rseed (9  \u03c1) let cm   NoteCommitment(n). if (for Orchard) cm   , return  if I2LEOSP256 ExtractG(\ud835\udc5f)(cm  cm, return  if reprG KA.DerivePublic(esk, gd)  ephemeralKey, return  return (n, memo). Notes:  gd has already been computed when applying NoteCommitment, and need not be computed again. A previous version of this speci\ufb01cation did not have the requirement for the decoded point pkd of a Sapling note to be in the set of prime-order points J(\ud835\udc5f) (i.e. if ... pkd J(\ud835\udc5f), return ). That did not match the implementation in zcashd. In fact the history is a little more complicated. The current speci\ufb01cation matches the implementation in librustzcash as of librustzcash-109, which has been used in zcashd since zcashd v2.1.2.",
      "However, there was another implementation of Sapling note decryption used in zcashd for consensus checks, speci\ufb01cally the check that a shielded coinbase output decrypts successfully with the zero ovk. This was corrected to enforce the same restriction on the decrypted pkd in zcashd v5.5.0, originally set to activate in a soft fork at block height 2121200 on both Mainnet and Testnet zcashd-6459. (On Testnet this height was in the past as of the zcashd v5.5.0 release, and so the change would have been immediately enforced on upgrade.) Since the soft fork was observed to be retrospectively valid after that height, the implementation was simpli\ufb01ed in zcashd-6725 to use the librustzcash implementation in all cases, which re\ufb02ects the speci\ufb01cation above. zebra always used the librustzcash implementation. As explained in the note in section5.4.9.3 Jubjub on page 102, abstJ accepts non-canonical compressed encodings of Jubjub curve points.",
      "Therefore, an implementation MUST use the original ephemeralKey \ufb01eld as encoded in the transaction as input to PRFock and KDFSapling, and in the comparison against reprG KASapling.DerivePublic(esk, gd) For consistency this is also what is speci\ufb01ed for Orchard. For Sapling outgoing ciphertexts, pkd could also be non-canonical. The above algorithm explicitly returns if reprP  pkd. However, this is technically redundant with the later check that returns if pkd  J(\ud835\udc5f), because only small-order Jubjub curve points have non-canonical encodings. This check is enforced retrospectively for consensus by current zcashd and zebra versions, and for wallet rescanning by current zcashd. Versions of zcashd prior to zcashd-6725 could however have accepted notes for which the outgoing ciphertext contains either a canonical or a non-canonical encoding of \ud835\udcaaJ for pkd. NU55 onward For Orchard outgoing ciphertexts, it is not possible for pkd to be non-canonical.",
      "NU55 onward The domain separators 4 and 5 used in the input to PRFexpand rseed are swapped for Orchard relative to Sapling. This was due to an oversight and there is no good reason for it. The comments in section4.20.2 Decryption using an Incoming Viewing Key (Sapling and Orchard) on page 68 concerning calculation of \u03c1, detection of spent notes, and decryption of transmitted note ciphertexts for transactions in the mempool also apply to notes decrypted by this procedure.",
      "Non-normative note: Implementors should pay close attention to similarities and differences between this procedure and section4.20.2 Decryption using an Incoming Viewing Key (Sapling and Orchard) on page 68, especially that:  in this procedure, the ephemeral private key esk derived from rseed is checked to be identical to that obtained from op (when leadByte  0x01);  in this procedure, pkd is obtained from op rather than being derived as KASapling.DerivePublic(ivk, gd);  in this procedure, the check that KASapling.DerivePublic(esk, gd)  epk is unconditional rather than being dependent on leadByte  0x01, and it uses the esk obtained from op;  NU55 onward for the same reason as in section4.20.2 on page 68, epk cannot be \ud835\udcaaP. 4.21 Block Chain Scanning (Sprout) Let \u2113Sprout be as de\ufb01ned in section5.3 Constants on page 74. Let NoteSprout be as de\ufb01ned in section3.2 Notes on page 14. Let KASprout be as de\ufb01ned in section5.4.5.1 Sprout Key Agreement on page 88.",
      "Let ivk  (apk B\u2113Sprout PRF , skenc KASprout.Private) be the incoming viewing key corresponding to ask, and let pkenc be the associated transmission key, as speci\ufb01ed in section4.2.1 Sprout Key Components on page 36. The following algorithm can be used, given the block chain and a Sprout spending key ask, to obtain each note sent to the corresponding shielded payment address, its memo \ufb01eld, and its \ufb01nal status (spent or unspent). let mutable ReceivedSet NoteSprout  BY512) let mutable SpentSet NoteSprout) let mutable NullifierMap B\u2113Sprout PRF  NoteSprout the empty mapping for each transaction tx: for each JoinSplit description in tx: let (epk, Cenc 1..Nnew) be the transmitted notes ciphertext of the JoinSplit description for \ud835\udc56in 1..Nnew: Attempt to decrypt the transmitted notes ciphertext component (epk, Cenc ) using ivk with the algorithm in section4.19.2 Decryption (Sprout) on page 66. If this succeeds with (n, memo): Add (n, memo) to ReceivedSet.",
      "Calculate the nulli\ufb01er nf of n using ask as in section4.16 Computing \u03c1 values and Nulli\ufb01ers on page 57. Add the mapping nf n to NullifierMap. let nf1..Nold be the nulli\ufb01ers of the JoinSplit description for \ud835\udc56in 1..Nold: if nf\ud835\udc56is present in NullifierMap, add NullifierMap(nf\ud835\udc56) to SpentSet return (ReceivedSet, SpentSet). 4.22 Block Chain Scanning (Sapling and Orchard) In Sapling and Orchard, block chain scanning requires only the nk and ivk key components, rather than a spending key as in Sprout. Typically, these components are derived from a full viewing key (section4.2.2 Sapling Key Components on page 36 or section4.2.3 Orchard Key Components on page 38). Let \u2113PRFnfSapling be as de\ufb01ned in section5.3 Constants on page 74. Let Note be NoteSapling or NoteOrchard as de\ufb01ned in section3.2 Notes on page 14. Let KA be either KASapling as de\ufb01ned in section5.4.5.3 on page 89, or KAOrchard as de\ufb01ned in section5.4.5.5 on page 90. Let NullifierType be BY\u2113PRFnfSapling8 for Sapling, or F\ud835\udc5eP for Orchard.",
      "The following algorithm can be used, given the block chain and (nk, ivk), to obtain each note sent to the corre- sponding shielded payment address, its memo \ufb01eld, and its \ufb01nal status (spent or unspent). let mutable ReceivedSet Note  BY512) let mutable SpentSet Note let mutable NullifierMap (NullifierType Note) the empty mapping for each transaction tx: for each Output description or Action description in tx: Attempt to decrypt the transmitted note ciphertext components epk and Cenc using ivk with the algorithm section4.20.2 Decryption using an Incoming Viewing Key (Sapling and Orchard) on page 68. If this succeeds with (n, memo): Add (n, memo) to ReceivedSet. Calculate the nulli\ufb01er nf of n using nk as in section4.16 Computing \u03c1 values and Nulli\ufb01ers on page 57. (This also requires pos from the Output description for Sapling notes.) Add the mapping nf n to NullifierMap.",
      "for each nulli\ufb01er nf of a Spend description or Action description in tx: if nf is present in NullifierMap, add NullifierMap(nf) to SpentSet return (ReceivedSet, SpentSet). Non-normative notes:  The above algorithm does not use the ovk key component, or the Cout transmitted note ciphertext component. When scanning the whole block chain, these are indeed not necessary. The advantage of supporting decryption using ovk as described in section4.20.3 Decryption using an Outgoing Viewing Key (Sapling and Orchard) on page 70, is that it allows recovering information about the note plaintexts sent in a transaction from that trans- action alone. When scanning only part of a block chain, it may be useful to augment the above algorithm with decryption of Cout components for each transaction, in order to obtain information about notes that were spent in the scanned period but received outside it.",
      "The above algorithm does not detect notes that were sent out-of-band or with incorrect transmitted note ciphertexts. It is possible to detect whether such notes were spent only if their nulli\ufb01ers are known. Concrete Protocol Integers, Bit Sequences, and Endianness All integers in Zcash-speci\ufb01c encodings are unsigned, have a \ufb01xed bit length, and are encoded in little-endian byte order unless otherwise speci\ufb01ed. The following functions convert between sequences of bits, sequences of bytes, and integers:  I2LEBSP N)  0 .. 2\u21131 B\u2113, such that I2LEBSP\u2113(\ud835\udc65) is the sequence of \u2113bits representing \ud835\udc65in little-endian order;  I2LEOSP N)  0 .. 2\u21131 BYceiling(\u21138), such that I2LEBSP\u2113(\ud835\udc65) is the sequence of ceiling(\u21138) bytes representing \ud835\udc65in little-endian order;  I2BEBSP N)  0 .. 2\u21131 B\u2113 such that I2BEBSP\u2113(\ud835\udc65) is the sequence of \u2113bits representing \ud835\udc65in big-endian order. LEBS2IP N)  B\u2113 0 .. 2\u21131 such that LEBS2IP\u2113(\ud835\udc46) is the integer represented in little-endian order by the bit sequence \ud835\udc46of length \u2113.",
      "LEOS2IP N  \u2113mod 8  0)  BY\u21138 0 .. 2\u21131 such that LEOS2IP\u2113(\ud835\udc46) is the integer represented in little-endian order by the byte sequence \ud835\udc46of length \u21138. BEOS2IP N  \u2113mod 8  0)  BY\u21138 0 .. 2\u21131 such that BEOS2IP\u2113(\ud835\udc46) is the integer represented in big-endian order by the byte sequence \ud835\udc46of length \u21138. LEBS2OSP N)  B\u2113 BYceiling(\u21138) de\ufb01ned as follows: pad the input on the right with 8  ceiling(\u21138) \u2113 zero bits so that its length is a multiple of 8 bits. Then convert each group of 8 bits to a byte value with the least signi\ufb01cant bit \ufb01rst, and concatenate the resulting bytes in the same order as the groups. LEOS2BSP N  \u2113mod 8  0)  BYceiling(\u21138) B\u2113 de\ufb01ned as follows: convert each byte to a group of 8 bits with the least signi\ufb01cant bit \ufb01rst, and concatenate the resulting groups in the same order as the bytes. Bit layout diagrams We sometimes use bit layout diagrams, in which each box of the diagram represents a sequence of bits.",
      "Diagrams are read from left to right, with lines read from top to bottom; the breaking of boxes across lines has no signi\ufb01cance. The bit length \u2113is given explicitly in each box, except when it is obvious (e.g. for a single bit, or for the notation 0\u2113 representing the sequence of \u2113zero bits, or for the output of LEBS2OSP\u2113). The entire diagram represents the sequence of bytes formed by \ufb01rst concatenating these bit sequences, and then treating each subsequence of 8 bits as a byte with the bits ordered from most signi\ufb01cant to least signi\ufb01cant. Thus the most signi\ufb01cant bit in each byte is toward the left of a diagram. (This convention is used only in descriptions of the Sprout design; in the Sapling and Orchard additions, bit sequencebyte sequence conversions are always speci\ufb01ed explicitly.) Where bit \ufb01elds are used, the text will clarify their position in each case.",
      "Constants De\ufb01ne: MerkleDepthSprout  N : 29 MerkleDepthSapling  N : 32 MerkleDepthOrchard  N : 32 \u2113Sprout Merkle N : 256 \u2113Sapling Merkle N : 255 \u2113Orchard Merkle N : 255 Nold N : 2 Nnew N : 2 \u2113value N : 64 \u2113hSig N : 256 \u2113Sprout N : 256 \u2113PRFexpand N : 512 \u2113PRFnfSapling N : 256 \u2113Sprout N : 256 \u2113Seed N : 256 \u2113ask N : 252 \u2113Sprout N : 252 N : 256 N : 88 N : 256 \u2113Sapling N : 251 \u2113ovk N : 256 \u2113Sapling scalar N : 252 \u2113Orchard scalar N : 255 \u2113Orchard base N : 255 UncommittedSprout  B\u2113Sprout Merkle : 0\u2113Sprout Merkle UncommittedSapling  B\u2113Sapling Merkle  : I2LEBSP\u2113Sapling Merkle (1) UncommittedOrchard  0 ..",
      "\ud835\udc5eP 1 : 2 MAX_MONEY N : 2.11015 (zatoshi) BlossomActivationHeight N : 653600, for Mainnet 584000, for Testnet CanopyActivationHeight N : 1046400, for Mainnet 1028500, for Testnet ZIP212GracePeriod N : 32256 NUFiveActivationHeight N : 1687104, for Mainnet 1842420, for Testnet SlowStartInterval N : 20000 PreBlossomHalvingInterval N : 840000 MaxBlockSubsidy N : 1.25109 (zatoshi) NumFounderAddresses N : 48 FoundersFraction Q : 1 PoWLimit N : 2243 1, for Mainnet 2251 1, for Testnet PoWAveragingWindow N : 17 PoWMedianBlockSpan N : 11 PoWMaxAdjustDown Q : PoWMaxAdjustUp Q : PoWDampingFactor N : 4 PreBlossomPoWTargetSpacing N : 150 (seconds). PostBlossomPoWTargetSpacing N : 75 (seconds). Concrete Cryptographic Schemes 5.4.1 Hash Functions 5.4.1.1 SHA-256, SHA-256d, SHA256Compress, and SHA-512 Hash Functions SHA-256 and SHA-512 are de\ufb01ned by NIST2015. Zcash uses the full SHA-256 hash function to instantiate NoteCommitmentSprout.",
      "SHA-256 BYN BY32 NIST2015 strictly speaking only speci\ufb01es the application of SHA-256 to messages that are bit sequences, producing outputs (message digests) that are also bit sequences. In practice, SHA-256 is universally implemented with a byte-sequence interface for messages and outputs, such that the most signi\ufb01cant bit of each byte corresponds to the \ufb01rst bit of the associated bit sequence. (In the NIST speci\ufb01cation \ufb01rst is con\ufb02ated with leftmost.) SHA-256d, de\ufb01ned as a double application of SHA-256, is used to hash block headers: SHA-256d BYN BY32 Zcash also uses the SHA-256 compression function, SHA256Compress. This operates on a single 512-bit block and excludes the padding step speci\ufb01ed in NIST2015, section 5.1. That is, the input to SHA256Compress is what NIST2015, section 5.2 refers to as the message and its padding. The Initial Hash Value is the same as for full SHA-256. SHA256Compress is used to instantiate several Pseudo Random Functions and MerkleCRHSprout.",
      "SHA256Compress B512 B256 The ordering of bits within words in the interface to SHA256Compress is consistent with NIST2015, section 3.1, i.e. big-endian. Ed25519 uses SHA-512: SHA-512 BYN BY64 The comment above concerning bit vs byte-sequence interfaces also applies to SHA-512. 5.4.1.2 BLAKE2 Hash Functions BLAKE2 is de\ufb01ned by ANWW2013. Zcash uses both the BLAKE2b and BLAKE2s variants. BLAKE2b-\u2113(\ud835\udc5d, \ud835\udc65) refers to unkeyed BLAKE2b-\u2113in sequential mode, with an output digest length of \u21138 bytes, 16-byte personalization string \ud835\udc5d, and input \ud835\udc65. BLAKE2b is used to instantiate hSigCRH, EquihashGen, and KDFSprout. From Overwinter onward, it is used to compute SIGHASH transaction hashes as speci\ufb01ed in ZIP-143, or as in ZIP-243 after Sapling activation, or as in ZIP-244 for version 5 transactions. For Sapling, it is also used to instantiate PRFexpand, PRFockSapling, KDFSapling, and in the RedJubjub signature scheme which instantiates SpendAuthSigSapling and BindingSigSapling.",
      "BLAKE2b-\u2113 BY16  BYN BY\u21138 Note: BLAKE2b-\u2113is not the same as BLAKE2b-512 truncated to \u2113bits, because the digest length is encoded in the parameter block. BLAKE2s-\u2113(\ud835\udc5d, \ud835\udc65) refers to unkeyed BLAKE2s-\u2113in sequential mode, with an output digest length of \u21138 bytes, 8-byte personalization string \ud835\udc5d, and input \ud835\udc65. BLAKE2s is used to instantiate PRFnfSapling, CRHivk, and GroupHashJ(\ud835\udc5f) BLAKE2s-\u2113 BY8  BYN BY\u21138 5.4.1.3 Merkle Tree Hash Function MerkleCRHSprout and MerkleCRHSapling and MerkleCRHOrchard are used to hash incremental Merkle tree hash values for Sprout and Sapling and Orchard respectively. MerkleCRHSprout Hash Function MerkleCRHSprout  0 .. MerkleDepthSprout 1  B\u2113Sprout Merkle  B\u2113Sprout Merkle B\u2113Sprout Merkle is de\ufb01ned as follows: MerkleCRHSprout(layer, left, right) : SHA256Compress 256-bit left 256-bit right SHA256Compress is de\ufb01ned in section5.4.1.1 SHA-256, SHA-256d, SHA256Compress, and SHA-512 Hash Functions on page 75.",
      "Security requirement: SHA256Compress must be collision-resistant, and it must be infeasible to \ufb01nd a preimage \ud835\udc65such that SHA256Compress(\ud835\udc65)  0256. Notes:  The layer argument does not affect the output. SHA256Compress is not the same as the SHA-256 function, which hashes arbitrary-length byte sequences. MerkleCRHSapling Hash Function Let PedersenHash be as speci\ufb01ed in section5.4.1.7 Pedersen Hash Function on page 79. MerkleCRHSapling  0 .. MerkleDepthSapling 1  B\u2113Sapling Merkle   B\u2113Sapling Merkle  B\u2113Sapling Merkle  is de\ufb01ned as follows: MerkleCRHSapling(layer, left, right) : PedersenHash(Zcash_PH, \ud835\udc59 left right) where \ud835\udc59 I2LEBSP6 MerkleDepthSapling 1 layer Security requirement: PedersenHash must be collision-resistant . Note: The pre\ufb01x \ud835\udc59provides domain separation between inputs at different layers of the note commitment tree.",
      "NoteCommitSapling, like PedersenHash, is de\ufb01ned in terms of PedersenHashToPoint, but using a pre\ufb01x that cannot collide with a layer pre\ufb01x, as noted in section5.4.8.2 Windowed Pedersen commitments on page 96. MerkleCRHOrchard Hash Function Let SinsemillaHash be as speci\ufb01ed in section5.4.1.9 Sinsemilla Hash Function on page 81. MerkleCRHOrchard  0 .. MerkleDepthOrchard 1  0 .. \ud835\udc5eP 1  0 .. \ud835\udc5eP 1 0 .. \ud835\udc5eP 1 is de\ufb01ned as follows: MerkleCRHOrchard(layer, left, right) : if hash   hash, otherwise where hash  SinsemillaHash(z.cash:Orchard-MerkleCRH, \ud835\udc59 left right) \ud835\udc59 I2LEBSP10 MerkleDepthOrchard 1 layer left I2LEBSP\u2113Orchard Merkle left right I2LEBSP\u2113Orchard Merkle right Security requirements:  SinsemillaHash must be collision-resistant. It must be infeasible to \ufb01nd a input of length 10  2  \u2113Orchard Merkle bits to SinsemillaHash that yields output . Note: The pre\ufb01x \ud835\udc59provides domain separation between inputs at different layers of the note commitment tree.",
      "5.4.1.4 hSig Hash Function hSigCRH is used to compute the value hSig in section4.3 JoinSplit Descriptions on page 39. hSigCRH(randomSeed, nfold 1..Nold, joinSplitPubKey) : BLAKE2b-256(ZcashComputehSig, hSigInput) where hSigInput : 256-bit randomSeed 256-bit nfold 256-bit nfold Nold 256-bit joinSplitPubKey . BLAKE2b-256(\ud835\udc5d, \ud835\udc65) is de\ufb01ned in section5.4.1.2 BLAKE2 Hash Functions on page 76. Security requirement: BLAKE2b-256(ZcashComputehSig, \ud835\udc65) must be collision-resistant on \ud835\udc65. 5.4.1.5 CRHivk Hash Function CRHivk is used to derive the incoming viewing key ivk for a Sapling shielded payment address. For its use when generating an address see section4.2.2 Sapling Key Components on page 36, and for its use in the Spend statement see section4.18.2 Spend Statement (Sapling) on page 61.",
      "It is de\ufb01ned as follows: CRHivk(ak, nk) : LEOS2IP256(BLAKE2s-256(Zcashivk, crhInput)) mod 2\u2113Sapling where crhInput : LEBS2OSP256(ak) LEBS2OSP256(nk) BLAKE2s-256(\ud835\udc5d, \ud835\udc65) is de\ufb01ned in section5.4.1.2 BLAKE2 Hash Functions on page 76. Security requirement: LEOS2IP256(BLAKE2s-256(Zcashivk, \ud835\udc65)) mod 2\u2113Sapling must be collision-resistant on a 64- byte input \ud835\udc65. Note that this does not follow from collision resistance of BLAKE2s-256 (and the best possible concrete security is that of a 251-bit hash rather than a 256-bit hash), but it is a reasonable assumption given the design, structure, and cryptanalysis to date of BLAKE2s. Non-normative note: BLAKE2s has a variable output digest length feature, but it does not support arbitrary bit lengths, otherwise it would have been used rather than external truncation. However, the protocol-speci\ufb01c personalization string together with truncation achieve essentially the same effect as using that feature.",
      "5.4.1.6 DiversifyHashSapling and DiversifyHashOrchard Hash Functions DiversifyHashSapling  B\u2113d J(\ud835\udc5f)  is used to derive a diversi\ufb01ed base in section4.2.2 Sapling Key Components on page 36. Let GroupHashJ(\ud835\udc5f) and \ud835\udc48be as de\ufb01ned in section5.4.9.5 Group Hash into Jubjub on page 104. De\ufb01ne DiversifyHashSapling(d) : GroupHashJ(\ud835\udc5f) Zcash_gd, LEBS2OSP\u2113d(d) DiversifyHashOrchard  B\u2113d P is used to derive a diversi\ufb01ed base in section4.2.3 Orchard Key Components on page 38. Let GroupHashP be as de\ufb01ned in section5.4.9.8 Group Hash into Pallas and Vesta on page 107. De\ufb01ne DiversifyHashOrchard(d) : GroupHashP(z.cash:Orchard-gd, ), if \ud835\udc43 \ud835\udcaaP otherwise where \ud835\udc43 GroupHashP( z.cash:Orchard-gd, LEBS2OSP\u2113d(d) The following security property and notes apply to both Sapling and Orchard.",
      "Security requirement: Unlinkability: Given two randomly selected shielded payment addresses from different spend authorities, and a third shielded payment address which could be derived from either of those authorities, such that the three addresses use different diversi\ufb01ers, it is not possible to tell which authority the third address was derived from. Non-normative notes:  Suppose that GroupHashJ(\ud835\udc5f) (restricted to inputs for which it does not return ) is modelled as a random oracle from diversi\ufb01ers to points of order \ud835\udc5fJ on the Jubjub curve. In this model, Unlinkability of DiversifyHashSapling holds under the Decisional Dif\ufb01eHellman assumption on the prime-order subgroup of points on the Jubjub curve. To prove this, consider the ElGamal encryption scheme ElGamal1985 on this prime-order subgroup, re- stricted to encrypting plaintexts encoded as the group identity \ud835\udcaaJ.",
      "(ElGamal was originally de\ufb01ned for F but works in any prime-order group.) ElGamal public keys then have the same form as diversi\ufb01ed payment addresses. If we make the assumption above on GroupHashJ(\ud835\udc5f) , then generating a new diversi\ufb01ed payment address from a given address pk, gives the same distribution of (gd , pkd ) pairs as the distribution of ElGamal ciphertexts obtained by encrypting \ud835\udcaaJ under pk. TODO: check whether this is justi\ufb01ed. Then, the de\ufb01nition of key privacy (IK-CPA as de\ufb01ned in BBDP2001, De\ufb01nition 1) for ElGamal corresponds to the de\ufb01nition of Unlinkability for DiversifyHashSapling. (IK-CCA corresponds to the potentially stronger requirement that DiversifyHashSapling remains Unlinkable when given Dif\ufb01eHellman key agreement oracles for each of the candidate diversi\ufb01ed payment addresses.) So if ElGamal is key-private, then DiversifyHashSapling is Unlinkable under the same conditions.",
      "BBDP2001, Appendix A gives a security proof for key privacy (both IK-CPA and IK-CCA) of ElGamal under the Decisional Dif\ufb01eHellman assumption on the relevant group. (In fact the proof needed is the small modi\ufb01cation described in the last paragraph in which the generator is chosen at random for each key.)  It is assumed (also for the security of other uses of the group hash, such as Pedersen hashes and commitments) that the discrete logarithm of the output group element with respect to any other generator is unknown. This assumption is justi\ufb01ed if the group hash acts as a random oracle. Essentially, diversi\ufb01ers act as handles to unknown random numbers.",
      "(The group hash inputs used with different personalizations are in different namespaces.)  Informally, the random self-reducibility property of DDH implies that an adversary would gain no advantage from being able to query an oracle for additional (gd, pkd) pairs with the same spending authority as an existing shielded payment address, since they could also create such pairs on their own. This justi\ufb01es only considering two shielded payment addresses in the security de\ufb01nition. TODO: FIXME This is not correct, because additional pairs dont quite follow the same distribution as an address with a valid diversi\ufb01er. The security de\ufb01nition may need to be more complex to model this properly. An 88-bit diversi\ufb01er cannot be considered cryptographically unguessable at a 128-bit security level; also, randomly chosen diversi\ufb01ers are likely to suffer birthday collisions when the number of choices approaches 244.",
      "If most users are choosing diversi\ufb01ers randomly (as recommended in section4.2.2 Sapling Key Components on page 36), then the fact that they may accidentally choose diversi\ufb01ers that collide (and therefore reveal the fact that they are not derived from the same incoming viewing key) does not appreciably reduce the anonymity set. In ZIP-32 and section4.2.3 Orchard Key Components on page 38 an 88-bit Pseudo Random Permutation, keyed differently for each node of the derivation tree, is used to select new diversi\ufb01ers. This resolves the potential problem, provided that the input to the Pseudo Random Permutation does not repeat for a given node. If the holder of an incoming viewing key permits an adversary to ask for a new address for that incoming viewing key with a given diversi\ufb01er, then it can trivially break Unlinkability for the other diversi\ufb01ed payment addresses associated with the incoming viewing key (this does not compromise other privacy properties).",
      "Implementations SHOULD avoid providing such a chosen diversi\ufb01er oracle. 5.4.1.7 Pedersen Hash Function PedersenHash is an algebraic hash function with collision resistance (for \ufb01xed input length) derived from assumed hardness of the Discrete Logarithm Problem on the Jubjub curve. It is based on the work of David Chaum, Ivan Damg\u00e5rd, Jeroen van de Graaf, Jurgen Bos, George Purdy, Eug\u00e8ne van Heijst and Birgit P\ufb01tzmann in CDvdG1987, BCP1988 and CvHP1991, and of Mihir Bellare, Oded Goldreich, and Sha\ufb01Goldwasser in BGG1995, with optimiza- tions for ef\ufb01cient instantiation in zk-SNARK circuits by Sean Bowe and Daira-Emma Hopwood. PedersenHash is used in the de\ufb01nitions of Pedersen commitments (section5.4.8.2 Windowed Pedersen commitments on page 96), and of the Pedersen hash for the Sapling incremental Merkle tree (section5.4.1.3 MerkleCRHSapling Hash Function on page 76). Let J, J(\ud835\udc5f), \ud835\udcaaJ, \ud835\udc5eJ, \ud835\udc5fJ, \ud835\udc4eJ, and \ud835\udc51J be as de\ufb01ned in section5.4.9.3 Jubjub on page 102.",
      "Let ExtractJ(\ud835\udc5f) J(\ud835\udc5f) B\u2113Sapling Merkle  be as de\ufb01ned in section5.4.9.4 Coordinate Extractor for Jubjub on page 104. Let FindGroupHashJ(\ud835\udc5f) be as de\ufb01ned in section5.4.9.5 Group Hash into Jubjub on page 104. Let UncommittedSapling be as de\ufb01ned in section5.3 Constants on page 74. Let \ud835\udc50be the largest integer such that 4  24\ud835\udc501 \ud835\udc5fJ 1 , i.e. \ud835\udc50: 63. De\ufb01ne \u2110 BY8  N J(\ud835\udc5f) by: \u2110(\ud835\udc37, \ud835\udc56) : FindGroupHashJ(\ud835\udc5f)( 32-bit \ud835\udc561 De\ufb01ne PedersenHashToPoint(\ud835\udc37 BY8, \ud835\udc40 BN) J(\ud835\udc5f) as follows: Pad \ud835\udc40to a multiple of 3 bits by appending zero bits, giving \ud835\udc40. Let \ud835\udc5b ceiling (length(\ud835\udc40) 3  \ud835\udc50 Split \ud835\udc40 into \ud835\udc5bsegments \ud835\udc401 .. \ud835\udc5bso that \ud835\udc40  concatB(\ud835\udc401 .. \ud835\udc5b), and each of \ud835\udc401 .. \ud835\udc5b1 is of length 3\ud835\udc50bits. (\ud835\udc40\ud835\udc5bmay be shorter.) Return \ud835\udc5b \ud835\udc561\ud835\udc40\ud835\udc56 \u2110(\ud835\udc37, \ud835\udc56) J(\ud835\udc5f). where  B31 .. \ud835\udc50  \ud835\udc5fJ1 .. \ud835\udc5fJ1 0 is de\ufb01ned as: Let \ud835\udc58\ud835\udc56 length(\ud835\udc40\ud835\udc56)3. Split \ud835\udc40\ud835\udc56into 3-bit chunks \ud835\udc5a1 .. \ud835\udc58\ud835\udc56so that \ud835\udc40\ud835\udc56 concatB(\ud835\udc5a1 .. \ud835\udc58\ud835\udc56). Write each \ud835\udc5a\ud835\udc57as \ud835\udc60\ud835\udc57 0, \ud835\udc60\ud835\udc57 1, \ud835\udc60\ud835\udc57 2, and let enc(\ud835\udc5a\ud835\udc57)  (1 2\ud835\udc60\ud835\udc57 2)  (1  \ud835\udc60\ud835\udc57 0  2\ud835\udc60\ud835\udc57 Let \ud835\udc40\ud835\udc56 \ud835\udc58\ud835\udc56 \ud835\udc571enc(\ud835\udc5a\ud835\udc57)  24(\ud835\udc571).",
      "Finally, de\ufb01ne PedersenHash BY8  BN B\u2113Sapling Merkle  by: PedersenHash(\ud835\udc37, \ud835\udc40) : ExtractJ(\ud835\udc5f) PedersenHashToPoint(\ud835\udc37, \ud835\udc40) See sectionA.3.3.9 Pedersen hash on page 210 for rationale and ef\ufb01cient circuit implementation of these functions. Security requirement: PedersenHash and PedersenHashToPoint are required to be collision-resistant between inputs of \ufb01xed length, for a given personalization input \ud835\udc37. No other security properties commonly associated with hash functions are needed. Non-normative note: These hash functions are not collision-resistant for variable-length inputs. Theorem 5.4.1. The encoding function is injective. Proof. We \ufb01rst check that the range of enc(\ud835\udc5a\ud835\udc57)  24(\ud835\udc571) is a subset of the allowable range \ud835\udc5fJ1 .. \ud835\udc5fJ1 0. The range of this expression is a subset of \u0394 ..",
      "\u0394 0 where \u0394  4  24(\ud835\udc561)  4  24\ud835\udc501 When \ud835\udc50 63, we have 4  24\ud835\udc501  0x444444444444444444444444444444444444444444444444444444444444444 \ud835\udc5fJ 1  0x73EDA753299D7D483339D80809A1D8053341049E6640841684B872F6B7B965B so the required condition is met. This implies that there is no wrap around and so \ud835\udc58\ud835\udc56 \ud835\udc571enc(\ud835\udc5a\ud835\udc57)  24(\ud835\udc571) may be treated as an integer expression. enc is injective. In orderto prove that is injective, consider\u0394 B31 .. \ud835\udc50 0 .. 2\u0394 such that \ud835\udc40\ud835\udc56\u0394  \ud835\udc40\ud835\udc56\u0394. With \ud835\udc58\ud835\udc56and \ud835\udc5a\ud835\udc57de\ufb01ned as above, we have \ud835\udc40\ud835\udc56\u0394  \ud835\udc58\ud835\udc56 \ud835\udc571enc(\ud835\udc5a\ud835\udc57)  24(\ud835\udc571) where enc(\ud835\udc5a\ud835\udc57)  enc(\ud835\udc5a\ud835\udc57)  4 is in 0 .. 8 and enc is injective. Express this sum in hexadecimal; then each \ud835\udc5a\ud835\udc57affects only one hex digit, and it is easy to see that \u0394 is injective. Therefore so is . Since the security proof from BGG1995, Appendix A depends only on the encoding being injective and its range not including zero, the proof can be adapted straightforwardly to show that PedersenHashToPoint is collision-resistant under the same assumptions and security bounds.",
      "Because ExtractJ(\ud835\udc5f) is injective, it follows that PedersenHash is equally collision-resistant . 5.4.1.8 Mixing Pedersen Hash Function A mixing Pedersen hash is used to compute \u03c1 from cm and pos in section4.16 Computing \u03c1 values and Nulli\ufb01ers on page 57. It takes as input a Pedersen commitment \ud835\udc43, and hashes it with another input \ud835\udc65. De\ufb01ne \ud835\udca5Sapling : FindGroupHashJ(\ud835\udc5f) (Zcash_J_, ). We de\ufb01ne MixingPedersenHash J  0 .. \ud835\udc5fJ 1 J by: MixingPedersenHash(\ud835\udc43, \ud835\udc65) : \ud835\udc43 \ud835\udc65 \ud835\udca5Sapling. Security requirement: The function (\ud835\udc5f, \ud835\udc40, \ud835\udc65) 0 .. \ud835\udc5fJ 1  BN  0 .. \ud835\udc5fJ 1 MixingPedersenHash(WindowedPedersenCommit\ud835\udc5f(\ud835\udc40), \ud835\udc65) must be collision-resistant on (\ud835\udc5f, \ud835\udc40, \ud835\udc65). See sectionA.3.3.10 Mixing Pedersen hash on page 212 for ef\ufb01cient circuit implementation of this function. 5.4.1.9 Sinsemilla Hash Function SinsemillaHash is an algebraic hash function with collision resistance (for \ufb01xed input length) derived from assumed hardness of the Discrete Logarithm Problem. It is designed by Sean Bowe and Daira-Emma Hopwood.",
      "The motivation for introducing a new discrete-logarithm-based hash function (rather than using PedersenHash) is to make ef\ufb01cient use of the lookups available in recent proof systems including Halo 2. SinsemillaHash is used in the de\ufb01nition of SinsemillaCommit (section5.4.8.4 Sinsemilla commitments on page 98), and for the Orchard incremental Merkle tree (section5.4.1.3 MerkleCRHOrchard Hash Function on page 77). Let P, \ud835\udcaaP, \ud835\udc5eP, \ud835\udc5fP, and \ud835\udc4fP be as de\ufb01ned in section5.4.9.6 Pallas and Vesta on page 105. Let Extract P  0 .. \ud835\udc5eP 1  be as de\ufb01ned in section5.4.9.7 Coordinate Extractor for Pallas on page 106. Let GroupHashP be as de\ufb01ned in section5.4.9.8 Group Hash into Pallas and Vesta on page 107. Let UncommittedOrchard be as de\ufb01ned in section5.3 Constants on page 74. Let I2LEOSP N)  0 .. 2\u21131 BYceiling(\u21138) and LEBS2IP N)  B\u2113 0 .. 2\u21131 be as de\ufb01ned in section5.1 Integers, Bit Sequences, and Endianness on page 73. Let \ud835\udc58: 10. Let \ud835\udc50be the largest integer such that 2\ud835\udc50\ud835\udc5fP 1 , i.e. \ud835\udc50: 253.",
      "De\ufb01ne \ud835\udcac BYN P and \ud835\udcae 0 .. 2\ud835\udc581 P by: \ud835\udcac(\ud835\udc37) : GroupHashP( z.cash:SinsemillaQ, \ud835\udc37 \ud835\udcae(\ud835\udc57) : GroupHashP( z.cash:SinsemillaS, I2LEOSP32(\ud835\udc57) De\ufb01ne  P   P  P  as incomplete addition on the Pallas curve:    \ud835\udc43 \ud835\udc43  \ud835\udcaaP  \ud835\udcaaP \ud835\udcaaP  (\ud835\udc65, \ud835\udc66)   (\ud835\udc65, \ud835\udc66)  \ud835\udcaaP (\ud835\udc65, \ud835\udc66)  (\ud835\udc65, \ud835\udc66)  if \ud835\udc65 \ud835\udc65 (\ud835\udc65, \ud835\udc66)  (\ud835\udc65, \ud835\udc66), otherwise. De\ufb01ne pad(\ud835\udc5b 0 .. \ud835\udc50, \ud835\udc40 B\ud835\udc5b(\ud835\udc581)1 .. \ud835\udc5b\ud835\udc58) 0 .. 2\ud835\udc581\ud835\udc5b as follows: pad \ud835\udc40to \ud835\udc5b \ud835\udc58bits by appending zero bits, giving \ud835\udc40padded. split \ud835\udc40padded into \ud835\udc5bpieces \ud835\udc40pieces 1 .. \ud835\udc5b, each of length \ud835\udc58bits, so that \ud835\udc40padded  concatB(\ud835\udc40pieces 1 .. \ud835\udc5b). return  LEBS2IP\ud835\udc58(\ud835\udc40pieces ) for \ud835\udc56from 1 up to \ud835\udc5b. De\ufb01ne SinsemillaHashToPoint(\ud835\udc37 BYN, \ud835\udc40 B0 .. \ud835\udc58\ud835\udc50) P  as follows: let \ud835\udc5b 0 .. \ud835\udc50  ceiling (length(\ud835\udc40) let \ud835\udc5a pad\ud835\udc5b(\ud835\udc40) let mutable Acc \ud835\udcac(\ud835\udc37) for \ud835\udc56from 1 up to \ud835\udc5b: set Acc  Acc  \ud835\udcae(\ud835\udc5a\ud835\udc56)  Acc return Acc. Finally, de\ufb01ne SinsemillaHash BYN  B0 .. \ud835\udc58\ud835\udc50 0 .. \ud835\udc5eP 1  by: SinsemillaHash(\ud835\udc37, \ud835\udc40) : Extract SinsemillaHashToPoint(\ud835\udc37, \ud835\udc40) See Zcash-Orchard, section Sinsemilla for rationale and ef\ufb01cient circuit implementation of these functions.",
      "Security requirement: SinsemillaHash and SinsemillaHashToPoint are required to be collision-resistant between inputs of \ufb01xed length, for a given personalization input \ud835\udc37. It must also be infeasible to \ufb01nd inputs (\ud835\udc37, \ud835\udc40) such that SinsemillaHashToPoint(\ud835\udc37, \ud835\udc40)  . No other security properties commonly associated with hash functions are needed. Non-normative notes:  These hash functions are not collision-resistant across variable-length inputs for the same \ud835\udc37(that is, it is assumed that a single input length will be used for any given \ud835\udc37). The intermediate value 2 GroupHashP( z.cash:SinsemillaQ, \ud835\udc37 for the \ufb01rst iteration of the loop can be precomputed, if \ud835\udc37is known in advance. Security argument We show a correspondence between Sinsemilla and a vector Pedersen hash, which allows using the security argument from BGG1995 to show that collision-resistance can be tightly reduced to the Discrete Logarithm Problem in P. De\ufb01ne \ud835\udeff(\ud835\udc4e, \ud835\udc4f)  0, if \ud835\udc4e \ud835\udc4f 1, if \ud835\udc4e \ud835\udc4f. Lemma 5.4.2.",
      "An injectivity property for Sinsemilla. Let \ud835\udc5b 0 .. \ud835\udc50, and consider a sequence of message pieces \ud835\udc5a 0 .. 2\ud835\udc581\ud835\udc5b. Collect the scalars by which each generator \ud835\udcae(\ud835\udc57) is multiplied in the algorithm for SinsemillaHashToPoint: De\ufb01ne \ud835\udf12(\ud835\udc5a)  2\ud835\udc5b\ud835\udc56 \ud835\udeff(\ud835\udc5a\ud835\udc56, \ud835\udc57) (mod \ud835\udc5fP) for \ud835\udc57from 0 up to 2\ud835\udc581 The mapping \ud835\udc5a 0 .. 2\ud835\udc581\ud835\udc5b \ud835\udf12(\ud835\udc5a) F\ud835\udc5fP 2\ud835\udc58 is injective. Proof. There is an injective mapping from \ud835\udc5ato the matrix of bits with 2\ud835\udc58columns and \ud835\udc5brows, such that the bit at (1-based) column \ud835\udc57 1 and row \ud835\udc56is set if and only if \ud835\udc5a\ud835\udc56 \ud835\udc57. Then the binary representations of the elements of \ud835\udf12(\ud835\udc5a) are given by the columns of this matrix, and they do not over\ufb02ow due to the requirement that 2\ud835\udc5b2\ud835\udc50\ud835\udc5fP1 The claim follows. Theorem 5.4.3. Collision resistance of SinsemillaHash and SinsemillaHashToPoint. Let \ud835\udc37 BYN be a personalization input, and let \u2113 0 .. \ud835\udc58 \ud835\udc50.",
      "Finding a collision \ud835\udc40, \ud835\udc40 B\u2113 with \ud835\udc40 \ud835\udc40 such that SinsemillaHashToPoint(\ud835\udc37, \ud835\udc40)  SinsemillaHashToPoint(\ud835\udc37, \ud835\udc40)  ef\ufb01ciently yields a nontrivial discrete logarithm relation, and similarly for SinsemillaHash(\ud835\udc37, \ud835\udc40)  SinsemillaHash(\ud835\udc37, \ud835\udc40)  . Proof. Without loss of generality we can restrict to the case where \u2113is a multiple of \ud835\udc58: since pad\ud835\udc5bis injective on inputs of a given bit length, collision resistance for \u2113 \ud835\udc5b \ud835\udc58bits implies collision resistance for each length that pads to \ud835\udc5b \ud835\udc58bits. Since \u21130 .. \ud835\udc58 \ud835\udc50 we have \ud835\udc5b0 .. \ud835\udc50. Then whenever SinsemillaHashToPoint(\ud835\udc37, \ud835\udc40)  , SinsemillaHashToPoint(\ud835\udc37, \ud835\udc40)  2\ud835\udc5b \ud835\udcac(\ud835\udc37)  2\ud835\udc581 \ud835\udc570 \ud835\udf12(\ud835\udc5a)\ud835\udc571 \ud835\udcae(\ud835\udc57), where \ud835\udc5a pad\ud835\udc5b(\ud835\udc40). (The \ud835\udc57 1 is just because sequence indices are 1-based.) This is a Pedersen vector hash of the \ud835\udf12(\ud835\udc5a) elements, with a \ufb01xed offset 2\ud835\udc5b \ud835\udcac(\ud835\udc37). The \ufb01xed offset does not affect collision resistance in this context.",
      "(See below for why it cannot be eliminated for SinsemillaHash, or when using incomplete addition.) Theorem 5.4.4 on page 84 will prove that a output from SinsemillaHashToPoint yields a nontrivial discrete log relation. It follows that the collision resistance of SinsemillaHashToPoint can be tightly reduced, via the proof in BGG1995, Appendix A, to the Discrete Logarithm Problem over P. Note that BGG1995 requires for their main scheme that the scalars are nonzero, which is not necessarily the case in our context. However, their proof in Appendix A does not depend on this, given that \ud835\udc5bis \ufb01xed. The restriction that scalars are nonzero appears to have been motivated by wanting to support variable-length messages and incremental hashing, which we do not. Now we consider SinsemillaHash.",
      "We want to prove that, for given \ud835\udc37, if we can \ufb01nd two distinct messages \ud835\udc40and \ud835\udc40 such that Extract SinsemillaHashToPoint(\ud835\udc37, \ud835\udc40)  Extract SinsemillaHashToPoint(\ud835\udc37, \ud835\udc40)  then we can ef\ufb01ciently extract a discrete logarithm. The inputs to Extract P are not , therefore they are in P. Extract P maps \ud835\udc43, \ud835\udc44P to the same output if and only if \ud835\udc43 \ud835\udc44. So either SinsemillaHashToPoint(\ud835\udc37, \ud835\udc40)  SinsemillaHashToPoint(\ud835\udc37, \ud835\udc40) (in which case use the original Pedersen hash proof) or SinsemillaHashToPoint(\ud835\udc37, \ud835\udc40)  SinsemillaHashToPoint(\ud835\udc37, \ud835\udc40). In the latter case, let \ud835\udc5a pad\ud835\udc5b(\ud835\udc40) and \ud835\udc5a  pad\ud835\udc5b(\ud835\udc40), then we have 2\ud835\udc5b \ud835\udcac(\ud835\udc37)  2\ud835\udc581 \ud835\udc570 \ud835\udf12(\ud835\udc5a)\ud835\udc571 \ud835\udcae(\ud835\udc57)   2\ud835\udc5b \ud835\udcac(\ud835\udc37)  2\ud835\udc581 \ud835\udc570 \ud835\udf12(\ud835\udc5a)\ud835\udc571 \ud835\udcae(\ud835\udc57) 2\ud835\udc5b1 \ud835\udcac(\ud835\udc37)  2\ud835\udc581 \ud835\udc570 \ud835\udf12(\ud835\udc5a)\ud835\udc571  \ud835\udf12(\ud835\udc5a)\ud835\udc571 \ud835\udcae(\ud835\udc57)  0 Because 2\ud835\udc5b1 \ud835\udc5fP 1, the coef\ufb01cients (mod \ud835\udc5fP) are not all zero, and therefore this is a nontrivial discrete logarithm relation between independent bases.",
      "Non-normative notes:  JT2020, Lemma 3 proves a tight reduction from \ufb01nding a nontrivial discrete logarithm relation in a prime- order group to solving the Discrete Logarithm Problem in that group. The above theorem easily extends to the case where additional scalar multiplication terms with independent bases may be added to the SinsemillaHashToPoint output before applying Extract P. This is needed to show secu- rity of the SinsemillaShortCommit commitment scheme de\ufb01ned in section5.4.8.4 Sinsemilla commitments on page 98. It is also needed to show security of nulli\ufb01er derivation de\ufb01ned in section4.16 Computing \u03c1 values and Nulli\ufb01ers on page 57 against Faerie Gold attacks, as described in section8.4 Faerie Gold attack and \ufb01x on page 143. Assuming that GroupHashG acts as a random oracle, it can also be proven that SinsemillaHashToPoint and SinsemillaHash are collision-resistant across different personalization inputs (regardless of input length). Theorem 5.4.4.",
      "A output from SinsemillaHashToPoint yields a nontrivial discrete log relation. Proof. For convenience of reference, we repeat the algorithm for SinsemillaHashToPoint in terms of the message pieces \ud835\udc5a 0 .. 2\ud835\udc581\ud835\udc5b, with indexing of the intermediate values of Acc: let Acc0 \ud835\udcac(\ud835\udc37) for \ud835\udc56from 1 up to \ud835\udc5b: let Acc\ud835\udc56 Acc\ud835\udc561  \ud835\udcae(\ud835\udc5a\ud835\udc56)  Acc\ud835\udc561 return Acc\ud835\udc5b. We have an exceptional case if and only if Acc\ud835\udc561   \ud835\udcae(\ud835\udc5a\ud835\udc56) or Acc\ud835\udc561  \ud835\udcae(\ud835\udc5a\ud835\udc56)   Acc\ud835\udc561. (Since none of \ud835\udcac(\ud835\udc37) or \ud835\udcae(\ud835\udc57)  \ud835\udc570 .. 2\ud835\udc581 are \ud835\udcaaP, no intermediate results can be \ud835\udcaaP unless one of the preceding conditions occurs.) If Acc\ud835\udc561  \ud835\udcae(\ud835\udc5a\ud835\udc56)  Acc\ud835\udc561, then we have \ud835\udcae(\ud835\udc5a\ud835\udc56)  \ud835\udcaaP contrary to assumption. So exceptional cases occur only if \ud835\udefc Acc\ud835\udc561  \ud835\udcae(\ud835\udc5a\ud835\udc56)  \ud835\udcaaP for some \ud835\udc561 .. \ud835\udc5b, and \ud835\udefc 1 (for the case Acc\ud835\udc561  \ud835\udcae(\ud835\udc5a\ud835\udc56)) or \ud835\udefc 1 (for Acc\ud835\udc561  \ud835\udcae(\ud835\udc5a\ud835\udc56)) or \ud835\udefc 2 (for Acc\ud835\udc561  \ud835\udcae(\ud835\udc5a\ud835\udc56)  Acc\ud835\udc561). Acc\ud835\udc56has a representation 2\ud835\udc56 \ud835\udcac(\ud835\udc37)  2\ud835\udc581 \ud835\udc570 \ud835\udc4b\ud835\udc56,\ud835\udc571 \ud835\udcae(\ud835\udc57) for some \ud835\udc4b\ud835\udc56 0 .. 2\ud835\udc5612\ud835\udc57.",
      "So given \ud835\udc5athat results in an exceptional case, the nontrivial discrete logarithm relation \ud835\udefc 2\ud835\udc56 \ud835\udcac(\ud835\udc37)  (2\ud835\udc581 \ud835\udc570 \ud835\udefc \ud835\udc4b\ud835\udc56,\ud835\udc571 \ud835\udcae(\ud835\udc57)  \ud835\udcae(\ud835\udc5a\ud835\udc56)  \ud835\udcaaP is easily computable from \ud835\udc5a. The coef\ufb01cients in this representation do not over\ufb02ow since \ud835\udc4b\ud835\udc56,\ud835\udc571  2\ud835\udc56for all \ud835\udc561 .. \ud835\udc5b and \ud835\udc570 .. 2\ud835\udc581; and \ud835\udefc 2\ud835\udc56 \ud835\udc5fP 1 for all \ud835\udc561 .. \ud835\udc5b and \ud835\udefc1, 1, 2. Similarly, a output from SinsemillaHash yields a nontrivial discrete logarithm relation, because Extract P only returns when its input is . Since by assumption it is hard to \ufb01nd a nontrivial discrete logarithm relation, we can argue that it is safe to use incomplete additions when computing Sinsemilla inside a circuit. 5.4.1.10 PoseidonHash Function Poseidon is a cryptographic permutation described in GKRRS2019. It operates over a sequence of \ufb01nite \ufb01eld elements, which we instantiate as F\ud835\udc5eP 3. The following speci\ufb01cation is intended to follow GKRRS2019 and Version 1.1 of the Poseidon reference implemen- tation Poseidon-1.1.8 The S-box function is \ud835\udc65\ud835\udc655.",
      "The number of full rounds \ud835\udc45\ud835\udc39is 8, and the number of partial rounds \ud835\udc45\ud835\udc43is 56. We use Poseidon in a sponge con\ufb01guration BDPA2011 (with elementwise addition in F\ud835\udc5eP replacing exclusive-or of bit strings9) to construct a hash function. The sponge capacity is one \ufb01eld element, the rate is two \ufb01eld elements, and the output is one \ufb01eld element. We use the Constant-Input-Length mode described in GKRRS2019, section 4.2: for a 2-element input, the initial value of the capacity element is 265, and no padding of the input message is needed. That is, if \ud835\udc53 F\ud835\udc5eP 3 F\ud835\udc5eP 3 is the Poseidon permutation, then the hash function PoseidonHash F\ud835\udc5eP  F\ud835\udc5eP F\ud835\udc5eP is speci\ufb01ed as: PoseidonHash(\ud835\udc65, \ud835\udc66)  \ud835\udc53(\ud835\udc65, \ud835\udc66, 265)1 (using 1-based indexing). The MDS matrix and round constants are generated by generate_parameters_grain.sage in Version 1.1 of the reference implementation. The number of full and partial rounds are as calculated by calc_round_numbers.py in that implementation, for a 128-bit security level with margin.",
      "8 Previous versions of the reference implementation were inconsistent with the paper. For verifying the parameters used in Zcash, we recommend the fork Poseidon-Zc1.1 which avoids use of the obsolete PyCrypto library. 9 The sponge construction was originally proposed as operating on an arbitrary group. BDPA2007 Non-normative notes:  The choice of MDS matrix and the number of rounds take into account cryptanalytic results in KR2020 and BCD2020. A detailed analysis of related matrix properties is given in GRS2020. BCD2020 says that ... \ufb01nite \ufb01elds F\ud835\udc5ewith a limited number of multiplicative subgroups might be preferable, i.e. one might want to avoid \ud835\udc5e1 being smooth. This implies that the \ufb01elds which are suitable for implementing FFT may be more vulnerable to integral attacks. F\ud835\udc5eP is such a \ufb01eld; the factorization of \ud835\udc5eP 1 is 232  3  463  539204044132271846773  8999194758858563409123804352480028797519453.",
      "Furthermore, previous cryptanalysis of Poseidon has focussed mainly on the case of S-box \ud835\udc65\ud835\udc653. That variant cannot be used in F\ud835\udc5eP because \ud835\udc65\ud835\udc653 would not be a permutation. \ud835\udefc 5 is the smallest integer for which \ud835\udc65\ud835\udc65\ud835\udefcis a permutation in F\ud835\udc5eP. On the other hand, the number of rounds chosen includes a signi\ufb01cant security margin, even taking into account these considerations. For small \ud835\udc61, such as \ud835\udc61 3 as used here, the results of KR2020 are positive for security since they indicate that the number of active S-boxes through the middle rounds is larger than originally estimated by the Poseidon designers (and the number of rounds is based on this original conservative estimate). Also note that the use of Poseidon in Orchard is very conservative. First, the sponge mode limits an adversary to only being able to in\ufb02uence part of the Poseidon permutation input, and we use it only to construct a PRF (PRFnfOrchard as described in section5.4.2 Pseudo Random Functions on page 86).",
      "Half of the sponge input is a random key nk, known only to holders of a full viewing key, and the remaining half \u03c1 comes from a previous nulli\ufb01er which is effectively a random af\ufb01ne-short-Weierstrass \ud835\udc65-coordinate on the Pallas curve. Then the PRF is used to enhance the security of a discrete-logarithm-based nulli\ufb01er construction (described in Zcash-Orchard, Section 3.5 Nulli\ufb01ers) against a potential discrete-log-breaking adversary. Given the weak assumption that the PoseidonHash sponge produces output that preserves suf\ufb01cient entropy from the inputs nk and \u03c1, this nulli\ufb01er construction would still be secure under a Decisional Dif\ufb01eHellman assumption on the Pallas curve, even if the Poseidon-based PRF were distinguishable from an ideal PRF. The constant 265 comes from GKRRS2019, section 4.2: Constant-Input-Length Hashing. The capacity value is length  (264)  (\ud835\udc5c1) where \ud835\udc5cis the output length.",
      "In this case the input length (length) is 2 \ufb01eld elements, and the output length is 1 \ufb01eld element. 5.4.1.11 Equihash Generator EquihashGen\ud835\udc5b,\ud835\udc58is a specialized hash function that maps an input and an index to an output of length \ud835\udc5bbits. It is used in section7.7.1 Equihash on page 133. Let powtag : 64-bit ZcashPoW 32-bit \ud835\udc5b 32-bit \ud835\udc58 Let powcount(\ud835\udc54) : 32-bit \ud835\udc54 Let EquihashGen\ud835\udc5b,\ud835\udc58(\ud835\udc46, \ud835\udc56) : \ud835\udc47\u210e1 .. \u210e\ud835\udc5b, where \ud835\udc5a floor (512 \u210e (\ud835\udc561 mod \ud835\udc5a)  \ud835\udc5b; \ud835\udc47 BLAKE2b-(\ud835\udc5b \ud835\udc5a) powtag, \ud835\udc46 powcount(floor (\ud835\udc561 Indices of bits in \ud835\udc47are 1-based. BLAKE2b-\u2113(\ud835\udc5d, \ud835\udc65) is de\ufb01ned in section5.4.1.2 BLAKE2 Hash Functions on page 76. Security requirement: BLAKE2b-\u2113(powtag, \ud835\udc65) must generate output that is suf\ufb01ciently unpredictable to avoid short-cuts to the Equihash solution process. It would suf\ufb01ce to model it as a random oracle.",
      "Note: When EquihashGen is evaluated for sequential indices, as in the Equihash solving process (section7.7.1 Equihash on page 133), the number of calls to BLAKE2b can be reduced by a factor of floor (512 in the best case (which is a factor of 2 for \ud835\udc5b 200). 5.4.2 Pseudo Random Functions Let SHA256Compress be as given in section5.4.1.1 SHA-256, SHA-256d, SHA256Compress, and SHA-512 Hash Functions on page 75. The Pseudo Random Functions PRFaddr, PRFnfSprout, PRFpk, and PRF\u03c1 from section4.1.2 Pseudo Random Functions on page 25, are all instantiated using SHA256Compress: PRFaddr (\ud835\udc61) : SHA256Compress 1 1 0 0 252-bit \ud835\udc65 8-bit \ud835\udc61 0248 PRFnfSprout (\u03c1) : SHA256Compress 1 1 1 0 252-bit ask 256-bit \u03c1 PRFpk ask(\ud835\udc56, hSig) : SHA256Compress 0 \ud835\udc56-1 0 0 252-bit ask 256-bit hSig PRF\u03c1 \u03d5(\ud835\udc56, hSig) : SHA256Compress 0 \ud835\udc56-1 1 0 252-bit \u03d5 256-bit hSig Security requirements:  SHA256Compress must be collision-resistant .",
      "SHA256Compress must be a PRF when keyed by the bits corresponding to \ud835\udc65, ask or \u03d5 in the above diagrams, with input in the remaining bits. Note: The \ufb01rst four bits i.e. the most signi\ufb01cant four bits of the \ufb01rst byte are used to separate distinct uses of SHA256Compress, ensuring that the functions are independent. As well as the inputs shown here, bits 1011 in this position are used to distinguish uses of the full SHA-256 hash function; see section5.4.8.1 Sprout Note Commitments on page 95. (The speci\ufb01c bit patterns chosen here were motivated by the possibility of future extensions that might have increased Nold andor Nnew to 3, or added an additional bit to ask to encode a new key type, or that would have required an additional PRF.",
      "In fact since Sapling switches to non-SHA256Compress-based cryptographic primitives, these extensions are unlikely to be necessary.) PRFexpand is used in section4.2.2 Sapling Key Components on page 36 to derive the Spend authorizing key ask and the proof authorizing key nsk. It is instantiated using the BLAKE2b hash function de\ufb01ned in section5.4.1.2 BLAKE2 Hash Functions on page 76: PRFexpand (\ud835\udc61) : BLAKE2b-512(Zcash_ExpandSeed, LEBS2OSP256(sk)  \ud835\udc61) Security requirement: BLAKE2b-512(Zcash_ExpandSeed, LEBS2OSP256(sk)  \ud835\udc61) must be a PRF for output range BY\u2113PRFexpand8 when keyed by the bits corresponding to sk, with input in the bits corresponding to \ud835\udc61. PRFockSapling is used in section4.20.1 Encryption (Sapling and Orchard) on page 67 to derive the outgoing cipher key ock used to encrypt an outgoing ciphertext.",
      "It is instantiated using the BLAKE2b hash function de\ufb01ned in section5.4.1.2 BLAKE2 Hash Functions on page 76: PRFockSapling (cv, cmu, ephemeralKey) : BLAKE2b-256(Zcash_Derive_ock, ockInput) where ockInput  LEBS2OSP256(ovk) 32-byte cv 32-byte cmu 32-byte ephemeralKey . Security requirement: BLAKE2b-512(Zcash_Derive_ock, ockInput) must be a PRF for output range Sym.K (de- \ufb01ned in section5.4.3 Symmetric Encryption on page 88) when keyed by the bits corresponding to ovk, with input in the bits corresponding to cv, cmu, and ephemeralKey. PRFnfSapling is used to derive the nulli\ufb01er for a Sapling note.",
      "It is instantiated using the BLAKE2s hash function de\ufb01ned in section5.4.1.2 BLAKE2 Hash Functions on page 76: PRFnfSapling (\u03c1) : BLAKE2s-256 Zcash_nf, LEBS2OSP256(nk) LEBS2OSP256(\u03c1) Security requirement: The function BLAKE2s-256 Zcash_nf, LEBS2OSP256(nk) LEBS2OSP256(\u03c1) must be a collision-resistant PRF for output range BY32 when keyed by the bits corresponding to nk, with input in the bits corresponding to \u03c1. Note that nk (\ud835\udc5f) is a representation of a point in the \ud835\udc5fJ-order subgroup of the Jubjub curve, and therefore is not uniformly distributed on B\u2113J. J (\ud835\udc5f) is de\ufb01ned in section5.4.9.3 Jubjub on page 102. PRFockOrchard is used in section4.20.1 Encryption (Sapling and Orchard) on page 67 to derive the outgoing cipher key ock used to encrypt an outgoing ciphertext.",
      "It is instantiated using the BLAKE2b hash function de\ufb01ned in section5.4.1.2 BLAKE2 Hash Functions on page 76: PRFockOrchard (cv, cmx, ephemeralKey) : BLAKE2b-256(Zcash_Orchardock, ockInput) where ockInput  LEBS2OSP256(ovk) 32-byte cv 32-byte cmx 32-byte ephemeralKey . Security requirement: BLAKE2b-512(Zcash_Orchardock, ockInput) must be a PRF for output range Sym.K (de- \ufb01ned in section5.4.3 Symmetric Encryption on page 88) when keyed by the bits corresponding to ovk, with input in the bits corresponding to cv, cmx, and ephemeralKey. Let \ud835\udc5eP be as de\ufb01ned in section5.4.9.6 Pallas and Vesta on page 105. PRFnfOrchard F\ud835\udc5eP  F\ud835\udc5eP F\ud835\udc5eP is used as part of deriving the nulli\ufb01er for an Orchard note. It is instantiated using the PoseidonHash hash function GKRRS2019 de\ufb01ned in section5.4.1.10 PoseidonHash Function on page 84: PRFnfOrchard (\u03c1) : PoseidonHash(nk, \u03c1). Security requirement: PoseidonHash F\ud835\udc5eP  F\ud835\udc5eP F\ud835\udc5eP must be a PRF when keyed by its \ufb01rst argument, with its second argument as input.",
      "Non-normative notes:  This construction of a PRF from a sponge is described in BDPA2011, section 3.12. It is called outer-keyed sponge in ADMA2015, or black-box keying in GPT2015. The results of these papers do not directly apply because the key is smaller than the rate. However, the result of GG2015 provides evidence for the security of this construction (even if it technically considers a situation in which the distinguishing adversary cannot evaluate the full permutation). See section5.4.1.10 PoseidonHash Function on page 84 for further security discussion of how Orchard uses Poseidon. 5.4.3 Symmetric Encryption Let Sym.K : B256, Sym.P : BYN, and Sym.C : BYN. Let the authenticated one-time symmetric encryption scheme Sym.EncryptK(P) be authenticated encryption using AEAD_CHACHA20_POLY1305 RFC-7539 encryption of plaintext P Sym.P, with empty associated data\", all-zero nonce 096, and 256-bit key K Sym.K.",
      "Similarly, let Sym.DecryptK(C) be AEAD_CHACHA20_POLY1305 decryption of ciphertext C Sym.C, with empty associated data\", all-zero nonce 096, and 256-bit key K Sym.K. The result is either the plaintext byte sequence, or indicating failure to decrypt. Note: The IETF\" de\ufb01nition of AEAD_CHACHA20_POLY1305 from RFC-7539 is used; this has a 32-bit block count and a 96-bit nonce, rather than a 64-bit block count and 64-bit nonce as in the original de\ufb01nition of ChaCha20. 5.4.4 Pseudo Random Permutations Let \u2113dk and \u2113d be as de\ufb01ned in section5.3 Constants on page 74. PRPd BY\u2113dk8  B\u2113d B\u2113d is a Pseudo Random Permutation speci\ufb01ed in section4.1.3 Pseudo Random Permutations on page 26. In this speci\ufb01cation, it is used to generate diversi\ufb01ers for Orchard shielded payment addresses in section4.2.3 Orchard Key Components on page 38.",
      "(ZIP-32 uses an identical construction to generate diversi\ufb01ers for Sapling shielded payment addresses.) Let FF1-AES256\ud835\udc3e(tweak, \ud835\udc65) be the FF1 format-preserving encryption algorithm NIST2016 using AES with a 256-bit key \ud835\udc3e, and parameters radix  2, minlen  88, maxlen  88. It will be used only with the empty string  as the tweak. \ud835\udc65is a sequence of 88 bits, as is the output. De\ufb01ne PRPd \ud835\udc3e(d) : FF1-AES256\ud835\udc3e(, d). Security requirement: FF1-AES256 with tweak \ufb01xed to  must be a secure Pseudo Random Permutation. Non-normative note: DKLS2020 describes attacks against FF1 that are practical for some parameterizations. However, for an 88-bit domain, and 10 rounds as speci\ufb01ed in NIST2016, even the distinguishing attack is no better than a brute force search for the 256-bit key. Speci\ufb01cally we have \ud835\udc5f 5 (half the number of rounds) and \ud835\udc5b 44 (half the domain size in bits), so according to DKLS2020, section 4.2 the data complexity is 22\ud835\udc5b((\ud835\udc5f1)1 2 )\ud835\udc5b 2264, and the time complexity is 22\ud835\udc5b((\ud835\udc5f1)1 2 )  2308.",
      "5.4.5 Key Agreement And Derivation 5.4.5.1 Sprout Key Agreement KASprout is a key agreement scheme as speci\ufb01ed in section4.1.5 Key Agreement on page 26. It is instantiated as Curve25519 key agreement, described in Bernstein2006, as follows. Let KASprout.Public and KASprout.SharedSecret be the type of Curve25519 public keys (i.e. BY32), and let KASprout.Private be the type of Curve25519 secret keys. Let Curve25519(\ud835\udc5b, \ud835\udc5e) be the result of point multiplication of the Curve25519 public key represented by the byte se- quence \ud835\udc5eby the Curve25519 secret key represented by the byte sequence \ud835\udc5b, as de\ufb01ned in Bernstein2006, section 2. Let KASprout.Base : 9 be the public byte sequence representing the Curve25519 base point. Let clampCurve25519(\ud835\udc65) take a 32-byte sequence \ud835\udc65as input and return a byte sequence representing a Curve25519 private key, with bits clamped as described in Bernstein2006, section 3: clear bits 0, 1, 2 of the \ufb01rst byte, clear bit 7 of the last byte, and set bit 6 of the last byte.",
      "Here the bits of a byte are numbered such that bit \ud835\udc4fhas numeric weight 2\ud835\udc4f. De\ufb01ne KASprout.FormatPrivate(\ud835\udc65) : clampCurve25519(\ud835\udc65). De\ufb01ne KASprout.DerivePublic(\ud835\udc5b, \ud835\udc5e) : Curve25519(\ud835\udc5b, \ud835\udc5e). De\ufb01ne KASprout.Agree(\ud835\udc5b, \ud835\udc5e) : Curve25519(\ud835\udc5b, \ud835\udc5e). 5.4.5.2 Sprout Key Derivation KDFSprout is a Key Derivation Function as speci\ufb01ed in section4.1.6 Key Derivation on page 27. It is instantiated using BLAKE2b-256 as follows: KDFSprout(\ud835\udc56, hSig, sharedSecret\ud835\udc56, epk, pknew enc,\ud835\udc56) : BLAKE2b-256(kdftag, kdfinput) where: kdftag : 64-bit ZcashKDF 8-bit \ud835\udc561 056 kdfinput : 256-bit hSig 256-bit sharedSecret\ud835\udc56 256-bit epk 256-bit pknew enc,\ud835\udc56 BLAKE2b-256(\ud835\udc5d, \ud835\udc65) is de\ufb01ned in section5.4.1.2 BLAKE2 Hash Functions on page 76. 5.4.5.3 Sapling Key Agreement KASapling is a key agreement scheme as speci\ufb01ed in section4.1.5 Key Agreement on page 26. It is instantiated as Dif\ufb01eHellman with cofactor multiplication on Jubjub as follows: Let J, J(\ud835\udc5f), J(\ud835\udc5f), and the cofactor \u210eJ be as de\ufb01ned in section5.4.9.3 Jubjub on page 102.",
      "De\ufb01ne KASapling.Public : J. De\ufb01ne KASapling.PublicPrimeOrder : J(\ud835\udc5f). De\ufb01ne KASapling.SharedSecret : J(\ud835\udc5f). De\ufb01ne KASapling.Private : F\ud835\udc5fJ. De\ufb01ne KASapling.DerivePublic(sk, \ud835\udc35) : sk \ud835\udc35. De\ufb01ne KASapling.Agree(sk, \ud835\udc43) : \u210eJ  sk \ud835\udc43. 5.4.5.4 Sapling Key Derivation KDFSapling is a Key Derivation Function as speci\ufb01ed in section4.1.6 Key Derivation on page 27. It is instantiated using BLAKE2b-256 as follows: KDFSapling(sharedSecret, ephemeralKey) : BLAKE2b-256(Zcash_SaplingKDF, kdfinput). where: kdfinput : LEBS2OSP256 reprJ(sharedSecret) ephemeralKey BLAKE2b-256(\ud835\udc5d, \ud835\udc65) is de\ufb01ned in section5.4.1.2 BLAKE2 Hash Functions on page 76. 5.4.5.5 Orchard Key Agreement KAOrchard is a key agreement scheme as speci\ufb01ed in section4.1.5 Key Agreement on page 26. It is instantiated as Dif\ufb01eHellman on Pallas as follows: Let P be as de\ufb01ned in section5.4.9.6 Pallas and Vesta on page 105. De\ufb01ne KAOrchard.Public : P. De\ufb01ne KAOrchard.PublicPrimeOrder : P. De\ufb01ne KAOrchard.SharedSecret : P.",
      "De\ufb01ne KAOrchard.Private : F De\ufb01ne KAOrchard.DerivePublic(sk, \ud835\udc35) : sk \ud835\udc35. De\ufb01ne KAOrchard.Agree(sk, \ud835\udc43) : sk \ud835\udc43. 5.4.5.6 Orchard Key Derivation KDFOrchard is a Key Derivation Function as speci\ufb01ed in section4.1.6 Key Derivation on page 27. It is instantiated using BLAKE2b-256 as follows: KDFOrchard(sharedSecret, ephemeralKey) : BLAKE2b-256(Zcash_OrchardKDF, kdfinput). where: kdfinput : LEBS2OSP256(reprP(sharedSecret)) ephemeralKey BLAKE2b-256(\ud835\udc5d, \ud835\udc65) is de\ufb01ned in section5.4.1.2 BLAKE2 Hash Functions on page 76. 5.4.6 Ed25519 Ed25519 is a signature scheme as speci\ufb01ed in section4.1.7 Signature on page 28. It is used to instantiate JoinSplitSig as described in section4.11 Non-malleability (Sprout) on page 51.",
      "Let PreCanopyExcludedPointEncodings BY32)  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00 ,  0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00 ,  0x26, 0xe8, 0x95, 0x8f, 0xc2, 0xb2, 0x27, 0xb0, 0x45, 0xc3, 0xf4, 0x89, 0xf2, 0xef, 0x98, 0xf0, 0xd5, 0xdf, 0xac, 0x05, 0xd3, 0xc6, 0x33, 0x39, 0xb1, 0x38, 0x02, 0x88, 0x6d, 0x53, 0xfc, 0x05 ,  0xc7, 0x17, 0x6a, 0x70, 0x3d, 0x4d, 0xd8, 0x4f, 0xba, 0x3c, 0x0b, 0x76, 0x0d, 0x10, 0x67, 0x0f, 0x2a, 0x20, 0x53, 0xfa, 0x2c, 0x39, 0xcc, 0xc6, 0x4e, 0xc7, 0xfd, 0x77, 0x92, 0xac, 0x03, 0x7a ,  0x13, 0xe8, 0x95, 0x8f, 0xc2, 0xb2, 0x27, 0xb0, 0x45, 0xc3, 0xf4, 0x89, 0xf2, 0xef, 0x98, 0xf0, 0xd5, 0xdf, 0xac, 0x05, 0xd3, 0xc6, 0x33, 0x39, 0xb1, 0x38, 0x02, 0x88, 0x6d, 0x53, 0xfc, 0x85 ,  0xb4, 0x17, 0x6a, 0x70, 0x3d, 0x4d, 0xd8, 0x4f, 0xba, 0x3c, 0x0b, 0x76, 0x0d, 0x10, 0x67, 0x0f, 0x2a, 0x20, 0x53, 0xfa, 0x2c, 0x39, 0xcc, 0xc6, 0x4e, 0xc7, 0xfd, 0x77, 0x92, 0xac, 0x03, 0xfa ,  0xec, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x7f ,  0xed, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x7f ,  0xee, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x7f ,  0xd9, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff ,  0xda, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff  Let \ud835\udc5d 2255 19.",
      "Let \ud835\udc4e 1. Let \ud835\udc51 121665121666 (mod \ud835\udc5d). Let \u2113 2252  27742317777372353535851937790883648493 (the order of the Ed25519 curves prime-order subgroup). Let \ud835\udc35be the base point given in BDLSY2012. De\ufb01ne the notation ? as in section2 Notation on page 10. De\ufb01ne I2LEOSP, LEOS2BSP, and LEBS2IP as in section5.1 Integers, Bit Sequences, and Endianness on page 73. De\ufb01ne reprBytesEd25519 Ed25519 BY32 such that reprBytesEd25519((\ud835\udc65, \ud835\udc66)) I2LEOSP256 (\ud835\udc66mod \ud835\udc5d)2255\ud835\udc65 , where \ud835\udc65 \ud835\udc65mod 2.10 De\ufb01ne abstBytesEd25519 BY32 Ed25519  such that abstBytesEd25519 is computed as follows: let \ud835\udc66 B255 be the \ufb01rst 255 bits of LEOS2BSP256 and let \ud835\udc65 B be the last bit. let \ud835\udc66 F\ud835\udc5d LEBS2IP255(\ud835\udc66) (mod \ud835\udc5d). let \ud835\udc65 ? 1 \ud835\udc662 \ud835\udc4e\ud835\udc51\ud835\udc662 . (The denominator \ud835\udc4e\ud835\udc51\ud835\udc662 cannot be zero, since \ud835\udc4e \ud835\udc51is not square in F\ud835\udc5d.) if \ud835\udc65 , return . if \ud835\udc65mod 2  \ud835\udc65then return (\ud835\udc65, \ud835\udc66) else return (\ud835\udc5d\ud835\udc65, \ud835\udc66). Note: This de\ufb01nition of point decoding differs from that of RFC-8032, section 5.1.3, as corrected by the errata.",
      "In the latter there is an additional step If x  0, and x_0  1, decoding fails., which rejects the encodings   0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x80 ,  0xee, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff ,  0xec, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff  In this speci\ufb01cation, the \ufb01rst two of these are accepted as encodings of (0, 1), and the third is accepted as an encoding of (0, 1). Ed25519 is de\ufb01ned as in BDLSY2012, using SHA-512 as the internal hash function, with the additional requirements below.",
      "A valid Ed25519 validating key is de\ufb01ned as a sequence of 32 bytes encoding a point on the Ed25519 curve. The requirements on a signature (\ud835\udc45, \ud835\udc46) with validating key \ud835\udc34on a message \ud835\udc40are:  \ud835\udc46MUST represent an integer less than \u2113. \ud835\udc45and \ud835\udc34MUST be encodings of points \ud835\udc45and \ud835\udc34respectively on the Ed25519 curve;  Pre-Canopy \ud835\udc45MUST NOT be in PreCanopyExcludedPointEncodings;  Pre-Canopy The validation equation MUST be equivalent to \ud835\udc46 \ud835\udc35 \ud835\udc45 \ud835\udc50 \ud835\udc34. Canopy onward The validation equation MUST be equivalent to 8 \ud835\udc46 \ud835\udc35 8 \ud835\udc45 8 \ud835\udc50 \ud835\udc34for single- signature validation. where \ud835\udc50is computed as the integer corresponding to SHA-512(\ud835\udc45 \ud835\udc34 \ud835\udc40) as speci\ufb01ed in BDLSY2012. If these requirements are not met or the validation equation does not hold, then the signature is considered invalid. The encoding of an Ed25519 signature is: 256-bit \ud835\udc45 256-bit \ud835\udc46 where \ud835\udc45and \ud835\udc46are as de\ufb01ned in BDLSY2012.",
      "10Here we use the (\ud835\udc65, \ud835\udc66) naming of coordinates in BDLSY2012, which is different from the (\ud835\udc62, v) naming used for coordinates of ctEdwards curves in section5.4.9.3 Jubjub on page 102 and in sectionA.2 Elliptic curve background on page 200. Notes:  It is not required that the integer encoding of the \ud835\udc66-coordinate10 of the points represented by \ud835\udc45or \ud835\udc34are less than 2255 19. It is not required that \ud835\udc34PreCanopyExcludedPointEncodings. Canopy onward Appendix sectionB.3 Ed25519 batch validation on page 223 describes an optimization that MAY be used to speed up validation of batches of Ed25519 signatures. Non-normative note: The exclusion, before Canopy activation, of PreCanopyExcludedPointEncodings from \ud835\udc45is due to a quirk of version 1.0.15 of the libsodium library libsodium which was initially used to implement Ed25519 signature validation in zcashd. (The ED25519_COMPAT compile-time option was not set.) The intent was to exclude points of order less than \u2113; however, not all such points were covered.",
      "Canopy onward Non-normative note: Because the post-Canopy rules for Ed25519 signatures are a relaxation of the pre-Canopy rules, a full validator implementation that checkpoints on the Canopy activation block MAY validate using the post-Canopy rules for the whole chain (and zcashd does so since zcashd v4.2.0). We retain the pre-Canopy rules in the speci\ufb01cation in order to accurately document the history of consensus changes. 5.4.7 RedDSA, RedJubjub, and RedPallas RedDSA is a Schnorr-based signature scheme, optionally supporting key re-randomization as described in section4.1.7.1 Signature with Re-Randomizable Keys on page 29. It also supports a Secret Key to Public Key Monomorphism as described in section4.1.7.2 Signature with Signing Key to Validating Key Monomorphism on page 30. It is based on a scheme from FKMSSS2016, section 3, with some ideas from EdDSA BJLSY2015.",
      "RedJubjub is a specialization of RedDSA to the Jubjub curve (section5.4.9.3 Jubjub on page 102), using the BLAKE2b-512 hash function. The spend authorization signature scheme SpendAuthSigSaplingis instantiated by RedJubjub, using parameters de\ufb01ned in section5.4.7.1 Spend Authorization Signature (Sapling and Orchard) on page 95. The binding signature scheme BindingSigSapling is instantiated by RedJubjub without key re-randomization, using parameters de\ufb01ned in section5.4.7.2 Binding Signature (Sapling and Orchard) on page 95. RedPallas is a specialization of RedDSA to the Pallas curve de\ufb01ned in section5.4.9.6 Pallas and Vesta on page 105, using the BLAKE2b-512 hash function. The spend authorization signature scheme SpendAuthSigOrchardis instantiated by RedPallas, using parameters de\ufb01ned in section5.4.7.1 Spend Authorization Signature (Sapling and Orchard) on page 95.",
      "The binding signature scheme BindingSigOrchard is instantiated by RedPallas without key re-randomization, using parameters de\ufb01ned in section5.4.7.2 Binding Signature (Sapling and Orchard) on page 95. Let I2LEBSP, I2LEOSP, LEOS2IP, and LEBS2OSP be as de\ufb01ned in section5.1 Integers, Bit Sequences, and Endianness on page 73. We \ufb01rst describe the scheme RedDSA over a general represented group. Its parameters are:  a represented group G, which also de\ufb01nes a subgroup G(\ud835\udc5f) of order \ud835\udc5fG, a cofactor \u210eG, a group operation , an additive identity \ud835\udcaaG, a bit-length \u2113G, a representation function reprG, and an abstraction function abstG, as speci\ufb01ed in section4.1.9 Represented Group on page 32;  \ud835\udcabG, a generator of G(\ud835\udc5f);  a bit-length \u2113H N such that 2\u2113H128 \ud835\udc5fG and \u2113H mod 8  0;  a cryptographic hash function H BYN BY\u2113H8. Its associated types are de\ufb01ned as follows: RedDSA.Message : BYN RedDSA.Signature : BYceiling(\u2113G8)  ceiling(bitlength(\ud835\udc5fG)8) RedDSA.Public : G RedDSA.Private : F\ud835\udc5fG. RedDSA.Random : F\ud835\udc5fG.",
      "De\ufb01ne H BYN F\ud835\udc5fG by: H(\ud835\udc35)  LEOS2IP\u2113H H(\ud835\udc35) (mod \ud835\udc5fG) De\ufb01ne RedDSA.GenPrivate ()  R RedDSA.Private as: Return sk  R F\ud835\udc5fG. De\ufb01ne RedDSA.DerivePublic RedDSA.Private RedDSA.Public by: RedDSA.DerivePublic(sk) : sk \ud835\udcabG. De\ufb01ne RedDSA.GenRandom ()  R RedDSA.Random as: Choose a byte sequence \ud835\udc47uniformly at random on BY(\u2113H128)8. Return H(\ud835\udc47). De\ufb01ne \ud835\udcaaRedDSA.Random : 0 (mod \ud835\udc5fG). De\ufb01ne RedDSA.RandomizePrivate RedDSA.Random  RedDSA.Private RedDSA.Private by: RedDSA.RandomizePrivate(\ud835\udefc, sk) : sk  \ud835\udefc(mod \ud835\udc5fG). De\ufb01ne RedDSA.RandomizePublic RedDSA.Random  RedDSA.Public RedDSA.Public as: RedDSA.RandomizePublic(\ud835\udefc, vk) : vk  \ud835\udefc \ud835\udcabG. De\ufb01ne RedDSA.Sign (sk RedDSA.Private)  (\ud835\udc40 RedDSA.Message)  R RedDSA.Signature as: Choose a byte sequence \ud835\udc47uniformly at random on BY(\u2113H128)8. Let vk  LEBS2OSP\u2113G reprG(RedDSA.DerivePublic(sk)) Let \ud835\udc5f H(\ud835\udc47 vk  \ud835\udc40). Let \ud835\udc45 \ud835\udc5f \ud835\udcabG. Let \ud835\udc45 LEBS2OSP\u2113G reprG(\ud835\udc45) Let \ud835\udc46 (\ud835\udc5f H(\ud835\udc45 vk  \ud835\udc40)  sk) mod \ud835\udc5fG. Let \ud835\udc46 I2LEOSPbitlength(\ud835\udc5fG)(\ud835\udc46). Return \ud835\udc45 \ud835\udc46.",
      "De\ufb01ne RedDSA.Validate (vk RedDSA.Public)  (\ud835\udc40 RedDSA.Message)  (\ud835\udf0e RedDSA.Signature) B as: Let \ud835\udc45be the \ufb01rst ceiling \u2113G8 bytes of \ud835\udf0e, and let \ud835\udc46be the remaining ceiling(bitlength(\ud835\udc5fG)8) bytes. Let \ud835\udc45 abstG LEOS2BSP\u2113G(\ud835\udc45) , and let \ud835\udc46 LEOS2IP8length(\ud835\udc46)(\ud835\udc46). Let vk  LEBS2OSP\u2113G reprG(vk) Let \ud835\udc50 H(\ud835\udc45 vk  \ud835\udc40). NU55 onward If reprG(\ud835\udc45) \ud835\udc45, return 0. Return 1 if \ud835\udc45 and \ud835\udc46 \ud835\udc5fG and \u210eG \ud835\udc46 \ud835\udcabG  \ud835\udc45 \ud835\udc50 vk  \ud835\udcaaG, otherwise 0. Notes:  The validation algorithm does not check that \ud835\udc45is a point of order at least \ud835\udc5fG. After the activation of ZIP-216 with NU55, validation returns 0 if \ud835\udc45is a non-canonical compressed point encoding. This change is also retrospectively valid on Mainnet and Testnet before NU55. The value \ud835\udc45used as part of the input to HMUST be exactly as encoded in the signature. Appendix sectionB.1 RedDSA batch validation on page 220 describes an optimization that MAY be used to speed up validation of batches of RedDSA signatures.",
      "Non-normative notes:  The randomization used in RedDSA.RandomizePrivate and RedDSA.RandomizePublic may interact with other uses of additive properties of keys forSchnorr-based signature schemes. In the Zcash protocol, such properties are used for binding signatures but not at the same time as key randomization. They are also used in ZIP-32 when deriving child extended keys, but this does not result in any practical security weakness as long as the security recommendations of ZIP 32 are followed. If RedDSA is reused in other protocols making use of these additive properties, careful analysis of potential interactions is required. It is RECOMMENDED that, for deployments of RedDSA in other protocols than Zcash, the requirement for \ud835\udc45 to be canonically encoded is always enforced (which was the original intent of the design).",
      "The two abelian groups speci\ufb01ed in section4.1.7.2 Signature with Signing Key to Validating Key Monomorphism on page 30 are instantiated for RedDSA as follows: : 0 (mod \ud835\udc5fG)  sk1 sk2 : sk1  sk2 (mod \ud835\udc5fG) : \ud835\udcaaG  vk1 vk2 : vk1  vk2. As required, RedDSA.DerivePublic is a group monomorphism, since it is injective and: RedDSA.DerivePublic(sk1 sk2)  sk1  sk2 (mod \ud835\udc5fG) \ud835\udcabG  sk1 \ud835\udcabG  sk2 \ud835\udcabG (since \ud835\udcabG has order \ud835\udc5fG)  RedDSA.DerivePublic(sk1) RedDSA.DerivePublic(sk2). A RedDSA validating key vk can be encoded as a bit sequence reprG(vk) of length \u2113G bits (or as a corresponding byte sequence vk by then applying LEBS2OSP\u2113G). The scheme RedJubjub specializes RedDSA with:  G : J as de\ufb01ned in section5.4.9.3 Jubjub on page 102;  \u2113H : 512;  H(\ud835\udc65) : BLAKE2b-512(Zcash_RedJubjubH, \ud835\udc65) as de\ufb01ned in section5.4.1.2 BLAKE2 Hash Functions on page 76.",
      "The scheme RedPallas specializes RedDSA with:  G : P as de\ufb01ned in section5.4.9.6 Pallas and Vesta on page 105;  \u2113H : 512;  H(\ud835\udc65) : BLAKE2b-512(Zcash_RedPallasH, \ud835\udc65) as de\ufb01ned in section5.4.1.2 BLAKE2 Hash Functions on page 76. The generator \ud835\udcabG G(\ud835\udc5f) is left as an unspeci\ufb01ed parameter, different between BindingSigSapling, SpendAuthSigSapling, BindingSigOrchard, and SpendAuthSigOrchard. 5.4.7.1 Spend Authorization Signature (Sapling and Orchard) Let RedJubjub be as de\ufb01ned in section5.4.7 RedDSA, RedJubjub, and RedPallas on page 92. De\ufb01ne \ud835\udca2Sapling : FindGroupHashJ(\ud835\udc5f) (Zcash_G_, ). The spend authorization signature scheme SpendAuthSigSaplingis instantiated as RedJubjub with keyre-randomization and with generator \ud835\udcabG  \ud835\udca2Sapling. Let RedPallas be as de\ufb01ned in section5.4.7 RedDSA, RedJubjub, and RedPallas on page 92. De\ufb01ne \ud835\udca2Orchard : GroupHashP(z.cash:Orchard, G).",
      "The spend authorization signature scheme SpendAuthSigOrchardis instantiated as RedPallas with keyre-randomization and with generator \ud835\udcabG  \ud835\udca2Orchard. See section4.15 Spend Authorization Signature (Sapling and Orchard) on page 56 for details on the use of this signature scheme. Security requirement: Each instantiation of SpendAuthSig must be a SURK-CMA-secure signature scheme with re-randomizable keys as de\ufb01ned in section4.1.7.1 Signature with Re-Randomizable Keys on page 29. 5.4.7.2 Binding Signature (Sapling and Orchard) Let RedJubjub and RedPallas be as de\ufb01ned in section5.4.7 RedDSA, RedJubjub, and RedPallas on page 92. The Sapling binding signature scheme, BindingSigSapling, is instantiated as RedJubjub without key re-randomization, using generator \ud835\udcabG  \u211bSapling de\ufb01ned in section5.4.8.3 Homomorphic Pedersen commitments (Sapling and Orchard) on page 97. See section4.13 Balance and Binding Signature (Sapling) on page 52 for details on the use of this signature scheme.",
      "The Orchard binding signature scheme, BindingSigOrchard, is instantiated as RedPallas without key re-randomization, using generator\ud835\udcabG  \u211bOrchardde\ufb01ned in section5.4.8.3 Homomorphic Pedersen commitments (Sapling and Orchard) on page 97. See section4.14 Balance and Binding Signature (Orchard) on page 54 for details on the use of this signature scheme. Security requirement: Each instantiation of BindingSig must be a SUF-CMA-secure signature scheme with key monomorphism as de\ufb01ned in section4.1.7.2 Signature with Signing Key to Validating Key Monomorphism on page 30. A signature must prove knowledge of the discrete logarithm of the validating key with respect to the base \u211bSapling or \u211bOrchard.",
      "5.4.8 Commitment schemes 5.4.8.1 Sprout Note Commitments The note commitment scheme NoteCommitSprout speci\ufb01ed in section4.1.8 Commitment on page 31 is instantiated using SHA-256 as follows: NoteCommitSprout (apk, v, \u03c1) : SHA-256 1 0 1 1 0 0 0 0 256-bit apk 64-bit v 256-bit \u03c1 256-bit rcm NoteCommitSprout.GenTrapdoor() generates the uniform distribution on NoteCommitSprout.Trapdoor. Note: The leading byte of the SHA-256 input is 0xB0. Security requirements:  SHA256Compress must be collision-resistant . SHA256Compress must be a PRF when keyed by the bits corresponding to the position of rcm in the second block of SHA-256 input, with input to the PRF in the remaining bits of the block and the chaining variable. 5.4.8.2 Windowed Pedersen commitments section5.4.1.7 Pedersen Hash Function on page 79 de\ufb01nes a Pedersen hash construction.",
      "We construct windowed Pedersen commitments by reusing that construction, and adding a randomized point on the Jubjub curve (see section5.4.9.3 Jubjub on page 102): WindowedPedersenCommit\ud835\udc5f(\ud835\udc60) : PedersenHashToPoint(Zcash_PH, \ud835\udc60)  \ud835\udc5f FindGroupHashJ(\ud835\udc5f) (Zcash_PH, r) See sectionA.3.5 Windowed Pedersen Commitment on page 213 for rationale and ef\ufb01cient circuit implementation of this function. The note commitment scheme NoteCommitSapling speci\ufb01ed in section4.1.8 Commitment on page 31 is instantiated as follows using WindowedPedersenCommit: NoteCommitSapling (gd, pkd, v) : WindowedPedersenCommitrcm 16  I2LEBSP64(v)  gd  pkd NoteCommitSapling.GenTrapdoor() generates the uniform distribution on F\ud835\udc5fJ. Security requirements:  WindowedPedersenCommit, and hence NoteCommitSapling, must be computationally binding and at least com- putationally hiding commitment schemes.",
      "(They are in fact unconditionally hiding commitment schemes.) Notes:  MerkleCRHSaplingis also de\ufb01ned in terms of PedersenHashToPoint (see section5.4.1.3 MerkleCRHSapling Hash Function on page 76). The pre\ufb01x 16 distinguishes the use of WindowedPedersenCommit in NoteCommitSapling from the layer pre\ufb01x used in MerkleCRHSapling. That layer pre\ufb01x is a 6-bit little-endian encoding of an integer in the range 0 .. MerkleDepthSapling 1; because MerkleDepthSapling  64, it cannot collide with 16. The arguments to NoteCommitSapling are in a different order to their encodings in WindowedPedersenCommit. There is no particularly good reason for this. Theorem 5.4.5. UncommittedSapling is not in the range of NoteCommitSapling. Proof. UncommittedSapling is de\ufb01ned as I2LEBSP\u2113Sapling Merkle (1).",
      "By injectivity of I2LEBSP\u2113Sapling Merkle and de\ufb01nitions of ExtractJ(\ud835\udc5f), WindowedPedersenCommit, and NoteCommitSapling, I2LEBSP\u2113Sapling Merkle (1) can be in the range of NoteCommitSapling only if there exist rcm NoteCommitSapling.Trapdoor, \ud835\udc37 BY8, and \ud835\udc40 BN such that\ud835\udc62(WindowedPedersenCommitrcm(\ud835\udc37, \ud835\udc40))  1. The latter can only be the af\ufb01ne-ctEdwards \ud835\udc62-coordinate of a point in J. We show that there are no points in J with af\ufb01ne-ctEdwards \ud835\udc62-coordinate 1. Suppose for a contradiction that (\ud835\udc62, v) J for \ud835\udc62 1 and some v F\ud835\udc5fS. By writing the curve equation as v2  (1 \ud835\udc4eJ\ud835\udc622)(1 \ud835\udc51J\ud835\udc622), and noting that 1 \ud835\udc51J\ud835\udc622  0 because \ud835\udc51J is nonsquare, we have v2  (1 \ud835\udc4eJ)(1 \ud835\udc51J). The right-hand-side is a nonsquare in F\ud835\udc5fS (for the Jubjub curve parameters), so there are no solutions for v (contradiction).",
      "5.4.8.3 Homomorphic Pedersen commitments (Sapling and Orchard) The windowed Pedersen commitments de\ufb01ned in the preceding section are highly ef\ufb01cient, but they do not support the homomorphic property we need when instantiating ValueCommit. For more details on the use of this property, see section4.13 Balance and Binding Signature (Sapling) on page 52 and section4.14 Balance and Binding Signature (Orchard) on page 54. Useful background is given in section3.6 Spend Transfers, Output Transfers, and their Descriptions on page 20 and section3.7 Action Transfers and their Descriptions on page 20. In order to support this property, we also de\ufb01ne homomorphic Pedersen commitments for Sapling: HomomorphicPedersenCommitSapling (\ud835\udc37, v) : v FindGroupHashJ(\ud835\udc5f) (\ud835\udc37, v) rcv FindGroupHashJ(\ud835\udc5f) (\ud835\udc37, r) ValueCommitSapling.GenTrapdoor() generates the uniform distribution on F\ud835\udc5fJ. See sectionA.3.6 Homomorphic Pedersen Commitment on page 213 for rationale and ef\ufb01cient circuit implementation of this function.",
      "We also de\ufb01ne homomorphic Pedersen commitments for Orchard: HomomorphicPedersenCommitOrchard (\ud835\udc37, v) : v GroupHashP(\ud835\udc37, v) rcv GroupHashP(\ud835\udc37, r) ValueCommitOrchard.GenTrapdoor() generates the uniform distribution on F\ud835\udc5fP. De\ufb01ne: \ud835\udcb1Sapling : FindGroupHashJ(\ud835\udc5f) (Zcash_cv, v) \u211bSapling : FindGroupHashJ(\ud835\udc5f) (Zcash_cv, r) \ud835\udcb1Orchard : GroupHashP(z.cash:Orchard-cv, v) \u211bOrchard : GroupHashP(z.cash:Orchard-cv, r) The commitment scheme ValueCommitSapling speci\ufb01ed in section4.1.8 Commitment on page 31 is instantiated as follows using HomomorphicPedersenCommitSapling on the Jubjub curve: ValueCommitrcv(v) : HomomorphicPedersenCommitSapling (Zcash_cv, v). which is equivalent to: ValueCommitSapling (v) : v \ud835\udcb1Sapling  rcv \u211bSapling. The commitment scheme ValueCommitOrchard speci\ufb01ed in section4.1.8 Commitment on page 31 is instantiated as follows using HomomorphicPedersenCommitOrchard on the Pallas curve: ValueCommitOrchard (v) : HomomorphicPedersenCommitOrchard (z.cash:Orchard-cv, v).",
      "which is equivalent to: ValueCommitOrchard (v) : v \ud835\udcb1Orchard  rcv \u211bOrchard. Security requirements:  HomomorphicPedersenCommitSapling and HomomorphicPedersenCommitOrchard must be computationally binding and at least computationally hiding commitment schemes, for a given personalization input \ud835\udc37. ValueCommitSaplingand ValueCommitOrchardmust be computationallybinding and at least computationallyhiding commitment schemes. (They are in fact unconditionally hiding commitment schemes.) Non-normative note: The output of HomomorphicPedersenCommitSapling may (with negligible probability for a randomly chosen commitment trapdoor) be the zero point of the curve, \ud835\udcaaJ. This would be rejected by consensus if it appeared as the cv \ufb01eld of a Spend description (section4.4 Spend Descriptions on page 40) or Output description (section4.5 Output Descriptions on page 41). An implementation of HomomorphicPedersenCommitSapling MAY resample the commitment trapdoor until the resulting commitment is not \ud835\udcaaJ.",
      "5.4.8.4 Sinsemilla commitments Let \u2113Orchard base be as de\ufb01ned in section5.3 Constants on page 74. Let P and \ud835\udc5fP be as de\ufb01ned in section5.4.9.6 Pallas and Vesta on page 105. Let Extract P be as de\ufb01ned in section5.4.9.7 Coordinate Extractor for Pallas on page 106. Let SinsemillaHashToPoint be as de\ufb01ned in section5.4.1.9 Sinsemilla Hash Function on page 81. We construct Sinsemilla commitments by reusing the Sinsemilla hash construction, and adding a randomized point on the Pallas curve (see section5.4.9.6 Pallas and Vesta on page 105): SinsemillaCommit\ud835\udc5f(\ud835\udc37, \ud835\udc40) : \ud835\udc40  \ud835\udc5f GroupHashP(\ud835\udc37 -r, ), if \ud835\udc40   otherwise where \ud835\udc40  SinsemillaHashToPoint(\ud835\udc37 -M, \ud835\udc40). SinsemillaShortCommit\ud835\udc5f(\ud835\udc37, \ud835\udc40) : Extract SinsemillaCommit\ud835\udc5f(\ud835\udc37, \ud835\udc40) See Zcash-Orchard, section 3.7.1.2 for rationale and ef\ufb01cient circuit implementation of this function. The probability of SinsemillaHashToPoint returning is insigni\ufb01cant (and would yield a nontrivial discrete logarithm relation).",
      "The binding property of SinsemillaCommit follows from collision resistance of SinsemillaHashToPoint proven in Theorem 5.4.3 on page 83, given that GroupHashP(\ud835\udc37 -r, )is independent of any of the bases used in SinsemillaHashToPoint. The binding property of SinsemillaShortCommit can be proven by a similar argument to that used for SinsemillaHash. Provided that SinsemillaHashToPoint does not return , SinsemillaCommit is perfectly hiding because the output distribution is perfectly indistinguishable from a random point in P, given that \ud835\udc5fis a uniformly random scalar on 0, \ud835\udc5e). It follows that SinsemillaShortCommit is also perfectly hiding under the same condition, since hiding cannot be affected by applying any \ufb01xed function to the output of SinsemillaCommit.",
      "The note commitment scheme NoteCommitOrchardspeci\ufb01ed in section4.1.8 Commitment on page 31 is instantiated as follows using SinsemillaCommit: NoteCommitOrchard (gd, pkd, v, \u03c1, \u03c8) : SinsemillaCommitrcm z.cash:Orchard-NoteCommit, gd  pkd  I2LEBSP64(v)  I2LEBSP\u2113Orchard base (\u03c1)  I2LEBSP\u2113Orchard base NoteCommitOrchard.GenTrapdoor() generates the uniform distribution on F\ud835\udc5fP. Note: The arguments to NoteCommitOrchard are the same order as their encodings in the input to SinsemillaCommit; this is different to NoteCommitSapling. The commitment scheme Commitivk speci\ufb01ed in section4.1.8 Commitment on page 31 is instantiated as follows using SinsemillaShortCommit: Commitivk rivk(ak, nk) : SinsemillaShortCommitrivk z.cash:Orchard-CommitIvk, I2LEBSP\u2113Orchard base (ak)  I2LEBSP\u2113Orchard base (nk) Commitivk.GenTrapdoor() generates the uniform distribution on F\ud835\udc5fP.",
      "Security requirements:  SinsemillaCommit and SinsemillaShortCommit, and hence NoteCommitOrchard and Commitivk, must be computa- tionally binding and at least computationally hiding commitment schemes. They are in fact unconditionally hiding commitment schemes provided that no output is observed. Theorem 5.4.6. UncommittedOrchard is not in the range of NoteCommitOrchard. Proof. UncommittedOrchardis de\ufb01ned as 2. By the de\ufb01nitions of Extract P, SinsemillaShortCommit, and NoteCommitOrchard, 2 can be in the range of NoteCommitOrchardonlyif there exist rcm NoteCommitOrchard.Trapdoor, \ud835\udc37 BYN, and \ud835\udc40 BN such that Extract SinsemillaCommitrcm(\ud835\udc37, \ud835\udc40)  2. Extract SinsemillaCommitrcm(\ud835\udc37, \ud835\udc40) can only be or 0 or the af\ufb01ne-short-Weierstrass \ud835\udc65-coordinate of a point in P. But 0  2 (mod \ud835\udc5eP), and there are no points in P with af\ufb01ne-short-Weierstrass \ud835\udc65-coordinate 2 (mod \ud835\udc5eP), since 23  \ud835\udc4fP  13 is not square in F\ud835\udc5eP.",
      "Non-normative notes:  Although the given theorem is correct for the de\ufb01nition of NoteCommitOrchard in this speci\ufb01cation, the imple- mentation in the Action circuit constrains the result to an unspeci\ufb01ed set of values when an input results in an exceptional case for any incomplete addition. If this occurs then it yields a nontrivial discrete logarithm relation for the Pallas curve, as proven in Theorem 5.4.4 on page 84. We can therefore assume that it is infeasible to \ufb01nd such inputs with nonnegligible probability. There are also no points in P with af\ufb01ne-short-Weierstrass \ud835\udc65-coordinate 0 (mod \ud835\udc5eP), as shown in a note at section5.4.9.7 Coordinate Extractor for Pallas on page 106. We do not choose UncommittedOrchard  0 because MerkleCRHOrchard returns 0 in exceptional cases. Although the hash values of leaf nodes are separated from the hash values at other layers by the layer input to MerkleCRHOrchard, it would arguably be confusing to rely on that.",
      "5.4.9 Represented Groups and Pairings 5.4.9.1 BN-254 The represented pairing BN-254 is de\ufb01ned in this section. Let \ud835\udc5eG : 21888242871839275222246405745257275088696311157297823662689037894645226208583. Let \ud835\udc5fG : 21888242871839275222246405745257275088548364400416034343698204186575808495617. Let \ud835\udc4fG : 3. (\ud835\udc5eG and \ud835\udc5fG are prime.) Let G(\ud835\udc5f) be the group (of order \ud835\udc5fG) of rational points on a BarretoNaehrig (BN2005) curve \ud835\udc38G1 over F\ud835\udc5eG with equation \ud835\udc662  \ud835\udc653  \ud835\udc4fG. This curve has embedding degree 12 with respect to \ud835\udc5fG. Let G(\ud835\udc5f) 2 be the subgroup of order \ud835\udc5fG in the sextic twist \ud835\udc38G2 of \ud835\udc38G1 over F\ud835\udc5eG 2 with equation \ud835\udc662  \ud835\udc653  \ud835\udc4fG \ud835\udf09, where F\ud835\udc5eG We represent elements of F\ud835\udc5eG 2 as polynomials \ud835\udc4e1  \ud835\udc61 \ud835\udc4e0 F\ud835\udc5eG\ud835\udc61, modulo the irreducible polynomial \ud835\udc612  1; in this representation, \ud835\udf09is given by \ud835\udc61 9. Let G(\ud835\udc5f) \ud835\udc47be the subgroup of \ud835\udc5fG th roots of unity in F 12, with multiplicative identity 1G. Let \ud835\udc52G be the optimal ate pairing (see Vercauter2009 and AKLGL2010, section 2) of type G(\ud835\udc5f) 1  G(\ud835\udc5f) 2 G(\ud835\udc5f) For \ud835\udc56 1 ..",
      "2, let \ud835\udcaaG\ud835\udc56be the point at in\ufb01nity (which is the additive identity) in G(\ud835\udc5f) \ud835\udc56, and let G(\ud835\udc5f) : G(\ud835\udc5f) \ud835\udcaaG\ud835\udc56. Let \ud835\udcabG1 G(\ud835\udc5f) : (1, 2). Let \ud835\udcabG2 G(\ud835\udc5f) : (11559732032986387107991004021392285783925812861821192530917403151452391805634  \ud835\udc61 10857046999023057135944570762232829481370756359578518086990519993285655852781, 4082367875863433681332203403145435568316851327593401208105741076214120093531  \ud835\udc61 8495653923123431417604973247489272438418190587263600148770280649306958101930). \ud835\udcabG1 and \ud835\udcabG2 are generators of G(\ud835\udc5f) 1 and G(\ud835\udc5f) 2 respectively. De\ufb01ne I2BEBSP N)  0 .. 2\u21131 B\u2113 as in section5.1 Integers, Bit Sequences, and Endianness on page 73. For a point \ud835\udc43 G(\ud835\udc5f)  (\ud835\udc65\ud835\udc43, \ud835\udc66\ud835\udc43):  The \ufb01eld elements \ud835\udc65\ud835\udc43and \ud835\udc66\ud835\udc43 F\ud835\udc5eare represented as integers \ud835\udc65and \ud835\udc66 0 .. \ud835\udc5e1. Let \ud835\udc66 \ud835\udc66mod 2. \ud835\udc43is encoded as 0 0 0 0 0 0 1 1-bit \ud835\udc66 256-bit I2BEBSP256(\ud835\udc65) For a point \ud835\udc43 G(\ud835\udc5f)  (\ud835\udc65\ud835\udc43, \ud835\udc66\ud835\udc43):  De\ufb01ne FE2IP F\ud835\udc5eG\ud835\udc61(\ud835\udc612  1) 0 .. \ud835\udc5eG 21 such that FE2IP(\ud835\udc4e\ud835\udc64,1  \ud835\udc61 \ud835\udc4e\ud835\udc64,0)  \ud835\udc4e\ud835\udc64,1  \ud835\udc5e \ud835\udc4e\ud835\udc64,0. Let \ud835\udc65 FE2IP(\ud835\udc65\ud835\udc43), \ud835\udc66 FE2IP(\ud835\udc66\ud835\udc43), and \ud835\udc66  FE2IP(\ud835\udc66\ud835\udc43). Let \ud835\udc66 1, if \ud835\udc66 \ud835\udc66 0, otherwise.",
      "\ud835\udc43is encoded as 0 0 0 0 1 0 1 1-bit \ud835\udc66 512-bit I2BEBSP512(\ud835\udc65) Non-normative notes:  Only the \ud835\udc5fG-order subgroups G(\ud835\udc5f) 2,\ud835\udc47are used in the protocol, not their containing groups G2,\ud835\udc47. Points in G(\ud835\udc5f) are always checked to be of order \ud835\udc5fG when decoding from external representation. (The group of rational points G1 on \ud835\udc38G1F\ud835\udc5eG is of order \ud835\udc5fG so no subgroup checks are needed in that case, and elements of G(\ud835\udc5f) \ud835\udc47are never represented externally.) The (\ud835\udc5f) superscripts on G(\ud835\udc5f) 1,2,\ud835\udc47are used for consistency with notation elsewhere in this speci\ufb01cation. The points at in\ufb01nity \ud835\udcaaG1,2 never occur in proofs and have no de\ufb01ned encodings in this protocol. A rational point \ud835\udc43 \ud835\udcaaG2 on the curve \ud835\udc38G2 can be veri\ufb01ed to be of order \ud835\udc5fG, and therefore in G(\ud835\udc5f) , by checking that \ud835\udc5fG  \ud835\udc43 \ud835\udcaaG2. The use of big-endian order by I2BEBSP is different from the encoding of most other integers in this pro- tocol.",
      "The encodings for G(\ud835\udc5f) 1,2 are consistent with the de\ufb01nition of EC2OSP for compressed curve points in IEEE2004, section 5.5.6.2. The LSB compressed form (i.e. EC2OSP-XL) is used for points in G(\ud835\udc5f) , and the SORT compressed form (i.e. EC2OSP-XS) for points in G(\ud835\udc5f)  Testing \ud835\udc66 \ud835\udc66 for the compression of G(\ud835\udc5f) points is equivalent to testing whether (\ud835\udc4e\ud835\udc66,1, \ud835\udc4e\ud835\udc66,0)  (\ud835\udc4e\ud835\udc66,1, \ud835\udc4e\ud835\udc66,0) in lexicographic order. Algorithms for decompressing points from the above encodings are given in IEEE2000, Appendix A.12.8 for G(\ud835\udc5f) , and IEEE2004, Appendix A.12.11 for G(\ud835\udc5f) When computing square roots in F\ud835\udc5eG or F\ud835\udc5eG 2 in order to decompress a point encoding, the implementation MUST NOT assume that the square root exists, or that the encoding represents a point on the curve. 5.4.9.2 BLS12-381 The represented pairing BLS12-381 is de\ufb01ned in this section. Parameters are taken from Bowe2017. Let \ud835\udc5eS : 4002409555221667393417789825735904156556882819939007885332058136124031650490837864442687629129015664037894272559787.",
      "Let \ud835\udc5fS : 52435875175126190479447740508185965837690552500527637822603658699938581184513. Let \ud835\udc62S : 15132376222941642752. Let \ud835\udc4fS : 4. (\ud835\udc5eS and \ud835\udc5fS are prime.) Let S(\ud835\udc5f) 1 be the subgroup of order \ud835\udc5fS of the group of rational points on a BarretoLynnScott (BLS2002) curve \ud835\udc38S1 over F\ud835\udc5eS with equation \ud835\udc662  \ud835\udc653  \ud835\udc4fS. This curve has embedding degree 12 with respect to \ud835\udc5fS. Let S(\ud835\udc5f) 2 be the subgroup of order \ud835\udc5fS in the sextic twist \ud835\udc38S2 of \ud835\udc38S1 over F\ud835\udc5eS 2 with equation \ud835\udc662  \ud835\udc653  4(\ud835\udc56 1), where F\ud835\udc5eS We represent elements of F\ud835\udc5eS 2 as polynomials \ud835\udc4e1  \ud835\udc61 \ud835\udc4e0 F\ud835\udc5eS\ud835\udc61, modulo the irreducible polynomial \ud835\udc612  1; in this representation, \ud835\udc56is given by \ud835\udc61. Let S(\ud835\udc5f) \ud835\udc47be the subgroup of \ud835\udc5fS th roots of unity in F 12, with multiplicative identity 1S. Let \ud835\udc52S be the optimal ate pairing of type S(\ud835\udc5f) 1  S(\ud835\udc5f) 2 S(\ud835\udc5f) For \ud835\udc56 1 .. 2, let \ud835\udcaaS\ud835\udc56be the point at in\ufb01nity in S(\ud835\udc5f) \ud835\udc56, and let S(\ud835\udc5f) : S(\ud835\udc5f) \ud835\udcaaS\ud835\udc56.",
      "Let \ud835\udcabS1 S(\ud835\udc5f) (3685416753713387016781088315183077757961620795782546409894578378688607592378376318836054947676345821548104185464507, 1339506544944476473020471379941921221584933875938349620426543736416511423956333506472724655353366534992391756441569). Let \ud835\udcabS2 S(\ud835\udc5f) (3059144344244213709971259814753781636986470325476647558659373206291635324768958432433509563104347017837885763365758  \ud835\udc61 352701069587466618187139116011060144890029952792775240219908644239793785735715026873347600343865175952761926303160, 927553665492332455747201965776037880757740193453592970025027978793976877002675564980949289727957565575433344219582  \ud835\udc61 1985150602287291935568054521177171638300868978215655730859378665066344726373823718423869104263333984641494340347905). \ud835\udcabS1 and \ud835\udcabS2 are generators of S(\ud835\udc5f) 1 and S(\ud835\udc5f) 2 respectively. De\ufb01ne I2BEBSP N)  0 .. 2\u21131 B\u2113 as in section5.1 Integers, Bit Sequences, and Endianness on page 73. For a point \ud835\udc43 S(\ud835\udc5f)  (\ud835\udc65\ud835\udc43, \ud835\udc66\ud835\udc43):  The \ufb01eld elements \ud835\udc65\ud835\udc43and \ud835\udc66\ud835\udc43 F\ud835\udc5eS are represented as integers \ud835\udc65and \ud835\udc66 0 .. \ud835\udc5eS1.",
      "Let \ud835\udc66 1, if \ud835\udc66 \ud835\udc5eS \ud835\udc66 0, otherwise. \ud835\udc43is encoded as 1 0 1-bit \ud835\udc66 381-bit I2BEBSP381(\ud835\udc65) For a point \ud835\udc43 S(\ud835\udc5f)  (\ud835\udc65\ud835\udc43, \ud835\udc66\ud835\udc43):  De\ufb01ne FE2IPP F\ud835\udc5eS\ud835\udc61(\ud835\udc612  1) 0 .. \ud835\udc5eS12 such that FE2IPP(\ud835\udc4e\ud835\udc64,1  \ud835\udc61 \ud835\udc4e\ud835\udc64,0)  \ud835\udc4e\ud835\udc64,1, \ud835\udc4e\ud835\udc64,0. Let \ud835\udc65 FE2IPP(\ud835\udc65\ud835\udc43), \ud835\udc66 FE2IPP(\ud835\udc66\ud835\udc43), and \ud835\udc66  FE2IPP(\ud835\udc66\ud835\udc43). Let \ud835\udc66 1, if \ud835\udc66 \ud835\udc66 lexicographically 0, otherwise. \ud835\udc43is encoded as 1 0 1-bit \ud835\udc66 381-bit I2BEBSP381(\ud835\udc651) 384-bit I2BEBSP384(\ud835\udc652) Non-normative notes:  Only the \ud835\udc5fS-order subgroups S(\ud835\udc5f) 1,2,\ud835\udc47are used in the protocol, not their containing groups S1,2,\ud835\udc47. Points in S(\ud835\udc5f) are always checked to be of order \ud835\udc5fS when decoding from external representation. (Elements of S(\ud835\udc5f) \ud835\udc47are never represented externally.) The (\ud835\udc5f) superscripts on S(\ud835\udc5f) 1,2,\ud835\udc47are used for consistency with notation elsewhere in this speci\ufb01cation. The points at in\ufb01nity \ud835\udcaaS1,2 never occur in proofs and have no de\ufb01ned encodings in this protocol. In contrast to the corresponding BN-254 curve, \ud835\udc38S1 over F\ud835\udc5eS is not a prime-order curve.",
      "A rational point \ud835\udc43 \ud835\udcaaS\ud835\udc56on the curve \ud835\udc38S\ud835\udc56for \ud835\udc561, 2 can be veri\ufb01ed to be of order \ud835\udc5fS, and therefore in S(\ud835\udc5f) by checking that \ud835\udc5fS  \ud835\udc43 \ud835\udcaaS\ud835\udc56. The use of big-endian order by I2BEBSP is different from the encoding of most other integers in this protocol. The encodings for S(\ud835\udc5f) 1,2 are speci\ufb01c to Zcash. Algorithms for decompressing points from the encodings of S(\ud835\udc5f) 1,2 are de\ufb01ned analogously to those for G(\ud835\udc5f) 1,2 in section5.4.9.1 BN-254 on page 99, taking into account that the SORT compressed form (not the LSB compressed form) is used for S(\ud835\udc5f) When computing square roots in F\ud835\udc5eS or F\ud835\udc5eS 2 in order to decompress a point encoding, the implementation MUST NOT assume that the square root exists, or that the encoding represents a point on the curve. 5.4.9.3 Jubjub You boil it in sawdust: you salt it in glue: You condense it with locusts and tape: Still keeping one principal object in view To preserve its symmetrical shape.",
      "Lewis Carroll, The Hunting of the Snark Carroll1876 Sapling uses an elliptic curve, Jubjub, designed to be ef\ufb01cientlyimplementable in zk-SNARK circuits. The represented group J of points on this curve is de\ufb01ned in this section. A complete twisted Edwards elliptic curve, as de\ufb01ned in BL2017, section 4.3.4, is an elliptic curve \ud835\udc38over a non- binary \ufb01eld F\ud835\udc5e, parameterized by distinct \ud835\udc4e, \ud835\udc51 F\ud835\udc5e0 such that \ud835\udc4eis square and \ud835\udc51is nonsquare, with equation \ud835\udc38: \ud835\udc4e\ud835\udc622  v2  1  \ud835\udc51\ud835\udc622v2. We use the abbreviation ctEdwards to refer to complete twisted Edwards elliptic curves and coordinates. Let \ud835\udc5eJ : \ud835\udc5fS, as de\ufb01ned in section5.4.9.2 BLS12-381 on page 101. Let \ud835\udc5fJ : 6554484396890773809930967563523245729705921265872317281365359162392183254199. (\ud835\udc5eJ and \ud835\udc5fJ are prime.) Let \u210eJ : 8. Let \ud835\udc4eJ : 1. Let \ud835\udc51J : 1024010241 (mod \ud835\udc5eJ). Let J be the group of points (\ud835\udc62, v) on a ctEdwards curve \ud835\udc38J over F\ud835\udc5eJ with equation \ud835\udc4eJ\ud835\udc622  v2  1  \ud835\udc51J\ud835\udc622v2. The zero point with coordinates (0, 1) is denoted \ud835\udcaaJ. J has order \u210eJ\ud835\udc5fJ. Let \u2113J : 256.",
      "De\ufb01ne the notation ? as in section2 Notation on page 10. De\ufb01ne I2LEBSP N)  0 .. 2\u21131 B\u2113 as in section5.1 Integers, Bit Sequences, and Endianness on page 73, and similarly for LEBS2IP N)  B\u2113 0 .. 2\u21131. De\ufb01ne reprJ J B\u2113J such that reprJ (\ud835\udc62, v)  I2LEBSP256 (v mod \ud835\udc5eJ)  2255\ud835\udc62 , where \ud835\udc62 \ud835\udc62mod 2. De\ufb01ne abstJ B\u2113J J  such that abstJ(\ud835\udc43)is computed as follows: let v B255 be the \ufb01rst 255 bits of \ud835\udc43and let \ud835\udc62 B be the last bit. if LEBS2IP255(v) \ud835\udc5eJ then return , otherwise let v F\ud835\udc5eJ  LEBS2IP255(v) (mod \ud835\udc5eJ). let \ud835\udc62 ? 1 v2 \ud835\udc4eJ \ud835\udc51J  v2 . (The denominator \ud835\udc4eJ \ud835\udc51Jv2 cannot be zero, since \ud835\udc4eJ \ud835\udc51J is not square in F\ud835\udc5eJ.) if \ud835\udc62 , return . if \ud835\udc62mod 2  \ud835\udc62then return (\ud835\udc62, v) else return (\ud835\udc5eJ \ud835\udc62, v). Note: In earlier versions of this speci\ufb01cation, abstJ was de\ufb01ned as the left inverse of reprJ such that if \ud835\udc46is not in the range of reprJ, then abstJ(\ud835\udc46) . This differs from the speci\ufb01cation above:  Previously, abstJ I2LEBSP256 2255  1 )) and abstJ I2LEBSP256 2255  \ud835\udc5eJ 1 )) were de\ufb01ned as .",
      "In the current speci\ufb01cation, abstJ I2LEBSP256 2255  1 ))  abstJ I2LEBSP256(1)  (0, 1)  \ud835\udcaaJ, and also abstJ I2LEBSP256 2255  \ud835\udc5eJ 1 ))  abstJ I2LEBSP256 \ud835\udc5eJ 1 ))  (0, 1). De\ufb01ne J(\ud835\udc5f) as the order-\ud835\udc5fJ subgroup of J. Note that this includes \ud835\udcaaJ. For the set of points of order \ud835\udc5fJ (which excludes \ud835\udcaaJ), we write J(\ud835\udc5f). De\ufb01ne J (\ud835\udc5f) : reprJ(\ud835\udc43) B\u2113J  \ud835\udc43J(\ud835\udc5f) Non-normative notes:  The ctEdwards compressed encoding used here is consistent with that used in EdDSA BJLSY2015 for validating keys and the \ud835\udc45element of a signature. BJLSY2015, Encoding and parsing curve points gives algorithms for decompressing points from the encod- ing of J. BJLSY2015, Encoding and parsing integers describes several possibilities for parsing of integers; the speci- \ufb01cation of abstJ above requires strict parsing. When computing square roots in F\ud835\udc5eJ in order to decompress a point encoding, the implementation MUST NOT assume that the square root exists, or that the encoding represents a point on the curve.",
      "Note that algorithms elsewhere in this speci\ufb01cation that use Jubjub may impose other conditions on points, for example that they have order at least \ud835\udc5fJ. 5.4.9.4 Coordinate Extractor for Jubjub Let \ud835\udc62 (\ud835\udc62, v)  \ud835\udc62and let v (\ud835\udc62, v)  v. De\ufb01ne ExtractJ(\ud835\udc5f) J(\ud835\udc5f) B\u2113Sapling Merkle  by ExtractJ(\ud835\udc5f)(\ud835\udc43) : I2LEBSP\u2113Sapling Merkle (\ud835\udc62(\ud835\udc43) Facts: The point (0, 1)  \ud835\udcaaJ, and the point (0, 1) has order 2 in J. J(\ud835\udc5f) is of odd-prime order. Lemma 5.4.7. Let \ud835\udc43 (\ud835\udc62, v) J(\ud835\udc5f). Then (\ud835\udc62, v) J(\ud835\udc5f). Proof. If \ud835\udc43 \ud835\udcaaJ then (\ud835\udc62, v)  (0, 1) J(\ud835\udc5f). Else, \ud835\udc43is of odd-prime order. Note that v  0. (If v  0 then \ud835\udc4e \ud835\udc622  1, and so applying the doubling formula gives 2 \ud835\udc43 (0, 1), then 4 \ud835\udc43 (0, 1)  \ud835\udcaaJ; contradiction since then \ud835\udc43would not be of odd-prime order.) Therefore, v  v. Now suppose (\ud835\udc62, v)  \ud835\udc44is a point in J(\ud835\udc5f). Then by applying the doubling formula we have 2 \ud835\udc44 2 \ud835\udc43. But also 2 (\ud835\udc43)  2 \ud835\udc43. Therefore either \ud835\udc44 \ud835\udc43(then v(\ud835\udc44) v(\ud835\udc43); contradiction since v  v), or doubling is not injective on J(\ud835\udc5f) (contradiction since J(\ud835\udc5f) is of odd order KvE2013).",
      "Theorem 5.4.8. \ud835\udc62is injective on J(\ud835\udc5f). Proof. By writing the curve equation as v2  (1 \ud835\udc4e\ud835\udc622)(1 \ud835\udc51\ud835\udc622), and noting that the potentially exceptional case 1 \ud835\udc51\ud835\udc622  0 does not occur for a ctEdwards curve, we see that for a given \ud835\udc62there can be at most two possible solutions for v, and that if there are two solutions they can be written as v and v. In that case by the Lemma, at most one of (\ud835\udc62, v) and (\ud835\udc62, v) is in J(\ud835\udc5f). Therefore, \ud835\udc62is injective on points in J(\ud835\udc5f). Since I2LEBSP\u2113Sapling Merkle is injective, it follows that ExtractJ(\ud835\udc5f) is injective on J(\ud835\udc5f). 5.4.9.5 Group Hash into Jubjub Let URS be the MPC randomness beacon de\ufb01ned in section5.9 Randomness Beacon on page 120. Let BLAKE2s-256 be as de\ufb01ned in section5.4.1.2 BLAKE2 Hash Functions on page 76. Let LEOS2IP be as de\ufb01ned in section5.1 Integers, Bit Sequences, and Endianness on page 73. Let J(\ud835\udc5f), J(\ud835\udc5f), and abstJ be as de\ufb01ned in section5.4.9.3 Jubjub on page 102. Let GroupHashJ(\ud835\udc5f) .Input : BY8  BYN, and let GroupHashJ(\ud835\udc5f) .URSType : BY64.",
      "(The input element with type BY8 is intended to act as a personalization parameter to distinguish uses of the group hash for different purposes.) Let \ud835\udc37 BY8 be an 8-byte domain separator, and let \ud835\udc40 BYN be the hash input. The hash GroupHashJ(\ud835\udc5f) URS (\ud835\udc37, \ud835\udc40) J(\ud835\udc5f)  is calculated as follows: let \ud835\udc3b BLAKE2s-256(\ud835\udc37, URS  \ud835\udc40) let \ud835\udc43 abstJ LEOS2BSP256(\ud835\udc3b) if \ud835\udc43 then return  let \ud835\udc44 \u210eJ \ud835\udc43 if \ud835\udc44 \ud835\udcaaJ then return , else return \ud835\udc44. Notes:  The use of GroupHashJ(\ud835\udc5f) URS for DiversifyHashSapling and to generate independent bases needs a random oracle (for inputs on which GroupHashJ(\ud835\udc5f) URS does not return ); here we show that it is suf\ufb01cient to employ a simpler random oracle instantiated by BLAKE2s-256 in the security analysis. BY32 , \ud835\udcaaJ, (0,1) abstJ LEOS2BSP256(\ud835\udc3b) J is injective, and both it and its inverse are ef\ufb01ciently computable. J \ud835\udcaaJ \u210eJ \ud835\udc43 J(\ud835\udc5f) is exactly \u210eJ-to-1, and both it and its inverse relation are ef\ufb01ciently computable.",
      "It follows that when BY8, \ud835\udc40 BYN) BLAKE2s-256(\ud835\udc37, URS  \ud835\udc40) BY32 is modelled as a random oracle, BY8, \ud835\udc40 BYN)  GroupHashJ(\ud835\udc5f) \ud835\udc37, \ud835\udc40 J(\ud835\udc5f) also acts as a random oracle. The BLAKE2s-256 chaining variable after processing URS may be precomputed. De\ufb01ne first (BY \ud835\udc47) \ud835\udc47 so that first(\ud835\udc53)  \ud835\udc53(\ud835\udc56) where \ud835\udc56is the least integer in BY such that \ud835\udc53(\ud835\udc56)  , or if no such \ud835\udc56exists. De\ufb01ne FindGroupHashJ(\ud835\udc5f)( \ud835\udc37, \ud835\udc40 : first(\ud835\udc56 BY GroupHashJ(\ud835\udc5f) URS (\ud835\udc37, \ud835\udc40 \ud835\udc56) J(\ud835\udc5f) ). Note: For random input, FindGroupHashJ(\ud835\udc5f) returns with probability approximately 2256. In the Zcash protocol, most uses of FindGroupHashJ(\ud835\udc5f) are for constants and do not return ; the only use that could potentially return  is in the computation of a default diversi\ufb01ed payment address in section4.2.2 Sapling Key Components on page 36. 5.4.9.6 Pallas and Vesta Orchard uses two elliptic curves, Pallas and Vesta, that form a cycle: the base \ufb01eld of each is the scalar \ufb01eld of the other.",
      "In Orchard, we use Vesta for the proof system (playing a similar r\u00f4le to BLS12-381 in Sapling), and Pallas for the application circuit (similar to Jubjub in Sapling). Both curves are designed to be ef\ufb01ciently implementable in zk-SNARK circuits, although we only use Pallas in that way for Orchard. The represented groups P and V of points on Pallas and Vesta respectively are de\ufb01ned in this section. A short Weierstrass elliptic curve over a \ufb01eld F\ud835\udc5eof characteristic greater than 3, as de\ufb01ned for example in H\u0131s\u0131l2010, De\ufb01nition 2.3.1, is an elliptic curve \ud835\udc38over F\ud835\udc5e, parameterized by \ud835\udc4e, \ud835\udc4f F\ud835\udc5esuch that 4  \ud835\udc4e3  27  \ud835\udc4f2  0, with equation \ud835\udc38: \ud835\udc662  \ud835\udc653  \ud835\udc4e \ud835\udc65 \ud835\udc4f. The curve has a distinguished zero point \ud835\udcaa, also called the point at in\ufb01nity. For Pallas and Vesta we have \ud835\udc4e 0 and so we will omit that term below. Let \ud835\udc5eP : 0x40000000000000000000000000000000224698fc094cf91b992d30ed00000001. Let \ud835\udc5eV : 0x40000000000000000000000000000000224698fc0994a8dd8c46eb2100000001. (\ud835\udc5eP and \ud835\udc5eV are prime.) Let \ud835\udc5fP : \ud835\udc5eV and \ud835\udc5fV : \ud835\udc5eP.",
      "Let \ud835\udc4fP  \ud835\udc4fV : 5. Let P be the group of points (\ud835\udc65, \ud835\udc66) with zero point \ud835\udcaaP, on a short Weierstrass curve \ud835\udc38P over F\ud835\udc5eP with equation \ud835\udc662  \ud835\udc653  \ud835\udc4fP. P has order \ud835\udc5fP. Let V be the group of points (\ud835\udc65, \ud835\udc66) with zero point \ud835\udcaaV, on a short Weierstrass curve \ud835\udc38V over F\ud835\udc5eV with equation \ud835\udc662  \ud835\udc653  \ud835\udc4fV. V has order \ud835\udc5fV. For the set of points on Pallas of order \ud835\udc5fP (which excludes \ud835\udcaaP), we write P. For the set of points on Vesta of order \ud835\udc5fV (which excludes \ud835\udcaaV), we write V. Let \u2113P  \u2113V : 256. De\ufb01ne the notation ? as in section2 Notation on page 10. De\ufb01ne I2LEBSP N)  0 .. 2\u21131 B\u2113 as in section5.1 Integers, Bit Sequences, and Endianness on page 73, and similarly for LEBS2IP N)  B\u2113 0 .. 2\u21131. Let G be either P or V. De\ufb01ne reprG G B\u2113G such that reprG  I2LEBSP256(0) reprG (\ud835\udc65, \ud835\udc66)  I2LEBSP256 (\ud835\udc65mod \ud835\udc5eG)  2255\ud835\udc66 , where \ud835\udc66 \ud835\udc66mod 2. De\ufb01ne abstG B\u2113G G  such that abstG(\ud835\udc43)is computed as follows: let \ud835\udc65 B255 be the \ufb01rst 255 bits of \ud835\udc43and let \ud835\udc66 B be the last bit. if LEBS2IP255(\ud835\udc65) \ud835\udc5eG then return , otherwise let \ud835\udc65 F\ud835\udc5eG  LEBS2IP255(\ud835\udc65) (mod \ud835\udc5eG).",
      "let \ud835\udc66 ? \ud835\udc653  \ud835\udc4fG . if \ud835\udc65 0 and \ud835\udc66 0, return \ud835\udcaaG. if \ud835\udc66 , return . if \ud835\udc66mod 2  \ud835\udc66then return (\ud835\udc65, \ud835\udc66) else return (\ud835\udc65, \ud835\udc5eG \ud835\udc66). Notes:  There is no solution to 0  \ud835\udc653  5 in either F\ud835\udc5eP or F\ud835\udc5eV, and so \ud835\udc66cannot be zero. Therefore there is only one valid representation of each point on Pallas and of each point on Vesta; in particular abstP(nc) and abstV(nc)  for nc  I2LEBSP256 2255) . This differs from the corresponding case of abstJ(nc)for Jubjub, for example. When computing square roots in F\ud835\udc5eP or F\ud835\udc5eV in order to decompress a point encoding, the implementation MUST NOT assume that the square root exists, or that the encoding represents a point on the curve. 5.4.9.7 Coordinate Extractor for Pallas Let P, \ud835\udcaaP, \ud835\udc5eP, and \ud835\udc4fP be as de\ufb01ned in section5.4.9.6 Pallas and Vesta on page 105. De\ufb01ne \ud835\udc65 P F\ud835\udc5eP and \ud835\udc66 P F\ud835\udc5eP such that: (\ud835\udc65, \ud835\udc66) (\ud835\udc65, \ud835\udc66)  \ud835\udc66. De\ufb01ne ExtractP P 0 .. \ud835\udc5eP 1 such that ExtractP(\ud835\udc43)  \ud835\udc65(\ud835\udc43) mod \ud835\udc5eP. We also de\ufb01ne Extract P  0 .. \ud835\udc5eP 1  such that Extract Extract  ExtractP(\ud835\udc43).",
      "Note: There is no solution to \ud835\udc662  03  5 in F\ud835\udc5eP, and so ExtractP(\ud835\udc43) can only be 0 when \ud835\udc43 \ud835\udcaaP. 5.4.9.8 Group Hash into Pallas and Vesta Orchard uses the simpli\ufb01ed SWU algorithm for random-oracle hashing to elliptic curves with \ud835\udc57-invariant 0, consistent with ID-hashtocurve, section 6.6.3, based on a method by Riad Wahby and Dan Boneh WB2019. It is adapted from work of Eric Brier, Jean-S\u00e9bastien Coron, Thomas Icart, David Madore, Hugues Randriam, and Mehdi Tibouchi in BCIMRT2010; Andrew Shallue and Christiaan van de Woestijne in SvdW2006; and Maciej Ulas in Ulas2007. Let P and V be the represented groups of points on the Pallas curve and the Vesta curve respectively, as de\ufb01ned in section5.4.9.6 Pallas and Vesta on page 105. Let G be either P or V according to the desired target curve. Also de\ufb01ne \ud835\udcaaG, G, \ud835\udc5eG, and abstG by replacing G with P or V, using de\ufb01nitions from section5.4.9.6 Pallas and Vesta on page 105. Let curveNameG be pallas when G  P, or vesta when G  V.",
      "The algorithm makes use of a curve \ud835\udc38iso-P, called iso-Pallas, that is isogenous11 to \ud835\udc38P; or \ud835\udc38iso-V, called iso-Vesta, that is isogenous to \ud835\udc38V. Let \ud835\udc4eiso-P : 0x18354a2eb0ea8c9c49be2d7258370742b74134581a27a59f92bb4b0b657a014b. Let \ud835\udc4eiso-V : 0x267f9b2ee592271a81639c4d96f787739673928c7d01b212c515ad7242eaa6b1. Let \ud835\udc4fiso-P  \ud835\udc4fiso-V : 1265. Let iso-P be the group of points (\ud835\udc65, \ud835\udc66) with zero point \ud835\udcaaiso-P, on a short Weierstrass curve \ud835\udc38iso-P over F\ud835\udc5eP with equation \ud835\udc662  \ud835\udc653  \ud835\udc4eiso-P  \ud835\udc65 \ud835\udc4fiso-P. Since \ud835\udc38iso-P is isogenous to \ud835\udc38P, it has the same order \ud835\udc5fiso-P  \ud835\udc5fP  \ud835\udc5eV. Let iso-V be the group of points (\ud835\udc65, \ud835\udc66) with zero point \ud835\udcaaiso-V, on a short Weierstrass curve \ud835\udc38iso-V over F\ud835\udc5eV with equation \ud835\udc662  \ud835\udc653  \ud835\udc4eiso-V  \ud835\udc65 \ud835\udc4fiso-V. Since \ud835\udc38iso-V is isogenous to \ud835\udc38V, it has the same order \ud835\udc5fiso-V  \ud835\udc5fV  \ud835\udc5eP.",
      "Let \ud835\udc9eP F\ud835\udc5eP 13 :  0x0e38e38e38e38e38e38e38e38e38e38e4081775473d8375b775f6034aaaaaaab, 0x3509afd51872d88e267c7ffa51cf412a0f93b82ee4b994958cf863b02814fb76, 0x17329b9ec525375398c7d7ac3d98fd13380af066cfeb6d690eb64faef37ea4f7, 0x1c71c71c71c71c71c71c71c71c71c71c8102eea8e7b06eb6eebec06955555580, 0x1d572e7ddc099cff5a607fcce0494a799c434ac1c96b6980c47f2ab668bcd71f, 0x325669becaecd5d11d13bf2a7f22b105b4abf9fb9a1fc81c2aa3af1eae5b6604, 0x1a12f684bda12f684bda12f684bda12f7642b01ad461bad25ad985b5e38e38e4, 0x1a84d7ea8c396c47133e3ffd28e7a09507c9dc17725cca4ac67c31d8140a7dbb, 0x3fb98ff0d2ddcadd303216cce1db9ff11765e924f745937802e2be87d225b234, 0x025ed097b425ed097b425ed097b425ed0ac03e8e134eb3e493e53ab371c71c4f, 0x0c02c5bcca0e6b7f0790bfb3506defb65941a3a4a97aa1b35a28279b1d1b42ae, 0x17033d3c60c68173573b3d7f7d681310d976bbfabbc5661d4d90ab820b12320a, 0x40000000000000000000000000000000224698fc094cf91b992d30ecfffffde5 Let \ud835\udc9eV F\ud835\udc5eV 13 :  0x38e38e38e38e38e38e38e38e38e38e390205dd51cfa0961a43cd42c800000001, 0x1d935247b4473d17acecf10f5f7c09a2216b8861ec72bd5d8b95c6aaf703bcc5, 0x18760c7f7a9ad20ded7ee4a9cdf78f8fd59d03d23b39cb11aeac67bbeb586a3d, 0x31c71c71c71c71c71c71c71c71c71c71e1c521a795ac8356fb539a6f0000002b, 0x0a2de485568125d51454798a5b5c56b2a3ad678129b604d3b7284f7eaf21a2e9, 0x14735171ee5427780c621de8b91c242a30cd6d53df49d235f169c187d2533465, 0x12f684bda12f684bda12f684bda12f685601f4709a8adcb36bef1642aaaaaaab, 0x2ec9a923da239e8bd6767887afbe04d121d910aefb03b31d8bee58e5fb81de63, 0x19b0d87e16e2578866d1466e9de10e6497a3ca5c24e9ea634986913ab4443034, 0x1ed097b425ed097b425ed097b425ed098bc32d36fb21a6a38f64842c55555533, 0x2f44d6c801c1b8bf9e7eb64f890a820c06a767bfc35b5bac58dfecce86b2745e, 0x3d59f455cafc7668252659ba2b546c7e926847fb9ddd76a1d43d449776f99d2f, 0x40000000000000000000000000000000224698fc0994a8dd8c46eb20fffffde5 11 For a brief introduction to isogenies between elliptic curves, see Cook2019.",
      "For deeper mathematical background, see the notes for lectures 4, 5, and 6 at Sutherland2021. Let iso_mapG iso-G G be the isogeny map given by: iso_mapG( \ud835\udcaaiso-G  \ud835\udcaaG iso_mapG( (\ud835\udc65, \ud835\udc66) 1  \ud835\udc653  \ud835\udc9eG 2  \ud835\udc652  \ud835\udc9eG 3  \ud835\udc65 \ud835\udc9eG \ud835\udc652  \ud835\udc9eG 5  \ud835\udc65 \ud835\udc9eG 7  \ud835\udc653  \ud835\udc9eG 8  \ud835\udc652  \ud835\udc9eG 9  \ud835\udc65 \ud835\udc9eG \ud835\udc653  \ud835\udc9eG 11  \ud835\udc652  \ud835\udc9eG 12  \ud835\udc65 \ud835\udc9eG Let BLAKE2b-512 BY16  BYN BY\u21138 be as de\ufb01ned in section5.4.1.2 BLAKE2 Hash Functions on page 76. De\ufb01ne the notation ? as in section2 Notation on page 10. Let BEOS2IP be as de\ufb01ned in section5.1 Integers, Bit Sequences, and Endianness on page 73. De\ufb01ne hash_to_field XMD:BLAKE2b(msg BYN, DST BY0 .. 255) F\ud835\udc5eG 2 as follows: let DST  DST   length(DST)  let msg  0x00128  msg   0, 128    0   DST let \ud835\udc4f0  BLAKE2b-512 0x0016, msg) let \ud835\udc4f1  BLAKE2b-512 0x0016, \ud835\udc4f0   1   DST) let \ud835\udc4f2  BLAKE2b-512 0x0016, (\ud835\udc4f0 \ud835\udc4f1)   2   DST) return  BEOS2IP512(\ud835\udc4f1) (mod \ud835\udc5eG), BEOS2IP512(\ud835\udc4f2) (mod \ud835\udc5eG) .",
      "Non-normative notes:  This algorithm is intended to correspond to hash_to_field(msg, 2) de\ufb01ned in ID-hashtocurve, section 5.3, using as its expand_message parameter the function XMD:BLAKE2b corresponding to expand_message_xmd de\ufb01ned in ID-hashtocurve, section 5.4.1, and with domain separation tag DST. In expand_message_xmd, H is instantiated as BLAKE2b-512 with b_in_bytes  64 and r_in_bytes  128, and we specialize to len_in_bytes  128 since that is the only case we need. In the event of any discrepancy or change to the Internet Draft, the de\ufb01nition here takes precedence. The security level \ud835\udc58in the Internet Draft is taken to be 256. Although this is greater than the conjectured 126-bit security of the Pallas curve against generic (e.g.",
      "Pollard rho) attacks Hopwood2020, this design choice is consistent with other instances of extracting a uniformly distributed \ufb01eld element from a hash output in the Orchard protocol, such as ToScalarOrchard and ToBaseOrchard de\ufb01ned in section4.2.3 Orchard Key Components on page 38, and Hde\ufb01ned in section5.4.7 RedDSA, RedJubjub, and RedPallas on page 92. Unlike other uses of BLAKE2b in Zcash, zero bytes are used for the BLAKE2b personalization, in order to follow the Internet Draft which encodes DST in the hash inputs instead. The conversion from bytes to \ufb01eld elements uses big-endian order, again in order to follow the Internet Draft. A minor optimization is to cache the state of the BLAKE2b-512 instance used to compute \ud835\udc4f0 after processing 0x00128, since this state does not depend on the message. Let \ud835\udf06G be any \ufb01xed nonsquare in F\ud835\udc5eG. De\ufb01ne sqrt_ratioF\ud835\udc5eG(num, div) F\ud835\udc5eG  F \ud835\udc5eG F\ud835\udc5eG  B as follows: sqrt_ratioF\ud835\udc5eG(num, div)  (? numdiv , , if numdiv is square in F\ud835\udc5eG (? \ud835\udf06G  numdiv , 0 , otherwise.",
      "Non-normative notes:  An arbitrary square root may be chosen in either case of the de\ufb01nition. The result is never . The choice of the nonsquare \ud835\udf06G is also arbitrary and will not affect the output of map_to_curve_simple_swuiso-G de\ufb01ned below. The computation of sqrt_ratioF\ud835\udc5eG can be optimized as described in Zcash-halo2, section 3.2.1 Fields. De\ufb01ne \ud835\udc4diso-G : 13 (mod \ud835\udc5eG). (This value is suitable for both iso-Pallas and iso-Vesta.) Precompute \ud835\udf03iso-G : ? \ud835\udc4diso-G\ud835\udf06G , which is not .12 By de\ufb01nition we have that \ud835\udc38G is the short Weierstrass curve with equation \ud835\udc662  \ud835\udc653  \ud835\udc4fG, and \ud835\udc38iso-G is the short Weierstrass curve with equation \ud835\udc662  \ud835\udc653  \ud835\udc4eiso-G  \ud835\udc65 \ud835\udc4fiso-G. De\ufb01ne map_to_curve_simple_swuiso-G(\ud835\udc62 F\ud835\udc5eG) iso-G as follows: let Zuu  \ud835\udc4diso-G  \ud835\udc622 let ta  Zuu2  Zuu let x1num  \ud835\udc4fiso-G  (ta  1) let xdiv  \ud835\udc4eiso-G  (ta  0) ?",
      "\ud835\udc4diso-G : ta compute x2 div and x3 let U  (x12 num  \ud835\udc4eiso-G  x2 div)  x1num  \ud835\udc4fiso-G  x3 let x2num  Zuu  x1num let (y1, is_gx1_square)  sqrt_ratioF\ud835\udc5eG(U, x3 div) let y2  \ud835\udf03iso-G  Zuu  \ud835\udc62 y1 let xnum  is_gx1_square ? x1num : x2num let y  is_gx1_square ? y1 : y2 let y  (\ud835\udc62mod 2  \ud835\udc66mod 2) ? y : y return the \ud835\udc38iso-G point with af\ufb01ne-short-Weierstrass coordinates (xnumxdiv, y). Let GroupHashG.Input : BYN  BYN. The \ufb01rst input element acts as a domain separator to distinguish uses of the group hash for different purposes; the second input element is the message. This hash-to-curve algorithm does not have a URS, i.e. GroupHashG.URSType : (). The hash GroupHashG(\ud835\udc37 BYN, \ud835\udc40 BYN) G is calculated as follows: let DST  \ud835\udc37 -  curveNameG  _XMD:BLAKE2b_SSWU_RO_ fail if length(DST)  255 let  \ud835\udc620, \ud835\udc621   hash_to_field XMD:BLAKE2b(\ud835\udc40, DST) let \ud835\udc44\ud835\udc56 map_to_curve_simple_swuiso-G(\ud835\udc62\ud835\udc56) for \ud835\udc560, 1 return iso_mapG(\ud835\udc440  \ud835\udc441). 12Both \ud835\udc4diso-G and \ud835\udf06G are nonsquare, and so their ratio is square in F\ud835\udc5eG.",
      "An arbitrary square root may be chosen. Non-normative notes:  The length of \ud835\udc37is in practice limited to 233 length(curveNameG) bytes due to the restriction of DST to at most 255 bytes. This limit is not exceeded by any use of GroupHashP or GroupHashV in this speci\ufb01cation. GroupHashP and GroupHashV are intended to be instantiations of hash_to_curve using Simpli\ufb01ed SWU for \ud835\udc34\ud835\udc35 0 described in ID-hashtocurve, section 6.6.3. In the event of any discrepancy or change to the Internet Draft, the de\ufb01nition here takes precedence. It is not necessary to use the clear_cofactor function speci\ufb01ed in the Internet Draft, because Pallas and Vesta (and therefore iso-Pallas and iso-Vesta) are prime-order curves. The above description incorporates optimizations from WB2019 that avoid inversions and unnecessary square tests in the computation of map_to_curve_simple_swuiso-G.",
      "In order to fully avoid inversions, the output of map_to_curve_simple_swuiso-G can be expressed in Jacobian coordinates, as can the input and output of iso_mapG. It is outside the scope of this document to describe Jacobian coordinates, but for example, the \ud835\udc38iso-G point with af\ufb01ne-short-Weierstrass coordinates xnumxdiv, y , has Jacobian coordinates xnumxdiv : yx3 div : xdiv Note: The uses of GroupHashP for DiversifyHashOrchard, and of both GroupHashP and GroupHashV to generate indepen- dent bases, need a random oracle. The hash_to_curve algorithm in ID-hashtocurve is designed to be indifferentiable from a random oracle (in the framework of MRH2003), given that XMD:BLAKE2b satis\ufb01es the requirements of ID-hashtocurve, section 5.5.4. The security of the Brier et al. construction on which this algorithm is based is analysed in FFSTV2013 and KT2015, with a veri\ufb01ed proof in BGHOZ2013.",
      "5.4.10 Zero-Knowledge Proving Systems 5.4.10.1 BCTV14 Before Sapling activation, Zcash uses zk-SNARKs generated by a fork of libsnark Zcash-libsnark with the BCTV14 proving system described in BCTV2014a, which is a modi\ufb01cation of the systems in PHGR2013 and BCGTV2013. A BCTV14 proof comprises (\ud835\udf0b\ud835\udc34 G(\ud835\udc5f) , \ud835\udf0b G(\ud835\udc5f) , \ud835\udf0b\ud835\udc35 G(\ud835\udc5f) , \ud835\udf0b G(\ud835\udc5f) , \ud835\udf0b\ud835\udc36 G(\ud835\udc5f) , \ud835\udf0b G(\ud835\udc5f) , \ud835\udf0b\ud835\udc3e G(\ud835\udc5f) , \ud835\udf0b\ud835\udc3b G(\ud835\udc5f) It is computed as described in BCTV2014a, Appendix B, using the pairing parameters speci\ufb01ed in section5.4.9.1 BN-254 on page 99. Note: Many details of the proving system are beyond the scope of this protocol document. For example, the quadratic constraint program verifying the JoinSplit statement, or its translation to a Quadratic Arithmetic Program BCTV2014a, section 2.3, are not speci\ufb01ed in this document. In 2015, Bryan Parno found a bug in this transla- tion, which is corrected by the libsnark implementation13 WCBTV2015 Parno2015 BCTV2014a, Remark 2.5.",
      "In practice it will be necessary to use the speci\ufb01c proving and verifying keys that were generated for the Zcash production block chain, given in section5.7 BCTV14 zk-SNARK Parameters on page 119, together with a proving system implementation that is interoperable with the Zcash fork of libsnark, to ensure compatibility. Vulnerability disclosure: BCTV14 is subject to a security vulnerability, separate from Parno2015, that could allow violation of Knowledge Soundness (and Soundness) CVE-2019-7167 SWB2019 Gabizon2019. The consequence for Zcash is that balance violation could have occurred before activation of the Sapling network upgrade, although there is no evidence of this having happened. Use of the vulnerability to produce false proofs is believed to have been fully mitigated by activation of Sapling. The use of BCTV14 in Zcash is now limited to verifying proofs that were made prior to the Sapling network upgrade.",
      "Due to this issue, new forks of Zcash MUST NOT use BCTV14, and any other users of the Zcash protocol SHOULD discontinue use of BCTV14 as soon as possible. 13Confusingly, the bug found by Bryan Parno was \ufb01xed in libsnark in 2015, but that \ufb01x was incompletely described in the May 2015 update BCTV2014a-old, Theorem 2.4. It is described completely in BCTV2014a, Theorem 2.4 and in Gabizon2019. The vulnerability does not affect the Zero Knowledge property of the scheme (as described in any version of BCTV2014a or as implemented in any version of libsnark that has been used in Zcash), even under subversion of the parameter generation BGG2017, Theorem 4.10. Sapling onward An implementation of Zcash that checkpoints on a block after Sapling MAY choose to skip veri\ufb01cation of BCTV14 proofs.",
      "Note that in section3.3 The Block Chain on page 18, there is a requirement that a full validator that potentially risks Mainnet funds or displays Mainnet transaction information to a user MUST do so only for a block chain that includes the activation block of the most recent settled network upgrade, with its known block hash as speci\ufb01ed in section3.12 Mainnet and Testnet on page 22. Since the most recent settled network upgrade is after the Sapling network upgrade, this mitigates the potential risks due to skipping BCTV14 proof veri\ufb01cation. Encoding of BCTV14 Proofs A BCTV14 proof is encoded by concatenating the encodings of its elements; for the BN-254 pairing this is: 264-bit \ud835\udf0b\ud835\udc34 264-bit \ud835\udf0b 520-bit \ud835\udf0b\ud835\udc35 264-bit \ud835\udf0b 264-bit \ud835\udf0b\ud835\udc36 264-bit \ud835\udf0b 264-bit \ud835\udf0b\ud835\udc3e 264-bit \ud835\udf0b\ud835\udc3b The resulting proof size is 296 bytes.",
      "In addition to the steps to verify a proof given in BCTV2014a, Appendix B, the veri\ufb01er MUST check, for the encoding of each element, that:  the lead byte is of the required form;  the remaining bytes encode a big-endian representation of an integer in 0 .. \ud835\udc5eS1 or (for \ud835\udf0b\ud835\udc35) 0 .. \ud835\udc5eS 21;  the encoding represents a point in G(\ud835\udc5f) or (for \ud835\udf0b\ud835\udc35) G(\ud835\udc5f) , including checking that it is of order \ud835\udc5fG in the latter case. 5.4.10.2 Groth16 After Sapling activation, Zcash uses zk-SNARKs with the Groth16 proving system described in BGM2017, which is a modi\ufb01cation of the system in Groth2016. An independent security proof of this system and its setup is given in Maller2018. Groth16 zk-SNARK proofs are used in transaction version 4 and later (section7.1 Transaction Encoding and Consensus on page 122), both in Sprout JoinSplit descriptions and in Sapling Spend descriptions and Output descriptions. They are generated by the bellman library Bowe-bellman. A Groth16 proof comprises (\ud835\udf0b\ud835\udc34 S(\ud835\udc5f) , \ud835\udf0b\ud835\udc35 S(\ud835\udc5f) , \ud835\udf0b\ud835\udc36 S(\ud835\udc5f) ).",
      "It is computed as described in Groth2016, section 3.2, using the pairing parameters speci\ufb01ed in section5.4.9.2 BLS12-381 on page 101. The proof elements are in a different order to the presentation in Groth2016. Note: The quadratic constraint programs verifying the Spend statement and Output statement are described in Appendix sectionA Circuit Design on page 200. However, many other details of the proving system are beyond the scope of this protocol document. For example, certain details of the translations of the Spend statement and Output statement to Quadratic Arithmetic Programs are not speci\ufb01ed in this document. In practice it will be necessary to use the speci\ufb01c proving and verifying keys generated for the Zcash production block chain (see section5.8 Groth16 zk-SNARK Parameters on page 119), and a proving system implementation that is interoperable with the bellman library used by Zcash, to ensure compatibility.",
      "Encoding of Groth16 Proofs A Groth16 proof is encoded by concatenating the encodings of its elements; for the BLS12-381 pairing this is: 384-bit \ud835\udf0b\ud835\udc34 768-bit \ud835\udf0b\ud835\udc35 384-bit \ud835\udf0b\ud835\udc36 The resulting proof size is 192 bytes. In addition to the steps to verify a proof given in Groth2016, the veri\ufb01er MUST check, for the encoding of each element, that:  the leading bit\ufb01eld is of the required form;  the remaining bits encode a big-endian representation of an integer in 0 .. \ud835\udc5eS1 or (in the case of \ud835\udf0b\ud835\udc35) two integers in that range;  the encoding represents a point in S(\ud835\udc5f) or (in the case of \ud835\udf0b\ud835\udc35) S(\ud835\udc5f) , including checking that it is of order \ud835\udc5fS in each case. 5.4.10.3 Halo 2 For Orchard Action descriptions in version 5 transactions, Zcash uses zk-SNARKs with the Halo 2 proving system described in Zcash-halo2. Encoding of Halo 2 Proofs Halo 2 proofs are de\ufb01ned as byte sequences, and so the encoding is the proof itself.",
      "Encodings of Note Plaintexts and Memo Fields As explained in section3.2.1 Note Plaintexts and Memo Fields on page 15, transmitted notes are stored on the block chain in encrypted form. The components and usage of note plaintexts, and which keys they are encrypted to, are de\ufb01ned in that section. The encoding of a Sprout note plaintext consists of: 8-bit leadByte 64-bit v 256-bit \u03c1 256-bit rcm memo (512 bytes)  A byte, 0x00, indicating this version of the encoding of a Sprout note plaintext. 8 bytes specifying v. 32 bytes specifying \u03c1. 32 bytes specifying rcm. 512 bytes specifying memo. The encoding of a Sapling or Orchard note plaintext consists of: 8-bit leadByte 88-bit d 64-bit v 256-bit rseed memo (512 bytes)  A byte, 0x01 or 0x02 as speci\ufb01ed in section3.2.1 Note Plaintexts and Memo Fields on page 15, indicating this version of the encoding of a Sapling or Orchard note plaintext. 11 bytes specifying d. 8 bytes specifying v. 32 bytes specifying rseed. 512 bytes specifying memo.",
      "Encodings of Addresses and Keys This section describes how Zcash encodes shielded payment addresses, incoming viewing keys, and spending keys. Addresses and keys can be encoded as a byte sequence; this is called the raw encoding. For Sprout shielded payment addresses, this byte sequence can then be further encoded using Base58Check. The Base58Check layer is the same as for upstream Bitcoin addresses Bitcoin-Base58. For Sapling-speci\ufb01c key and address formats, Bech32 ZIP-173 is used instead of Base58Check. Non-normative note: ZIP 173 is similar to Bitcoins BIP 173, except for dropping the limit of 90 characters on an encoded Bech32 string (which does not hold for Sapling viewing keys, for example), and requirements speci\ufb01c to Bitcoins Segwit addresses. Orchard introduces a new address format called a uni\ufb01ed payment address.",
      "This can encode an Orchard ad- dress, but also a Sapling address, a transparent address, and potentially future address formats, all in the same uni\ufb01ed payment address. It is RECOMMENDED to use uni\ufb01ed payment addresses for all new applications, unless compatibility with software that only accepts previous address formats is required. Uni\ufb01ed payment addresses and Orchard spending keys are encoded with Bech32m BIP-350 rather than Bech32. Payment addresses MAY be encoded as QR codes; in this case, the RECOMMENDED format for a Sapling payment address is the Bech32 form converted to uppercase, using the Alphanumeric mode ISO2015, sections 7.3.4 and 7.4.4. Similarly, the RECOMMENDED format for a uni\ufb01ed payment address is the Bech32m form converted to uppercase, using the Alphanumeric mode. 5.6.1 Transparent Encodings 5.6.1.1 Transparent Addresses Transparent addresses are either P2SH (Pay to Script Hash) addresses BIP-13 or P2PKH (Pay to Public Key Hash) addresses Bitcoin-P2PKH.",
      "The raw encoding of a P2SH address consists of: 8-bit 0x1C 8-bit 0xBD 160-bit script hash  Two bytes 0x1C, 0xBD, indicating this version of the raw encoding of a P2SH address on Mainnet. (Addresses on Testnet use 0x1C, 0xBA instead.)  20 bytes specifying a script hash Bitcoin-P2SH. The raw encoding of a P2PKH address consists of: 8-bit 0x1C 8-bit 0xB8 160-bit validating key hash  Two bytes 0x1C, 0xB8, indicating this version of the raw encoding of a P2PKH address on Mainnet. (Addresses on Testnet use 0x1D, 0x25 instead.)  20 bytes specifying a validating key hash, which is a RIPEMD-160 hash RIPEMD160 of a SHA-256 hash NIST2015 of a compressed ECDSA key encoding. Notes:  In Bitcoin a single byte is used for the version \ufb01eld identifying the address type. In Zcash two bytes are used. For addresses on Mainnet, this and the encoded length cause the \ufb01rst two characters of the Base58Check encoding to be \ufb01xed as t3 for P2SH addresses, and as t1 for P2PKH addresses.",
      "(This does not imply that a transparent Zcash address can be parsed identically to a Bitcoin address just by removing the t.)  Zcash does not yet support Hierarchical Deterministic Wallet addresses BIP-32. 5.6.1.2 Transparent Private Keys These are encoded in the same way as in Bitcoin Bitcoin-Base58, for both Mainnet and Testnet. 5.6.2 Sprout Encodings 5.6.2.1 Sprout Payment Addresses Let KASprout be as de\ufb01ned in section5.4.5.1 Sprout Key Agreement on page 88. A Sprout shielded payment address consists of apk B\u2113Sprout PRF  and pkenc KASprout.Public. apk is a SHA256Compress output. pkenc is a KASprout.Public key, for use with the encryption scheme de\ufb01ned in section4.19 In-band secret distribution (Sprout) on page 65. These components are derived from a spending key as described in section4.2.1 Sprout Key Components on page 36.",
      "The raw encoding of a Sprout shielded payment address consists of: 8-bit 0x16 8-bit 0x9A 256-bit apk 256-bit pkenc  Two bytes 0x16, 0x9A, indicating this version of the raw encoding of a Sprout shielded payment address on Mainnet. (Addresses on Testnet use 0x16, 0xB6 instead.)  32 bytes specifying apk. 32 bytes specifying pkenc, using the normal encoding of a Curve25519 public key Bernstein2006. Note: For addresses on Mainnet, the lead bytes and encoded length cause the \ufb01rst two characters of the Base58Check encoding to be \ufb01xed as zc. For Testnet, the \ufb01rst two characters are \ufb01xed as zt. 5.6.2.2 Sprout Incoming Viewing Keys Let KASprout be as de\ufb01ned in section5.4.5.1 Sprout Key Agreement on page 88. A Sprout incoming viewing key consists of apk B\u2113Sprout PRF  and skenc KASprout.Private. apk is a SHA256Compress output. skenc is a KASprout.Private key, for use with the encryption scheme de\ufb01ned in section4.19 In-band secret distribution (Sprout) on page 65.",
      "These components are derived from a spending key as described in section4.2.1 Sprout Key Components on page 36. The raw encoding of a Sprout incoming viewing key consists of: 8-bit 0xA8 8-bit 0xAB 8-bit 0xD3 256-bit apk 256-bit skenc  Three bytes 0xA8, 0xAB, 0xD3, indicating this version of the raw encoding of a Zcash incoming viewing key on Mainnet. (Addresses on Testnet use 0xA8, 0xAC, 0x0C instead.)  32 bytes specifying apk. 32 bytes specifying skenc, using the normal encoding of a Curve25519 private key Bernstein2006. skenc MUST be clamped using KASprout.FormatPrivate as speci\ufb01ed in section4.2.1 Sprout Key Components on page 36. That is, a decoded incoming viewing key MUST be considered invalid if skenc  KASprout.FormatPrivate(skenc). KASprout.FormatPrivate is de\ufb01ned in section5.4.5.1 Sprout Key Agreement on page 88. Note: For addresses on Mainnet, the lead bytes and encoded length cause the \ufb01rst four characters of the Base58Check encoding to be \ufb01xed as ZiVK.",
      "For Testnet, the \ufb01rst four characters are \ufb01xed as ZiVt. 5.6.2.3 Sprout Spending Keys A Sprout spending key consists of ask, which is a sequence of 252 bits (see section4.2.1 Sprout Key Components on page 36). The raw encoding of a Sprout spending key consists of: 8-bit 0xAB 8-bit 0x36 04 252-bit ask  Two bytes 0xAB, 0x36, indicating this version of the raw encoding of a Zcash spending key on Mainnet. (Addresses on Testnet use 0xAC, 0x08 instead.)  32 bytes: 4 zero padding bits and 252 bits specifying ask. The zero padding occupies the most signi\ufb01cant 4 bits of the third byte. Notes:  If an implementation represents ask internally as a sequence of 32 bytes with the 4 bits of zero padding intact, it will be in the correct form for use as an input to PRFaddr, PRFnfSprout, and PRFpk without need for bit-shifting. For addresses on Mainnet, the lead bytes and encoded length cause the \ufb01rst two characters of the Base58Check encoding to be \ufb01xed as SK.",
      "For Testnet, the \ufb01rst two characters are \ufb01xed as ST. 5.6.3 Sapling Encodings 5.6.3.1 Sapling Payment Addresses Let KASapling be as de\ufb01ned in section5.4.5.3 Sapling Key Agreement on page 89. Let \u2113d be as de\ufb01ned in section5.3 Constants on page 74. Let J(\ud835\udc5f), abstJ, and reprJ be as de\ufb01ned in section5.4.9.3 Jubjub on page 102. Let LEBS2OSP N)  B\u2113 BYceiling(\u21138) be as de\ufb01ned in section5.1 Integers, Bit Sequences, and Endianness on page 73. A Sapling shielded payment address consists of d B\u2113d and pkd KASapling.PublicPrimeOrder. pkd is an encoding of a KASapling public key of type KASapling.PublicPrimeOrder, for use with the encryption scheme de- \ufb01ned in section4.20 In-band secret distribution (Sapling and Orchard) on page 67. d is a diversi\ufb01er. These components are derived as described in section4.2.2 Sapling Key Components on page 36. The raw encoding of a Sapling shielded payment address consists of: LEBS2OSP88(d) LEBS2OSP256 reprJ(pkd)  11 bytes specifying d.",
      "32 bytes specifying the ctEdwards compressed encoding of pkd (see section5.4.9.3 Jubjub on page 102). When decoding the representation of pkd, the address MUST be considered invalid if abstJ returns , or the encoding of pkd is a non-canonical encoding as de\ufb01ned in section4.1.9 Represented Group on page 32, or the resulting pkd is not in J(\ud835\udc5f). Non-normative notes:  There are no non-canonical encodings of Jubjub curve points in J(\ud835\udc5f). The restriction on pkd re\ufb02ects its current type KASapling.PublicPrimeOrder  J(\ud835\udc5f). In versions of this speci\ufb01cation prior to v2025.6.0, pkd had type KASapling.PublicPrimeSubgroup  J(\ud835\udc5f), i.e. including \ud835\udcaaJ. Implementations of consumers for this encoding may need to be updated to exclude \ud835\udcaaJ, and should be checked for consistency with the current version of ZIP-216. For addresses on Mainnet, the Human-Readable Part (as de\ufb01ned in ZIP-173) is zs. For addresses on Testnet, the Human-Readable Part is ztestsapling.",
      "5.6.3.2 Sapling Incoming Viewing Keys Let KASapling be as de\ufb01ned in section5.4.5.3 Sapling Key Agreement on page 89. Let \u2113Sapling be as de\ufb01ned in section5.3 Constants on page 74. A Sapling incoming viewing key consists of ivk 1 .. 2\u2113Sapling 1. ivk is a KASapling.Private key (restricted to \u2113Sapling bits), derived as described in section4.2.2 Sapling Key Components on page 36. It is used with the encryption scheme de\ufb01ned in section4.20 In-band secret distribution (Sapling and Orchard) on page 67. The raw encoding of a Sapling incoming viewing key consists of: 256-bit ivk  32 bytes (little-endian) specifying ivk, padded with zeros in the most signi\ufb01cant bits. ivk MUST be in the range 1 .. 2\u2113Sapling 1 as speci\ufb01ed in section4.2.2 Sapling Key Components on page 36. That is, a decoded incoming viewing key MUST be considered invalid if ivk is not in this range. For incoming viewing keys on Mainnet, the Human-Readable Part is zivks.",
      "For incoming viewing keys on Testnet, the Human-Readable Part is zivktestsapling. Non-normative notes:  The diversi\ufb01er key is not present in this encoding, so it does not provide suf\ufb01cient information to decrypt the diversi\ufb01er to obtain a diversi\ufb01er index. This encoding is therefore deprecated: the preferred way to encode a Sapling incoming viewing key is as a component of a uni\ufb01ed incoming viewing key using the IVK Encoding speci\ufb01ed in ZIP-316, which does include the diversi\ufb01er key. In versions of this speci\ufb01cation prior to v2025.6.0, the range of ivk was de\ufb01ned as 0 .. 2\u2113Sapling 1, i.e. including 0. Implementations of consumers for this encoding may need to be updated to exclude 0. 5.6.3.3 Sapling Full Viewing Keys Let KASapling be as de\ufb01ned in section5.4.5.3 Sapling Key Agreement on page 89. A Sapling full viewing key consists of ak J(\ud835\udc5f), nk J(\ud835\udc5f), and ovk BY\u2113ovk8. ak and nk are points on the Jubjub curve (see section5.4.9.3 Jubjub on page 102).",
      "They are derived as described in section4.2.2 Sapling Key Components on page 36. The raw encoding of a Sapling full viewing key consists of: LEBS2OSP256 reprJ(ak) LEBS2OSP256 reprJ(nk) 32-byte ovk  32 bytes specifying the ctEdwards compressed encoding of ak (see section5.4.9.3 Jubjub on page 102). 32 bytes specifying the ctEdwards compressed encoding of nk. 32 bytes specifying the outgoing viewing key ovk. When decoding this representation, the key MUST be considered invalid if abstJ returns for either ak or nk, or if ak J(\ud835\udc5f), or if nk J(\ud835\udc5f). For full viewing keys on Mainnet, the Human-Readable Part is zviews. For full viewing keys on Testnet, the Human-Readable Part is zviewtestsapling. 5.6.3.4 Sapling Spending Keys A Sapling spending key consists of sk B\u2113sk (see section4.2.2 Sapling Key Components on page 36). The raw encoding of a Sapling spending key consists of: LEBS2OSP256(sk)  32 bytes specifying sk.",
      "For spending keys on Mainnet, the Human-Readable Part is secret-spending-key-main. For spending keys on Testnet, the Human-Readable Part is secret-spending-key-test. 5.6.4 Uni\ufb01ed and Orchard Encodings 5.6.4.1 Uni\ufb01ed Payment Addresses and Viewing Keys Rather than de\ufb01ning a Bech32 string encoding of Orchard shielded payment addresses, we instead de\ufb01ne, in ZIP-316, a uni\ufb01ed payment address format that is able to encode a set of payment addresses of different types. This enables the consumer of an address to choose the best address type it supports, providing a better user experience as new formats are added in the future. Similarly, uni\ufb01ed incoming viewing keys and uni\ufb01ed full viewing keys are de\ufb01ned to encode sets of incoming viewing keys and full viewing keys respectively. Since ZIP-316 includes a full speci\ufb01cation of encoding, decoding, and other processing of uni\ufb01ed payment addresses, uni\ufb01ed incoming viewing keys, and uni\ufb01ed full viewing keys, we give only a summary here.",
      "A uni\ufb01ed payment address includes zero or one address of each type in the following Priority List:  typecode 0x03  section5.6.4.2 Orchard Raw Payment Addresses on page 118;  typecode 0x02  section5.6.3.1 Sapling Payment Addresses on page 115;  typecode 0x01  transparent P2SH address, or typecode 0x00  transparent P2PKH address. with the restrictions that there MUST be at least one shielded payment address (typecodes 0x02), and that both P2SH and P2PKH cannot be present. When sending a payment, the consumer of a uni\ufb01ed payment address MUST use the most preferred address type that it supports from the set, i.e. the \ufb01rst in the above list. See ZIP-316 for additional requirements, and for discussion of uni\ufb01ed incoming viewing keys and uni\ufb01ed full viewing keys. Note that there is intentionally no typecode de\ufb01ned for a Sprout shielded payment address (or Sprout viewing keys).",
      "Since it is no longer possible (since activation of ZIP-211 in the Canopy network upgrade) to send funds into the Sprout chain value pool, this would not be generally useful. The format uses Bech32m BIP-350 (ignoring any length restrictions) for the checksum algorithm and string encoding. This is chosen over Bech32 in order for the checksum to better handle variable-length inputs. A jumbling algorithm is used in order to mitigate address replacement attacks given that a user might only check part of the address. See ZIP-316 for full details. 5.6.4.2 Orchard Raw Payment Addresses Let KAOrchard be as de\ufb01ned in section5.4.5.5 Orchard Key Agreement on page 90. An Orchard shielded payment address consists of d B\u2113d and pkd KAOrchard.PublicPrimeOrder. pkd is an encoding of a KAOrchard public key of type KAOrchard.PublicPrimeOrder, for use with the encryption scheme de\ufb01ned in section4.20 In-band secret distribution (Sapling and Orchard) on page 67. d is a sequence of 11 bytes.",
      "These components are derived as described in section4.2.3 Orchard Key Components on page 38. The raw encoding of an Orchard shielded payment address consists of: LEBS2OSP88(d) LEBS2OSP256 reprP(pkd)  11 bytes specifying d. 32 bytes specifying the short Weierstrass compressed encoding of pkd (see section5.4.9.6 Pallas and Vesta on page 105). When decoding the representation of pkd, the address MUST be considered invalid if abstP returns or \ud835\udcaaP. There is no Bech32m encoding de\ufb01ned for an individual Orchard shielded payment address; instead use a uni\ufb01ed payment address as de\ufb01ned in ZIP-316. 5.6.4.3 Orchard Raw Incoming Viewing Keys Let KAOrchard be as de\ufb01ned in section5.4.5.5 Orchard Key Agreement on page 90. An Orchard incoming viewing key consists of a diversi\ufb01er key dk, and a KAOrchard.Private key ivk restricted to the range 1 .. \ud835\udc5eP 1.",
      "It is derived as described in section4.2.3 Orchard Key Components on page 38, and is used with the encryption scheme de\ufb01ned in section4.20 In-band secret distribution (Sapling and Orchard) on page 67. Let I2LEOSP be as de\ufb01ned in section5.1 Integers, Bit Sequences, and Endianness on page 73. The raw encoding of an Orchard incoming viewing key consists of: I2LEOSP256(ivk)  32 bytes specifying dk. 32 bytes (little-endian) specifying ivk. ivk MUST be in the range 1 .. \ud835\udc5eP 1 as speci\ufb01ed in section4.2.3 Orchard Key Components on page 38. That is, a decoded incoming viewing key MUST be considered invalid if ivk is not in this range. There is no Bech32m encoding de\ufb01ned for an individual Orchard incoming viewing key; instead use a uni\ufb01ed incoming viewing key as de\ufb01ned in ZIP-316. 5.6.4.4 Orchard Raw Full Viewing Keys Let KAOrchard be as de\ufb01ned in section5.4.5.5 Orchard Key Agreement on page 90. Let ExtractP be as de\ufb01ned in section5.4.9.7 Coordinate Extractor for Pallas on page 106.",
      "An Orchard full viewing key consists of ak 0 .. \ud835\udc5eP 1, nk F\ud835\udc5eP, and rivk F\ud835\udc5fP. ak is the Spend validating key, a result of applying ExtractP to a point on the Pallas curve (see section5.4.9.6 Pallas and Vesta on page 105). nk is the nulli\ufb01er deriving key, a \ufb01eld element in F\ud835\udc5eP. rivk is the Commitivk randomness, a \ufb01eld element in F\ud835\udc5fP. They are derived as described in section4.2.3 Orchard Key Components on page 38. Let I2LEOSP be as de\ufb01ned in section5.1 Integers, Bit Sequences, and Endianness on page 73. The raw encoding of an Orchard full viewing key consists of: I2LEOSP256(ak) I2LEOSP256(nk) I2LEOSP256(rivk)  32 bytes (little-endian) specifying ak. 32 bytes (little-endian) specifying nk. 32 bytes (little-endian) specifying rivk.",
      "When decoding this representation, the key MUST be considered invalid if ak, nk, or rivk are not canonically encoded elements of their respective \ufb01elds, or if ak is not a valid af\ufb01ne \ud835\udc65-coordinate of a Pallas curve point in P, or if either the external or internal incoming viewing keys derived as speci\ufb01ed in section4.2.3 Orchard Key Components on page 38 are 0 or . There is no Bech32m encoding de\ufb01ned for an individual Orchard full viewing key; instead use a uni\ufb01ed full viewing key as de\ufb01ned in ZIP-316. 5.6.4.5 Orchard Spending Keys An Orchard spending key consists of sk B\u2113sk (see section4.2.3 Orchard Key Components on page 38). The raw encoding of an Orchard spending key consists of: LEBS2OSP256(sk)  32 bytes specifying sk. Orchard spending keys are encoded using Bech32m (not Bech32). For spending keys on Mainnet, the Human-Readable Part is secret-orchard-sk-main. For spending keys on Testnet, the Human-Readable Part is secret-orchard-sk-test.",
      "BCTV14 zk-SNARK Parameters The SHA-256 hashes of the proving key and verifying key for the Sprout JoinSplit circuit, encoded in libsnark format, are: 8bc20a7f013b2b58970cddd2e7ea028975c88ae7ceb9259a5344a16bc2c0eef7 sprout-proving.key 4bd498dae0aacfd8e98dc306338d017d9c08dd0918ead18172bd0aec2fc5df82 sprout-verifying.key These parameters were obtained by a multi-party computation described in BGG-mpc and BGG2017. They are used only before Sapling activation. Due to the security vulnerability described in section5.4.10.1 BCTV14 on page 110, it is not recommended to use these parameters in new protocols, and it is recommended to stop using them in protocols other than Zcash where they are currently used. Groth16 zk-SNARK Parameters bellman Bowe-bellman encodes the proving key and verifying key fora zk-SNARK circuit in a single parameters \ufb01le.",
      "The BLAKE2b-512 hashes of this \ufb01le for the Sapling Spend circuit and Output circuit, and for the implementation of the Sprout JoinSplit circuit used after Sapling activation, are respectively: 8270785a1a0d0bc77196f000ee6d221c9c9894f55307bd9357c3f0105d31ca63 991ab91324160d8f53e2bbd3c2633a6eb8bdf5205d822e7f3f73edac51b2b70c sapling-spend.params 657e3d38dbb5cb5e7dd2970e8b03d69b4787dd907285b5a7f0790dcc8072f60b f593b32cc2d1c030e00ff5ae64bf84c5c3beb84ddc841d48264b4a171744d028 sapling-output.params e9b238411bd6c0ec4791e9d04245ec350c9c5744f5610dfcce4365d5ca49dfef d5054e371842b3f88fa1b9d7e8e075249b3ebabd167fa8b0f3161292d36c180a sprout-groth16.params These parameters were obtained by a multi-party computation described in BGM2017. Randomness Beacon Let URS : 096b36a5804bfacef1691e173c366a47ff5ba84a44f26ddd7e8d9f79d5b42df0.",
      "This value is used in the de\ufb01nition of GroupHashJ(\ud835\udc5f) in section5.4.9.5 Group Hash into Jubjub on page 104, and in the multi- party computation to obtain the Sapling parameters given in section5.8 Groth16 zk-SNARK Parameters on page 119. It is derived as described in Bowe2018:  Take the hash of the Bitcoin block at height 514200 in RPC byte order, i.e. the big-endian 32-byte representation of 0x00000000000000000034b33e842ac1c50456abe5fa92b60f6b3dfc5d247f7b58. Apply SHA-256 242 times. Convert to a US-ASCII lowercase hexadecimal string. Note: URS is a 64-byte US-ASCII string, i.e. the \ufb01rst byte is 0x30, not 0x09. Network Upgrades Zcash launched with a protocol revision that we call Sprout. A \ufb01rst upgrade, called Overwinter, activated on Mainnet on 26 June, 2018 at block height 347500 Swihart2018. Its speci\ufb01cations are described in this document, ZIP-201, ZIP-202, ZIP-203, and ZIP-143.",
      "A second upgrade, called Sapling, activated on Mainnet on 28 October, 2018 at block height 419200 Hamdon2018. Its speci\ufb01cations are described in this document, ZIP-205, and ZIP-243. A third upgrade, called Blossom, activated on Mainnet on 11 December, 2019 at block height 653600 Zcash-Blossom. Its speci\ufb01cations are described in this document, ZIP-206, and ZIP-208. A fourth upgrade, called Heartwood, activated on Mainnet on 16 July, 2020 at block height 903000 Zcash-Heartwd. Its speci\ufb01cations are described in this document, ZIP-250, ZIP-213, and ZIP-221. A \ufb01fth upgrade, called Canopy, activated on Mainnet on 18 November, 2020 at block height 1046400 (coinciding with the \ufb01rst block subsidy halving) Zcash-Canopy. Its speci\ufb01cations are described in this document, ZIP-251, ZIP-207, ZIP-211, ZIP-212, ZIP-214, and ZIP-215. Additional information and rationale is given in ZIP-1014. A sixth upgrade, called NU55, activated on Mainnet on 31 May, 2022 at block height 1687104 Zcash-Nu5.",
      "Its speci\ufb01- cations are described in this document, ZIP-252, ZIP-216, ZIP-221, ZIP-224, ZIP-225, ZIP-239, ZIP-244, and ZIP-316, with updates to ZIP-32, ZIP-203, ZIP-209, ZIP-212, ZIP-213, and ZIP-221. Additional information and rationale is given in Zcash-Orchard and Zcash-halo2. A seventh upgrade, called NU66, activated on Mainnet on 23 November, 2024 at block height 2726400 (coinciding with the second block subsidy halving) Zcash-Nu6. Its speci\ufb01cations are described in this document, ZIP-253, ZIP-236, and ZIP-2001, with updates to ZIP-207 and ZIP-214. Additional information and rationale is given in ZIP-1015. This draft speci\ufb01cation describes the set of changes proposed for the NU6.6.1 network upgrade Zcash-Nu6.1, for which the Mainnet activation block height has not yet been set. Its speci\ufb01cations are described in this document, ZIP-255, and ZIP-271, with updates to ZIP-214. Additional information and rationale is given in ZIP-1016.",
      "This section summarizes the strategy for upgrading from Sprout to subsequent versions of the protocol (Overwinter, Sapling, Blossom, Heartwood, Canopy, NU55, NU66, and NU6.6.1), and for future upgrades. The network upgrade mechanism is described in ZIP-200. Each network upgrade is introduced as a bilateral consensus rule change. In this kind of upgrade,  there is an activation block height at which the consensus rule change takes effect;  blocks and transactions that are valid according to the post-upgrade rules are not valid before the upgrade block height;  blocks and transactions that are valid according to the pre-upgrade rules are no longer valid at or after the activation block height. Full support for each network upgrade is indicated by a minimum version of the peer-to-peer protocol. At the planned activation block height, nodes that support a given upgrade will disconnect from (and will not reconnect to) nodes with a protocol version lower than this minimum.",
      "See ZIP-201 for how this applies to the Overwinter upgrade, for example. This ensures that upgrade-supporting nodes transition cleanly from the old protocol to the new protocol. Nodes that do not support the upgrade will \ufb01nd themselves on a network that uses the old protocol and is fully partitioned from the upgrade-supporting network. This allows us to specify arbitrary protocol changes that take effect at a given block height. Note, however, that a block chain reorganization across the upgrade activation block height is possible. In the case of such a reorganization, blocks at a height before the activation block height will still be created and validated according to the pre-upgrade rules, and upgrade-supporting nodes MUST allow for this.",
      "Consensus Changes from Bitcoin Transaction Encoding and Consensus The Zcash transaction format up to and including transaction version 4 is as follows (this should be read in the context of consensus rules later in the section): Version Bytes Name Data Type Description 1 .. 4 header uint32 Contains:  fOverwintered \ufb02ag (bit 31)  version (bits 30 .. 0)  transaction version. 3 .. 4 nVersionGroupId uint32 Version group ID (nonzero). 1 .. 4 Varies tx_in_count compactSize Number of transparent inputs. 1 .. 4 Varies tx_in tx_in Transparent inputs, encoded as in Bitcoin. 1 .. 4 Varies tx_out_count compactSize Number of transparent outputs. 1 .. 4 Varies tx_out tx_out Transparent outputs, encoded as in Bitcoin. 1 .. 4 lock_time uint32 Unix-epoch UTC time or block height, encoded as in Bitcoin. 3 .. 4 nExpiryHeight uint32 A block height after which the transaction will expire, or 0 to disable expiry. ZIP-203 valueBalanceSapling int64 The net value of Sapling spends minus outputs.",
      "Varies nSpendsSapling compactSize The number of Spend descriptions in vSpendsSapling. 384 nSpendsSapling vSpendsSapling SpendDescriptionV4 nSpendsSapling A sequence of Spend descriptions, encoded per section7.3 Spend Description Encoding and Consensus on page 128. Varies nOutputsSapling compactSize The number of Output descriptions in vOutputsSapling. 948 nOutputsSapling vOutputsSapling OutputDescriptionV4 nOutputsSapling A sequence of Output descriptions, encoded per section7.4 Output Description Encoding and Consensus on page 129. 2 .. 4 Varies nJoinSplit compactSize The number of JoinSplit descriptions in vJoinSplit. 2 .. 3 1802 nJoinSplit vJoinSplit JSDescriptionBCTV14 nJoinSplit A sequence of JoinSplit descriptions using BCTV14 proofs, encoded per section7.2 JoinSplit Description Encoding and Consensus on page 128.",
      "1698 nJoinSplit vJoinSplit JSDescriptionGroth16 nJoinSplit A sequence of JoinSplit descriptions using Groth16 proofs, encoded per section7.2 JoinSplit Description Encoding and Consensus on page 128. 2 .. 4  joinSplitPubKey byte32 An encoding of a JoinSplitSig public validating key. 2 .. 4  joinSplitSig byte64 A signature on a pre\ufb01x of the transaction encoding, validated using joinSplitPubKey as speci\ufb01ed in section4.11 Non-malleability (Sprout) on page 51. bindingSigSapling byte64 A Sapling binding signature on the SIGHASH transaction hash, validated as speci\ufb01ed in section5.4.7.2 Binding Signature (Sapling and Orchard) on page 95. Version constraints apply to the effectiveVersion, which is equal to min(2, version) when fOverwintered  0 and to version otherwise. If effectiveVersion 5 once header has been parsed, the remainder of the transaction encoding MUST be parsed according to the v5 format described in the next table.",
      "The consensus rules later in this section specify constraints on nVersionGroupId depending on effectiveVersion. The joinSplitPubKey and joinSplitSig \ufb01elds are present if and only if effectiveVersion 2 and nJoinSplit  0. bindingSigSapling is present if and only if effectiveVersion  4 and nSpendsSapling  nOutputsSapling  0. Note that the valueBalanceSapling \ufb01eld is always present for these transaction versions. Several Sapling \ufb01elds have been renamed from previous versions of this speci\ufb01cation: valueBalance valueBalanceSapling; nShieldedSpend nSpendsSapling; vShieldedSpend vSpendsSapling; nShieldedOutput nOutputsSapling; vShieldedOutput vOutputsSapling; bindingSig bindingSigSapling. The Zcash transaction format for transaction version 5 is as follows (this should be read in the context of consensus rules later in the section): Note Bytes Name Data Type Description header uint32 Contains:  fOverwintered \ufb02ag (bit 31, always set)  version (bits 30 .. 0)  transaction version.",
      "nVersionGroupId uint32 Version group ID (nonzero). nConsensusBranchId uint32 Consensus branch ID. lock_time uint32 Unix-epoch UTC time or block height, encoded as in Bitcoin . nExpiryHeight uint32 A block height after which the transaction will expire, or 0 to disable expiry. ZIP-203 Varies tx_in_count compactSize Number of transparent inputs. Varies tx_in tx_in Transparent inputs, encoded as in Bitcoin. Varies tx_out_count compactSize Number of transparent outputs. Varies tx_out tx_out Transparent outputs, encoded as in Bitcoin. Varies nSpendsSapling compactSize The number of Spend descriptions in vSpendsSapling. nSpendsSapling vSpendsSapling SpendDescriptionV5 nSpendsSapling A sequence of Spend descriptions, encoded per section7.3 Spend Description Encoding and Consensus on page 128. Varies nOutputsSapling compactSize The number of Output descriptions in vOutputsSapling.",
      "756 nOutputsSapling vOutputsSapling OutputDescriptionV5 nOutputsSapling A sequence of Output descriptions, encoded per section7.4 Output Description Encoding and Consensus on page 129. valueBalanceSapling int64 The net value of Sapling spends minus outputs. anchorSapling byte32 A root of the Sapling note commitment tree at some block height in the past, LEBS2OSP256 rtSapling) 192 nSpendsSapling vSpendProofsSapling byte192 nSpendsSapling Encodings of the zk-SNARK proofs for each Sapling Spend description. nSpendsSapling vSpendAuthSigsSapling byte64 nSpendsSapling Authorizing signatures for each Sapling Spend description. 192 nOutputsSapling vOutputProofsSapling byte192 nOutputsSapling Encodings of the zk-SNARK proofs for each Sapling Output description. bindingSigSapling byte64 A Sapling binding signature on the SIGHASH transaction hash, validated per section5.4.7.2 Binding Signature (Sapling and Orchard) on page 95.",
      "Varies nActionsOrchard compactSize The number of Action descriptions in vActionsOrchard. 820 nActionsOrchard vActionsOrchard ActionDescription nActionsOrchard A sequence of Action descriptions, encoded per section7.5 Action Description Encoding and Consensus on page 130. flagsOrchard byte Contains:  enableSpendsOrchard \ufb02ag (bit 0)  enableOutputsOrchard \ufb02ag (bit 1)  Reserved, zeros (bits 2 .. 7). valueBalanceOrchard int64 The net value of Orchard spends minus outputs. anchorOrchard byte32 A root of the Orchard note commitment tree at some block height in the past, LEBS2OSP256 rtOrchard) Varies sizeProofsOrchard compactSize The length of the aggregated zk-SNARK proof \ud835\udf0bZKAction. Value is 2720  2272  nActionsOrchard. sizeProofsOrchard proofsOrchard bytesizeProofsOrchard The aggregated zk-SNARK proof \ud835\udf0bZKAction (see section5.4.10.3 Halo 2 on page 112). nActionsOrchard vSpendAuthSigsOrchard byte64 nActionsOrchard Authorizing signatures for each spend of an Orchard Action description.",
      "bindingSigOrchard byte64 An Orchard binding signature on the SIGHASH transaction hash, validated per section5.4.7.2 Binding Signature (Sapling and Orchard) on page 95. The \ufb01elds valueBalanceSapling and bindingSigSapling are present if and only if nSpendsSapling  nOutputsSapling  0. If valueBalanceSapling is not present, then vbalanceSapling is de\ufb01ned to be 0. The \ufb01eld anchorSapling is present if and only if nSpendsSapling  0. The \ufb01elds flagsOrchard, valueBalanceOrchard, anchorOrchard, sizeProofsOrchard, proofsOrchard, and bindingSigOrchard are present if and only if nActionsOrchard  0. If valueBalanceOrchard is not present, then vbalanceOrchard is de\ufb01ned to be 0. Transaction version 5 does not support JoinSplit transfers. Several \ufb01elds are reordered andor renamed relative to prior versions. 7.1.1 Transaction Identi\ufb01ers The transaction ID of a version 4 or earlier transaction is the SHA-256d hash of the transaction encoding in the pre-v5 format described above.",
      "The transaction ID of a version 5 transaction is as de\ufb01ned in ZIP-244. A v5 transaction also has a wtxid (used for example in the peer-to-peer protocol) as de\ufb01ned in ZIP-239. 7.1.2 Transaction Consensus Rules Consensus rules:  The transaction version number MUST be greater than or equal to 1. Pre-Overwinter The fOverwintered \ufb02ag MUST NOT be set. Overwinter onward The fOverwintered \ufb02ag MUST be set. Overwinter onward The version group ID MUST be recognized. Overwinter only, pre-Sapling The transaction version number MUST be 3, and the version group ID MUST be 0x03C48270. Sapling to Canopy inclusive, pre-NU55 The transaction version number MUST be 4, and the version group ID MUST be 0x892F2085. NU55 onward The transaction version number MUST be 4 or 5. If the transaction version number is 4 then the version group ID MUST be 0x892F2085. If the transaction version number is 5 then the version group ID MUST be 0x26A7270A.",
      "NU55 onward If effectiveVersion 5, the nConsensusBranchId \ufb01eld MUST match the consensus branch ID used for SIGHASH transaction hashes, as speci\ufb01ed in ZIP-244. Pre-Sapling The encoded size of the transaction MUST be less than or equal to 100000 bytes. NU55 onward nSpendsSapling, nOutputsSapling, and nActionsOrchard MUST all be less than 216. Pre-Sapling If effectiveVersion  1 or nJoinSplit  0, then both tx_in_count and tx_out_count MUST be nonzero. Sapling onward If effectiveVersion  5, then at least one of tx_in_count, nSpendsSapling, and nJoinSplit MUST be nonzero. Sapling onward If effectiveVersion  5, then at least one of tx_out_count, nOutputsSapling, and nJoinSplit MUST be nonzero. NU55 onward If effectiveVersion 5 then this condition MUST hold: tx_in_count  0 or nSpendsSapling  0 or (nActionsOrchard  0 and enableSpendsOrchard  1).",
      "NU55 onward If effectiveVersion 5 then this condition MUSThold: tx_out_count  0 ornOutputsSapling  0 or (nActionsOrchard  0 and enableOutputsOrchard  1). NU55 onward If effectiveVersion 5 and nActionsOrchard  0, then at least one of enableSpendsOrchard and enableOutputsOrchard MUST be 1. A transaction with one or more transparent inputs from coinbase transactions MUST have no transparent outputs (i.e. tx_out_count MUST be 0). Inputs from coinbase transactions include Founders Reward outputs and funding stream outputs. If effectiveVersion 2 and nJoinSplit  0, then:  joinSplitPubKey MUST be a valid encoding (see section5.4.6 Ed25519 on page 90) of an Ed25519 validating key. joinSplitSig MUST represent a valid signature under joinSplitPubKey of dataToBeSigned, as de\ufb01ned in section4.11 Non-malleability (Sprout) on page 51.",
      "Sapling onward If effectiveVersion 4 and nSpendsSapling  nOutputsSapling  0, then:  let bvkSapling and SigHash be as de\ufb01ned in section4.13 Balance and Binding Signature (Sapling) on page 52;  bindingSigSapling MUST represent a valid signature under the transaction binding validating key bvkSapling of SigHash  i.e. BindingSigSapling.ValidatebvkSapling(SigHash, bindingSigSapling)  1. NU55 onward As speci\ufb01ed in section5.4.7 RedDSA, RedJubjub, and RedPallas on page 92, the validation of the \ud835\udc45component of the signature changes to prohibit non-canonical encodings. This change is also retrospectively valid on Mainnet and Testnet before NU55. Sapling onward If effectiveVersion  4 and there are no Spend descriptions or Output descriptions, then valueBalanceSapling MUST be 0.",
      "NU55 onward If effectiveVersion 5 and nActionsOrchard  0, then:  let bvkOrchard and SigHash be as de\ufb01ned in section4.14 Balance and Binding Signature (Orchard) on page 54;  bindingSigOrchard MUST represent a valid signature under the transaction binding validating key bvkOrchard of SigHash  i.e. BindingSigOrchard.ValidatebvkOrchard(SigHash, bindingSigOrchard)  1. As speci\ufb01ed in section5.4.7 RedDSA, RedJubjub, and RedPallas on page 92, validation of the \ud835\udc45component of the signature prohibits non-canonical encodings. Let totalDeferredOutput and totalDeferredInput be as de\ufb01ned in section7.8 on page 136.",
      "For the block at block height height:  de\ufb01ne the total output value of its coinbase transaction to be the total value in zatoshi of its transparent outputs, minus vbalanceSapling, minus vbalanceOrchard, plus totalDeferredOutput(height);  de\ufb01ne the total input value of its coinbase transaction to be the value in zatoshi of the block subsidy, plus the transaction fees paid by transactions in the block, plus totalDeferredInput(height). Pre-NU66 The total output value of a coinbase transaction MUST NOT be greater than its total input value. NU66 onward The total output value of a coinbase transaction MUST be equal to its total input value. A coinbase transaction MUST NOT have any JoinSplit descriptions. A coinbase transaction MUST NOT have any Spend descriptions. Pre-Heartwood A coinbase transaction MUST NOT have any Output descriptions. NU55 onward In a version 5 coinbase transaction, the enableSpendsOrchard \ufb02ag MUST be 0. NU55 onward In a version 5 transaction, the reserved bits 2 ..",
      "7 of the flagsOrchard \ufb01eld MUST be zero. A coinbase transaction for a block at block height greater than 0 MUST have a script that, as its \ufb01rst item, encodes the block height height as follows. For height in the range 1 .. 16, the encoding is a single byte of value 0x50  height. Otherwise, let heightBytes be the signed little-endian representation of height, using the minimum nonzero number of bytes such that the most signi\ufb01cant byte is  0x80. The length of heightBytes MUST be in the range 1 .. 5. Then the encoding is the length of heightBytes encoded as one byte, followed by heightBytes itself. This matches the encoding used by Bitcoin in the implementation of BIP-34 (but the description here is to be considered normative). A coinbase transaction script MUST have length in 2 .. 100 bytes. A transparent input in a non-coinbase transaction MUST NOT have a null prevout.",
      "Every non-null prevout MUST point to a unique UTXO in either a preceding block, or a previous transaction in the same block. A transaction MUST NOT spend a transparent output of a coinbase transaction from a block less than 100 blocks prior to the spend. Note that transparent outputs of coinbase transactions include Founders Reward outputs, transparent funding stream outputs ZIP-207, and lockbox disbursement outputs ZIP-271. A transaction MUST NOT spend an output of the genesis block coinbase transaction. (There is one such zero-valued output, on each of Testnet and Mainnet.)  Overwinter to Canopy inclusive, pre-NU55 nExpiryHeight MUST be less than or equal to 499999999. NU55 onward nExpiryHeight MUST be less than or equal to 499999999 for non-coinbase transactions. Overwinter onward If a transaction is not a coinbase transaction and its nExpiryHeight \ufb01eld is nonzero, then it MUST NOT be mined at a block height greater than its nExpiryHeight.",
      "NU55 onward The nExpiryHeight \ufb01eld of a coinbase transaction MUST be equal to its block height. Sapling onward valueBalanceSapling MUST be in the range MAX_MONEY .. MAX_MONEY. NU55 onward valueBalanceOrchard MUST be in the range MAX_MONEY .. MAX_MONEY for version 5 transactions. Heartwood onward All Sapling and Orchard outputs in coinbase transactions MUST decrypt to a note plaintext, i.e. the procedure in section4.20.3 Decryption using an Outgoing Viewing Key (Sapling and Orchard) on page 70 does not return , using a sequence of 32 zero bytes as the outgoing viewing key. (This implies that before Canopy activation, Sapling outputs of a coinbase transaction MUST have note plaintext lead byte equal to 0x01.)  Canopy onward Any Sapling or Orchard output of a coinbase transaction decrypted to a note plaintext according to the preceding rule MUST have note plaintext lead byte equal to 0x02.",
      "(This applies even during the grace period speci\ufb01ed in ZIP-212.)  TODO: Other rules inherited from Bitcoin. The types speci\ufb01ed in section7.1 Transaction Encoding and Consensus on page 122 are part of the consensus rules. Consensus rules associated with each JoinSplit description (section7.2 JoinSplit Description Encoding and Consensus on page 128), each Spend description (section7.3 Spend Description Encoding and Consensus on page 128), each Output description (section7.4 Output Description Encoding and Consensus on page 129), and each Action description (section7.5 Action Description Encoding and Consensus on page 130) MUST also be followed. Notes:  Previous versions of this speci\ufb01cation de\ufb01ned what is now the header \ufb01eld as a signed int32 \ufb01eld which was required to be positive. The consensus rule that the fOverwintered \ufb02ag MUST NOT be set before Overwinter has activated, has the same effect.",
      "The semantics of transactions with version number not equal to 1, 2, 3, 4, or 5 is not currently de\ufb01ned. The exclusion of transactions with transaction version number greater than 2 is not a consensus rule before Overwinter activation. Such transactions may exist in the block chain and MUST be treated identically to version 2 transactions. Overwinter onward Once Overwinter has activated, limits on the maximum transaction version number are consensus rules. The transaction version number 0x7FFFFFFF, and the version group ID 0xFFFFFFFF, are reserved for use in experimental extensions to transaction format or semantics on private testnets. They MUST NOT be used on the Zcash Mainnet or Testnet. Note that a future upgrade might use any transaction version number or version group ID. It is likely that an upgrade that changes the transaction version number or version group ID will also change the transaction format, and software that parses transactions SHOULD take this into account.",
      "Overwinter onward The purpose of version group ID is to allow unambiguous parsing of loose transactions, independent of the context of a block chain. Code that parses transactions is likely to be reused between consensus branches as de\ufb01ned in ZIP-200, and in that case the fOverwintered and version \ufb01elds alone may be insuf\ufb01cient to determine the format to be used for parsing. A transaction version number of 2 does not have the same meaning as in Bitcoin, where it is associated with support for OP_CHECKSEQUENCEVERIFY as speci\ufb01ed in BIP-68. Zcash was forked from Bitcoin Core v0.11.2 and does not currently support BIP 68. Sapling onward Because coinbase transactions have no Spend descriptions, the valueBalanceSapling \ufb01eld of a coinbase transaction must have a negative or zero value. The negative case can only occur after Heartwood activation, for transactions with ZIP-213 shielded outputs.",
      "Prior to the Heartwood network upgrade, it was not possible for coinbase transactions to have shielded outputs, and therefore the coinbase maturity rule and the requirement to spend coinbase outputs only in transactions with no transparent outputs, applied to all coinbase outputs. Canopy onward The rule that Sapling outputs in coinbase transactions MUST decrypt to a note plaintext with lead byte 0x02, also applies to funding stream outputs that specify Sapling shielded payment addresses, if there are any. NU55 onward The \ufb02ags in flagsOrchard allow a version 5 transaction to declare that no funds are spent from Orchard notes (by setting enableSpendsOrchard to 0), or that no new Orchard notes with nonzero values are created (bysetting enableOutputsOrchard to 0). This has two primarypurposes. First, the enableSpendsOrchard \ufb02ag is set to 0 in version 5 coinbase transactions to ensure that they cannot spend from existing Orchard outputs.",
      "This maintains a restriction present in coinbase transactions for transparent, Sprout, or Sapling funds, which would not otherwise be enforceable in the combined Action transfer design. Second, if a security vulnerability were found that affected only the input side, or only the output side of the Action circuit, it would be possible to use these \ufb02ags in a soft fork (i.e. a strictly contracting consensus change) to effectively switch off non-zero-valued transfers only on the relevant side. Setting either of these \ufb02ags to 0 does not affect the presence or validation of spend authorization signatures, or other consensus rules associated with Action descriptions. These note spending and creation consensus rules are speci\ufb01ed as part of the Orchard Action statement (section4.18.4 Action Statement (Orchard) on page 63).",
      "NU55 onward Because enableSpendsOrchard is set to 0 in version 5 coinbase transactions which disables non-zero-valued Orchard spends the valueBalanceOrchard \ufb01eld of a coinbase transaction must have a negative or zero value. The negative case can only occur for transactions with ZIP-213 shielded outputs. NU55 onward The rule that nSpendsSapling, nOutputsSapling, and nActionsOrchard MUST all be less than 216, is technically redundant because a transaction that could violate this rule would not \ufb01t within the 2 MB block size limit. It is included in order to simplify the security argument for balance preservation. NU55 onward The rule that from NU55 activation, the nExpiryHeight \ufb01eld of a coinbase transaction MUST be equal to the block height, is needed to maintain the property that all transactions have unique transaction IDs.",
      "All non-coinbase transactions necessarily have some effecting data that is unique across all transactions in a valid block chain: either a tx_in referring to a previous unique tx_out, or a Spend description or Action description referring to a unique nulli\ufb01er. However, coinbase transactions do not necessarily have any such unique effecting data; the block height encoded in the coinbase script is unique in a valid block chain, but for v5 transactions, it is not included in the transaction ID hash speci\ufb01ed by ZIP-244. Requiring nExpiryHeight to be set to the block height ensures that the effecting data that contributes to the transaction ID is unique, even for v5 coinbase transactions. In order to avoid the block height being limited to 499999999, we also remove that bound on nExpiryHeight for coinbase transactions. For consistency, these changes apply to all coinbase transactions, not just v5 coinbase transactions.",
      "The changes relative to Bitcoin version 1 transactions as described in Bitcoin-Format are:  Transaction version 0 is not supported. A version 1 transaction is equivalent to a version 2 transaction with nJoinSplit  0. The \ufb01elds nJoinSplit, vJoinSplit, joinSplitPubKey, and joinSplitSig have been added. Overwinter onward The \ufb01eld nVersionGroupId has been added. Sapling onward The following \ufb01elds have been added: nSpendsSapling, vSpendsSapling, nOutputsSapling, vOutputsSapling, and bindingSigSapling. NU55 onward In version 5 transactions, these \ufb01elds have been added: nConsensusBranchId, nActionsOrchard, vActionsOrchard, flagsOrchard, valueBalanceOrchard, anchorOrchard, sizeProofsOrchard, proofsOrchard, bindingSigOrchard, and vSpendAuthSigsOrchard. In Zcash it is permitted for a transaction to have no transparent inputs, provided at least one of nJoinSplit, nSpendsSapling, nOutputsSapling, and nActionsOrchard are nonzero. A consensus rule limiting transaction size has been added.",
      "In Bitcoin there is a corresponding standard rule but no consensus rule. JoinSplit Description Encoding and Consensus An abstract JoinSplit description, as described in section3.5 JoinSplit Transfers and Descriptions on page 19, is encoded in a transaction as an instance of a JoinSplitDescription type: Bytes Name Data Type Description vpub_old uint64 A value vold pub that the JoinSplit transfer removes from the transparent transaction value pool. vpub_new uint64 A value vnew pub that the JoinSplit transfer inserts into the transparent transaction value pool. anchor byte32 A root rtSprout of the Sprout note commitment tree at some block height in the past, or the root produced by a previous JoinSplit transfer in this transaction. nullifiers byte32Nold A sequence of nulli\ufb01ers of the input notes nfold 1..Nold. commitments byte32Nnew A sequence of note commitments for the output notes cmnew 1..Nnew. ephemeralKey byte32 A Curve25519 public key epk.",
      "randomSeed byte32 A 256-bit seed that must be chosen independently at random for each JoinSplit description. vmacs byte32Nold A sequence of message authentication tags h1..Nold binding hSig to each ask of the JoinSplit description, computed as described in section4.11 Non-malleability (Sprout) on page 51. 296  zkproof byte296 An encoding of the zk-SNARK proof \ud835\udf0bZKJoinSplit (see section5.4.10.1 BCTV14 on page 110). 192  zkproof byte192 An encoding of the zk-SNARK proof \ud835\udf0bZKJoinSplit (see section5.4.10.2 Groth16 on page 111). encCiphertexts byte601Nnew A sequence of ciphertext components for the encrypted output notes, Cenc 1..Nnew. BCTV14 proofs are used when the transaction version is 2 or 3, i.e. before Sapling activation. Groth16 proofs are used when the transaction version is 4, i.e. after Sapling activation.",
      "The ephemeralKey and encCiphertexts \ufb01elds together form the transmitted notes ciphertext, which is computed as described in section4.19 In-band secret distribution (Sprout) on page 65. Consensus rules applying to a JoinSplit description are given in section4.3 JoinSplit Descriptions on page 39. Spend Description Encoding and Consensus Let LEBS2OSP be as de\ufb01ned in section5.1 Integers, Bit Sequences, and Endianness on page 73. Let reprJ and \ud835\udc5eJ be as de\ufb01ned in section5.4.9.3 Jubjub on page 102. Let spendAuthSig be the spend authorization signature for this Spend transfer, and let \ud835\udf0bZKSpend be the zk-SNARK proof of the corresponding Spend statement. In a version 4 transaction these are encoded in the spendAuthSig \ufb01eld and zkproof \ufb01eld respectively of the Spend description. In a version 5 transaction, spend authorization signatures in vSpendAuthSigsSapling and proofs in vSpendProofsSapling are in one-to-one correspondence with Spend descriptions in vSpendsSapling.",
      "An abstract Spend description, as described in section3.6 Spend Transfers, Output Transfers, and their Descriptions on page 20, is encoded in a transaction as an instance of a SpendDescriptionV4 or SpendDescriptionV5 type: Bytes Name Data Type Description byte32 A value commitment to the value of the input note, LEBS2OSP256 reprJ(cv) 32  anchor byte32 A root of the Sapling note commitment tree at some block height in the past, LEBS2OSP256 rtSapling) nullifier byte32 The nulli\ufb01er of the input note, nf. byte32 The randomized validating key for spendAuthSig, LEBS2OSP256 reprJ(rk) 192  zkproof byte192 An encoding of the zk-SNARK proof \ud835\udf0bZKSpend (see section5.4.10.2 Groth16 on page 111). 64  spendAuthSig byte64 A signature authorizing this Spend. The anchor, zkproof, and spendAuthSig \ufb01elds are only present in a Spend description if the transaction version is 4.",
      "For v5 transactions, all Spend descriptions share the same anchor, which is encoded once as the anchorSapling \ufb01eld of the transaction as described in section7.1 Transaction Encoding and Consensus on page 122. The zkproof and spendAuthSig \ufb01elds have been moved into vSpendProofsSapling and vSpendAuthSigsSapling respectively for v5. Consensus rule: LEOS2IP256(anchorSapling), if present, MUST be less than \ud835\udc5eJ. Other consensus rules applying to a Spend description are given in section4.4 Spend Descriptions on page 40. Output Description Encoding and Consensus Let LEBS2OSP be as de\ufb01ned in section5.1 Integers, Bit Sequences, and Endianness on page 73. Let reprJ and \ud835\udc5eJ be as in section5.4.9.3 Jubjub on page 102, and ExtractJ(\ud835\udc5f) as in section5.4.9.4 Coordinate Extractor for Jubjub on page 104. Let \ud835\udf0bZKOutput be the zk-SNARK proof of the Output statement for this Output statement. In a version 4 transaction this is encoded in the zkproof \ufb01eld of the Spend description.",
      "In a v5 transaction, proofs in vOutputProofsSapling are in one-to-one correspondence with Output descriptions in vOutputsSapling. An abstract Output description, described in section3.6 Spend Transfers, Output Transfers, and their Descriptions on page 20, is encoded in a transaction as an instance of an OutputDescriptionV4 or OutputDescriptionV5 type: Bytes Name Data Type Description byte32 A value commitment to the value of the output note, LEBS2OSP256 reprJ(cv) byte32 The \ud835\udc62-coordinate of the note commitment for the output note, LEBS2OSP256(cm\ud835\udc62) where cm\ud835\udc62 ExtractJ(\ud835\udc5f)(cm). ephemeralKey byte32 An encoding of an ephemeral Jubjub public key, LEBS2OSP256 reprJ(epk) encCiphertext byte580 A ciphertext component for the encrypted output note, Cenc. outCiphertext byte80 A ciphertext component that allows the holder of the outgoing cipher key to recover the diversi\ufb01ed transmission key pkd and ephemeral private key esk, hence the entire note plaintext.",
      "192  zkproof byte192 An encoding of the zk-SNARK proof \ud835\udf0bZKOutput (see section5.4.10.2 Groth16 on page 111). The zkproof \ufb01eld is only present in a Spend description if the transaction version is 4. This \ufb01eld has been moved into the vOutputProofsSapling \ufb01eld of version 5 transactions. The ephemeralKey, encCiphertext, and outCiphertext \ufb01elds together form the transmitted note ciphertext, which is computed as described in section4.20 In-band secret distribution (Sapling and Orchard) on page 67. Consensus rule: LEOS2IP256(cmu) MUST be less than \ud835\udc5eJ. Other consensus rules applying to an Output description are given in section4.5 Output Descriptions on page 41. Action Description Encoding and Consensus Let LEBS2OSP be as de\ufb01ned in section5.1 Integers, Bit Sequences, and Endianness on page 73. Let reprP and \ud835\udc5eP be as de\ufb01ned in section5.4.9.6 Pallas and Vesta on page 105.",
      "Let spendAuthSig be the spend authorization signature for this Action transfer from vSpendAuthSigsOrchard, and let \ud835\udf0bZKAction be the zk-SNARK proof of the corresponding Action statement. Spend authorization signatures in the vSpendAuthSigsOrchard \ufb01eld of a version 5 transaction and aggregated proofs in the proofsOrchard \ufb01eld are in one-to-one correspondence with Action descriptions in vActionsOrchard. An abstract Action description, as described in section3.7 Action Transfers and their Descriptions on page 20, is encoded in a transaction as an instance of an ActionDescription type: Bytes Name Data Type Description byte32 A value commitment to the net value of the input note minus the output note, LEBS2OSP256 reprP(cv) nullifier byte32 The nulli\ufb01er of the input note, nf. byte32 The randomized validating key for spendAuthSig, LEBS2OSP256 reprP(rk) byte32 The \ud835\udc65-coordinate of the note commitment for the output note, LEBS2OSP256(cm\ud835\udc65) where cm\ud835\udc65 ExtractP(cm).",
      "ephemeralKey byte32 An encoding of an ephemeral Pallas public key, LEBS2OSP256 reprP(epk) encCiphertext byte580 A ciphertext component for the encrypted output note, Cenc. outCiphertext byte80 A ciphertext component that allows the holder of the outgoing cipher key (which can be derived from a full viewing key) to recover the recipient diversi\ufb01ed transmission key pkd and the ephemeral private key esk, hence the entire note plaintext. The ephemeralKey, encCiphertext, and outCiphertext \ufb01elds together form the transmitted note ciphertext, which is computed as described in section4.20 In-band secret distribution (Sapling and Orchard) on page 67. Consensus rule: LEOS2IP256(cmx) MUST be less than \ud835\udc5eP. Other consensus rules applying to an Action description are given in section4.6 Action Descriptions on page 42.",
      "Block Header Encoding and Consensus The Zcash block header format is as follows (this should be read in the context of consensus rules later in the section): Bytes Name Data Type Description nVersion int32 The block version number indicates which set of block validation rules to follow. The current and only de\ufb01ned block version number for Zcash is 4. hashPrevBlock byte32 A SHA-256d hash in internal byte order of the previous blocks header. This ensures no previous block can be changed without also changing this blocks header. hashMerkleRoot byte32 A SHA-256d hash in internal byte order. The merkle root is derived from the hashes of all transactions included in this block, ensuring that none of those transactions can be modi\ufb01ed without modifying the header. hashReserved  hashFinalSaplingRoot  hashLightClientRoot  hashBlockCommitments byte32 Pre-Sapling A reserved \ufb01eld, to be ignored.",
      "Sapling and Blossom only, pre-Heartwood The root LEBS2OSP256 rtSapling) of the Sapling note commitment tree corresponding to the \ufb01nal Sapling treestate of this block. Heartwood and Canopy only, pre-NU55 The hashChainHistoryRoot of this block ZIP-221. NU55 onward The hashBlockCommitments of this block ZIP-244. nTime uint32 The block timestamp is a Unix epoch time (UTC) when the miner started hashing the header (according to the miner). nBits uint32 An encoded version of the target threshold this blocks header hash must be less than or equal to, in the same nBits format used by Bitcoin. Bitcoin-nBits nNonce byte32 An arbitrary \ufb01eld that miners can change to modify the header hash in order to produce a hash less than or equal to the target threshold. solutionSize compactSize The size of an Equihash solution in bytes (always 1344). solution byte1344 The Equihash solution. A block consists of a block header and a sequence of transactions.",
      "How transactions are encoded in a block is part of the Zcash peer-to-peer protocol but not part of the consensus protocol. Let ThresholdBits be as de\ufb01ned in section7.7.3 Dif\ufb01culty adjustment on page 134, and let PoWMedianBlockSpan be the constant de\ufb01ned in section5.3 Constants on page 74. De\ufb01ne the median-time-past of a block to be the median (as de\ufb01ned in section7.7.3 Dif\ufb01culty adjustment on page 134) of the nTime \ufb01elds of the preceding PoWMedianBlockSpan blocks (or all preceding blocks if there are fewer than PoWMedianBlockSpan). The median-time-past of a genesis block is not de\ufb01ned. Consensus rules:  The block version number MUST be greater than or equal to 4. For a block at block height height, nBits MUST be equal to ThresholdBits(height). The block MUST pass the dif\ufb01culty \ufb01lter de\ufb01ned in section7.7.2 Dif\ufb01culty \ufb01lter on page 134. solution MUST represent a valid Equihash solution as de\ufb01ned in section7.7.1 Equihash on page 133.",
      "For each block other than the genesis block, nTime MUST be strictly greater than the median-time-past of that block. For each block at block height 2 or greater on Mainnet, or block height 653606 or greater on Testnet, nTime MUST be less than or equal to the median-time-past of that block plus 90  60 seconds. The size of a block MUST be less than or equal to 2000000 bytes. Sapling and Blossom only, pre-Heartwood hashLightClientRoot MUST be LEBS2OSP256 rtSapling) where rtSapling is the root of the Sapling note commitment tree for the \ufb01nal Sapling treestate of this block. Heartwood and Canopy only, pre-NU55 hashLightClientRoot MUST be set to the hashChainHistoryRoot for this block, as speci\ufb01ed in ZIP-221. NU55 onward hashBlockCommitments MUST be set to the value of hashBlockCommitments for this block, as speci\ufb01ed in ZIP-244. A block MUST have at least one transaction.",
      "The \ufb01rst transaction in a block MUST be a coinbase transaction, and subsequent transactions MUST NOT be coinbase transactions. TODO: Other rules inherited from Bitcoin. In addition, a full validator MUST NOT accept blocks with nTime more than two hours in the future according to its clock. This is not strictly a consensus rule because it is nondeterministic, and clock time varies between nodes. Also note that a block that is rejected by this rule at a given point in time may later be accepted. Notes:  The semantics of blocks with block version number not equal to 4 is not currently de\ufb01ned. Miners MUST NOT create such blocks. The exclusion of blocks with block version number greater than 4 is not a consensus rule; such blocks may exist in the block chain and MUST be treated identically to version 4 blocks by full validators. Note that a future upgrade might use block version number either greater than or less than 4.",
      "It is likely that such an upgrade will change the block header andor transaction format, and software that parses blocks SHOULD take this into account. The nVersion \ufb01eld is a signed integer. (It was speci\ufb01ed as unsigned in a previous version of this speci\ufb01cation.) A future upgrade might use negative values for this \ufb01eld, or otherwise change its interpretation. There is no relation between the values of the version \ufb01eld of a transaction, and the nVersion \ufb01eld of a block header. Like other serialized \ufb01elds of type compactSize, the solutionSize \ufb01eld MUST be encoded with the minimum number of bytes (3 in this case), and other encodings MUST be rejected. This is necessary to avoid a potential attack in which a miner could test several distinct encodings of each Equihash solution against the dif\ufb01culty \ufb01lter, rather than only the single intended encoding.",
      "As in Bitcoin, the nTime \ufb01eld MUST represent a time strictly greater than the median of the timestamps of the past PoWMedianBlockSpan blocks. The Bitcoin Developer Reference Bitcoin-Block was previously in error on this point, but has now been corrected. The rule limiting nTime to be no later than 90  60 seconds after the median-time-past is a retrospective consensus change, applied as a soft fork in zcashd v2.1.1-1. It had not been violated by any block from the given block heights in the consensus block chains of either Mainnet or Testnet. There are no changes to the block version number or format for Overwinter. Although the block version number does not change for Sapling, the previously reserved (and ignored) \ufb01eld hashReserved has been repurposed for hashFinalSaplingRoot. There are no other format changes. There are no changes to the block version number or format for Blossom. For Heartwood, the hashFinalSaplingRoot \ufb01eld is renamed to hashLightClientRoot.",
      "Once Heartwood acti- vates, the meaning of this \ufb01eld changes according to ZIP-221. There are no changes to the block version number or format for Canopy. For NU55, the hashLightClientRoot \ufb01eld is renamed to hashBlockCommitments. Once NU55 activates, the meaning of this \ufb01eld changes according to ZIP-244. There are no changes to the block version number or format for NU66. There are no changes to the block version number or format for NU6.6.1. The changes relative to Bitcoin version 4 blocks as described in Bitcoin-Block are:  Block versions less than 4 are not supported. The hashReserved  hashFinalSaplingRoot  hashLightClientRoot  hashBlockCommitments, solutionSize, and solution \ufb01elds have been added. The type of the nNonce \ufb01eld has changed from uint32 to byte32. The maximum block size has been doubled to 2000000 bytes. Proof of Work Zcash uses Equihash BK2016 as its proof-of-work.",
      "The original motivations for changing the proof-of-work from SHA-256d used by Bitcoin were described in WG2016. A block satis\ufb01es the proof-of-work if and only if:  The solution \ufb01eld encodes a valid Equihash solution according to section7.7.1 Equihash on page 133. The block header satis\ufb01es the dif\ufb01culty check according to section7.7.2 Dif\ufb01culty \ufb01lter on page 134. 7.7.1 Equihash An instance of the Equihash algorithm is parameterized by positive integers \ud835\udc5band \ud835\udc58, such that \ud835\udc5bis a multiple of \ud835\udc58 1. We assume \ud835\udc583. The Equihash parameters for Mainnet and Testnet are \ud835\udc5b 200, \ud835\udc58 9. Equihash is based on a variation of the Generalized Birthday Problem AR2017: given a sequence \ud835\udc4b1 .. N of \ud835\udc5b-bit strings, \ufb01nd 2\ud835\udc58distinct \ud835\udc4b\ud835\udc56\ud835\udc57such that 2\ud835\udc58 \ud835\udc571\ud835\udc4b\ud835\udc56\ud835\udc57 0. In Equihash, N  2 \ud835\udc581 1, and the sequence \ud835\udc4b1 .. N is derived from the block header and a nonce. Let powheader : 32-bit nVersion 256-bit hashPrevBlock 256-bit hashMerkleRoot 256-bit hashReserved 32-bit nTime 32-bit nBits 256-bit nNonce For \ud835\udc561 ..",
      "\ud835\udc41, let \ud835\udc4b\ud835\udc56 EquihashGen\ud835\udc5b,\ud835\udc58(powheader, \ud835\udc56). EquihashGen is instantiated in section5.4.1.11 Equihash Generator on page 85. De\ufb01ne I2BEBSP N)  0 .. 2\u21131 B\u2113 as in section5.1 Integers, Bit Sequences, and Endianness on page 73. A valid Equihash solution is then a sequence \ud835\udc56 1 .. \ud835\udc412\ud835\udc58 that satis\ufb01es the following conditions: Generalized Birthday condition \ud835\udc4b\ud835\udc56\ud835\udc57 0. Algorithm Binding conditions  For all \ud835\udc5f1 .. \ud835\udc581, for all \ud835\udc640 .. 2\ud835\udc58\ud835\udc5f1 : \ud835\udc4b\ud835\udc56\ud835\udc642\ud835\udc5f\ud835\udc57has \ud835\udc5b\ud835\udc5f \ud835\udc581 leading zeros; and  For all \ud835\udc5f1 .. \ud835\udc58, for all \ud835\udc640 .. 2\ud835\udc58\ud835\udc5f1 : \ud835\udc56\ud835\udc642\ud835\udc5f1..\ud835\udc642\ud835\udc5f2\ud835\udc5f1  \ud835\udc56\ud835\udc642\ud835\udc5f2\ud835\udc5f11..\ud835\udc642\ud835\udc5f2\ud835\udc5flexicographically. Notes:  This does not include a dif\ufb01culty condition, because here we are de\ufb01ning validity of an Equihash solution independent of dif\ufb01culty. Previous versions of this speci\ufb01cation incorrectly speci\ufb01ed the range of \ud835\udc5fto be 1 .. \ud835\udc581 for both parts of the algorithm binding condition. The implementation in zcashd was as intended.",
      "An Equihash solution with \ud835\udc5b 200 and \ud835\udc58 9 is encoded in the solution \ufb01eld of a block header as follows: I2BEBSP21(\ud835\udc561 1) I2BEBSP21(\ud835\udc562 1)    I2BEBSP21(\ud835\udc56512 1) Recall from section5.2 Bit layout diagrams on page 73 that the bits in the above diagram are ordered from most to least signi\ufb01cant in each byte. For example, if the \ufb01rst 3 elements of \ud835\udc56are 69, 42, 221, then the corresponding bit array is: I2BEBSP21(68) I2BEBSP21(41) I2BEBSP21(221 1) 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 8-bit 0 8-bit 2 8-bit 32 8-bit 0 8-bit 10 8-bit 127 8-bit 255    and so the \ufb01rst 7 bytes of solution would be 0, 2, 32, 0, 10, 127, 255. Note: I2BEBSP is big-endian, while integer \ufb01eld encodings in powheader and in the instantiation of EquihashGen are little-endian.",
      "The rationale for this is that little-endian serialization of block headers is consistent with Bitcoin, but little-endian ordering of bits in the solution encoding would require bit-reversal (as opposed to only shifting). 7.7.2 Dif\ufb01culty \ufb01lter Let ToTarget be as de\ufb01ned in section7.7.4 nBits conversion on page 136. Dif\ufb01culty is de\ufb01ned in terms of a target threshold, which is adjusted for each block according to the algorithm de\ufb01ned in section7.7.3 Dif\ufb01culty adjustment on page 134. The dif\ufb01culty\ufb01lteris unchanged from Bitcoin, and is calculated using SHA-256d on the whole block header (including solutionSize and solution). The result is interpreted as a 256-bit integer represented in little-endian byte order, which MUST be less than or equal to the target threshold given by ToTarget(nBits). 7.7.3 Dif\ufb01culty adjustment The desired time between blocks is called the block target spacing.",
      "Zcash uses a dif\ufb01culty adjustment algorithm based on DigiShield v3v4 DigiByte-PoW, with simpli\ufb01cations and altered parameters, to adjust dif\ufb01culty to target the desired block target spacing. Unlike Bitcoin, the dif\ufb01culty adjustment occurs after every block. The constants PoWLimit, PreBlossomHalvingInterval, PoWAveragingWindow, PoWMaxAdjustDown, PoWMaxAdjustUp, PoWDampingFactor, PreBlossomPoWTargetSpacing, and PostBlossomPoWTargetSpacing are speci\ufb01ed in section section5.3 Constants on page 74. Let ToCompact and ToTarget be as de\ufb01ned in section7.7.4 nBits conversion on page 136. Let nTime(height) be the value of the nTime \ufb01eld in the header of the block at block height height. Let nBits(height) be the value of the nBits \ufb01eld in the header of the block at block height height. Block header \ufb01elds are speci\ufb01ed in section7.6 Block Header Encoding and Consensus on page 131.",
      "De\ufb01ne: mean(\ud835\udc46) : length(\ud835\udc46) length(\ud835\udc46) median(\ud835\udc46) : sorted(\ud835\udc46)ceiling((length(\ud835\udc46)1)2) bound upper lower (\ud835\udc65) : max(lower, min(upper, \ud835\udc65))) trunc(\ud835\udc65) : floor(\ud835\udc65) , if \ud835\udc650 floor(\ud835\udc65) , otherwise IsBlossomActivated(height N) : (height BlossomActivationHeight) BlossomPoWTargetSpacingRatio : PreBlossomPoWTargetSpacing PostBlossomPoWTargetSpacing PostBlossomHalvingInterval : floor(PreBlossomHalvingInterval  BlossomPoWTargetSpacingRatio) PoWTargetSpacing(height N) : PreBlossomPoWTargetSpacing, if not IsBlossomActivated(height) PostBlossomPoWTargetSpacing, otherwise AveragingWindowTimespan(height N) : PoWAveragingWindow  PoWTargetSpacing(height) MinActualTimespan(height N) : floor(AveragingWindowTimespan(height)  (1 PoWMaxAdjustUp)) MaxActualTimespan(height N) : floor(AveragingWindowTimespan(height)  (1  PoWMaxAdjustDown)) MedianTime(height N) : median( nTime(\ud835\udc56) for \ud835\udc56from max(0, height PoWMedianBlockSpan) up to height 1 ) ActualTimespan(height N) : MedianTime(height) MedianTime(height PoWAveragingWindow) ActualTimespanDamped(height N) : AveragingWindowTimespan(height)  trunc (ActualTimespan(height) AveragingWindowTimespan(height) PoWDampingFactor ActualTimespanBounded(height N) : bound MaxActualTimespan(height) MinActualTimespan(height) (ActualTimespanDamped(height)) MeanTarget(height N) : PoWLimit, if height PoWAveragingWindow mean(ToTarget(nBits(\ud835\udc56)) for \ud835\udc56from heightPoWAveragingWindow up to height1), otherwise.",
      "The target threshold for a given block height height is then calculated as: Threshold(height N) : PoWLimit, if height  0 min(PoWLimit, floor MeanTarget(height) AveragingWindowTimespan  ActualTimespanBounded(height)), otherwise ThresholdBits(height N) : ToCompact(Threshold(height)). Notes:  The convention used for the height parameters to the functions MedianTime, MeanTarget, ActualTimespan, ActualTimespanDamped, ActualTimespanBounded, Threshold, and ThresholdBits is that these functions use only information from blocks preceding the given block height. When the median function is applied to a sequence of even length (which only happens in the de\ufb01nition of MedianTime during the \ufb01rst PoWAveragingWindow 1 blocks of the block chain), the element that begins the second half of the sequence is taken. This corresponds to the zcashd implementation, but was not speci\ufb01ed correctly in versions of this speci\ufb01cation prior to v2019.0.0.",
      "On Testnet from block height 299188 onward, the dif\ufb01culty adjustment algorithm is changed to allow minimum- dif\ufb01culty blocks, as described in ZIP-205. The Blossom network upgrade changes the minimum-dif\ufb01culty time threshold to 6 times the block target spacing, as described in ZIP-208. These changes do not apply to Mainnet. 7.7.4 nBits conversion Deterministic conversions between a target threshold and a compact\" nBits value are not fully de\ufb01ned in the Bitcoin documentation Bitcoin-nBits, and so we de\ufb01ne them here: size(\ud835\udc65) : ceiling (bitlength(\ud835\udc65) mantissa(\ud835\udc65) : floor \ud835\udc65 2563size(\ud835\udc65)) ToCompact(\ud835\udc65) : mantissa(\ud835\udc65)  224size(\ud835\udc65), if mantissa(\ud835\udc65)  223 floor (mantissa(\ud835\udc65)  224(size(\ud835\udc65)  1), otherwise ToTarget(\ud835\udc65) : if \ud835\udc65\u00ee 223  223 (\ud835\udc65\u00ee (223 1))  256floor(\ud835\udc65224)3, otherwise. 7.7.5 De\ufb01nition of Work As explained in section3.3 The Block Chain on page 18, a node chooses the best block chain visible to it by \ufb01nding the chain of valid blocks with the greatest total work.",
      "Let ToTarget be as de\ufb01ned in section7.7.4 nBits conversion on page 136. The work of a block with value nBits for the nBits \ufb01eld in its block header is de\ufb01ned as floor ToTarget(nBits)  1 Calculating Block Subsidy, Funding Streams, Lockbox Disbursement, and Founders Reward In section3.10 Block Subsidy, Funding Streams, and Founders Reward on page 22 the block subsidy, miner subsidy, Founders Reward, and funding streams are de\ufb01ned. Their amounts in zatoshi are calculated from the block height using the formulae below. Let the constants SlowStartInterval, PreBlossomHalvingInterval, PostBlossomHalvingInterval, BlossomActivationHeight, MaxBlockSubsidy, and FoundersFraction be as de\ufb01ned in section5.3 Constants on page 74. Let FundingStreams be as speci\ufb01ed in section7.10.1 ZIP 214 Funding Streams on page 140. Let ZIP271ActivationHeight and ZIP271DisbursementAmount be as de\ufb01ned in ZIP-271.",
      "SlowStartShift N : SlowStartInterval SlowStartRate N : MaxBlockSubsidy SlowStartInterval Halving(height N) : if height  SlowStartShift floor (height SlowStartShift PreBlossomHalvingInterval , if not IsBlossomActivated(height) floor (BlossomActivationHeight SlowStartShift PreBlossomHalvingInterval  height BlossomActivationHeight PostBlossomHalvingInterval , otherwise BlockSubsidy(height N) : SlowStartRate  height, if height  SlowStartShift SlowStartRate  (height  1), if SlowStartShift height and height  SlowStartInterval floor (MaxBlockSubsidy 2Halving(height) if SlowStartInterval height and not IsBlossomActivated(height) floor MaxBlockSubsidy BlossomPoWTargetSpacingRatio  2Halving(height) , otherwise FoundersReward(height N) : BlockSubsidy(height)  FoundersFraction, if Halving(height)  1 otherwise for fs FundingStreams, fs.Value(height) : if height  CanopyActivationHeight floor BlockSubsidy(height)  fs.Numerator fs.Denominator , if fs.StartHeight height and height  fs.EndHeight otherwise totalDeferredOutput(height) :  fs FundingStreams : fs.Recipient(height)  DEFERRED_POOLfs.Value(height) totalDeferredInput(height) : ZIP271DisbursementAmount, if height  ZIP271ActivationHeight otherwise.",
      "MinerSubsidy(height) : BlockSubsidy(height) FoundersReward(height)  fs FundingStreamsfs.Value(height). Payment of Founders Reward The Founders Reward is paid by a transparent output in the coinbase transaction, to one of NumFounderAddresses transparent addresses, depending on the block height.",
      "For Mainnet, FounderAddressList1..NumFounderAddresses is:  t3Vz22vK5z2LcKEdg16Yv4FFneEL1zg9ojd, t3cL9AucCajm3HXDhb5jBnJK2vapVoXsop3, t3fqvkzrrNaMcamkQMwAyHRjfDdM2xQvDTR, t3TgZ9ZT2CTSK44AnUPi6qeNaHa2eC7pUyF, t3SpkcPQPfuRYHsP5vz3Pv86PgKo5m9KVmx, t3Xt4oQMRPagwbpQqkgAViQgtST4VoSWR6S, t3ayBkZ4w6kKXynwoHZFUSSgXRKtogTXNgb, t3adJBQuaa21u7NxbR8YMzp3km3TbSZ4MGB, t3K4aLYagSSBySdrfAGGeUd5H9z5Qvz88t2, t3RYnsc5nhEvKiva3ZPhfRSk7eyh1CrA6Rk, t3Ut4KUq2ZSMTPNE67pBU5LqYCi2q36KpXQ, t3ZnCNAvgu6CSyHm1vWtrx3aiN98dSAGpnD, t3fB9cB3eSYim64BS9xfwAHQUKLgQQroBDG, t3cwZfKNNj2vXMAHBQeewm6pXhKFdhk18kD, t3YcoujXfspWy7rbNUsGKxFEWZqNstGpeG4, t3bLvCLigc6rbNrUTS5NwkgyVrZcZumTRa4, t3VvHWa7r3oy67YtU4LZKGCWa2J6eGHvShi, t3eF9X6X2dSo7MCvTjfZEzwWrVzquxRLNeY, t3esCNwwmcyc8i9qQfyTbYhTqmYXZ9AwK3X, t3M4jN7hYE2e27yLsuQPPjuVek81WV3VbBj, t3gGWxdC67CYNoBbPjNvrrWLAWxPqZLxrVY, t3LTWeoxeWPbmdkUD3NWBquk4WkazhFBmvU, t3P5KKX97gXYFSaSjJPiruQEX84yF5z3Tjq, t3f3T3nCWsEpzmD35VK62JgQfFig74dV8C9, t3Rqonuzz7afkF7156ZA4vi4iimRSEn41hj, t3fJZ5jYsyxDtvNrWBeoMbvJaQCj4JJgbgX, t3Pnbg7XjP7FGPBUuz75H65aczphHgkpoJW, t3WeKQDxCijL5X7rwFem1MTL9ZwVJkUFhpF, t3Y9FNi26J7UtAUC4moaETLbMo8KS1Be6ME, t3aNRLLsL2y8xcjPheZZwFy3Pcv7CsTwBec, t3gQDEavk5VzAAHK8TrQu2BWDLxEiF1unBm, t3Rbykhx1TUFrgXrmBYrAJe2STxRKFL7G9r, t3aaW4aTdP7a8d1VTE1Bod2yhbeggHgMajR, t3YEiAa6uEjXwFL2v5ztU1fn3yKgzMQqNyo, t3g1yUUwt2PbmDvMDevTCPWUcbDatL2iQGP, t3dPWnep6YqGPuY1CecgbeZrY9iUwH8Yd4z, t3QRZXHDPh2hwU46iQs2776kRuuWfwFp4dV, t3enhACRxi1ZD7e8ePomVGKn7wp7N9fFJ3r, t3PkLgT71TnF112nSwBToXsD77yNbx2gJJY, t3LQtHUDoe7ZhhvddRv4vnaoNAhCr2f4oFN, t3fNcdBUbycvbCtsD2n9q3LuxG7jVPvFB8L, t3dKojUU2EMjs28nHV84TvkVEUDu1M1FaEx, t3aKH6NiWN1ofGd8c19rZiqgYpkJ3n679ME, t3MEXDF9Wsi63KwpPuQdD6by32Mw2bNTbEa, t3WDhPfik343yNmPTqtkZAoQZeqA83K7Y3f, t3PSn5TbMMAEw7Eu36DYctFezRzpX1hzf3M, t3R3Y5vnBLrEn8L6wFjPjBLnxSUQsKnmFpv, t3Pcm737EsVkGTbhsu2NekKtJeG92mvYyoN  For Testnet, FounderAddressList1..NumFounderAddresses is:  t2UNzUUx8mWBCRYPRezvA363EYXyEpHokyi, t2N9PH9Wk9xjqYg9iin1Ua3aekJqfAtE543, t2NGQjYMQhFndDHguvUw4wZdNdsssA6K7x2, t2ENg7hHVqqs9JwU5cgjvSbxnT2a9USNfhy, t2BkYdVCHzvTJJUTx4yZB8qeegD8QsPx8bo, t2J8q1xH1EuigJ52MfExyyjYtN3VgvshKDf, t2Crq9mydTm37kZokC68HzT6yez3t2FBnFj, t2EaMPUiQ1kthqcP5UEkF42CAFKJqXCkXC9, t2F9dtQc63JDDyrhnfpzvVYTJcr57MkqA12, t2LPirmnfYSZc481GgZBa6xUGcoovfytBnC, t26xfxoSw2UV9Pe5o3C8V4YybQD4SESfxtp, t2D3k4fNdErd66YxtvXEdft9xuLoKD7CcVo, t2DWYBkxKNivdmsMiivNJzutaQGqmoRjRnL, t2C3kFF9iQRxfc4B9zgbWo4dQLLqzqjpuGQ, t2MnT5tzu9HSKcppRyUNwoTp8MUueuSGNaB, t2AREsWdoW1F8EQYsScsjkgqobmgrkKeUkK, t2Vf4wKcJ3ZFtLj4jezUUKkwYR92BLHn5UT, t2K3fdViH6R5tRuXLphKyoYXyZhyWGghDNY, t2VEn3KiKyHSGyzd3nDw6ESWtaCQHwuv9WC, t2F8XouqdNMq6zzEvxQXHV1TjwZRHwRg8gC, t2BS7Mrbaef3fA4xrmkvDisFVXVrRBnZ6Qj, t2FuSwoLCdBVPwdZuYoHrEzxAb9qy4qjbnL, t2SX3U8NtrT6gz5Db1AtQCSGjrpptr8JC6h, t2V51gZNSoJ5kRL74bf9YTtbZuv8Fcqx2FH, t2FyTsLjjdm4jeVwir4xzj7FAkUidbr1b4R, t2EYbGLekmpqHyn8UBF6kqpahrYm7D6N1Le, t2NQTrStZHtJECNFT3dUBLYA9AErxPCmkka, t2GSWZZJzoesYxfPTWXkFn5UaxjiYxGBU2a, t2RpffkzyLRevGM3w9aWdqMX6bd8uuAK3vn, t2JzjoQqnuXtTGSN7k7yk5keURBGvYofh1d, t2AEefc72ieTnsXKmgK2bZNckiwvZe3oPNL, t2NNs3ZGZFsNj2wvmVd8BSwSfvETgiLrD8J, t2ECCQPVcxUCSSQopdNquguEPE14HsVfcUn, t2JabDUkG8TaqVKYfqDJ3rqkVdHKp6hwXvG, t2FGzW5Zdc8Cy98ZKmRygsVGi6oKcmYir9n, t2DUD8a21FtEFn42oVLp5NGbogY13uyjy9t, t2UjVSd3zheHPgAkuX8WQW2CiC9xHQ8EvWp, t2TBUAhELyHUn8i6SXYsXz5Lmy7kDzA1uT5, t2Tz3uCyhP6eizUWDc3bGH7XUC9GQsEyQNc, t2NysJSZtLwMLWEJ6MH3BsxRh6h27mNcsSy, t2KXJVVyyrjVxxSeazbY9ksGyft4qsXUNm9, t2J9YYtH31cveiLZzjaE4AcuwVho6qjTNzp, t2QgvW4sP9zaGpPMH1GRzy7cpydmuRfB4AZ, t2NDTJP9MosKpyFPHJmfjc5pGCvAU58XGa4, t29pHDBWq7qN4EjwSEHg8wEqYe9pkmVrtRP, t2Ez9KM8VJLuArcxuEkNRAkhNvidKkzXcjJ, t2D5y7J5fpXajLbGrMBQkFg2mFN8fo3n8cX, t2UV2wr1PTaUiybpkV3FdSdGxUJeZdZztyt  Note: For Testnet only, the addresses from index 4 onward have been changed from what was implemented at launch.",
      "This re\ufb02ects an upgrade on Testnet, starting from block height 53127. Zcash-Issue2113 Each address representation in FounderAddressList denotes a transparent P2SH multisig address. Let SlowStartShift and Halving be de\ufb01ned as in the previous section. De\ufb01ne: FounderAddressChangeInterval : ceiling (SlowStartShift  PreBlossomHalvingInterval NumFounderAddresses FounderAddressAdjustedHeight(height N) : height, if not IsBlossomActivated(height), BlossomActivationHeight  floor (height BlossomActivationHeight BlossomPoWTargetSpacingRatio , otherwise FounderAddressIndex(height N) : 1  floor (FounderAddressAdjustedHeight(height) FounderAddressChangeInterval FoundersRewardLastBlockHeight : max(height N  Halving(height)  1). Let FounderRedeemScriptHash(height N) be the standard redeem script hash, as speci\ufb01ed in Bitcoin-Multisig, for the P2SH multisig address with Base58Check form given by FounderAddressList FounderAddressIndex(height).",
      "Consensus rule: Pre-Canopy A coinbase transaction at height 1 .. FoundersRewardLastBlockHeight MUST include at least one output that pays exactly FoundersReward(height) zatoshi with a standard P2SH script of the form OP_HASH160 FounderRedeemScriptHash(height) OP_EQUAL as its scriptPubKey. Notes:  No Founders Reward is required to be paid for height  FoundersRewardLastBlockHeight (i.e. after the \ufb01rst halving), or for height  0 (i.e. the genesis block), or after Canopy activation. The Founders Reward addresses are not treated specially in any other way, and there can be other outputs to them, in coinbase transactions or otherwise. In particular, it is valid for a coinbase transaction with height 1 .. FoundersRewardLastBlockHeight to have other outputs, possibly to the same address, that do not meet the criterion in the above consensus rule, as long as at least one output meets it.",
      "The assertion FounderAddressIndex(FoundersRewardLastBlockHeight) NumFounderAddresses holds, ensuring that the Founders Reward address index remains in range for the whole period in which the Founders Reward is paid. Non-normative notes:  Blossom onward FoundersRewardLastBlockHeight  1046399. Blossom is not intended to change the total Founders Reward or the effective period over which it is paid. 7.10 Payment of Funding Streams, Deferred Lockbox, and Lockbox Disbursement Let PostBlossomHalvingInterval be as de\ufb01ned in section5.3 Constants on page 74. Let Halving be as de\ufb01ned in section7.8 on page 136. Let ZIP271ActivationHeight, ZIP271DisbursementAmount, ZIP271DisbursementChunks, and ZIP271DisbursementAddress be as de\ufb01ned for the relevant network (Mainnet or Testnet) in ZIP-271. ZIP-207 de\ufb01nes a consensus mechanism to require coinbase transactions to include funding stream outputs, intended to provide funds from issuance for Zcash development.",
      "ZIP-2001 extended this mechanism to support directing funds from issuance into a reserve, the deferred development fund lockbox. ZIP-271 de\ufb01nes a one-time disbursal of funds from this lockbox in order to support the Community And Coinholder Funding Model ZIP-1016. The funding streams are paid to one of a pre-de\ufb01ned set of recipients, depending on the block height. Each recipient identi\ufb01er is either the string encoding of an address to be paid by an output in the coinbase transaction, or the identi\ufb01er DEFERRED_POOL. The latter indicates that the value is to be paid into the deferred development fund lockbox. A funding stream fs is de\ufb01ned by a block subsidy fraction (represented as a numerator and denominator), a start block height (inclusive), an end block height (exclusive), and a sequence of recipients: fs.Numerator fs.Denominator fs.StartHeight fs.EndHeight fs.Recipients BYN DEFERRED_POOL )N.",
      "De\ufb01ne: HeightForHalving(halving N) : min(height N  Halving(height)  halving) FSRecipientChangeInterval : PostBlossomHalvingInterval48 FSRecipientPeriod(height) : floor (height (HeightForHalving(1) PostBlossomHalvingInterval) FSRecipientChangeInterval For each funding stream fs, de\ufb01ne: fs.RecipientIndex(height) : 1  FSRecipientPeriod(height) FSRecipientPeriod(fs.StartHeight) fs.Recipient(height) : fs.Recipients fs.RecipientIndex(height) fs.NumRecipients : fs.RecipientIndex(fs.EndHeight 1). fs.Recipients MUST be of length fs.NumRecipients. Each element of fs.Recipients MUST represent either a transparent P2SH address as speci\ufb01ed in section5.6.1.1 Transparent Addresses on page 113, or a Sapling shielded payment address as speci\ufb01ed in section5.6.3.1 Sapling Payment Addresses on page 115, or the identi\ufb01er DEFERRED_POOL.",
      "A funding stream fs is active at block height height when fs.Value(height)  0, where fs.Value is de\ufb01ned in section7.8 Calculating Block Subsidy, Funding Streams, Lockbox Disbursement, and Founders Reward on page 136. Consensus rule: Canopy onward In each block with coinbase transaction cb at block height height, cb MUST contain at least the given number of distinct outputs for each of the following:  for each funding stream fs active at that block height with a recipient identi\ufb01er other than DEFERRED_POOL given by fs.Recipient(height), one output that pays fs.Value(height) zatoshi in the prescribed way to the address represented by that recipient identi\ufb01er;  NU6.6.1 onward if the block height is ZIP271ActivationHeight, ZIP271DisbursementChunks equal outputs paying a total of ZIP271DisbursementAmount zatoshi in the prescribed way to the Key-Holder Organizations P2SH multisig address represented by ZIP271DisbursementAddress, as speci\ufb01ed by ZIP-271.",
      "The term prescribed way is de\ufb01ned as follows:  The prescribed way to pay a transparent P2SH address is to use a standard P2SH script of the form OP_HASH160 fs.RedeemScriptHash(height) OP_EQUAL as the scriptPubKey. Here fs.RedeemScriptHash(height) is the standard redeem script hash for the recipient address for fs.Recipient(height) in Base58Check form. Standard redeem script hashes are de\ufb01ned in ZIP-48 for P2SH multisig addresses, or Bitcoin-P2SH for other P2SH addresses. The prescribed way to pay a Sapling or Orchard payment address is de\ufb01ned in ZIP-213, using the post- Heartwood consensus rules speci\ufb01ed for Sapling and Orchard outputs of coinbase transactions in section7.1.2 Transaction Consensus Rules on page 124. Notes:  The funding stream addresses are not treated specially in any other way, and there can be other outputs to them, in coinbase transactions or otherwise.",
      "In particular, it is valid for a coinbase transaction to have other outputs, possibly to the same address, that do not meet the criterion in the above consensus rule, as long as there is at least the given number of distinct outputs that meet it, disjointly for each funding item. For clari\ufb01cation, if there are multiple active funding streams or lockbox disbursements with the same recipient identi\ufb01er andor value, there MUST be at least the given number of distinct outputs for each of them. Up to and including NU6.6.1 there have been no funding streams or lockbox disbursements de\ufb01ned with a shielded payment address as a recipient. That might change in future, so implementations are encouraged to support Sapling and Orchard outputs as recipients, as permitted by ZIP-213 and ZIP-207. 7.10.1 ZIP 214 Funding Streams Let CanopyActivationHeight be as de\ufb01ned in section5.3 Constants on page 74.",
      "ZIP-214 Revision 0 de\ufb01nes these funding streams for Mainnet: Stream Numerator Denominator Start height End height FS_ZIP214_BP FS_ZIP214_ZF FS_ZIP214_MG It also de\ufb01nes these funding streams for Testnet: Stream Numerator Denominator Start height End height FS_ZIP214_BP FS_ZIP214_ZF FS_ZIP214_MG Notes:  The block heights of halvings are different between Testnet and Mainnet, as a result of different activation block heights for the Blossom network upgrade (which changed the block target spacing). The end height of these funding streams corresponds to the second halving on each network. On Testnet, the activation block height of Canopy is before the \ufb01rst halving. Therefore, the consequence of the above rules for Testnet is that the amount sent to each Zcash Development Fund recipient address will initially (before Testnet block height 1116000) be double the number of currency units as the corresponding initial amount on Mainnet.",
      "This reduces to the same number of currency units as on Mainnet, from Testnet block heights 1116000 (inclusive) to 2796000 (exclusive). NU66 onward ZIP-214 Revision 1 de\ufb01nes these funding streams for Mainnet: Stream Numerator Denominator Start height End height FS_FPF_ZCG FS_DEFERRED It also de\ufb01nes these funding streams for Testnet: Stream Numerator Denominator Start height End height FS_FPF_ZCG FS_DEFERRED Note: The new funding streams begin at the second halving for Mainnet, but the second halving on Testnet occurred prior to the introduction of the new funding streams. For both new funding streams on each network, the associated duration corresponds to approximately one years worth of blocks.",
      "NU6.6.1 onward ZIP-214 Revision 2 de\ufb01nes these funding streams for Mainnet: Stream Numerator Denominator Start height End height FS_FPF_ZCG_H3 FS_CCF_H3 It also de\ufb01nes these funding streams for Testnet: Stream Numerator Denominator Start height End height FS_FPF_ZCG_H3 FS_CCF_H3 7.11 Changes to the Script System The OP_CODESEPARATOR opcode has been disabled. This opcode also no longer affects the calculation of SIGHASH transaction hashes. 7.12 Bitcoin Improvement Proposals In general, Bitcoin Improvement Proposals (BIPs) do not apply to Zcash unless otherwise speci\ufb01ed in this section. All of the BIPs referenced below should be interpreted by replacing BTC, or bitcoin used as a currency unit, with ZEC; and satoshi with zatoshi. The following BIPs apply, otherwise unchanged, to Zcash: BIP-11, BIP-14, BIP-31, BIP-35, BIP-37, BIP-61. The following BIPs apply starting from the Zcash genesis block, i.e.",
      "any activation rules or exceptions for particular blocks in the Bitcoin block chain are to be ignored: BIP-16, BIP-30, BIP-65, BIP-66. The effect of BIP-34 has been incorporated into the consensus rules (section7.1.2 Transaction Consensus Rules on page 124). This excludes the Mainnet and Testnet genesis blocks, for which the height in coinbase was inadvertently omitted. BIP-13 applies with the changes to address version bytes described in section5.6.1.1 Transparent Addresses on page 113. BIP-111 applies from peer-to-peer protocol version 170004 onward; that is:  references to protocol version 70002 are to be replaced by 170003;  references to protocol version 70011 are to be replaced by 170004;  the reference to protocol version 70000 is to be ignored (Zcash nodes have supported Bloom-\ufb01ltered connec- tions since launch).",
      "Differences from the Zerocash paper Transaction Structure Zerocash introduces two new operations, which are described in the paper as new transaction types, in addition to the original transaction type of the cryptocurrency on which it is based (e.g. Bitcoin). In Zcash, there is only the original Bitcoin transaction type, which is extended to contain a sequence of zero or more Zcash-speci\ufb01c operations. This allows for the possibility of chaining transfers of shielded value in a single Zcash transaction, e.g. to spend a shielded note that has just been created. (In Zcash, we refer to value stored in UTXOs as transparent, and value stored in output notes of JoinSplit transfers or Output transfers) as shielded.) This was not possible in the Zerocash design without using multiple transactions. It also allows transparent and shielded transfers to happen atomically  possibly under the control of nontrivial script conditions, at some cost in distinguishability.",
      "Computation of SIGHASH transaction hashes, as described in section4.10 SIGHASH Transaction Hashing on page 50, was changed to clean up handling of an error case for SIGHASH_SINGLE, to remove the special treatment of OP_CODESEPARATOR, and to include Zcash-speci\ufb01c \ufb01elds in the hash ZIP-76. Memo Fields Zcash adds a memo \ufb01eld sent from the creator of a JoinSplit description to the recipient of each output note. This feature is described in more detail in section3.2.1 Note Plaintexts and Memo Fields on page 15. Uni\ufb01cation of Mints and Pours In the original Zerocash protocol, there were two kinds of transaction relating to shielded notes:  a Mint transaction takes value from UTXOs (unspent transaction outputs) as input and produces a new shielded note as output. a Pour transaction takes up to Nold shielded notes as input, and produces up to Nnew shielded notes and a UTXO as output. Only Pour transactions included a zk-SNARK proof.",
      "Pre-Sapling In Zcash, the sequence of operations added to a transaction (see section8.1 Transaction Structure on page 142) consists only of JoinSplit transfers. A JoinSplit transfer is a Pour operation generalized to take a UTXO as input, allowing JoinSplit transfers to subsume the functionality of Mints. An advantage of this is that a Zcash transaction that takes input from a UTXO can produce up to Nnew output notes, improving the indistinguishability properties of the protocol. A related change conceals the input arity of the JoinSplit transfer: an unused (zero-valued) input is indistinguishable from an input that takes value from a note. This uni\ufb01cation also simpli\ufb01es the \ufb01x to the Faerie Gold attack described below, since no special case is needed for Mints. Sapling onward In Sapling, there are still no Mint transactions. Instead of JoinSplit transfers, there are Spend transfers and Output transfers.",
      "These make use of Pedersen value commitments to represent the shielded values that are transferred. Because these commitments are additively homomorphic, it is possible to check that all Spend transfers and Output transfers balance; see section4.13 Balance and Binding Signature (Sapling) on page 52 for detail. This reduces the granularity of the circuit, allowing a substantial performance improvement (orthogonal to other Sapling circuit improvements) when the numbers of shielded inputs and outputs are signi\ufb01cantly different. This comes at the cost of revealing the exact number of shielded inputs and outputs, but dummy (zero-valued) outputs are still possible. Faerie Gold attack and \ufb01x When a shielded note is created in Zerocash, the creator is supposed to choose a new \u03c1 value at random. The nulli\ufb01er of the note is derived from its spending key (ask) and \u03c1. The note commitment is derived from the recipient address component apk, the value v, and the commitment trapdoor rcm, as well as \u03c1.",
      "However nothing prevents creating multiple notes with different v and rcm (hence different note commitments) but the same \u03c1. An adversary can use this to mislead a note recipient, by sending two notes both of which are veri\ufb01ed as valid by Receive (as de\ufb01ned in BCGGMTV2014, Figure 2), but only one of which can be spent. We call this a Faerie Gold attack  referring to various Celtic legends in which faeries pay mortals in what appears to be gold, but which soon after reveals itself to be leaves, gorse blossoms, gingerbread cakes, or other less valuable things LG2004. This attack does not violate the security de\ufb01nitions given in BCGGMTV2014. The issue could be framed as a problem either with the de\ufb01nition of Completeness, or the de\ufb01nition of Balance:  The Completeness property asserts that a validly received note can be spent provided that its nulli\ufb01er does not appear on the ledger.",
      "This does not take into account the possibility that distinct notes, which are validly received, could have the same nulli\ufb01er. That is, the security de\ufb01nition depends on a protocol detail nulli\ufb01ers that is not part of the intended abstract security property, and that could be implemented incorrectly. The Balance property only asserts that an adversary cannot obtain more funds than they have minted or received via payments. It does not prevent an adversary from causing others funds to decrease. In a Faerie Gold attack, an adversary can cause spending of a note to reduce (to zero) the effective value of another note for which the adversary does not know the spending key, which violates an intuitive conception of global balance. These problems with the security de\ufb01nitions need to be repaired; how to do so is discussed in Hopwood2022, but that is outside the scope of this speci\ufb01cation. Here we only describe how Zcash addresses the immediate attack.",
      "It would be possible to address the attack by requiring that a recipient remember all of the \u03c1 values for all notes they have ever received, and reject duplicates (as proposed in GGM2016). However, this requirement would interfere with the intended Zcash feature that a holder of a spending key can recover access to (and be sure that they are able to spend) all of their funds, even if they have forgotten everything but the spending key. Sprout Instead, Zcash enforces that an adversary must choose distinct values for each \u03c1, by making use of the fact that all of the nulli\ufb01ers in JoinSplit descriptions that appear in a valid block chain must be distinct. This is true regardless of whether the nulli\ufb01ers corresponded to real or dummy notes (see section4.8.1 Dummy Notes (Sprout) on page 46). The nulli\ufb01ers are used as input to hSigCRH to derive a public value hSig which uniquely identi\ufb01es the transaction, as described in section4.3 JoinSplit Descriptions on page 39.",
      "(hSig was already used in Zerocash in a way that requires it to be unique in order to maintain indistinguishability of JoinSplit descriptions; adding the nulli\ufb01ers to the input of the hash used to calculate it has the effect of making this uniqueness property robust even if the transaction creator is an adversary.) Sprout The \u03c1 value for each output note is then derived from a random private seed \u03d5 and hSig using PRF\u03c1 \u03d5. The correct construction of \u03c1 for each output note is enforced by section4.18.1 JoinSplit Statement (Sprout) on page 60 in the JoinSplit statement. Sprout Now even if the creator of a JoinSplit description does not choose \u03d5 randomly, uniqueness of nulli\ufb01ers and collision resistance of both hSigCRH and PRF\u03c1 will ensure that the derived \u03c1 values are unique, at least for any two JoinSplit descriptions that get into a valid block chain. This is suf\ufb01cient to prevent the Faerie Gold attack.",
      "A variation on the attack attempts to cause the nulli\ufb01er of a sent note to be repeated, without repeating \u03c1. However, since the nulli\ufb01er is computed as PRFnfSprout (\u03c1) or PRFnfSapling (\u03c1) (for Orchard, see below); this is only possible if the adversary \ufb01nds a collision across both inputs on PRFnfSprout or PRFnfSapling, which is assumed to be infeasible  see section4.1.2 Pseudo Random Functions on page 25. Sprout Crucially, nulli\ufb01er integrity is enforced whether or not the enforceMerklePath\ud835\udc56\ufb02ag is set for an input note (section4.18.1 JoinSplit Statement (Sprout) on page 60). If this were not the case then an adversary could perform the attack by creating a zero-valued note with a repeated nulli\ufb01er, since the nulli\ufb01er would not depend on the value. Sprout Nulli\ufb01er integrity also prevents a roadblock attack in which the adversary sees a victims transaction, and is able to publish another transaction that is mined \ufb01rst and blocks the victims transaction.",
      "This attack would be possible if the public value(s) used to enforce uniqueness of \u03c1 could be chosen arbitrarily by the transaction creator: the victims transaction, rather than the adversarys, would be considered to be repeating these values. In the chosen solution that uses nulli\ufb01ers for these public values, they are enforced to be dependent on spending keys controlled by the original transaction creator (whether or not each input note is a dummy), and so a roadblock attack cannot be performed by another party who does not know these keys. Sapling onward In Sapling, uniqueness of \u03c1 is ensured by making it dependent on the position of the note commitment in the Sapling note commitment tree. Speci\ufb01cally, \u03c1  cmpos \ud835\udca5Sapling, where \ud835\udca5Sapling is a generator independent of the generators used in NoteCommitSapling. Therefore, \u03c1 commits uniquely to the note and its position, and this commitment is collision-resistant by the same argument used to prove collision resistance of Pedersen hashes.",
      "Note that it is possible for two distinct Sapling positioned notes (having different \u03c1 values and nulli\ufb01ers, but different note positions) to have the same note commitment, but this causes no security problem. Roadblock attacks are not possible because a given note position does not repeat for outputs of different transactions in the same block chain. Note that this depends on the fact that the value is bound by the note commitment: it could be the case that the adversary uses a dummy note that is not required to have a note commitment in the note commitment tree when it is spent. If this happens and the victims note is not a dummy, the note commitments will differ and so will the nulli\ufb01ers. If both notes are dummies, the adversary cannot know the inputs to the note commitment since they are generated at random for the victims spend, regardless of the adversarys potential knowledge of viewing keys.",
      "NU55 onward In Orchard, the nulli\ufb01er is computed using a construction that combines elliptic curve cryptography and the Poseidon-based PRFnfOrchard in a way that, for privacy, aims to provide defence in depth against potential weaknesses in either (see Zcash-Orchard, Section 3.5 Nulli\ufb01ers and section4.16 Computing \u03c1 values and Nulli\ufb01ers on page 57). Resistance to Faerie Gold attacks, on the other hand, depends entirely on hardness of the Discrete Logarithm Problem. The \u03c1 value of a note created in a given Action transfer is obtained from the nulli\ufb01er of the note spent in that Action transfer; this ensures (without any cryptographic assumption) that all \u03c1 values of notes added to the note commitment tree are unique. Then, the nulli\ufb01er derivation can be considered as computing a vector Pedersen commitment on input that includes \u03c1, so that the binding property of that commitment scheme ensures that Orchard nulli\ufb01ers will be unique.",
      "(Speci\ufb01cally, this is a Sinsemilla commitment with an additional term having base \ud835\udca6Orchard, truncated to its \ud835\udc65-coordinate. The \ud835\udc65-coordinate truncation cannot harm collision resistance because, assuming hardness of the Discrete Logarithm Problem on the Pallas curve, section5.4.1.9 Security argument on page 82 covers the case where the additional term is added.) Roadblock attacks are not possible because \u03c1 does not repeat for notes in the note commitment tree, and by a corresponding argument to Sapling for dummy notes. Internal hash collision attack and \ufb01x The Zerocash security proof requires that the composition of COMMrcm and COMMs is a computationally binding commitment to its inputs apk, v, and \u03c1. However, the instantiation of COMMrcm and COMMs in section 5.1 of the paper did not meet the de\ufb01nition of a binding commitment at a 128-bit security level. Speci\ufb01cally, the internal hash of apk and \u03c1 is truncated to 128 bits (motivated by providing statistical hiding security).",
      "This allows an attacker, with a work factor on the order of 264, to \ufb01nd distinct pairs (apk, \u03c1) and (apk , \u03c1) with colliding outputs of the truncated hash, and therefore the same note commitment. This would have allowed such an attacker to break the Balance property by double-spending notes, potentially creating arbitrary amounts of currency for themself HW2016. Zcash uses a simpler construction with a single hash evaluation for the commitment: SHA-256 for Sprout notes, PedersenHashToPoint for Sapling notes, and SinsemillaHashToPoint for Orchard notes. The motivation for the nested construction in Zerocash was to allow Mint transactions to be publically veri\ufb01ed without requiring zk-SNARK proofs (BCGGMTV2014, section 1.3, under step 3).",
      "Since Zcash combines Mint and Pour transactions into generalized JoinSplit transfers (for Sprout), or Spend transfers and Output transfers (for Sapling), or Action transfers (for Orchard), and each transfer always uses a zk-SNARK proof, Zcash does not require the nesting. A side bene\ufb01t is that this reduces the cost of computing the note commitments: for Sprout it reduces the number of SHA256Compress evaluations needed to compute each note commitment from three to two, saving a total of four SHA256Compress evaluations in the JoinSplit statement. Sprout Note: The full SHA-256 algorithm is used for NoteCommitSprout, with randomness appended after the commitment input. The commitment input can be split into two blocks, call them \ud835\udc65of length 64 bytes, and \ud835\udc66of the remaining length (9 bytes).",
      "Let COMM BY41) be the commitment scheme that applies SHA256Compress with the \ufb01rst 32 bytes of \ud835\udc67in the IV, and the rest of \ud835\udc67(9 bytes), the randomness \ud835\udc5f(32 bytes), and padding up to 64 bytes in the SHA256Compress input block. Then we have NoteCommitSprout (\ud835\udc65 \ud835\udc66)  COMM \ud835\udc5f(SHA256Compress(\ud835\udc65)  \ud835\udc66). Suppose we make the reasonable assumption that COMM is a computationally binding and hiding commitment scheme. If SHA256Compress is collision-resistant with the standard IV14, then NoteCommitSprout is as secure for binding as COMM. Also NoteCommitSprout is as secure for hiding as COMM (without any assumption on SHA256Compress). This effectively rules out potential concerns about the MerkleDamg\u00e5rd structure Damg\u00e5rd1989 of SHA-256 causing any security problem for NoteCommitSprout.",
      "Sprout Note: Sprout note commitments are not statistically hiding, so for Sprout notes, Zcash does not support the everlasting anonymity property described in BCGGMTV2014, section 8.1, even when used as described in that section. While it is possible to de\ufb01ne a statistically hiding, computationally binding commitment scheme for this use at a 128-bit security level, the overhead of doing so within the JoinSplit statement was not considered to justify the bene\ufb01ts. Sapling onward In Sapling, Pedersen or Sinsemilla commitments are used instead of SHA256Compress. These commitments are statistically hiding, and so everlasting anonymity is supported for Sapling and Orchard notes under the same conditions as in Zerocash (by the protocol, not necessarily by zcashd). Note that diversi\ufb01ed payment addresses can be linked if the Decisional Dif\ufb01eHellman Problem on the Jubjub curve or the Pallas curve can be broken.",
      "14If SHA256Compress is not collision-resistant with the standard IV, then SHA-256 is not collision-resistant for a 2-block input. Changes to PRF inputs and truncation The format of inputs to the PRFs instantiated in section5.4.2 Pseudo Random Functions on page 86 has changed relative to Zerocash. There is also a requirement for another PRF, PRF\u03c1, which must be domain-separated from the others. In the Zerocash protocol, \u03c1old is truncated from 256 to 254 bits in the input to PRFsn (which corresponds to PRFnfSprout in Zcash). Also, hSig is truncated from 256 to 253 bits in the input to PRFpk. These truncations are not taken into account in the security proofs. Both truncations affect the validity of the proof sketch for Lemma D.2 in the proof of Ledger Indistinguishability in BCGGMTV2014, Appendix D. In more detail:  In the argument relating H and 2, it is stated that in 2, for each \ud835\udc561, 2, sn\ud835\udc56: PRFsn ask(\u03c1) for a random (and not previously used) \u03c1.",
      "It is also argued that the calls to PRFsn ask are each by de\ufb01nition unique. The latter assertion depends on the fact that \u03c1 is not previously used. However, the argument is incorrect because the truncated input to PRFsn ask, i.e. \u03c1254, may repeat even if \u03c1 does not. In the same argument, it is stated that with overwhelming probability, hSig is unique. In fact what is required to be unique is the truncated input to PRFpk, i.e. hSig253  CRH(pksig)253. In practice this value will be unique under a plausible assumption on CRH provided that pksig is chosen randomly, but no formal argument for this is presented. Note that \u03c1 is truncated in the input to PRFsn but not in the input to COMMrcm, which further complicates the analysis. As further evidence that it is essential for the proofs to explicitly take any such truncations into account, consider a slightly modi\ufb01ed protocol in which \u03c1 is truncated in the input to COMMrcm but not in the input to PRFsn.",
      "In that case, it would be possible to violate balance by creating two notes for which \u03c1 differs only in the truncated bits. These notes would have the same note commitment but different nulli\ufb01ers, so it would be possible to spend the same value twice. Sprout For resistance to Faerie Gold attacks as described in section8.4 Faerie Gold attack and \ufb01x on page 143, Zcash depends on collision resistance of hSigCRH and PRF\u03c1 (instantiated using BLAKE2b-256 and SHA256Compress re- spectively). Collision resistance of a truncated hash does not follow from collision resistance of the original hash, even if the truncation is only by one bit. This motivated avoiding truncation along any path from the inputs to the computation of hSig to the uses of \u03c1.",
      "Sprout Since the PRFs are instantiated using SHA256Compress which has an input block size of 512 bits (of which 256 bits are used for the PRF input and 4 bits are used for domain separation), it was necessary to reduce the size of the PRF key to 252 bits. The key is set to ask in the case of PRFaddr, PRFnfSprout, and PRFpk, and to \u03d5 (which does not exist in Zerocash) for PRF\u03c1, and so those values have been reduced to 252 bits. This is preferable to requiring reasoning about truncation, and 252 bits is quite suf\ufb01cient for security of these cryptovalues. Sapling uses Pedersen hashes and BLAKE2s where Sprout used SHA256Compress. Pedersen hashes can be ef\ufb01ciently instantiated for arbitrary input lengths. BLAKE2s has an input block size of 512 bits, and uses a \ufb01nalization \ufb02ag rather than padding of the last input block; it also supports domain separation via a personalization parameter distinct from the input. Therefore, there is no need for truncation in the inputs to any of these hashes.",
      "Note however that the output of CRHivk is truncated, requiring a security assumption on BLAKE2s truncated to 251 bits (see section5.4.1.5 CRHivk Hash Function on page 77). Orchard replaces Pedersen hashes by Sinsemilla hashes which can also be ef\ufb01ciently instantiated for arbitrary input lengths. It replaces uses of BLAKE2s in the circuit by the commitment scheme Commitivk, and by a construction for nulli\ufb01er derivation that uses the Poseidon-based PRFnfOrchard (along with scalar multiplication on the Pallas curve). Again, there is no need for truncation in the inputs to any of these functions, and the need for truncation in the derivation of ivk is removed. In-band secret distribution Zerocash speci\ufb01ed ECIES (referencing Certicoms SEC 1 standard) as the encryption scheme used for the in-band secret distribution. This has been changed to a key agreement scheme based on Curve25519 (for Sprout) or Jubjub (for Sapling) and the authenticated encryption algorithm AEAD_CHACHA20_POLY1305.",
      "This scheme is still loosely based on ECIES, and on the crypto_box_seal scheme de\ufb01ned in libsodium libsodium-Seal. The motivations for this change were as follows:  The Zerocash paper did not specify the curve to be used. We believe that Curve25519 has signi\ufb01cant side- channel resistance, performance, implementation complexity, and robustness advantages over most other available curve choices, as explained in Bernstein2006. For Sapling, the Jubjub curve was designed according to a similar design process following the Safe curves criteria BL-SafeCurves Hopwood2018. This retains Curve25519s advantages while keeping shielded payment address sizes short, because the same public key material supports both encryption and spend authentication. For Orchard, we de\ufb01ne a prime-order curve Pallas Hopwood2020, with similar advantages to Jubjub. ECIES permits many options, which were not speci\ufb01ed.",
      "There are at least counting conservatively 576 possible combinations of options and algorithms over the four standards (ANSI X9.63, IEEE Std 1363a-2004, ISOIEC 18033-2, and SEC 1) that de\ufb01ne ECIES variants M\u00c1E\u00c12010. Although the Zerocash paper states that ECIES satis\ufb01es key privacy (as de\ufb01ned in BBDP2001), it is not clear that this holds for all curve parameters and key distributions. For example, if a group of non-prime order is used, the distribution of ciphertexts could be distinguishable depending on the order of the points representing the ephemeral and recipient public keys. Public key validity is also a concern. Curve25519 (and Jubjub) key agreement is de\ufb01ned in a way that avoids these concerns due to the curve structure and the clamping of private keys (or explicit cofactor multiplication and point validation for Sapling). The Pallas curve is prime- order, but we still validate points, and use a similar key agreement scheme to Sapling for consistency and ease of analysis.",
      "Unlike the DHAESDHIES proposal on which it is based ABR1999, ECIES does not require a representation of the senders ephemeral public key to be included in the input to the KDF, which may impair the security properties of the scheme. (The Std 1363a-2004 version of ECIES IEEE2004 has a DHAES mode that allows this, but the representation of the key input is underspeci\ufb01ed, leading to incompatible implementations.) The scheme we use for Sprout has both the ephemeral and recipient public key encodings which are unambiguous for Curve25519 and also hSig and a nonce as described below, as input to the KDF. For Sapling and Orchard, it is only possible to include the ephemeral public key encoding, but this is suf\ufb01cient to retain the original security properties of DHAES.",
      "Note that being able to break the Elliptic Curve Dif\ufb01eHellman Problem on Curve25519 or Jubjub or Pallas (without breaking AEAD_CHACHA20_POLY1305 as an authenticated encryption scheme or BLAKE2b-256 as a KDF) would not help to decrypt the transmitted note(s) ciphertext unless pkenc or pkd is known or guessed. Sprout The KDF also takes a public seed hSig as input. This can be modeled as using a different randomness extractor for each JoinSplit transfer, which limits degradation of security with the number of JoinSplit transfers. This facilitates security analysis as explained in DGKM2011  see section 7 of that paper for a security proof that can be applied to this construction under the assumption that single-block BLAKE2b-256 is a weak PRF. Note that hSig is authenticated, by the zk-SNARK proof, as having been chosen with knowledge of aold sk,1..Nold, so an adversary cannot modify it in a ciphertext from someone elses transaction for use in a chosen-ciphertext attack without detection.",
      "(In Sapling and Orchard, there is no equivalent to hSig, but the binding signature and spend authorization signatures prevent such modi\ufb01cations.)  Sprout The scheme used by Sprout includes an optimization that reuses the same ephemeral key (with different nonces) for the two ciphertexts encrypted in each JoinSplit description. The security proofs of ABR1999 can be adapted straightforwardly to the resulting scheme. Although DHAES as de\ufb01ned in that paper does not pass the recipient public key or a public seed to the hash function \ud835\udc3b, this does not impair the proof because we can consider \ud835\udc3bto be the specialization of our KDF to a given recipient key and seed. (Passing the recipient public key to the KDF could in principle compromise key privacy, but not con\ufb01dentiality of encryption.) Sprout It is necessary to adapt the HDH independence assumptions and the proof slightly to take into account that the ephemeral key is reused for two encryptions.",
      "Note that the 256-bit key for AEAD_CHACHA20_POLY1305 maintains a high concrete security level even under attacks using parallel hardware Bernstein2005 in the multi-user setting Zaverucha2012. This is especially neces- sary because the privacy of Zcash transactions may need to be maintained far into the future, and upgrading the encryption algorithm would not prevent a future adversary from attempting to decrypt ciphertexts encrypted before the upgrade. Other cryptovalues that could be attacked to break the privacy of transactions are also suf\ufb01ciently long to resist parallel brute force in the multi-user setting: for Sprout, ask is 252 bits, and skenc is no shorter than ask. In Sapling, ivk is an output of CRHivk, which is a 251-bit value. In Orchard, ivk is an \ud835\udc65-coordinate on the Pallas curve. This degree of divergence from a uniform distribution on the scalar \ufb01eld is not expected to cause any weakness in note encryption.",
      "For all shielded protocols, the checking of note commitments makes partitioning oracle attacks LGR2021 against the transmitted note ciphertext infeasible, at least in the absence of side-channel attacks. The following argument applies to Sapling and Orchard, but can be adapted to Sprout by replacing ivk with skenc, pkd with pkenc, and using a \ufb01xed base. The decryption procedure for transmitted note ciphertexts in Sapling and Orchard is speci\ufb01ed in section4.20.2 Decryption using an Incoming Viewing Key (Sapling and Orchard) on page 68; it ensures that a suc- cessful decryption cannot occur unless the decrypted note plaintext encodes a note consistent with the note commitment (encoded as the cm\ud835\udc62\ufb01eld of the Output description or the cm\ud835\udc65\ufb01eld of the Action description). Suppose that it were feasible to \ufb01nd a pair of transmitted note ciphertext and note commitment that decrypts successfully for two different incoming viewing keys ivk1 and ivk2.",
      "Assuming that the note commitment scheme is binding and that note commitment opens to a note with pkd and gd, we must have pkd  KA.Agree(ivk1, gd)  KA.Agree(ivk2, gd). But this is impossible given that gd has order greater than the maximum value of ivk that can be an output of CRHivk or Commitivk. There is also a decryption procedure making use of outgoing ciphertexts in Sapling and Orchard, as speci\ufb01ed in section4.20.3 on page 70. It checks (via KA.DerivePublic, and also via PRFexpand rseed in the case of post-ZIP-212 ciphertexts with note plaintext lead byte  0x01) that the decrypted esk value is consistent with the transmitted note ciphertext, which is protected from partitioning oracle attacks as described above. It also checks that the pkd value is consistent with the note commitment. Since these are the only \ufb01elds in an outgoing ciphertext, even if a partitioning oracle attack occurred against an outgoing ciphertext, it could not result in any equivocation of the decrypted data.",
      "Because ovk and ock are each 256 bits, partitioning oracle attacks that speed up a search for these keys (analogous to the attacks against Password-based AEAD in LGR2021) are infeasible, even given knowledge of ivk. Omission in Zerocash security proof The abstract Zerocash protocol requires PRFaddr only to be a PRF; it is not speci\ufb01ed to be collision-resistant . This reveals a \ufb02aw in the proof of the Balance property. Suppose that an adversary \ufb01nds a collision on PRFaddr such that a1 sk and a2 sk are distinct spending keys for the same apk. Because the note commitment is to apk, but the nulli\ufb01er is computed from ask (and \u03c1), the adversary is able to double-spend the note, once with each ask. This is not detected because each Spend reveals a different nulli\ufb01er. The JoinSplit statements are still valid because they can only check that the ask in the witness is some preimage of the apk used in the note commitment. The error is in the proof of Balance in BCGGMTV2014, Appendix D.3.",
      "For the \ud835\udc9cviolates Condition I case, the proof says: (i) If cmold  cmold 2 , then the fact that snold  snold implies that the witness \ud835\udc4econtains two distinct openings of cmold (the \ufb01rst opening contains (aold sk,1, \u03c1old 1 ), while the second opening contains (aold sk,2, \u03c1old 2 )). This violates the binding property of the commitment scheme COMM.\" In fact the openings do not contain aold sk,\ud835\udc56; they contain aold pk,\ud835\udc56. (In Sprout cmold opens directly to (aold pk,\ud835\udc56, vold \ud835\udc56, \u03c1old \ud835\udc56), and in Zerocash it opens to (vold \ud835\udc56, COMMs(aold pk,\ud835\udc56, \u03c1old \ud835\udc56).) A similar error occurs in the argument for the \ud835\udc9cviolates Condition II case. The \ufb02aw is not exploitable for the actual instantiations of PRFaddr in Zerocash and Sprout, which are collision- resistant assuming that SHA256Compress is. The proof can be straightforwardly repaired.",
      "The intuition is that we can rely on collision resistance of PRFaddr (on both its arguments) to argue that distinctness of aold sk,1 and aold sk,2, together with constraint 1(b) of the JoinSplit statement (see section4.18.1 JoinSplit Statement (Sprout) on page 60), implies distinctness of aold pk,1 and aold pk,2, therefore distinct openings of the note commitment when Condition I or II is violated. Miscellaneous  The paper de\ufb01nes a note as ((apk, pkenc), v, \u03c1, rcm, s, cm), whereas this speci\ufb01cation de\ufb01nes a Sprout note as (apk, v, \u03c1, rcm). The instantiation of COMMs in section 5.1 of the paper did not actually use s, and neither does the new instantiation of NoteCommitSprout in Sprout. pkenc is also not needed as part of a note: it is not an input to NoteCommitSprout nor is it constrained by the Zerocash POUR statement or the Zcash JoinSplit statement. cm can be computed from the other \ufb01elds.",
      "(The de\ufb01nition of notes for Sapling is different again.)  The length of proof encodings given in the paper is 288 bytes. Sprout This differs from the 296 bytes speci\ufb01ed in section5.4.10.1 BCTV14 on page 110, because both the \ud835\udc65-coordinate and compressed \ud835\udc66-coordinate of each point need to be represented. Although it is possible to encode a proof in 288 bytes by making use of the fact that elements of F\ud835\udc5ecan be represented in 254 bits, we prefer to use the standard formats for points de\ufb01ned in IEEE2004. The fork of libsnark used by Zcash uses this standard encoding rather than the less ef\ufb01cient (uncompressed) one used by upstream libsnark. In Sapling, a customized encoding is used for BLS12-381 points in Groth16 proofs to minimize length, and similarly for Pallas and Vesta points in Orchard. The range of monetary values differs. In Zcash this range is 0 .. MAX_MONEY, while in Zerocash it is 0 .. 2\u2113value1.",
      "(The JoinSplit statement still only directly enforces that the sum of amounts in a given JoinSplit transfer is in the latter range; this enforcement is technically redundant given that the Balance property holds.) The inventors of Zerocash are Eli Ben-Sasson, Alessandro Chiesa, Christina Garman, Matthew Green, Ian Miers, Eran Tromer, and Madars Virza. The designers of the Zcash protocol are the Zerocash inventors and also Daira-Emma Hopwood, Sean Bowe, Jack Grigg, Simon Liu, Taylor Hornby, Nathan Wilcox, Zooko Wilcox, Jay Graber, Eirik Ogilvie-Wigley, Ariel Gabizon, George Tankersley, Ying Tong Lai, Kris Nuttycombe, Jack Gavigan, Steven Smith, and Greg Pfeil. The Equihash proof-of-work algorithm was designed by Alex Biryukov and Dmitry Khovratovich.",
      "The authors would like to thank everyone with whom they have discussed the Zerocash and Zcash protocol designs; in addition to the preceding, this includes Mike Perry, isis agora lovecruft, Leif Ryge, Andrew Miller, Ben Blaxill, Samantha Hulsey, Alex Balducci, Jake Tarren, Solar Designer, Ling Ren, John Tromp, Paige Peterson, jl777, Alison Stevenson, Maureen Walsh, Filippo Valsorda, Zaki Manian, Kexin Hu, Brian Warner, Mary Maller, Michael Dixon, Andrew Poelstra, Benjamin Winston, Josh Cincinnati, Kobi Gurkan, Weikeng Chen, Henry de Valence, Deirdre Connolly, Chelsea Komlo, Zancas Wilcox, Jane Lusby, teor, Izaak Meckler, Zac Williamson, Vitalik Buterin, Jakub Zalewski, Oana Ciobotaru, Andre Serrano, Brad Miller, Charlie OKeefe, David Campbell, Elena Giralt, Francisco Gindre, Joseph Van Geffen, Josh Swihart, Kevin Gorham, Larry Ruane, Marshall Gaucher, Ryan Taylor, Sasha Meyer, Conrado Gouv\u00eaa, Aditya Bharadwaj, Andrew Arnott, Arya, Andrea Kobrlova, Luk\u00e1\u0161 Korba, Honza Rychnovsk\u00fd, Schell Scivally, and no doubt others.",
      "We would also like to thank the designers and developers of Bitcoin and Bitcoin Core. Zcash has bene\ufb01ted from security audits performed by NCC Group, Coinspect, Least Authority, Mary Maller, Kudelski Security, QEDIT, and Trail of Bits. We also thank Mary Maller for her work on reviewing the security proofs for Halo 2 (any remaining errors are ours). The Faerie Gold attack was found by Zooko Wilcox (who also came up with the name) and Brian Warner. The \ufb01x for this attack in Sprout was proposed by Daira-Emma Hopwood; subsequent analysis of variations on the attack was performed by Daira-Emma Hopwood and Sean Bowe. The internal hash collision attack was found by Taylor Hornby. The error in the Zerocash proof of Balance relating to collision resistance of PRFaddr was found by Daira-Emma Hopwood. The errors in the proof of Ledger Indistinguishability mentioned in section8.6 Changes to PRF inputs and truncation on page 146 were also found by Daira-Emma Hopwood.",
      "The 2015 Soundness vulnerability in BCTV14 Parno2015 was found by Bryan Parno. An additional condition needed to resist this attack was documented by Ariel Gabizon Gabizon2019, section 3. The 2019 Soundness vulnerability in BCTV14 Gabizon2019 was found by Ariel Gabizon. The design of Sapling is primarily due to Matthew Green, Ian Miers, Daira-Emma Hopwood, Sean Bowe, Jack Grigg, and Jack Gavigan. A potential attack linking diversi\ufb01ed payment addresses, avoided in the adopted design, was found by Brian Warner. The design of Orchard is primarilydue to Daira-Emma Hopwood, Sean Bowe, Jack Grigg, Kris Nuttycombe, Ying Tong Lai, and Steven Smith. The observation in section5.4.1.6 DiversifyHashSapling and DiversifyHashOrchard Hash Functions on page 78 that diversi\ufb01ed payment address unlinkability can be proven in the same way as key privacy for ElGamal, is due to Mary Maller.",
      "We thank Ariel Gabizon for teaching us the techniques of BFIJSV2010 used in sectionB.2 Groth16 batch veri\ufb01cation on page 221, by applying them to BCTV14. The arithmetization used by Halo 2 is based on that used by PLONK GWC2019, which was designed by Ariel Gabizon, Zachary Williamson, and Oana Ciobotaru. Numerous people have contributed to the science of zero-knowledge proving systems, but we would particularly like to acknowledge the work of Sha\ufb01Goldwasser, Silvio Micali, Oded Goldreich, Mihir Bellare, Charles Rackoff, Joe Kilian, Yael Tauman Kalai, Guy Rothblum, Rosario Gennaro, Bryan Parno, Jon Howell, Craig Gentry, Mariana Raykova, Jens Groth, Rafail Ostrovsky, and Amit Sahai. We thank the organizers of the ZKProof standardization effort and workshops; and also Anna Rose, Fredrik Harrysson, Terun Chitra, James Prestwich, Josh Cincinnati, Tanya Karsou, Henrik Jose, Chris Ward, and others for their work on the Zero Knowledge Podcast, ZK Summits, and ZK Study Club.",
      "These efforts have enriched the zero knowledge community immeasurably. Many of the ideas used in Zcash including the use of zero-knowledge proofs to resolve the tension between privacy and auditability, Merkle trees over note commitments (using Pedersen hashes as in Sapling), and the use of serial numbers or nulli\ufb01ers to detect or prevent double-spends were \ufb01rst applied to privacy-preserving digital currencies by Tomas Sander and Amnon Ta-Shma. To a large extent Zcash is a re\ufb01nement of their Auditable, Anonymous Electronic Cash proposal in ST1999. We thank Alexandra Elbakyan for her tireless work in dismantling barriers to scienti\ufb01c research. This document is set in the beautiful Quattrocento font designed by Pablo Impallari. The New Century Schoolbook font by URW Type Foundry, based on Century Schoolbook designed by Morris Fuller, is used for italics. Finally, we would like to thank the Internet Archive for their scan of Peter Newells illustration of the Jubjub bird, from Carroll1902.",
      "Change History 2025.6.3 2025-12-02  Specify in section3.3 The Block Chain on page 18 that NU6.6.1 is the most recent settled network upgrade on Testnet and Mainnet. Update the description in section3.12 Mainnet and Testnet on page 22 of protocol governance. 2025.6.2 2025-11-11  In section4.16 Computing \u03c1 values and Nulli\ufb01ers on page 57, add a note that the \u03c1 and \u03c8 inputs to DeriveNullifier must be consistent with the note committed to by cm. Fix an error in the statement of Merkle Path Validity for Orchard: the Merkle path should be from the leaf value ExtractP(cmold) rather than cmold. This was implemented as intended in the orchard crate. Add a note to the Orchard key components diagram in section3.1 Payment Addresses and Keys on page 13, say- ing that the derivations of ask and rivk shown there are not the only possibility, and referencing section4.2.3 Orchard Key Components on page 38.",
      "Also change the existing note in that section to say that most Zcash wallets, not just zcashd, derive Sapling and Orchard keys and addresses according to ZIP-32. Fix type errors in section4.20.2 Decryption using an Incoming Viewing Key (Sapling and Orchard) on page 68 and in section4.20.3 Decryption using an Outgoing Viewing Key (Sapling and Orchard) on page 70: ToScalar re- turns an integer, not a byte sequence, and so cannot be assigned to rcm. This was implemented as intended in the zcash_note_encryption crate. Rename section4.20.3 on page 70 from Decryption using a Full Viewing Key (Sapling and Orchard) to Decryption using an Outgoing Viewing Key (Sapling and Orchard). De\ufb01ne allowedLeadBytes in section3.2.1 Note Plaintexts and Memo Fields on page 15, and use it to refactor the con- straints on note plaintext lead bytes. Correct the accents on Luk\u00e1\u0161 Korbas name.",
      "Add acknowledgements to Joe Kilian, Yael Tauman Kalai, and Guy Rothblum for contributions to the science of zero-knowledge proving systems. 2025.6.1 2025-10-08  Update NU6.6.1 consensus changes with the one-time lockbox disbursement addresses, the splitting of the disbursement into ZIP271DisbursementChunks chunks, and other updates to ZIP-271. Remove \"proposal\" from the version string. 2025.6.0 2025-09-17  Apply NU6.6.1 consensus changes from ZIP-271. Add boilerplate support for NU6.6.1. Specify in section3.3 The Block Chain on page 18 that NU66 is the most recent settled network upgrade. Clarify section3.4 Transactions and Treestates on page 18, taking into account the NU66 consensus changes from ZIP-236. Adjust the recommendation in section4.2.2 Sapling Key Components on page 36 not to encode information in the diversi\ufb01er, to recommend instead using a ZIP-32 diversi\ufb01er index.",
      "Deprecate the encoding of a Sapling ivk in section5.6.3.2 Sapling Incoming Viewing Keys on page 116, in favour of using a uni\ufb01ed incoming viewing key with a Sapling component that includes the diversi\ufb01er key dk. Tighten the type of ivk in Sapling to 1 .. 2\u2113Sapling 1, and the type of pkd in Sapling to KASapling.PublicPrimeOrder, in order to make the exclusion of the zero point for pkd more obvious Zips-Issue664. This also has the effect that a Sapling incoming viewing key section5.6.3.2 Sapling Incoming Viewing Keys on page 116 or a Sapling IVK Encoding in a uni\ufb01ed incoming viewing key ZIP-316 that encodes the zero ivk MUST be considered invalid when imported. Notes have been added in section5.6.3.1 Sapling Payment Addresses on page 115 and section5.6.3.2 Sapling Incoming Viewing Keys on page 116 calling out these changes. In section4.20.3 Decryption using an Outgoing Viewing Key (Sapling and Orchard) on page 70, retrospectively enforce the check for canonicity of pkd.",
      "This has no effect on consensus relative to the previous version, because only small-order Jubjub curve points have non-canonical encodings, and so the check that returns if pkd J(\ud835\udc5f) would catch all such cases. Document in section5.4.7 RedDSA, RedJubjub, and RedPallas on page 92, section4.4 Spend Descriptions on page 40, and section7.1.2 Transaction Consensus Rules on page 124 that the canonicity restriction on the encoding of \ud835\udc45in RedDSA signature validation is retrospectively valid on Mainnet and Testnet before NU55. So that Sapling and Orchard are consistent, section5.4.5.5 Orchard Key Agreement on page 90 now also de\ufb01nes KAOrchard.PublicPrimeOrder, which is used instead of KAOrchard.Public as the type of an Orchard pkd. This has no effect on conformance since both KAOrchard.PublicPrimeOrder and KAOrchard.Public are de\ufb01ned as P.",
      "In section5.6.4.4 Orchard Raw Full Viewing Keys on page 118, clarify the requirement on ak by rewording valid Pallas \ud835\udc65-coordinate to valid af\ufb01ne \ud835\udc65-coordinate of a Pallas curve point in P. Implementations might use (0, 0) to represent \ud835\udcaaP, but 0 is not a valid \ud835\udc65-coordinate in the sense required here. Change incorrect uses of block reward to block subsidy. Added dark mode rendering (https:zips.z.cashprotocolprotocol-dark.pdf). Add an acknowledgement to Schell Scivally for discussions on the Zcash protocol. 2024.5.1 2024-09-26  Apply NU66 consensus changes from ZIP-2001 and ZIP-236. Collect the de\ufb01nitions of balances for each chain value pool into section4.17 Chain Value Pool Balances on page 58. Add a de\ufb01nition of total issued supply. Add acknowledgements to Aditya Bharadwaj, Andrew Arnott, Arya, Andrea Kobrlova, Luk\u00e1\u0161 Korba, and Honza Rychnovsk\u00fd for discussions on the Zcash protocol. 2024.5.0 2024-08-28  Add the hyphen in Daira-Emma Hopwood. Correct some author lists in the References.",
      "Prevent incorrect line-breaking on hyphens. In section5.4.1.9 Sinsemilla Hash Function on page 81, declare use of LEBS2IP instead of LEOS2IP. Add an acknowledgement to Conrado Gouv\u00eaa for discussions on the Zcash protocol. Add boilerplate support for NU66. 2023.4.0 2023-12-19  The domain separators 4 and 5 used in the input to PRFexpand rseed for Sapling were accidentally swapped in the protocol speci\ufb01cation relative to ZIP-212. The implementation in zcashd followed ZIP-212, using 4 to derive rcm and 5 to derive esk. This has been corrected in the protocol speci\ufb01cation. For Orchard, the implementation in the orchard crate (and therefore in zcashd) followed the protocol spec- i\ufb01cation, using 5  I2LEOSP256(\u03c1) to derive rcm and 4  I2LEOSP256(\u03c1) to derive esk. This cannot now be changed, and so ZIP-212 has been updated to follow this implementation. Notes have been added pointing out the discrepancy.",
      "Document that the attacks in DKLS2020 are no better than brute force key search against FF1-AES256 as speci\ufb01ed in section5.4.4 Pseudo Random Permutations on page 88. In the table of section7.6 Block Header Encoding and Consensus on page 131, clarify that hashLightClientRoot is used in Heartwood and Canopy, but not in NU55 or later. The return type of GroupHashJ(\ud835\udc5f) in section5.4.9.5 Group Hash into Jubjub on page 104 was incorrectly given as J(\ud835\udc5f), rather than the correct J(\ud835\udc5f) . In the discussion of partitioning oracle attacks on note encryption in section8.7 In-band secret distribution on page 147, we now use the fact that gd has order greater than the maximum value of ivk, rather than assuming that gd is a non-zero point in the prime-order subgroup. (In the case of Sapling, the circuits only enforce that gd is not a small-order point, not that it is in the prime-order subgroup.",
      "It is true that honestly generated addresses have prime-order gd which would have been suf\ufb01cient for the security argument against this class of attacks, but the chosen \ufb01x is more direct.)  Delete a confusing claim in section4.4 Spend Descriptions on page 40 that The check that rk is not of small order is technically redundant with a check in the Spend circuit.... The small-order check excludes the zero point \ud835\udcaaJ, which the Spend authority check that this claim was intending to reference does not. An implementation of HomomorphicPedersenCommitSapling MAY resample the commitment trapdoor until the resulting commitment is not \ud835\udcaaJ, in order to avoid it being rejected as the cv \ufb01eld of a Spend description or Output description. Add notes in section4.4 Spend Descriptions on page 40, section4.5 Output Descriptions on page 41, and section5.4.8.3 Homomorphic Pedersen commitments (Sapling and Orchard) on page 97 to that effect.",
      "Rename the section Note Commitments and Nulli\ufb01ers to section4.16 Computing \u03c1 values and Nulli\ufb01ers on page 57, to more accurately re\ufb02ect its contents. Split some of the content of the section Notes into subsections section3.2.2 Note Commitments on page 16 and section3.2.3 Nulli\ufb01ers on page 17. Make the descriptions of how note commitments and nulli\ufb01ers are used more precise and explicit, and add forward references where helpful. Remove redundancy in the de\ufb01nition of note plaintexts between section3.2.1 Note Plaintexts and Memo Fields on page 15 and section5.5 Encodings of Note Plaintexts and Memo Fields on page 112. The abstract no longer describes the NU55 version of the speci\ufb01cation as a draft. Acknowledge Greg Pfeil as a co-designer of the Zcash protocol. Acknowledge Daira-Emma Hopwood for the \ufb01x to the Faerie Gold attack in Sprout, and add a reference to hir Explaining the Security of Zcash talk at Zcon3 Hopwood2022 for repairs to the Zerocash security de\ufb01nitions.",
      "Acknowledge the font designers Pablo Impallari and Morris Fuller. Change Daira-Emma Hopwoods name. 2022.3.8 2022-09-15  Correct Jurgen Bos name. 2022.3.7 2022-09-10  Remove a now-unused sampling of rcv in section4.8.3 Dummy Notes (Orchard) on page 48. Specify in section3.3 The Block Chain on page 18 that NU55 is the most recent settled network upgrade. 2022.3.6 2022-09-01  Correct Kexin Hus name. Correct cross-references for the de\ufb01nition of an anchor. Remove a calculation of cv in section4.8.3 Dummy Notes (Orchard) on page 48 that is not applicable to Orchard (since cv for an Action description depends on both the spent and output notes). Clarify that the recommended format for a QR code starts with a Bech32 encoding for a Sapling payment address and with a Bech32m encoding for a uni\ufb01ed payment address. Replace ResearchGate links for CDvdG1987 and BDPA2007 with alternatives that do not cause false-positive link checker errors.",
      "In protocolREADME.rst: update the build dependency documentation for Debian Bullseye, mention the make linkcheck target, and correct the description of make all. Update the Makefile to build correctly with newer versions of latexmk. 2022.3.5 2022-08-02  ZIP 244 is not modi\ufb01ed by ZIP 225. section5.4.1.5 CRHivk Hash Function on page 77 incorrectly cross-referenced BLAKE2b-256 rather than BLAKE2s-256. The actual speci\ufb01cation was correct. 2022.3.4 2022-06-22  Document in section5.4.6 Ed25519 on page 90 that a full validator implementation that checkpoints on the Canopy activation block MAY validate Ed25519 signatures using the post-Canopy rules for the whole chain. Update references for ECCZF2019 and ZIP-302 and ZIP-252. 2022.3.3 2022-06-21  In section3.12 Mainnet and Testnet on page 22, update the settled activation block hashes to be those for NU55 on Mainnet and Testnet. Correct the history entry for v2022.3.2 to include the entry about the calculation for sizeProofsOrchard.",
      "Rename ExcludedPointEncodings to PreCanopyExcludedPointEncodings. In section5.6.2.3 Sprout Spending Keys on page 115, remove the statement that future key representations might use the padding bits of Sprout spending keys. Give a full-text URL for Nakamoto2008. 2022.3.2 2022-06-06  Set NUFiveActivationHeight for Testnet and Mainnet. An NU55 onward consensus rule requiring the nConsensusBranchId \ufb01eld to match the consensus branch ID used for SIGHASH transaction hashes, should apply only when effectiveVersion 5 (since v4 transactions did not explicitly encode the nConsensusBranchId \ufb01eld). Correction in section5.3 Constants on page 74: UncommittedOrchard  0 .. \ud835\udc5eP 1 is not a bit sequence. In section7.1 Transaction Encoding and Consensus on page 122, add the calculation for sizeProofsOrchard to the v5 transaction format table. Make section1.2 High-level Overview on page 8 more precise about chain value pools.",
      "2022.3.1 2022-04-28  In section4.2.3 Orchard Key Components on page 38, do not allow construction of Orchard spending keys such that the corresponding internal incoming viewing key is 0 or . (This was already speci\ufb01ed for the external incoming viewing key.) Similarly in section5.6.4.4 Orchard Raw Full Viewing Keys on page 118, do not consider a decoded key valid if either its external or internal incoming viewing key would be 0 or . Clarify how to determine which table in section7.1 Transaction Encoding and Consensus on page 122 to use for transaction parsing, depending on the effectiveVersion as determined by the header \ufb01eld. Correct block chain branch to consensus branch to match ZIP-200. Add an acknowledgement to Mary Maller for reviewing the Halo 2 security proofs. Add an acknowledgement to Josh Cincinnati for discussions on the Zcash protocol. Add acknowledgements to more people associated with the ZK Podcast.",
      "2022.3.0 2022-03-18  Correct a type error in the usage of Commitivk: the output type Commitivk.Output includes 0, but the type of incoming viewing keys should not include 0 because KAOrchard.Private does not. This is now handled by explicitly rejecting 0 as output from Commitivk when generating ivk in section4.2.3 Orchard Key Components on page 38. An encoding of ivk as 0 is also rejected in section5.6.4.3 Orchard Raw Incoming Viewing Keys on page 118 when parsing an incoming viewing key. The Action circuit needed no changes because pkd already could not be \ud835\udcaaP, and therefore the Diversi\ufb01ed address integrity condition fails when ivk  0. In section3.3 The Block Chain on page 18, de\ufb01ne what a settled network upgrade is, specify requirements for check- pointing, and allow nodes to impose a limitation on rollback depth. In section5.4.10.1 BCTV14 on page 110, note that the above checkpointing requirement mitigates the risks of not performing BCTV14 zk proof veri\ufb01cation.",
      "Document the consensus rule that coinbase script length MUST be 2 .. 100 bytes. section3.11 Coinbase Transactions on page 22 effectively de\ufb01ned a coinbase transaction as the \ufb01rst transaction in a block. This wording was copied from the Bitcoin Developer Reference Bitcoin-CbInput, but it does not match the implementation in zcashd that was inherited from Bitcoin Core. Instead, a coinbase transaction should be, and now is, de\ufb01ned as a transaction with a single null prevout.",
      "The speci\ufb01cations of consensus rules have been clari\ufb01ed and adjusted (without any actual consensus change) to take this into account, as follows:  a block MUST have at least one transaction;  the \ufb01rst transaction in a block MUST be a coinbase transaction, and subsequent transactions MUST NOT be coinbase transactions;  a transparent input in a non-coinbase transaction MUST NOT have a null prevout;  every non-null prevout MUST point to a unique UTXO in either a preceding block, or a previous trans- action in the same block (this rule was previously not given explicitly because it was assumed to be inherited from Bitcoin);  the rule that A coinbase transaction MUST NOT have any transparent inputs with non-null prevout \ufb01elds is removed as an explicit consensus rule because it is implied by the corrected de\ufb01nition of coinbase transaction.",
      "2022.2.19 2022-01-19  In section4.10 SIGHASH Transaction Hashing on page 50, add a consensus rule that SIGHASH type encodings MUST be canonical for v5 transactions. In section3.5 JoinSplit Transfers and Descriptions on page 19, clarify that balance for JoinSplit transfers is enforced by the JoinSplit statement, and that there is no consensus rule to check it directly. In section8.5 Internal hash collision attack and \ufb01x on page 145, add a security argument for why the SHA-256- based commitment scheme NoteCommitSprout is binding and hiding, under reasonable assumptions about SHA256Compress. 2022.2.18 2022-01-03  Change the types of cm\ud835\udc65, UncommittedOrchard, and ak in Orchard to 0 .. \ud835\udc5eP 1, avoiding type errors and re\ufb02ecting the implementation in zcashd. This eliminates all uses of P\ud835\udc65(except that ak in an Orchard full viewing key is still required to be a valid Pallas af\ufb01ne-short-Weierstrass \ud835\udc65-coordinate).",
      "Re\ufb01ne the security argument about partitioning oracle attacks in section8.7 In-band secret distribution on page 147:  The argument for decryption with an incoming viewing key does not need to depend on the Decisional Dif\ufb01eHellman Problem, since gd is committed to by the note commitment as well as pkd. It is necessary to say that the note commitment is always checked for a successful decryption. Pedantically, it was not correct to conclude from the given security argument that partitioning oracle attacks against an outgoing ciphertext are necessarily prevented, according to the de\ufb01nition in LGR2021. Instead, the correct conclusions are that such attacks could not feasibly result in any equivocation of the decrypted data, or in recovery of ovk or ock. Correct the note about domain separators for PRFexpand in section4.1.2 Pseudo Random Functions on page 25, and ensure that new domain separators for deriving internal keys from ZIP-32 and ZIP-316 are included.",
      "2021.2.17 2021-12-01  Add notes in sectionB.1 RedDSA batch validation on page 220, sectionB.2 Groth16 batch veri\ufb01cation on page 221 and sectionB.3 Ed25519 batch validation on page 223 that \ud835\udc67\ud835\udc57may be sampled from 0 .. 2128 1 instead of 1 .. 2128 1. Add note in section8.7 In-band secret distribution on page 147 about resistance of note encryption to partitioning oracle attacks LGR2021. Add acknowledgement to Mihir Bellare for contributions to the science of zero-knowledge proofs. Add acknowledgement to Sasha Meyer. 2021.2.16 2021-09-30  Use complete addition in SinsemillaCommit. Correct the proof of Theorem 5.4.6 on page 99. Change the type of cmold in Orchard to P rather than P, i.e. allow the identity point. Change the type of rtOrchard from P\ud835\udc65(i.e. a Pallas \ud835\udc65-coordinate or 0) to 0 .. \ud835\udc5eP 1. This re\ufb02ects the existing zcashd implementation; also checking rtOrchard P\ud835\udc65would require a square root and is unnecessary. Witness gnew and pknew in the Orchard Action circuit as P, i.e.",
      "non-identityPallas points, rather than witnessing their representations as bit sequences. This re\ufb02ects the existing zcashd implementation. Note that akP in Orchard cannot be the identity. Correct the consensus rule about the maximum value of outputs in a coinbase transaction: it should reference the block subsidy rather than the miner subsidy. 2021.2.15 2021-09-01  Correct a minor error in the proof of Theorem 5.4.3 on page 83: the condition SinsemillaHashToPoint(\ud835\udc37, \ud835\udc40)   is required in the proof. (The case SinsemillaHashToPoint(\ud835\udc37, \ud835\udc40)  is covered by Theorem 5.4.4 on page 84.) The proof had not been updated correctly when the statement was revised in v2021.2.0. Also add a missing \ud835\udc37 argument to SinsemillaHashToPoint in that proof. Fix a reference to nonexistent version 2019.0-beta-40 of this speci\ufb01cation (in section7.7.3 Dif\ufb01culty adjustment on page 134) that should be v2019.0.0. Fix URL links to BBDP2001 and BDJR2000.",
      "Improve protocollinks_and_dests.py to eliminate false positives when checking DOI links. 2021.2.14 2021-08-12  Fix the URL for ZIP-239 in the References. Reword the reference to a Sapling full viewing key in section4.8.2 Dummy Notes (Sapling) on page 47 (the full viewing key would include ovk, although it is not used in that section). 2021.2.13 2021-07-29  Add consensus rules in section3.8 Note Commitment Trees on page 21 that, for each note commitment tree, a block MUST NOT add note commitments that exceed the capacity of that tree. 2021.2.12 2021-07-29  Change the number of partial rounds, \ud835\udc45\ud835\udc43, for Poseidon from 58 to 56. This matches the number calcu- lated by calc_round_numbers.py (for 128-bit security with margin) in Version 1.1 of the Poseidon reference implementation Poseidon-1.1 Poseidon-Zc1.1. 2021.2.11 2021-07-20  Change the de\ufb01nition of inputs to the Action circuit to split enableSpends and enableOutputs into two \ufb01eld elements.",
      "2021.2.10 2021-07-13  Clarify that decomposition of scalars for scalar multiplication in the Action circuit MUST be canonical, unless a non-canonical decomposition can be proven to result in an equivalent statement  and clarify for which multiplications the latter case applies. The encoding of the block height in the scriptSig of a coinbase transaction is now at most 5 bytes (rather than 9 bytes), because block height MUST also be encoded in the 32-bit nExpiryHeight \ufb01eld of coinbase transactions after NU55 activation. Clarify in section3.4 Transactions and Treestates on page 18 that the remaining value in a transparent transaction value pool is only available to miners as a fee in the case of non-coinbase transactions, and that the remaining value in the transparent transaction value pool of a coinbase transaction is destroyed. Remove a spurious reference to rseed in section4.19 In-band secret distribution (Sprout) on page 65. There were no changes for Sprout in ZIP-212.",
      "2021.2.9 2021-07-01  Add a consensus rule for version 5 or later transactions, that if nActionsOrchard  0 then at least one of enableSpendsOrchard and enableOutputsOrchard MUST be 1. Delete the consensus rule in section3.4 Transactions and Treestates on page 18 that required checking that each intermediate root of the note commitment tree is not . Checking this rule would have imposed a signi\ufb01cant performance penalty, since intermediate roots do not otherwise need to be computed. Change the type of MerkleCRHOrchard to have 0 .. \ud835\udc5eP 1 in place of 0 .. \ud835\udc5eP 1  for the inputs and output, and map a output from SinsemillaHash to 0.",
      "(We retain the original de\ufb01nitions of SinsemillaHash and SinsemillaHashToPoint both because it would be disruptive to change them at this point in the Network Upgrade Process, and because it is necessary to track outputs in order to correctly model non-determinism in the Action circuit.)  Allow the Merkle path validity check in the Action circuit to pass if any output of MerkleCRHOrchard is 0, and add a note in section4.9 Merkle Path Validity on page 49 arguing that this is safe. Fix a typo in the Security Requirements for section5.4.1.3 MerkleCRHOrchard Hash Function on page 77: the length of the input to SinsemillaHash is 10  2  \u2113Orchard Merkle bits, not 6  2  \u2113Orchard Merkle bits. Replace must with MUST in two consensus rules speci\ufb01ed in section7.1 Transaction Encoding and Consensus on page 122.",
      "Add a clari\ufb01cation in section7.1.2 Transaction Consensus Rules on page 124 that after Heartwood and before Canopy activation, Sapling outputs of a coinbase transaction MUST have note plaintext lead byte equal to 0x01. This was implied by the existing rule that such outputs MUST decrypt successfully with an all-zero outgoing viewing key. Correct \ud835\udc59to \ud835\udc59in two places in section5.4.1.3 MerkleCRHSapling Hash Function on page 76. Correct an erroneous statement in section3.4 Transactions and Treestates on page 18 that claimed transaction IDs are not part of the consensus protocol. 2021.2.8 2021-06-29  Change one of the Sapling onward consensus rules in section7.1.2 Transaction Consensus Rules on page 124 to have the correct applicability: Sapling to Canopy inclusive, pre-NU55. Describe transaction IDs and wtxids in section3.4 Transactions and Treestates on page 18. Add a section section7.1.1 Transaction Identi\ufb01ers on page 124 on how to compute transaction IDs and wtxids.",
      "Split the transaction-related consensus rules into their own subsection section7.1.2 Transaction Consensus Rules on page 124, for more precise cross-referencing. 2021.2.7 2021-06-28  Correct the type of UncommittedOrchard, which should be P\ud835\udc65rather than a bit sequence. Explicitly say that padding in section5.4.1.9 Sinsemilla Hash Function on page 81 is by appending zero bits. Add a step to the algorithm for generating an Orchard note in section4.7.3 Sending Notes (Orchard) on page 45, to restart if esk  0. 2021.2.6 2021-06-26  Require that from NU55 activation, the nExpiryHeight\ufb01eld of a coinbase transaction is set to the block height. This is needed to maintain the property that all transactions have unique transaction IDs, as explained in a note in section7.1.2 Transaction Consensus Rules on page 124. In order to avoid the block height being limited to 499999999, we also remove that bound on nExpiryHeight for coinbase transactions.",
      "Remove the recommendation to support 63-bit block heights in section3.3 The Block Chain on page 18 (since it is incompatible with the above consensus rule for coinbase nExpiryHeight). Ensure that the layer number is passed to MerkleCRH in section4.9 Merkle Path Validity on page 49. Re\ufb01ne the key components diagram in section3.1 Payment Addresses and Keys on page 13 to show that Orchard incoming viewing keys include both dk and ivk. Clarify that the MAX_MONEY .. MAX_MONEY range restriction applies to both valueBalanceSapling and valueBalanceOrchard. Update section5.5 Encodings of Note Plaintexts and Memo Fields on page 112 for Orchard. Add ZIP-203, ZIP-212, and ZIP-213 to the list of ZIPs updated for NU55. Give cross-references to section2 Notation on page 10 where ? and  are used.",
      "2021.2.5 2021-06-19  Change the consensus rule that requires at least one input to, and at least one output from a v5 or later transaction, to take into account the enableSpendsOrchard and enableOutputsOrchard \ufb02ags. Correct the type of Extract P imported in section5.4.1.9 Sinsemilla Hash Function on page 81 (from P P\ud835\udc65to P  P\ud835\udc65). Add ZIP-209 to the list of ZIPs updated for NU55. 2021.2.4 2021-06-08  Add an explicit consensus rule in section7.1.2 Transaction Consensus Rules on page 124 that the reserved bits of the flagsOrchard \ufb01eld MUST be zero. Correct a cut-and-paste error in the algorithm for section4.8.3 Dummy Notes (Orchard) on page 48, which should refer to the Action statement rather than the Spend statement. 2021.2.3 2021-06-06  Specify (as a note in section4.18.4 Action Statement (Orchard) on page 63) the encoding of primary inputs to the Action circuit. This uses new helper functions \ud835\udc65and \ud835\udc66de\ufb01ned in section5.4.9.7 Coordinate Extractor for Pallas on page 106.",
      "The speci\ufb01cation of ExtractP has also been refactored to use \ud835\udc65(this does not change the Orchard protocol). In section5.4.1.10 PoseidonHash Function on page 84, say that the round constants as well as the MDS matrices are generated according to Version 1.1 of the reference implementation. Clarify that epk encoded in an Action description cannot be \ud835\udcaaP. Specify that Orchard spending keys are encoded using Bech32m. Add ZIP-239 to the list of ZIPs included in NU55. Move the section on abstraction (previously section 5.1) to section4 Abstract Protocol on page 23. Section 5.2 has been split into two (section5.1 Integers, Bit Sequences, and Endianness on page 73 and section5.2 Bit layout diagrams on page 73) to avoid renumbering later subsections. Correct an error in the encoding of height-in-coinbase for blocks at heights 1 .. 16. Clarify, in section3.3 The Block Chain on page 18, requirements on the range of block heights that should be sup- ported.",
      "Delete the sentence All conversions between Ed25519 points, byte sequences, and integers used in this section are as speci\ufb01ed in BDLSY2012. from section5.4.6 Ed25519 on page 90. This sentence was misleading given that the conversions in BDLSY2012 are not suf\ufb01ciently well-speci\ufb01ed for a consensus protocol; it should have been deleted earlier when explicit de\ufb01nitions for reprBytesEd25519 and abstBytesEd25519 were added. Make the NU55 speci\ufb01cation the default. 2021.2.2 2021-05-20  Clarify in section4.10 SIGHASH Transaction Hashing on page 50 that v4 transactions continue to use the ZIP-243 SIGHASH algorithm after NU55 activation. 2021.2.1 2021-05-20  Correct the size of vActionsOrchard in section7.1 Transaction Encoding and Consensus on page 122. Change the type of Orchard Merkle hash values to 0 .. \ud835\udc5eP 1, with a corresponding change to the signature of MerkleCRHOrchard.",
      "Add a note to section4.9 Merkle Path Validity on page 49 clarifying that non-canonical encodings are allowed as input to MerkleCRHOrchard. Clarify the distinction between Orchard incoming viewing keys and KAOrchard private keys. Add a note in section5.4.1.9 Sinsemilla Hash Function on page 81 that JT2020, Lemma 3 proves a tight reduction from \ufb01nding a nontrivial discrete logarithm relation to the Discrete Logarithm Problem. Add a note to section4.9 Merkle Path Validity on page 49 clarifying the encoding of rtSapling as a primary input to the Sapling Spend circuit, and that non-canonical encodings are allowed as input to MerkleCRHSapling. Change the notation \u2110\ud835\udc37 \ud835\udc56for a Sapling Pedersen generator to \u2110(\ud835\udc37, \ud835\udc56). 2021.2.0 2021-05-07  Include \u03c1 as an input to the derivation of \u03c8, esk, and rcm in Orchard. This was originally intended and as described in Zcash-Orchard, Section 3.5 Nulli\ufb01ers. Change the statement of Theorem 5.4.3 on page 83 to exclude outputs from SinsemillaHashToPoint.",
      "This does not affect security given Theorem 5.4.4 on page 84, but the case is only handled by the latter proof and not the former. Delegate to ZIP-316 for the speci\ufb01cation of uni\ufb01ed payment addresses, uni\ufb01ed incoming viewing keys, and uni\ufb01ed full viewing keys (section5.6.4.1 Uni\ufb01ed Payment Addresses and Viewing Keys on page 117). Specify that diversi\ufb01er indices for Orchard payment addresses should be chosen uniquely, not randomly. Vanity diversi\ufb01ers are not an issue for Orchard given that it does not have its own payment address format, and given the use of jumbling (ZIP-316) in uni\ufb01ed payment addresses. Remove the corresponding note from section4.2.3 Orchard Key Components on page 38. Clarify that the change to use hashBlockCommitments in a block header for NU55 is a consensus rule. Clarify that transparent inputs are prohibited in coinbase transactions only if they have a non-null prevout \ufb01eld.",
      "Caveat how the result of GG2015 applies to analysis of PRFnfOrchard in section5.4.2 Pseudo Random Functions on page 86. Unlinkability of diversi\ufb01ed payment addresses depends on the Decisional Dif\ufb01eHellman Problem, not the Discrete Logarithm Problem. Add a paragraph to section8.6 Changes to PRF inputs and truncation on page 146 covering Orchard. Clarify the de\ufb01nition of pad in section5.4.1.9 Sinsemilla Hash Function on page 81 by disambiguating \ud835\udc40pieces from \ud835\udc40padded. State explicitly that valueBalanceOrchard can only be negative in a coinbase transaction if it has ZIP-213 shielded outputs. Update the list of ZIPs relevant to NU55 in section6 Network Upgrades on page 120. Clarify notation by changing \u2113rcm to \u2113Sprout 2021.1.24 2021-04-23  Add the nConsensusBranchId \ufb01eld to v5 transactions, matching the consensus branch ID used for SIGHASH transaction hashes. Include the diversi\ufb01er key in an encoded Orchard Incoming Viewing Key.",
      "Remove an unused precomputation in section5.4.9.8 Group Hash into Pallas and Vesta on page 107. Clarify that only an outgoing cipher key is strictly needed to decrypt an outgoing ciphertext. Explicitly say that coinbase transactions MUST NOT have transparent inputs (this is a consensus rule inherited from Bitcoin which has been present since launch). 2021.1.23 2021-04-19  Correct errors in the de\ufb01nitions of ExtractP and Extract P in section5.4.9.7 Coordinate Extractor for Pallas on page 106: ExtractP(\ud835\udcaaP) should be 0, and Extract P() should be . Change the type of KAOrchard public keys and shared secrets to P (i.e. exclude \ud835\udcaaP), and the type of KAOrchard private keys to F \ud835\udc5fP (i.e. exclude 0). Change the type of an Orchard ivk to 1 .. \ud835\udc5eP 1 (i.e. exclude 0). Change the types of pkold d , cmold and akP to P in the auxiliary inputs to the Action statement. When creating Orchard notes, repeat with another rseed if cm is .",
      "Add a note in section4.2.3 Orchard Key Components on page 38 about non-uniformity of ivk. Fix a typo: Decription to Description. Add Action descriptions to the introduction of section4.16 Computing \u03c1 values and Nulli\ufb01ers on page 57. Use a different footnote symbol for each Sapling \ufb01eld cardinality rule in v5 transactions. Fix some URLs in references. 2021.1.22 2021-04-05  Specify that a uni\ufb01ed payment address MUST contain at least one shielded payment address. Further clari\ufb01cations to Theorem 5.4.3 on page 83. Correct ZKSpend.Verify to ZKOutput.Verify in section4.5 Output Descriptions on page 41. Make sure that Change History entries are URL destinations. 2021.1.21 2021-04-01  Correct and clarify Theorem 5.4.3 on page 83 and Theorem 5.4.4 on page 84. Clarify that a dummy note should be created if no real Orchard note is being spent in an Action transfer. Add a caveat in section4.2.3 Orchard Key Components on page 38 about reuse of rivk between PRFexpand and Commitivk.",
      "Expand the set of ZIPs associated with NU55 in section6 Network Upgrades on page 120, and reference Zcash-halo2 and Zcash-Orchard there. Section section5.4.5.6 Orchard Key Derivation on page 90 should be in slate blue. Explicitly note that the end of the ZIP-212 grace period precedes NU55 activation. Change the condition for presence of anchorSapling in a version 5 transaction to vSpendsSapling  0. Fix type error in kdfinput for KDFSapling and KDFOrchard (ephemeralKey is already a byte sequence). Make a note in section8.7 In-band secret distribution on page 147 of the divergence of ivk for Sapling and Orchard from a uniform scalar. Correct the set of inputs to PRFexpand used for ZIP-32 and Orchard in section4.1.2 Pseudo Random Functions on page 25. Write the caution about linkage between the abstract and concrete protocols in section4 Abstract Protocol on page 23. Update the Sprout keycomponent diagram in section3.1 Payment Addresses and Keys on page 13 to remove magenta highlighting.",
      "2021.1.20 2021-03-25  Credit Eirik Ogilvie-Wigley as a designer of the Zcash protocol. Add Andre Serrano, Brad Miller, Charlie OKeefe, David Campbell, Elena Giralt, Francisco Gindre, Joseph Van Geffen, Josh Swihart, Kevin Gorham, Larry Ruane, Marshall Gaucher, and Ryan Taylor to the acknowledgements. Add proof of collision resistance for Sinsemilla. Correct some interim \ufb01ndings of the NCC speci\ufb01cation audit:  Fix typos. Correct the de\ufb01nition of \ud835\udc50in section5.4.1.9 Sinsemilla Hash Function on page 81. Propagate intermediate results to the output of Sinsemilla primitives. Change the output types of NoteCommitOrchardand Commitivk to re\ufb02ect that these can return , and change the Action statement to be satis\ufb01ed if they do. Propagate from the inputs of MerkleCRHOrchard to its output, and add an explicit consensus rule that rtOrchard computed from appending a note commitment is not .",
      "Correct the de\ufb01nition of PRFnfOrchard in section5.4.2 Pseudo Random Functions on page 86 by changing Poseidon to PoseidonHash. Restrict the de\ufb01nition of a short Weierstrass elliptic curve in section5.4.9.6 Pallas and Vesta on page 105 to base \ufb01elds of characteristic greater than 3. De\ufb01ne G in section5.4.9.8 Group Hash into Pallas and Vesta on page 107. Fix type confusion between integers and \ufb01eld elements (including additional cases not found in the audit, involving nulli\ufb01ers and cm\ud835\udc65). Fix a discrepancy between section5.4.9.8 Group Hash into Pallas and Vesta on page 107 and ID-hashtocurve: the zero padding in expand_message_xmd should be 128 bytes (matching the input block size of BLAKE2b), rather than 64 bytes. Document that the use of \ud835\udc58 256 when extracting \ufb01eld elements in hash_to_field is intentional, despite the Pallas curve only having 126-bit conjectured security against generic attacks. Correct the output type of sqrt_ratioF\ud835\udc5eG.",
      "Document that the choice of nonsquare for \ud835\udf06G in section5.4.9.8 Group Hash into Pallas and Vesta on page 107 makes no difference to the output of map_to_curve_simple_swuiso-G. Document the limitation on the domain separation string for the group hash into Pallas and Vesta. Correct the sizes of SpendDescriptionV5 and OutputDescriptionV5 in the version 5 transaction format. Make the description of when \ufb01elds are included in v5 transactions consistent between the protocol speci\ufb01cation and ZIP-225. Make the naming of enableSpends and enableOutputs consistent. Change the speci\ufb01cations of note decryption in section4.19 In-band secret distribution (Sprout) on page 65 and section4.20 In-band secret distribution (Sapling and Orchard) on page 67 to return the note and memo \ufb01eld, rather than a note plaintext. Generalize the block chain scanning algorithm in section4.22 Block Chain Scanning (Sapling and Orchard) on page 72 to support Orchard.",
      "Update the hashFinalSaplingRoothashLightClientRoothashBlockCommitments \ufb01eld for NU55. Update speci\ufb01cation of Poseidon. Fix errors in Orchard due to cut-and-paste from Sapling. Add references to Zcash-halo2. Correct the description of length in section5.6.4.1 Uni\ufb01ed Payment Addresses and Viewing Keys on page 117. Correct the type signature of DiversifyHashOrchard in section4.1.1 Hash Functions on page 24. Various rationale updates for NU55. Other \ufb01xes to the Orchard speci\ufb01cation, including generation of dummy notes and output notes. Describe the recommended way to encode a Sapling or uni\ufb01ed payment address as a QR code. Move the de\ufb01nition of to before its \ufb01rst use. Delete a confusing part of the de\ufb01nition of concatB that we dont rely on. Add a de\ufb01nition for the  symbol in section1 Introduction on page 7, before its \ufb01rst use. Remove speci\ufb01cation of memo \ufb01eld contents, which will be in ZIP-302. Remove support for building the Sprout-only speci\ufb01cation (sprout.pdf).",
      "Remove magenta highlighting of differences from Zerocash. 2021.1.19 2021-03-17  Correct the range of input to ValueCommitOrchard in the Action statement, and the corresponding security argument in section4.14 Balance and Binding Signature (Orchard) on page 54. Update the consensus rules that prevent trivial transactions (with no inputs or outputs) to take into account Action transfers in the v5 transaction format. Make DiversifyHashOrchard total, by replacing an output of \ud835\udcaaP with another base. Fix a type error in the non-normative note at the end of section5.4.8.4 Sinsemilla commitments on page 98. 2021.1.18 2021-03-17  De\ufb01ne uni\ufb01ed payment addresses in place of the Bech32 form of Orchard shielded payment addresses. Remove Sprout-speci\ufb01c \ufb01elds from the v5 transaction format. The \u03c1 value for an Orchard output note was incorrectly described as being derived from rseed, instead of being set to the nulli\ufb01er from the same Action description as intended.",
      "The \u03c8 value is now derived using the PRFexpand input 9, instead of 10. Correct a note about the range of the Merkle hash inputs in section4.18.4 Action Statement (Orchard) on page 63. Correct the validity condition for ak in section5.6.4.4 Orchard Raw Full Viewing Keys on page 118. Add a de\ufb01nition for \ud835\udca6Orchard in section4.16 Computing \u03c1 values and Nulli\ufb01ers on page 57. Correct the number of full and partial rounds for Poseidon. Add a note explaining the origin of the 265 constant in the de\ufb01nition of PoseidonHash. The subgroup check added to section4.20.3 Decryption using an Outgoing Viewing Key (Sapling and Orchard) on page 70 for Sapling in v2021.1.17 was applied to the wrong variable (gd, when it should have been pkd), despite being described correctly in the Change History entry below. 2021.1.17 2021-03-15  Draft NU55 speci\ufb01cation.",
      "In the consensus rule that a transaction with one or more transparent inputs from coinbase transactions MUST have no transparent outputs, explicitly say that inputs from coinbase transactions include funding stream outputs. The de\ufb01nition of an abstraction function in section4.1.9 Represented Group on page 32 incorrectly required canon- icity, i.e. that abstG does not accept inputs outside the range of reprG. While this was originally intended, it is not true of abstJ. (It is also not true of abstBytesEd25519, but Ed25519 is not strictly de\ufb01ned as a represented group in this speci\ufb01cation.)  Correct Theorem 5.4.5 on page 96, which was proving the wrong thing. It needs to prove that NoteCommitSapling does not return UncommittedSapling, but was previously proving that PedersenHash does not return that value. The note about non-canonical encodings in section5.4.9.3 Jubjub on page 102 gave incorrect values for the encodings of the point of order 2, by omitting a \ud835\udc5eJ term.",
      "The speci\ufb01cation of decryption in section4.20.3 on page 70 differed from the zcashd implementation in two respects:  The speci\ufb01cation had a type error in that it failed to check whether abstJ(pkd) , which is needed in order for its use as input to KASapling.Agree to be well-typed. The speci\ufb01cation did not require pkd to be in the subgroup J(\ud835\udc5f), while the implementation in zcashd did. This check is not needed for security; however, since Jubjub public keys are normally of type KASapling.PublicPrimeOrder, we change the speci\ufb01cation to match zcashd. Correct the procedure for generating dummy Sapling notes in section4.8.2 Dummy Notes (Sapling) on page 47. Add a note in section5.4.10.1 BCTV14 on page 110 describing conditions under which an implementation that check- points on Sapling can omit verifying BCTV14 proofs. Rename hash extractor to coordinate extractor. This is a more accurate name since it is also used on commitments. Rename char to byte in \ufb01eld type declarations.",
      "2021.1.16 2021-01-11  Add macros and Makefile support for building the NU55 draft speci\ufb01cation. Clarify the encoding of block heights for the height in coinbase rule. The description of this rule has also moved from section7.6 on page 131 to section7.1.2 Transaction Consensus Rules on page 124. Include the activation dates of Heartwood and Canopy in section6 Network Upgrades on page 120. Section links in the Heartwood and Canopy versions of the speci\ufb01cation now go to the correct document URL. Attempt to improve search and cut-and-paste behaviour for ligatures in some PDF readers. 2020.1.15 2020-11-06  Add a missing consensus rule that has always been implemented in zcashd: there must be at least one transparent output, Output description, or JoinSplit description in a transaction. Add a consensus rule that the (zero-valued) coinbase transaction output of the genesis block cannot be spent.",
      "De\ufb01ne Sprout chain value pool balance and Sapling chain value pool balance, and include consensus rules from ZIP-209. Correct the Sapling note decryption algorithms:  ephemeralKey is kept as a byte sequence rather than immediately converted to a curve point; this matters because of non-canonical encoding. The representation of pkd in a note plaintext may also be non-canonical and need not be in the prime- order subgroup. Move checking of cm\ud835\udc62in decryption with ivk to the end of the algorithm, to more closely match the implementation. The note about decryption of outputs in mempool transactions should have been normative. Reserve transaction version number 0x7FFFFFFF and version group ID 0xFFFFFFFF for experimental use. Remove a statement that the language consisting of key and address encoding possibilities is pre\ufb01x-free.",
      "(The human-readable forms are pre\ufb01x-free but the raw encodings are not; for example, the raw encoding of a Sapling spending key can be a pre\ufb01x of several of the other encodings.)  Use let mutable to introduce mutable variables in algorithms. Include a reference to BFIJSV2010 for batch pairing veri\ufb01cation techniques. Acknowledge Jack Gavigan as a co-designer of Sapling and of the Zcash protocol. Acknowledge Izaak Meckler, Zac Williamson, Vitalik Buterin, and Jakub Zalewski. Acknowledge Alexandra Elbakyan. 2020.1.14 2020-08-19  The consensus rule that a coinbase transaction must not spend more than is available from the block subsidy and transaction fees, was not explicitly stated. (This rule was correctly implemented in zcashd.)  Fix a type error in the output of PRFnfSapling; a Sapling nulli\ufb01er is a sequence of 32 bytes, not a bit sequence.",
      "Correct an off-by-one in an expression used in the de\ufb01nition of \ud835\udc50in section5.4.1.7 Pedersen Hash Function on page 79 (this does not change the value of \ud835\udc50). 2020.1.13 2020-08-11  Rename the type of Sapling transmission keys from KASapling.PublicPrimeOrder to KASapling.PublicPrimeSubgroup. This type is de\ufb01ned as J(\ud835\udc5f), which re\ufb02ects the implementation in zcashd (subject to the next point below); it was never enforced that a transmission key (pkd) cannot be \ud835\udcaaJ. Add a non-normative note saying that zcashd does not fully conform to the requirement to treat transmission keys not in KASapling.PublicPrimeSubgroup as invalid when importing shielded payment addresses. Retrospective note: Changing KASapling.PublicPrimeOrder to KASapling.PublicPrimeSubgroup was a mistake and has since been reverted in speci\ufb01cation version v2025.6.0.",
      "As discussed in notes added in version v2023.4.0 at section4.20.3 Decryption using an Outgoing Viewing Key (Sapling and Orchard) on page 70, librustzcash changed in librustzcash-109 to enforce that pkd is not \ud835\udcaaJ. zcashd also used a different implementation for a consensus check on shielded coinbase outputs. The missing check on pkd for the latter was corrected in zcashd-6459, and simpli\ufb01ed when it was observed to be retrospectively valid in zcashd-6725. However ZIP-216 was only corrected later Zips-Issue664, at the same time as the publication of v2025.6.0. Set CanopyActivationHeight for Testnet. Modify the tables and notes in section7.10.1 ZIP 214 Funding Streams on page 140 to re\ufb02ect changes in ZIP-214. Updates to re\ufb02ect ZIP-211: add a consensus rule on vold pub in section4.3 JoinSplit Descriptions on page 39, and a rule about node and wallet support for sending to Sprout addresses in section4.7.1 Sending Notes (Sprout) on page 43. Re\ufb01ne the domain of HeightForHalving from N to N.",
      "Make Halving(height) return 0 (rather than 1) for height  SlowStartShift. This has no effect on consensus since the Halving function is not used in that case, but it makes the de\ufb01nition match the intuitive meaning of the function. Rename sections under section7 Consensus Changes from Bitcoin on page 122 to clarify that these sections do not only concern encoding, but also consensus rules. Make the Canopy speci\ufb01cation the default. 2020.1.12 2020-08-03  Include SHA-512 in section5.4.1.1 SHA-256, SHA-256d, SHA256Compress, and SHA-512 Hash Functions on page 75. Add a reference to BCCGLRT2014 in section4.1.13 Zero-Knowledge Proving System on page 34. Use abstBytesEd25519 and reprBytesEd25519 for conversions in sectionB.3 Ed25519 batch validation on page 223, and \ufb01x a missing requirement that \ud835\udc46\ud835\udc57 \u2113for all signatures. 2020.1.11 2020-07-13  Change instances of the production network to Mainnet, and the test network to Testnet. This follows the terminology used in ZIPs.",
      "Update stale references to Bitcoin documentation. Add changes for ZIP-207 and ZIP-214. 2020.1.10 2020-07-05  Corrections to a note in section5.4.6 Ed25519 on page 90. 2020.1.9 2020-07-05  Add section3.12 Mainnet and Testnet on page 22. Acknowledge Jane Lusby and teor. Precisely specify the encoding and decoding of Ed25519 points. Correct an error introduced in v2020.1.8; \ud835\udcaaJ was incorrectly used when the point (0, 1) on Jubjub was meant. Precisely specify the conversion from a bit sequence in abstJ. 2020.1.8 2020-07-04  Add Ying Tong Lai and Kris Nuttycombe as Zcash protocol designers. Change the speci\ufb01cation of abstJ in section5.4.9.3 Jubjub on page 102 to match the implementation. Repair the argument for GroupHashJ(\ud835\udc5f) URS being usable as a random oracle, which previously depended on abstJ being injective. In RedDSA veri\ufb01cation, clarify that \ud835\udc45used as part of the input to HMUST be exactly as encoded in the signature.",
      "Specify that shielded outputs of coinbase transactions MUST use v2 note plaintexts after Canopy activation. Correct a bug in section4.20.3 Decryption using an Outgoing Viewing Key (Sapling and Orchard) on page 70: esk is only to be checked against ToScalar PRFexpand rseed (4) when leadByte  0x01. Later edit: this should have been ToScalar PRFexpand rseed (5) 2020.1.7 2020-06-26  Delete some new superscripts that only added notational clutter. Add an explicit lead byte \ufb01eld to Sprout note plaintexts, and clearly specify the error handling when it is invalid. De\ufb01ne a Sapling note plaintext lead byte as having type BY (so that decoding to a note plaintext always succeeds, and error handling is more explicit). Fix a sign error in the \ufb01xed-base term of the batch validation equation in sectionB.1 RedDSA batch validation on page 220. Fix a sign error in the \ufb01xed-base term of the batch validation equation in sectionB.3 Ed25519 batch validation on page 223.",
      "2020.1.6 2020-06-17  Incorporate changes to Sapling note encryption from ZIP-212. Correct an error in the speci\ufb01cation of Ed25519 validating keys: they should not have been speci\ufb01ed to be checked against PreCanopyExcludedPointEncodings, since libsodium v1.0.15 does not do so. Incorporate Ed25519 changes for Canopy from ZIP-215. Add Appendix sectionB.3 Ed25519 batch validation on page 223. Consistently use validating for signatures and verifying for proofs. Use the symbol  for positive square root. 2020.1.5 2020-06-02  Reference ZIP-173 instead of BIP 173. Mark more index entries as de\ufb01nitions. 2020.1.4 2020-05-27  Reference BIP-32 and ZIP-32 when describing keys and their encodings. Network Upgrade 4 has been given the name Canopy. Reference ZIP-211, ZIP-212, and ZIP-215 for the Canopy upgrade. Improve LaTeX portability of this speci\ufb01cation. 2020.1.3 2020-04-22  Correct a wording error transposing transparent inputs and transparent outputs in section4.12 Balance (Sprout) on page 51.",
      "Minor wording clari\ufb01cations. Reference ZIP-251, ZIP-207, and ZIP-214 for the Canopy upgrade. 2020.1.2 2020-03-20  The implementation of Sprout Ed25519 signature validation in zcashd differed from what was speci\ufb01ed in section5.4.6 Ed25519 on page 90. The speci\ufb01cation has been changed to match the implementation. Add consensus rules for Heartwood. Remove pvc Makefile targets. Make the Heartwood speci\ufb01cation the default. Add macros and Makefile support for building the Canopy speci\ufb01cation. 2020.1.1 2020-02-13  Resolve con\ufb02icts in the speci\ufb01cation of memo \ufb01elds by deferring to ZIP-302. 2020.1.0 2020-02-06  Specify a retrospective soft fork implemented in zcashd v2.1.1-1 that limits the nTime \ufb01eld of a block relative to its median-time-past. Correct the de\ufb01nition of median-time-past for the \ufb01rst PoWMedianBlockSpan blocks in a block chain. Add acknowledgements to Henry de Valence, Deirdre Connolly, Chelsea Komlo, and Zancas Wilcox.",
      "Add an acknowledgement to Trail of Bits for their security audit. Change indices in the incremental Merkle tree diagram to be zero-based. Use the term monomorphism for an injective homomorphism, in the context of a signature scheme with key monomorphism. 2019.0.9 2019-12-27  No changes to Sprout or Sapling. Specify the height at which Blossom activated. Add Blossom to section6 Network Upgrades on page 120. Add a non-normative note giving the explicit value of FoundersRewardLastBlockHeight. Clarify the effect of Blossom on SIGHASH transaction hashes. Makefile updates for Heartwood. 2019.0.8 2019-09-24  Fix a typo in the generator \ud835\udcabS1 in section5.4.9.2 BLS12-381 on page 101 found by magrady. Clarify the type of vnew in section4.7.2 Sending Notes (Sapling) on page 44. 2019.0.7 2019-09-24  Fix a discrepancy in the number of constraints for BLAKE2s found by QED-it. Fix an error in the expression for \u0394 in sectionA.3.3.9 Pedersen hash on page 210, and add acknowledgement to Kobi Gurkan.",
      "Fix a typo in section4.9 Merkle Path Validity on page 49 and add acknowledgement to Weikeng Chen. Update references to ZIPs and to the Electric Coin Company blog. Makefile improvements to suppress unneeded output. 2019.0.6 2019-08-23  No changes to Sprout or Sapling. Replace dummy Blossom activation block height with the Testnet height, and a reference to ZIP-206. 2019.0.5 2019-08-23  Note the change to the minimum-dif\ufb01culty threshold time on Testnet for Blossom. Correct the packing of nfold into input elements in sectionA.4 The Sapling Spend circuit on page 217. Add an epigraph from Carroll1876 to the start of section5.4.9.3 Jubjub on page 102. Clarify how the constant \ud835\udc50in section5.4.1.7 Pedersen Hash Function on page 79 is obtained. Add a footnote that zcashd uses ZIP-32 extended spending keys instead of the derivation from sk in section3.1 Payment Addresses and Keys on page 13. Remove optimized Makefile targets (which actually produced a larger PDF, with TeXLive 2019).",
      "Remove html Makefile targets. Make the Blossom spec the default. 2019.0.4 2019-07-23  Clicking on a section heading now shows section labels. Add a List of Theorems and Lemmata. Changes needed to support TeXLive 2019. 2019.0.3 2019-07-08  Experimental support for building using LuaTEX and XeTEX. Add an Index. 2019.0.2 2019-06-18  Correct a misstatement in the security argument in section4.13 Balance and Binding Signature (Sapling) on page 52: binding for a commitment scheme does not imply that the commitment determines its randomness. The rest of the security argument did not depend on this; it is simpler to rely on knowledge soundness of the Spend and Output proofs. Give a de\ufb01nition for complete twisted Edwards elliptic curves in section5.4.9.3 Jubjub on page 102. Clarify that Theorem 5.4.5 on page 96 depends on the parameters of the Jubjub curve. Ensure that this document builds correctly and without missing characters on recent versions of TEXLive.",
      "Update the Makefile to use Ghostscript for PDF optimization. Ensure that hyperlinks are preserved, and available as Destination names in URL fragments and links from other PDF documents. 2019.0.1 2019-05-20  No changes to Sprout or Sapling. Minor \ufb01x to the list of integer constants in section2 Notation on page 10. Use IsBlossomActivated in the de\ufb01nition of FounderAddressAdjustedHeight for consistency. 2019.0.0 2019-05-01  Fix a speci\ufb01cation error in the Founders Reward calculation during the slow start period. Correct an inconsistency in dif\ufb01culty adjustment between the spec and zcashd implementation for the \ufb01rst PoWAveragingWindow 1 blocks of the block chain. This inconsistency was pointed out by NCC Group in their Blossom speci\ufb01cation audit. Revert changes for funding streams from Withdrawn ZIP 207. 2019.0-beta-39 2019-04-18  Change author af\ufb01liations from Zerocoin Electric Coin Company to Electric Coin Company.",
      "Add acknowledgement to Mary Maller for the observation that diversi\ufb01ed payment address unlinkability can be proven in the same way as key privacy for ElGamal. 2019.0-beta-38 2019-04-18  Update the following sections to match the current draft of ZIP-208:  section7.7.3 Dif\ufb01culty adjustment on page 134  section7.8 Calculating Block Subsidy, Funding Streams, Lockbox Disbursement, and Founders Reward on page 136  Specify funding streams, along with the draft funding streams de\ufb01ned in the current draft of ZIP 207. Update the following sections to match the current draft of ZIP 207:  section3.10 Block Subsidy, Funding Streams, and Founders Reward on page 22  section3.11 Coinbase Transactions on page 22  section7.8 Calculating Block Subsidy, Funding Streams, Lockbox Disbursement, and Founders Reward on page 136  section7.9 Payment of Founders Reward on page 137  Correct the generators \ud835\udcabS1 and \ud835\udcabS2 for BLS12-381. Update README.rst to include Makefile targets for Blossom.",
      "Makefile updates:  Fix a typo for the pvcblossom target. Update the pinned git hashes for sam2p and pdfsizeopt. 2019.0-beta-37 2019-02-22  The rule that miners SHOULD NOT mine blocks that chain to other blocks with a block version number greater than 4, has been removed. This is because such blocks (mined nonconformantly) exist in the current Mainnet consensus block chain. Clarify that Equihash is based on a variation of the Generalized Birthday Problem, and cite AR2017. Update reference BGG2017 (previously BGG2016). Clarify which transaction \ufb01elds are added by Overwinter and Sapling. Correct the rule about when a transaction is permitted to have no transparent inputs. Explain the differences between the system in Groth2016 and what we refer to as Groth16. Reference Mary Mallers security proof for Groth16 Maller2018. Correct BGM2018 to BGM2017. Fix a typo in sectionB.2 Groth16 batch veri\ufb01cation on page 221 and clarify the costs of Groth16 batch veri\ufb01cation.",
      "Add macros and Makefile support for building the Blossom speci\ufb01cation. 2019.0-beta-36 2019-02-09  Correct isis agora lovecrufts name. 2019.0-beta-35 2019-02-08  Cite Gabizon2019 and acknowledge Ariel Gabizon. Correct SBB2019 to SWB2019. The Gabizon2019 vulnerability affected Soundness of BCTV14 as well as Knowledge Soundness. Clarify the history of the Parno2015 vulnerability and acknowledge Bryan Parno. Specify the dif\ufb01culty adjustment change that occurred on Testnet at block height 299188. Add Eirik Ogilvie-Wigley and Benjamin Winston to acknowledgements. Rename zk-SNARK Parameters sections to be named according to the proving system (BCTV14 or Groth16), not the shielded protocol construction (Sprout or Sapling). In section6 Network Upgrades on page 120, say when Sapling activated. 2019.0-beta-34 2019-02-05  Disclose a security vulnerability in BCTV14 that affected Sprout before activation of the Sapling network upgrade (see section5.4.10.1 BCTV14 on page 110).",
      "Rename PHGR13 to BCTV2014. Rename reference BCTV2015 to BCTV2014a, and BCTV2014 to BCTV2014b. 2018.0-beta-33 2018-11-14  Complete sectionA.4 The Sapling Spend circuit on page 217. Add sectionA.5 The Sapling Output circuit on page 219. Change the description of window lookup in sectionA.3.3.7 Fixed-base Af\ufb01ne-ctEdwards scalar multiplication on page 208 to match sapling-crypto. Describe 2-bit window lookup with conditional negation in sectionA.3.3.9 Pedersen hash on page 210. Fix or complete various calculations of constraint costs. Adjust the notation used for scalar multiplication in Appendix A to allow bit sequences as scalars. 2018.0-beta-32 2018-10-24  Correct the input to Hused to derive the nonce \ud835\udc5fin RedDSA.Sign, from \ud835\udc47 \ud835\udc40to \ud835\udc47 vk  \ud835\udc40. This matches the sapling-crypto implementation; the speci\ufb01cation of this input was unintentionallychanged in v2018.0-beta-20. Clarify the description of the Merkle path check in sectionA.3.4 Merkle path check on page 213.",
      "2018.0-beta-31 2018-09-30  Correct some uses of \ud835\udc5fJ that should have been \ud835\udc5fS or \ud835\udc5e. Correct uses of LEOS2IP\u2113in RedDSA.Validate and RedDSA.BatchValidate to ensure that \u2113is a multiple of 8 as required. Minor changes to avoid clashing notation for Edwards curves \ud835\udc38Edwards(\ud835\udc4e,\ud835\udc51), Montgomery curves \ud835\udc38Mont(\ud835\udc34,\ud835\udc35), and extractors \u2130\ud835\udc9c. Correct a use of J that should have been M in the proof of Theorem A.3.4 on page 206, and make a minor tweak to the theorem statement (\ud835\udc582  \ud835\udc581 instead of \ud835\udc581  \ud835\udc582) to make the contradiction derived by the proof clearer. Clarify notation in the proof of Theorem A.3.3 on page 206. Address some of the \ufb01ndings of the QED-it report:  Improved cross-referencing in section5.4.1.7 Pedersen Hash Function on page 79. Clarify the notes concerning domain separation of pre\ufb01xes in section5.4.1.3 MerkleCRHSapling Hash Function on page 76 and section5.4.8.2 Windowed Pedersen commitments on page 96. Correct the statement and proof of Theorem A.3.2 on page 206.",
      "Add the QED-it report to the acknowledgements. 2018.0-beta-30 2018-09-02  Give an informal security argument for Unlinkability of diversi\ufb01ed payment addresses based on reduction to key privacy of ElGamal encryption, for which a security proof is given in BBDP2001. (This argument has gaps which will be addressed in a future version.)  Add a reference to BGM2017 for the Sapling zk-SNARK parameters. Write sectionA.4 The Sapling Spend circuit on page 217 (draft). Add a reference to the ristretto_bulletproofs design notes Dalek-notes for the synthetic blinding factor technique. Ensure that the constraint costs in sectionA.3.3.1 Checking that Af\ufb01ne-ctEdwards coordinates are on the curve on page 205 and sectionA.3.3.6 Af\ufb01ne-ctEdwards nonsmall-order check on page 208 accurately re\ufb02ect the implemen- tation in sapling-crypto. Minor correction to the non-normative note in sectionA.3.2.2 Range check on page 203.",
      "Clarify non-normative note in section4.1.8 Commitment on page 31 about the de\ufb01nitions of ValueCommitSapling.Output and NoteCommitSapling.Output. Clarify that the signer of a spend authorization signature is supposed to choose the spend authorization randomizer, \ud835\udefc, itself. Only step 4 in section4.15 Spend Authorization Signature (Sapling and Orchard) on page 56 may securely be delegated. Add a non-normative note to section5.4.7 RedDSA, RedJubjub, and RedPallas on page 92 explaining that RedDSA key randomization may interact with other uses of additive properties of Schnorr keys. Add dates to Change History entries. (These are the dates of the git tags in local, i.e. UK, time.) 2018.0-beta-29 2018-08-15  Finish sectionA.3.2.2 Range check on page 203. Change sectionA.3.7 BLAKE2s hashes on page 214 to correct the constraint count and to describe batched equality checks performed by the sapling-crypto implementation. 2018.0-beta-28 2018-08-14  Finish sectionA.3.7 BLAKE2s hashes on page 214.",
      "Minor corrections to sectionA.3.3.8 Variable-base Af\ufb01ne-ctEdwards scalar multiplication on page 209. 2018.0-beta-27 2018-08-12  Notational changes:  Use a superscript (\ud835\udc5f) to mark the subgroup order, instead of a subscript. Use G(\ud835\udc5f) for the set of \ud835\udc5fG-order points in G. Mark the subgroup order in pairing groups, e.g. use G(\ud835\udc5f) 1 instead of G1. Make the bit-representation indicator an af\ufb01x instead of a superscript. Clarify that when validating a Groth16 proof, it is necessary to perform a subgroup check for \ud835\udf0b\ud835\udc34and \ud835\udf0b\ud835\udc36as well as for \ud835\udf0b\ud835\udc35. Correct the description of Groth16 batch veri\ufb01cation to explicitly take account of how veri\ufb01cation depends on primary inputs. Add Charles Rackoff, Rafail Ostrovsky, and Amit Sahai to the acknowledgements section for their work on zero-knowledge proofs. 2018.0-beta-26 2018-08-05  Add sectionB.2 Groth16 batch veri\ufb01cation on page 221. 2018.0-beta-25 2018-08-05  Add the hashes of parameter \ufb01les for Sapling.",
      "Add cross references for parameters and functions used in RedDSA batch validation. Makefile changes: name the PDF \ufb01le for the Sprout version of the speci\ufb01cation as sprout.pdf, and make protocol.pdf link to the Sapling version. 2018.0-beta-24 2018-07-31  Add a missing consensus rule for version 4 transactions: if there are no Sapling Spends or Outputs, then valueBalanceSapling MUST be 0. 2018.0-beta-23 2018-07-27  Update RedDSA validation to use cofactor multiplication. This is necessary in order for the output of batch validation to match that of unbatched validation in all cases. Add sectionB.1 RedDSA batch validation on page 220. 2018.0-beta-22 2018-07-18  Update section6 Network Upgrades on page 120 to take account that Overwinter has activated. The recommendation for transactions without JoinSplit descriptions to be version 1 applies only before Overwinter, not before Sapling. Complete the proof of Theorem A.3.5 on page 211.",
      "Add a note about redundancy in the nonsmall-order checking of rk. Clarify the use of cvnew and cmnew, and the selection of outgoing viewing key, in sending Sapling notes. Delete the description of optimizations for the af\ufb01ne twisted Edwards nonsmall-order check, since the Sapling circuit does not use them. Also clarify that some other optimizations are not used. 2018.0-beta-21 2018-06-22  Remove the consensus rule If nJoinSplit  0, the transaction MUST NOT use SIGHASH types other than SIGHASH_ALL., which was never implemented. Add section on signature hashing. Brie\ufb02y describe the changes to computation of SIGHASH transaction hashes in Sprout. Clarify that interstitial treestates form a tree for each transaction containing JoinSplit descriptions. Correct the description of P2PKH addresses in section5.6.1.1 Transparent Addresses on page 113  they use a hash of a compressed, not an uncompressed ECDSA key representation.",
      "Clarify the wording of the caveat4 about the claimed security of shielded transactions. Correct the de\ufb01nition of set difference (\ud835\udc46\ud835\udc47). Add a note concerning malleability of zk-SNARK proofs. Clarify attribution of the Zcash protocol design. Acknowledge Alex Biryukov and Dmitry Khovratovich as the designers of Equihash. Acknowledge Sha\ufb01Goldwasser, Silvio Micali, Oded Goldreich, Rosario Gennaro, Bryan Parno, Jon Howell, Craig Gentry, Mariana Raykova, and Jens Groth for their work on zero-knowledge proving systems. Acknowledge Tomas Sander and Amnon Ta-Shma for ST1999. Acknowledge Kudelski Securitys audit. Use the more precise subgroup types G(\ud835\udc5f) and J(\ud835\udc5f) in preference to G and J where applicable. Change the types of auxiliary inputs to the Spend statement and Output statement, to be more faithful to the implementation. Rename the cm \ufb01eld of an Output description to cmu, re\ufb02ecting the fact that it is a Jubjub curve \ud835\udc62-coordinate.",
      "Add explicit consensus rules that the anchorSapling \ufb01eld of a Spend description and the cmu \ufb01eld of an Output description must be canonical encodings. Enforce that esk in outCiphertext is a canonical encoding. Add consensus rules that cv in a Spend description, and cv and epk in an Output description, are not of small order. Exclude 0 from the range of esk when encrypting Sapling notes. Add a consensus rule that valueBalanceSapling is in the range MAX_MONEY .. MAX_MONEY. Enforce stronger constraints on the types of key components pkd, ak, and nk. Correct the conformance rule for fOverwintered (it must not be set before Overwinter has activated, not before Sapling has activated). Correct the argument that v is in range in section4.13 Balance and Binding Signature (Sapling) on page 52. Correct an error in the algorithm for RedDSA.Validate: the validating key vk is given directly to this algorithm and should not be computed from the unknown signing key sk.",
      "Correct or improve the types of GroupHashJ(\ud835\udc5f) , FindGroupHashJ(\ud835\udc5f) , ExtractJ(\ud835\udc5f), PRFexpand, PRFockSapling, and CRHivk. Instantiate PRFockSapling using BLAKE2b-256. Change the syntax of a commitment scheme to add COMM.GenTrapdoor. This is necessary because the intended distribution of commitment trapdoors may not be uniform on all values that are acceptable trapdoor inputs. Add notes on the purpose of outgoing viewing keys. Correct the encoding of a full viewing key (ovk was missing). Ensure that Sprout functions and values are given Sprout-speci\ufb01c types where appropriate. Improve cross-referencing. Clarify the use of BCTV14 vs Groth16 proofs in JoinSplit statements. Clarify that the  \ud835\udc4enotation refers to the positive square root. (This matters for the conversion in sectionA.3.3.3 ctEdwards Montgomery conversion on page 205.)  Model the group hash as a random oracle. This appears to be unavoidable in order to allow proving unlink- ability of DiversifyHashSapling.",
      "Explain how this relates to the Discrete Logarithm Independence assumption used previously, and justify this modelling by showing that it follows from treating BLAKE2s-256 as a random oracle in the instantiation of GroupHashJ(\ud835\udc5f)  Rename CRS (Common Random String) to URS (Uniform Random String), to match the terminology adopted at the \ufb01rst ZKProof workshop held in Boston, Massachusetts on May 1011, 2018. Generalize PRFexpand to accept an arbitrary-length input. (This speci\ufb01cation does not use that generalization, but ZIP-32 does.)  Change the notation for a multiplication constraint in Appendix sectionA Circuit Design on page 200 to avoid potential confusion with cartesian product. Clarify the wording of the abstract. Correct statements about which algorithms are instantiated by BLAKE2s and BLAKE2b. Add a note explaining which conformance requirements of BIP 173 (de\ufb01ning Bech32) apply. Add the Jubjub bird image to the title page.",
      "This image has been edited from a scan of Peter Newells original illustration (as it appeared in Carroll1902) to remove the background and Bandersnatch, and to restore the birds clipped right wing. Change the light yellow background to white (indicating that this Overwinter and Sapling speci\ufb01cation is no longer a draft). 2018.0-beta-20 2018-05-22  Add Michael Dixon and Andrew Poelstra to acknowledgements. Minor improvements to cross-references. Correct the order of arguments to RedDSA.RandomizePrivate and RedDSA.RandomizePublic. Correct a reference to RedDSA.RandomizePrivate that was intended to be RedDSA.RandomizePublic. Fix the description of the Sapling balancing value in section4.13 Balance and Binding Signature (Sapling) on page 52. Correct a type error in section5.4.9.5 Group Hash into Jubjub on page 104. Correct a type error in RedDSA.Sign in section5.4.7 RedDSA, RedJubjub, and RedPallas on page 92.",
      "Ensure \ud835\udca2Sapling is de\ufb01ned in section5.4.7.1 Spend Authorization Signature (Sapling and Orchard) on page 95. Make the validating key pre\ufb01x part of the input to the hash function in RedDSA, not part of the message. Correct the statement about FindGroupHashJ(\ud835\udc5f) never returning . Correct an error in the computation of generators for Pedersen hashes. Change the order in which NoteCommitSapling commits to its inputs, to match the sapling-crypto implementa- tion. Fail Sapling key generation if ivk  0. (This has negligible probability.)  Change the notation Hto Hin section5.4.7 RedDSA, RedJubjub, and RedPallas on page 92, to avoid confusion with the convention for representations of group elements. cmu encodes only the \ud835\udc62-coordinate of the note commitment, not the full curve point. rk is checked to be not of small order outside the Spend statement, not in the Spend statement. Change terminology describing constraint systems. 2018.0-beta-19 2018-04-23  Minor clari\ufb01cations.",
      "2018.0-beta-18 2018-04-23  Clarify the security argument for balance in Sapling. Correct a subtle problem with the type of the value input to ValueCommitSapling: although it is only directly used to commit to values in 0 .. 2\u2113value1, the security argument depends on a sum of commitments being binding on \ud835\udc5fJ1 .. \ud835\udc5fJ1  Fix the loss of tightness in the use of PRFnfSapling by specifying the keyspace more precisely. Correct type ambiguities for \u03c1. Specify the representation of \ud835\udc56in group G2 of BLS12-381. 2018.0-beta-17 2018-04-21  Correct an error in the de\ufb01nition of DefaultDiversifier. 2018.0-beta-16 2018-04-21  Explicitly note that outputs from coinbase transactions include Founders Reward outputs. The point represented by \ud835\udc45in an Ed25519 signature is checked to not be of small order; this is not the same as checking that it is of prime order \u2113. Specify support for BIP-111 (the NODE_BLOOM service bit) in peer-to-peer protocol version 170004.",
      "Give references Vercauter2009 and AKLGL2010 for the optimal ate pairing. Give references for BLS BLS2002 and BN BN2005 curves. De\ufb01ne KASprout.DerivePublic for Curve25519. Caveat the claim about note traceability set in section1.2 High-level Overview on page 8 and link to Peterson2017 and Quesnelle2017. Do not require a generator as part of the speci\ufb01cation of a represented group; instead, de\ufb01ne it in the represented pairing or scheme using the group. Refactor the abstract de\ufb01nition of a signature scheme to allow derivation of validating keys independent of key pair generation. Correct the explanation in section1.2 High-level Overview on page 8 to apply to Sapling. Add the de\ufb01nition of a signing key to validating key homomorphism for signature schemes. Remove the output index as an input to KDFSapling. Allow dummy Sapling input notes. Specify RedDSA and RedJubjub. Specify Sapling binding signatures and spend authorization signatures. Specify the randomness beacon.",
      "Add outgoing ciphertexts and ock. De\ufb01ne DefaultDiversifier. Change the Spend circuit and Output circuit speci\ufb01cations to remove unintended differences from sapling- crypto. Use \u210eJ to refer to the Jubjub curve cofactor, rather than 8. Correct an error in the \ud835\udc66-coordinate formula for addition in sectionA.3.3.4 Af\ufb01ne-Montgomery arithmetic on page 206 (the constraints were correct). Add acknowledgements for Brian Warner, Mary Maller, and the Least Authority audit. Makefile improvements. 2018.0-beta-15 2018-03-19  Clarify the bit ordering of SHA-256. Drop _t from the names of representation types. Remove functions from the Sprout speci\ufb01cation that it does not use. Updates to transaction format and consensus rules for Overwinter and Sapling. Add speci\ufb01cation of the Output statement. Change MerkleDepthSapling from 29 to 32. Updates to Sapling construction, changing howthe nulli\ufb01er is computed and separating it from the randomized Spend validating key (rk).",
      "Clarify conversions between bit sequences and byte sequences for sk, reprJ(ak), and reprJ(nk). Change the Makefile to avoid multiple reloads in PDF readers while rebuilding the PDF. Spacing and pagination improvements. 2018.0-beta-14 2018-03-11  Only cosmetic changes to Sprout. Simplify FindGroupHashJ(\ud835\udc5f) to use a single-byte index. Changes to diversi\ufb01cation for Pedersen hashes and Pedersen commitments. Improve security de\ufb01nitions for signatures. 2018.0-beta-13 2018-03-11  Only cosmetic changes to Sprout. Change how (ask, nsk) are derived from the spending key sk to ensure they are on the full range of F\ud835\udc5fJ. Change PRFnr to produce output computationally indistinguishable from uniform on F\ud835\udc5fJ. Change UncommittedSapling to be a \ud835\udc62-coordinate for which there is no point on the curve.",
      "Appendix A updates:  categorize components into larger sections  \ufb01ll in the decompression and validation algorithm  more precisely state the assumptions for inputs and outputs  delete not-all-one component which is no longer needed  factor out xor into its own component  specify unpacking more precisely; separate it from boolean constraints  optimize checking for non-small order  notation in variable-base multiplication algorithm. 2018.0-beta-12 2018-03-06  Add references to Overwinter ZIPs and update the section on OverwinterSapling transitions. Add a section on re-randomizable signatures. Add de\ufb01nition of PRFnr. Work-in-progress on Sapling statements. Rename raw to homomorphic Pedersen commitments. Add packing modulo the \ufb01eld size and range checks to Appendix A. Update the algorithm for variable-base scalar multiplication to what is implemented by sapling-crypto. 2018.0-beta-11 2018-02-26  Add sections on Spend descriptions and Output descriptions.",
      "Swap order of cv and rt in a Spend description for consistency. Fix off-by-one error in the range of ivk. 2018.0-beta-10 2018-02-26  Split the descriptions of SHA-256 and SHA256Compress, and of BLAKE2, into their own sections. Specify SHA256Compress more precisely. Add Kexin Hu to acknowledgements (forthe idea of explicitlyencoding the root of the Sapling note commitment tree in block headers). Move bitbyteinteger conversion primitives into section5.1 Integers, Bit Sequences, and Endianness on page 73. Refer to Overwinter and Sapling just as upgrades in the abstract, not as the next minor version and major version. PRFnr must be collision-resistant . Correct an error in the Pedersen hash speci\ufb01cation. Use a named variable, \ud835\udc50, for chunks per segment in the Pedersen hash speci\ufb01cation, and change its value from 61 to 63. Add a proof justifying this value of \ud835\udc50. Specify Pedersen commitments. Notation changes.",
      "Generalize the distinct-\ud835\udc65criterion (Theorem A.3.4 on page 206) to allow negative indices. 2018.0-beta-9 2018-02-10  Specifythe coinbase maturityrule, and the rule that coinbase transactions cannot contain JoinSplit descriptions, Spend descriptions, or Output descriptions. Delay lifting the 100000-byte transaction size limit from Overwinter to Sapling. Improve presentation of the proof of injectivity for ExtractJ(\ud835\udc5f). Specify GroupHashJ(\ud835\udc5f)  Specify Pedersen hashes. 2018.0-beta-8 2018-02-08  Add instantiation of CRHivk. Add instantiation of a hash extractor (later renamed to coordinate extractor) for Jubjub. Make the background lighter and the Sapling green darker, for contrast. 2018.0-beta-7 2018-02-07  Specify the 100000-byte limit on transaction size. (The implementation in zcashd was as intended.)  Specify that 0xF6 followed by 511 zero bytes encodes an empty memo \ufb01eld. Reference security de\ufb01nitions for Pseudo Random Functions and Pseudo Random Generators.",
      "Rename clamp to bound and ActualTimespanClamped to ActualTimespanBounded in the dif\ufb01culty adjustment algorithm, to avoid a name collision with Curve25519 scalar clamping. Change uses of the term full node to full validator. A full node by de\ufb01nition participates in the peer-to-peer network, whereas a full validator just needs a copy of the block chain from somewhere. The latter is what was meant. Add an explanation of how Sapling prevents Faerie Gold and roadblock attacks. Sapling work in progress. 2018.0-beta-6 2018-01-31  Sapling work in progress, mainly on Appendix sectionA Circuit Design on page 200. 2018.0-beta-5 2018-01-30  Specify more precisely the requirements on Ed25519 validating keys and signatures. Sapling work in progress. 2018.0-beta-4 2018-01-25  Update key components diagram for Sapling. 2018.0-beta-3 2018-01-22  Explain how the chosen \ufb01x to Faerie Gold avoids a potential roadblock attack. Update some explanations of changes from Zerocash for Sapling.",
      "Add a description of the Jubjub curve. Add an acknowledgement to George Tankersley. Add an appendix on the design of the Sapling circuits at the quadratic constraint program level. 2017.0-beta-2.9 2017-12-17  Refer to skenc as a receiving key rather than as a viewing key. Updates for incoming viewing key support. Refer to Network Upgrade 0 as Overwinter. 2017.0-beta-2.8 2017-12-02  Correct the non-normative note describing how to check the order of \ud835\udf0b\ud835\udc35. Initial version of draft Sapling protocol speci\ufb01cation. 2017.0-beta-2.7 2017-07-10  Fix an off-by-one error in the speci\ufb01cation of the Equihash algorithm binding condition. (The implementation in zcashd was as intended.)  Correct the types and consensus rules for transaction version numbers and block version numbers. (Again, the implementation in zcashd was as intended.)  Clarify the computation of h\ud835\udc56in a JoinSplit statement. 2017.0-beta-2.6 2017-05-09  Be more precise when talking about curve points and pairing groups.",
      "2017.0-beta-2.5 2017-03-07  Clarify the consensus rule preventing double-spends. Clarify what a note commitment opens to in section8.8 Omission in Zerocash security proof on page 148. Correct the order of arguments to COMM in section5.4.8.1 Sprout Note Commitments on page 95. Correct a statement about indistinguishability of JoinSplit descriptions. Change the Founders Reward addresses, for Testnet only, to re\ufb02ect the hard-fork upgrade described in Zcash-Issue2113. 2017.0-beta-2.4 2017-02-25  Explain a variation on the Faerie Gold attack and why it is prevented. Generalize the description of the InternalH attack to include \ufb01nding collisions on (apk, \u03c1) rather than just on \u03c1. Rename enforce\ud835\udc56to enforceMerklePath\ud835\udc56. 2017.0-beta-2.3 2017-02-12  Specify the security requirements on the SHA256Compress function, in order for the scheme in section5.4.8.1 Sprout Note Commitments on page 95 to be a secure commitment. Specify G2 more precisely.",
      "Explain the use of interstitial treestates in chained JoinSplit transfers. 2017.0-beta-2.2 2017-02-11  Give de\ufb01nitions of computational binding and computational hiding for commitment schemes. Give a de\ufb01nition of statistical zero knowledge. Reference the white paper on MPC parameter generation BGG2017. 2017.0-beta-2.1 2017-02-06  \u2113Merkle is a bit length, not a byte length. Specify the maximum block size. 2017.0-beta-2 2017-02-04  Add abstract and keywords. Fix a typo in the de\ufb01nition of nulli\ufb01er integrity. Make the description of block chains more consistent with upstream Bitcoin documentation (referring to best chains rather than using the concept of a block chain view). De\ufb01ne how nodes select a best valid block chain. 2016.0-beta-1.13 2017-01-20  Specify the dif\ufb01culty adjustment algorithm. Clarify some de\ufb01nitions of \ufb01elds in a block header. De\ufb01ne PRFaddr in section4.2.1 Sprout Key Components on page 36.",
      "2016.0-beta-1.12 2017-01-09  Update the hashes of proving and verifying keys for the \ufb01nal Sprout parameters. Add cross references from shielded payment address and spending key encoding sections to where the key components are speci\ufb01ed. Add acknowledgements for Filippo Valsorda and Zaki Manian. 2016.0-beta-1.11 2016-12-19  Specify a check on the order of \ud835\udf0b\ud835\udc35in a zk-SNARK proof . Note that due to an oversight, the Zcash genesis block does not follow BIP-34. 2016.0-beta-1.10 2016-10-30  Update reference to the Equihash paper BK2016. (The newer version has no algorithmic changes, but the section discussing potential ASIC implementations is substantially expanded.)  Clarify the discussion of proof size in Differences from the Zerocash paper. 2016.0-beta-1.9 2016-10-28  Add Founders Reward addresses for Mainnet. Change protected terminology to shielded.",
      "2016.0-beta-1.8 2016-10-04  Revise the lead bytes for transparent P2SH and P2PKH addresses, and reencode the Testnet Founders Reward addresses. Add a section on which BIPs apply to Zcash. Specify that OP_CODESEPARATOR has been disabled, and no longer affects SIGHASH transaction hashes. Change the representation type of vpub_old and vpub_new to uint64. (This is not a consensus change be- cause the type of vold pub and vnew pub was already speci\ufb01ed to be 0 .. MAX_MONEY; it just better re\ufb02ects the implementation.)  Correct the representation type of the block nVersion \ufb01eld to uint32. 2016.0-beta-1.7 2016-10-02  Clarify the consensus rule for payment of the Founders Reward, in response to an issue raised by the NCC audit. 2016.0-beta-1.6 2016-09-26  Fix an error in the de\ufb01nition of the sortedness condition for Equihash: it is the sequences of indices that are sorted, not the sequences of hashes. Correct the number of bytes in the encoding of solutionSize.",
      "Update the section on encoding of transparent addresses. (The precise pre\ufb01xes are not decided yet.)  Clarify why BLAKE2b-\u2113is different from truncated BLAKE2b-512. Clarify a note about SU-CMA security for signatures. Add a note about PRFnfSprout corresponding to PRFsn in Zerocash. Add a paragraph about key length in section8.7 In-band secret distribution on page 147. Add acknowledgements for John Tromp, Paige Peterson, Maureen Walsh, Jay Graber, and Jack Gavigan. 2016.0-beta-1.5 2016-09-22  Update the Founders Reward address list. Add some clari\ufb01cations based on Eli Ben-Sassons review. 2016.0-beta-1.4 2016-09-19  Specify the block subsidy, miner subsidy, and the Founders Reward. Specify coinbase transaction outputs to Founders Reward addresses. Improve notation (for example  for multiplication and \ud835\udc47\u2113 for sequence types) to avoid ambiguity. 2016.0-beta-1.3 2016-09-16  Correct the omission of solutionSize from the block header format. Document that compactSize encodings must be canonical.",
      "Add a note about conformance language in the introduction. Add acknowledgements for Solar Designer, Ling Ren and Alison Stevenson, and for the NCC Group and Coinspect security audits. 2016.0-beta-1.2 2016-09-11  Remove GeneralCRH in favour of specifying hSigCRH and EquihashGen directly in terms of BLAKE2b-\u2113. Correct the security requirement for EquihashGen. 2016.0-beta-1.1 2016-09-05  Add a speci\ufb01cation of abstract signatures. Clarify what is signed in the Sending Notes section. Specify ZK parameter generation as a randomized algorithm, rather than as a distribution of parameters. 2016.0-beta-1 2016-09-04  Major reorganization to separate the abstract cryptographic protocol from the algorithm instantiations. Add type declarations. Add a High-level Overview section. Add a section specifying the zero-knowledge proving system and the encoding of proofs. Change the encoding of points in proofs to follow IEEE Std 1363a.",
      "Add a section on consensus changes from Bitcoin, and the speci\ufb01cation of Equihash. Complete the Differences from the Zerocash paper section. Correct the Merkle tree depth to 29. Change the length of memo \ufb01elds to 512 bytes. Switch the JoinSplit signature scheme to Ed25519, with consequent changes to the computation of hSig. Fix the lead bytes in shielded payment address and spending key encodings to match the implemented protocol. Add a consensus rule about the ranges of vold pub and vnew pub. Clarify cryptographic security requirements and added de\ufb01nitions relating to the in-band secret distribution. Add various citations: the Fixing Vulnerabilities in the Zcash Protocol and Why Equihash? blog posts, several crypto papers for security de\ufb01nitions, the Bitcoin whitepaper, the CryptoNote whitepaper, and several references to Bitcoin documentation. Reference the extended version of the Zerocash paper rather than the Oakland proceedings version.",
      "Add JoinSplit transfers to the Concepts section. Add a section on Coinbase Transactions. Add acknowledgements for Jack Grigg, Simon Liu, Ariel Gabizon, jl777, Ben Blaxill, Alex Balducci, and Jake Tarren. Fix a Makefile compatibility problem with the escaping behaviour of echo. Switch to biber for the bibliography generation, and add backreferences. Make the date format in references more consistent. Add visited dates to all URLs in references. Terminology changes. 2016.0-alpha-3.1 2016-05-20  Change main font to Quattrocento. 2016.0-alpha-3 2016-05-09  Change version numbering convention (no other changes). 2.0-alpha-3 2016-05-06  Allow anchoring to any previous output treestate in the same transaction, rather than just the immediately preceding output treestate. Add change history. 2.0-alpha-2 2016-04-21  Change from truncated BLAKE2b-512 to BLAKE2b-256. Clarify endianness, and that uses of BLAKE2b are unkeyed. Minor correction to what SIGHASH types cover.",
      "Add as intended for the Zcash release of summer 2016\" to title page. Require PRFaddr to be collision-resistant (see section8.8 Omission in Zerocash security proof on page 148). Add speci\ufb01cation of path computation for the incremental Merkle tree. Add a note in section4.18.1 JoinSplit Statement (Sprout) on page 60 about how this condition corresponds to con- ditions in the Zerocash paper. Changes to terminology around keys. 2.0-alpha-1 2016-03-30  First version intended for public review. ABR1999 Michel Abdalla, Mihir Bellare, and Phillip Rogaway. DHAES: An Encryption Scheme Based on the Dif\ufb01eHellman Problem. Cryptology ePrint Archive: Report 1999007. Received March 17, 1999. September 1998. URL: https:eprint.iacr.org1999007 (visited on 2016-08-21) (p27, 147). ADMA2015 Elena Andreeva, Joan Daemen, Bart Mennink, and Gilles Van Assche. Security of Keyed Sponge Constructions Using a Modular Proof Approach. Team Keccak web page, https:keccak. teampapers.html.",
      "URL: https:keccak.teamfilesModularKeyedSponge.pdf (visited on 2021-03-01). Originally published in Fast Software Encryption - Proceeedings of the 22nd International Workshop (Istanbul, Turkey, March 811, 2015), pages 364384; Springer, 2015. Note that the pre-proceedings version contained an oversight in the analysis of the outer-keyed sponge. (P87). AGRRT2017 Martin Albrecht, Lorenzo Grassi, Christian Rechberger, Arnab Roy, and Tyge Tiessen. MiMC: Ef\ufb01cient Encryption and Cryptographic Hashing with Minimal Multiplicative Complexity. Cryp- tology ePrint Archive: Report 2016492. Received May 21, 2016. January 5, 2017. URL: https: eprint.iacr.org2016492 (visited on 2018-01-12) (p216). AKLGL2010 Diego Aranha, Koray Karabina, Patrick Longa, Catherine Gebotys, and Julio L\u00f3pez. Faster Explicit Formulas for Computing Pairings over Ordinary Curves. Cryptology ePrint Archive: Report 2010526. Last revised September 12, 2011. URL: https:eprint.iacr.org2010526 (visited on 2018-04-03) (p99, 176).",
      "ANWW2013 Jean-Philippe Aumasson, Samuel Neves, Zooko Wilcox, and Christian Winnerlein. BLAKE2: simpler, smaller, fast as MD5. January 29, 2013. URL: https:blake2.netsp (visited on 2016-08-14) (p76, 214). AR2017 Leo Alcock and Ling Ren. A Note on the Security of Equihash. In: CCSW 17. Proceedings of the 2017 Cloud Computing Security Workshop (Dallas, TX, USA, November 3, 2017); post-workshop of the 2017 ACM SIGSAC Conference on Computer and Communications Security. ACM. URL: 170). BBDP2001 Mihir Bellare, Alexandra Boldyreva, Anand Desai, and David Pointcheval. Key-Privacy in Public- Key Encryption. September 2001. URL: https:cseweb.ucsd.edumihirpapersanonenc. pdf (visited on 2021-09-01). Full version. (P27, 78, 147, 157, 172). BBJLP2008 Daniel Bernstein, Peter Birkner, Marc Joye, Tanja Lange, and Christiane Peters. Twisted Edwards Curves. Cryptology ePrint Archive: Report 2008013. Received January 8, 2008. March 13, 2008.",
      "URL: https:eprint.iacr.org2008013 (visited on 2018-01-12) (p206, 207). BCCGLRT2014 Nir Bitansky, Ran Canetti, Alessandro Chiesa, Sha\ufb01Goldwasser, Huijia Lin, Aviad Rubinstein, and Eran Tromer. The Hunting of the SNARK. Cryptology ePrint Archive: Report 2014580. Received July 24, 2014. URL: https:eprint.iacr.org2014580 (visited on 2020-08-01) (p34, 166). BCD2020 Tim Beyne, Anne Canteaut, Itai Dinur, Maria Eichlseder, Gregor Leander, Ga\u00ebtan Leurent, Mar\u00eda Naya-Plasencia, L\u00e9o Perrin, Yu Sasaki, Yosuke Todo, and Friedrich Wiemer. Out of Oddity  New Cryptanalytic Techniques against Symmetric Primitives Optimized for Integrity Proof Systems. Cryptology ePrint Archive: Report 2020188. Last revised November 11, 2020. URL: differences) in Advances in Cryptology - CRYPTO 2020, Vol. 12172 pages 299328; Lecture Notes in Computer Science; Springer, 2020. (P85). BCGGMTV2014 Eli Ben-Sasson, Alessandro Chiesa, Christina Garman, Matthew Green, Ian Miers, Eran Tromer, and Madars Virza.",
      "Zerocash: Decentralized Anonymous Payments from Bitcoin (extended ver- sion). Cryptology ePrint Archive: Report 2014349. Received May 19, 2014. URL: https:eprint. iacr.org2014349 (visited on 2021-04-05). A condensed version appeared in Proceedings of the IEEE Symposium on Security and Privacy (Oakland) 2014, pages 459474; IEEE, 2014. (P7, 8, 10, 24, 26, 51, 60, 66, 143, 145, 146, 148). BCGTV2013 Eli Ben-Sasson, Alessandro Chiesa, Daniel Genkin, Eran Tromer, and Madars Virza. SNARKs for C: Verifying Program Executions Succinctly and in Zero Knowledge. Cryptology ePrint Archive: Report 2013507. Last revised October 7, 2013. URL: https:eprint.iacr.org2013507 (vis- ited on 2016-08-31). An earlier version appeared in Proceedings of the 33rd Annual International Cryptology Conference, CRYPTO 2013, pages 90108; IACR, 2013. (P110). BCIMRT2010 Eric Brier, Jean-S\u00e9bastien Coron, Thomas Icart, David Madore, Hugues Randriam, and Mehdi Tibouchi.",
      "Ef\ufb01cient Indifferentiable Hashing into Ordinary Elliptic Curves. In: Advances in Cryptology - CRYPTO 2010. Proceedings of the 30th Annual International Cryptology Conference (Santa Barbara, California, USA, August 1519, 2010). Ed. by Tal Rabin. Vol. 6223. Lecture Notes in Computer Science. Springer, 2010, pages 237254. ISBN: 978-3-642-14623-7. DOI: 10.1007978- 3-642-14623-7_13. URL: https:www.iacr.orgarchivecrypto20106223023862230238. pdf (visited on 2021-01-27) (p107). BCP1988 Jurgen Bos, David Chaum, and George Purdy. A Voting Scheme. Unpublished. Presented at the rump session of CRYPTO 88 (Santa Barbara, California, USA, August 2125, 1988); does not appear in the proceedings. (p79). BCTV2014a Eli Ben-Sasson, Alessandro Chiesa, Eran Tromer, and Madars Virza. Succinct Non-Interactive Zero Knowledge for a von Neumann Architecture. Cryptology ePrint Archive: Report 2013879. Last revised February 5, 2019. URL: https:eprint.iacr.org2013879 (visited on 2019-02- 08) (p110, 111, 171, 200).",
      "BCTV2014a-old Eli Ben-Sasson, Alessandro Chiesa, Eran Tromer, and Madars Virza. Succinct Non-Interactive Zero Knowledge for a von Neumann Architecture (May 19, 2015 version). Cryptology ePrint Archive: Report 2013879. Version: 20150519:172604. URL: https:eprint.iacr.org2013 87920150519:172604 (visited on 2019-02-08) (p110). BCTV2014b Eli Ben-Sasson, Alessandro Chiesa, Eran Tromer, and Madars Virza. Scalable Zero Knowledge via Cycles of Elliptic Curves (extended version). In: Advances in Cryptology - CRYPTO 2014. Vol. 8617. Lecture Notes in Computer Science. Springer, 2014, pages 276294. URL: https:www. cs.tau.ac.iltromerpapersscalablezk-20140803.pdf (visited on 2016-09-01) (p35, 171). BDEHR2011 Johannes Buchmann, Erik Dahmen, Sarah Ereth, Andreas H\u00fclsing, and Markus R\u00fcckert. On the Security of the Winternitz One-Time Signature Scheme (full version). Cryptology ePrint Archive: Report 2011191. Received April 13, 2011. URL: https:eprint.iacr.org2011191 (visited on 2016-09-05) (p28).",
      "BDJR2000 Mihir Bellare, Anand Desai, Eric Jokipii, and Phillip Rogaway. A Concrete Security Treatment of Symmetric Encryption: Analysis of the DES Modes of Operation. September 2000. URL: https: cseweb.ucsd.edumihirpaperssym- enc.pdf (visited on 2021-09-01). An extended abstract appeared in Proceedings of the 38th Annual Symposium on Foundations of Computer Science (Miami Beach, Florida, USA, October 2022, 1997), pages 394403; IEEE Computer Society Press, 1997; ISBN 0-8186-8197-7. (P26, 157). BDLSY2012 Daniel Bernstein, Niels Duif, Tanja Lange, Peter Schwabe, and Bo-Yin Yang. High-speed high- security signatures. In: Journal of Cryptographic Engineering 2 (September 26, 2011), pages 77 89. URL: https:cr.yp.topapers.htmled25519 (visited on 2021-04-05). Document ID: a1a62a2f76d23f65d622484ddd09caf8. (P91, 159, 221). BDPA2007 Guido Bertoni, Joan Daemen, Micha\u00ebl Peeters, and Gilles Van Assche. Sponge functions.",
      "ECRYPT Hash Workshop (May 2007), also available as a public comment to NIST as part of the Hash Algorithm Requirements and Evaluation Criteria for the SHA-3 competition. URL: https: keccak.teamfilesSpongeFunctions.pdf (visited on 2022-08-31) (p84, 154). BDPA2011 Guido Bertoni, Joan Daemen, Micha\u00ebl Peeters, and Gilles Van Assche. Cryptographic sponge functions. Team Keccak web page, https:keccak.teamsponge_duplex.html. Version 0.1, January 14, 2011. URL: https:keccak.teamfilesCSF-0.1.pdf (visited on 2021-03-01) (p84, 87). Bernstein2001 Daniel Bernstein. Pippengers exponentiation algorithm. December 18, 2001. URL: https: cr.yp.topapers.htmlpippenger (visited on 2018-07-27). Draft. Error pointed out by Sam Hocevar: the example in Figure 4 needs 2 and is thus of length 18. (P221, 222). Bernstein2005 Daniel Bernstein. Understanding brute force. In: ECRYPT STVL Workshop on Symmetric Key Encryption, eSTREAM report 2005036. April 25, 2005. URL: https:cr.yp.topapers.",
      "htmlbruteforce (visited on 2016-09-24). Document ID: 73e92f5b71793b498288efe81fe55dee. (P148). Bernstein2006 Daniel Bernstein. Curve25519: new Dif\ufb01e-Hellman speed records. In: Public Key Cryptogra- phy  PKC 2006. Proceedings of the 9th International Conference on Theory and Practice in Public-Key Cryptography (New York, NY, USA, April 2426, 2006). Springer, February 9, 2006. URL: https:cr.yp.topapers.htmlcurve25519 (visited on 2021-04-05). Document ID: 4230efdfa673480fc079449d90f322c0. (P27, 88, 114, 147). BFIJSV2010 Olivier Blazy, Georg Fuchsbauer, Malika Izabach\u00e8ne, Amandine Jambert, Herv\u00e9 Sibert, and Damien Vergnaud. Batch GrothSahai. Cryptology ePrint Archive: Report 2010040. Last revised February 3, 2010. URL: https:eprint.iacr.org2010040 (visited on 2020-10-17) (p150, 165, 221). BGG-mpc Sean Bowe, Ariel Gabizon, and Matthew Green. GitHub repository zcashmpc: zk-SNARK parameter multi-party computation protocol. URL: https:github.comzcashmpc (visited on 2017-01-06) (p119).",
      "BGG1995 Mihir Bellare, Oded Goldreich, and Sha\ufb01Goldwasser. Incremental Cryptography: The Case of Hashing and Signing. In: Advances in Cryptology - CRYPTO 94. Proceedings of the 14th Annual International Cryptology Conference (Santa Barbara, California, USA, August 2125, 1994). Ed. by Yvo Desmedt. Vol. 839. Lecture Notes in Computer Science. Springer, October 20, 1995, pages 216233. ISBN: 978-3-540-48658-9. DOI: 10.10073- 540- 48658- 5_22. URL: https: cseweb.ucsd.edumihirpapersinc1.pdf (visited on 2018-02-09) (p79, 80, 82, 83, 210). BGG2017 Sean Bowe, Ariel Gabizon, and Matthew Green. A multi-party protocol for constructing the public parameters of the Pinocchio zk-SNARK. Cryptology ePrint Archive: Report 2017602. Last revised June 25, 2017. URL: https:eprint.iacr.org2017602 (visited on 2019-02-10) (p111, 119, 170, 181). BGHOZ2013 Gilles Barthe, Benjamin Gr\u00e9goire, Sylvain Heraud, Frederico Olmedo, and Santiago Zanella-B\u00e9guelin. Veri\ufb01ed indifferentiable hashing into elliptic curves.",
      "In: Journal of Computer Security, Security and Trust Principles 21.6 (2013), pages 881917. URL: https:software.imdea. orgszanellaZanella.2012.POST.pdf (visited on 2021-01-28) (p110). BGM2017 Sean Bowe, Ariel Gabizon, and Ian Miers. Scalable Multi-party Computation for zk-SNARK Parameters in the Random Beacon Model. Cryptology ePrint Archive: Report 20171050. Last revised November 5, 2017. URL: https:eprint.iacr.org20171050 (visited on 2018-08-31) (p111, 119, 170, 172). BIP-11 Gavin Andresen. M-of-N Standard Transactions. Bitcoin Improvement Proposal 11. Created Oc- tober 18, 2011. URL: https:github.combitcoinbipsblobmasterbip-0011.mediawiki (visited on 2020-07-13) (p142). BIP-13 Gavin Andresen. Address Format for pay-to-script-hash. Bitcoin Improvement Proposal 13. Created October 18, 2011. URL: https:github.combitcoinbipsblobmasterbip- 0013.mediawiki (visited on 2020-07-13) (p113, 142). BIP-14 Amir Taaki and Patrick Strateman. Protocol Version and User Agent.",
      "Bitcoin Improvement Proposal 14. Created November 10, 2011. URL: https:github.combitcoinbipsblob masterbip-0014.mediawiki (visited on 2020-07-13) (p142). BIP-16 Gavin Andresen. Pay to Script Hash. Bitcoin Improvement Proposal 16. Created January 3, 2012. URL: https:github.combitcoinbipsblobmasterbip-0016.mediawiki (visited on 2020-07-13) (p142). BIP-30 Pieter Wuille. Duplicate transactions. Bitcoin Improvement Proposal 30. Created February 22, 2012. URL: https:github.combitcoinbipsblobmasterbip-0030.mediawiki (visited on 2020-07-13) (p142). BIP-31 Mike Hearn. Pong message. Bitcoin Improvement Proposal 31. Created April 11, 2012. URL: https: github.combitcoinbipsblobmasterbip-0031.mediawiki (visited on 2020-07-13) (p142). BIP-32 Pieter Wuille. Hierarchical Deterministic Wallets. Bitcoin Improvement Proposal 32. Created February 11, 2012. Last updated January 15, 2014. URL: https:github.combitcoinbips blobmasterbip-0032.mediawiki (visited on 2020-07-13) (p113, 167). BIP-34 Gavin Andresen.",
      "Block v2, Height in Coinbase. Bitcoin Improvement Proposal 34. Created July 6, 2012. URL: https:github.combitcoinbipsblobmasterbip-0034.mediawiki (visited on 2020-07-13) (p125, 142, 181). BIP-35 Jeff Garzik. mempool message. Bitcoin Improvement Proposal 35. Created August 16, 2012. URL: https:github.combitcoinbipsblobmasterbip-0035.mediawiki (visited on 2020-07-13) (p142). BIP-37 Mike Hearn and Matt Corallo. Connection Bloom \ufb01ltering. Bitcoin Improvement Proposal 37. Created October 24, 2012. URL: https:github.combitcoinbipsblobmasterbip- 0037.mediawiki (visited on 2020-07-13) (p142). BIP-61 Gavin Andresen. Reject P2P message. Bitcoin Improvement Proposal 61. Created June 18, 2014. URL: https:github.combitcoinbipsblobmasterbip-0061.mediawiki (visited on 2020-07-13) (p142). BIP-62 Pieter Wuille. Dealing with malleability. Bitcoin Improvement Proposal 62. Withdrawn Novem- ber 17, 2015. URL: https:github.combitcoinbipsblobmasterbip-0062.mediawiki (visited on 2020-07-13) (p29).",
      "BIP-65 Peter Todd. OP_CHECKLOCKTIMEVERIFY . Bitcoin Improvement Proposal 65. Created October 10, 2014. URL: https:github.combitcoinbipsblobmasterbip-0065.mediawiki (visited on 2020-07-13) (p142). BIP-66 Pieter Wuille. Strict DER signatures. Bitcoin Improvement Proposal 66. Created January 10, 2015. URL: https:github.combitcoinbipsblobmasterbip-0066.mediawiki (visited on 2020-07-13) (p142). BIP-68 Mark Friedenbach, BtcDrak, Nicolas Dorier, and kinoshitajona. Relative lock-time using consensus-enforced sequence numbers. Bitcoin Improvement Proposal 68. Last revised Novem- ber 21, 2015. URL: https:github.combitcoinbipsblobmasterbip-0068.mediawiki (visited on 2020-07-13) (p126). BIP-111 Matt Corallo and Peter Todd. NODE_BLOOM service bit. Bitcoin Improvement Proposal 111. Created August 20, 2015. URL: https:github.combitcoinbipsblobmasterbip-0111.mediawiki (visited on 2020-07-13) (p142, 176). BIP-350 Pieter Wuille. Bech32m format for v1 witness addresses. Bitcoin Improvement Proposal 350.",
      "Created December 16, 2020. URL: https:github.combitcoinbipsblobmasterbip- 0350.mediawiki (visited on 2021-03-17) (p113, 118). Bitcoin-Base58 Base58Check encoding  Bitcoin Wiki. URL: https:en.bitcoin.itwikiBase58Check_ encoding (visited on 2020-07-13) (p113, 114). Bitcoin-Block Block Headers  Bitcoin Developer Reference. URL: https :   developer . bitcoin . org  referenceblock_chain.htmlblock-headers (visited on 2020-07-13) (p132, 133). Bitcoin-CbInput Coinbase Input  Bitcoin Developer Reference. URL: https :   developer . bitcoin . org  referencetransactions.htmlcoinbase-input-the-input-of-the-first-transaction- in-a-block (visited on 2022-03-17) (p155). Bitcoin-CoinJoin CoinJoin  Bitcoin Wiki. URL: https:en.bitcoin.itwikiCoinJoin (visited on 2020-07-13) (p9). Bitcoin-Format Raw Transaction Format  Bitcoin Developer Reference. URL: https:developer.bitcoin. org  reference  transactions . html  raw - transaction - format (visited on 2020-07-13) (p127).",
      "Bitcoin-Multisig Transactions: Multisig  Bitcoin Developer Guide. URL: https:developer.bitcoin.org devguidetransactions.htmlmultisig (visited on 2020-07-13) (p138). Bitcoin-nBits Target nBits  Bitcoin DeveloperReference. URL: https:developer.bitcoin.orgreference block_chain.htmltarget-nbits (visited on 2020-07-13) (p131, 136). Bitcoin-P2PKH Transactions: P2PKH Script Validation  Bitcoin Developer Guide. URL: https:developer. bitcoin.orgdevguidetransactions.htmlp2pkh-script-validation (visited on 2020-07- 13) (p113). Bitcoin-P2SH Transactions: P2SH Scripts  Bitcoin Developer Guide. URL: https:developer.bitcoin.org devguidetransactions.htmlpay-to-script-hash-p2sh (visited on 2020-07-13) (p113, 140). Bitcoin-Protocol Protocol documentation  Bitcoin Wiki. URL: https:en.bitcoin.itwikiProtocol_ documentation (visited on 2020-07-13) (p8). Bitcoin-SigHash Signature Hash Types  Bitcoin Developer Guide.",
      "URL: https:developer.bitcoin.org devguidetransactions.htmlsignature-hash-types (visited on 2020-07-13) (p50). BJLSY2015 Daniel Bernstein, Simon Josefsson, Tanja Lange, Peter Schwabe, and Bo-Yin Yang. EdDSA for more curves. Technical Report. July 4, 2015. URL: https:cr.yp.topapers.htmleddsa (visited on 2018-01-22) (p92, 103). BK2016 Alex Biryukov and Dmitry Khovratovich. Equihash: Asymmetric Proof-of-Work Based on the Generalized Birthday Problem (full version). Cryptology ePrint Archive: Report 2015946. Last revised October 27, 2016. URL: https:eprint.iacr.org2015946 (visited on 2016-10-30) (p10, 133, 182). BKR2001 Mihir Bellare, Joe Kilian, and Phillip Rogaway. The Security of the Cipher Block Chaining Message Authentication Code. In: Journal of Computer and System Sciences 61.3 (December 2000), pages 362399. DOI: 10.1006jcss.1999.1694. URL: https:cseweb.ucsd.edumihir paperscbc.pdf (visited on 2021-03-08). Updated September 12, 2001. (P26).",
      "BL-SafeCurves Daniel Bernstein and Tanja Lange. SafeCurves: choosing safe curves for elliptic-curve cryptogra- phy. URL: https:safecurves.cr.yp.to (visited on 2018-01-29) (p147, 192). BL2017 Daniel Bernstein and Tanja Lange. Montgomery curves and the Montgomery ladder. Cryptology ePrint Archive: Report 2017293. Received March 30, 2017. URL: https:eprint.iacr.org 2017293 (visited on 2017-11-26) (p102, 200, 206, 207). BLS2002 Paulo Barreto, Ben Lynn, and Michael Scott. Constructing Elliptic Curves with Prescribed Em- bedding Degrees. Cryptology ePrint Archive: Report 2002088. Last revised February 22, 2005. URL: https:eprint.iacr.org2002088 (visited on 2018-04-20) (p101, 176). BN2005 Paulo Barreto and Michael Naehrig. Pairing-Friendly Elliptic Curves of Prime Order. Cryptology ePrint Archive: Report 2005133. Last revised February 28, 2006. URL: https:eprint.iacr. org2005133 (visited on 2018-04-20) (p99, 176). BN2007 Mihir Bellare and Chanathip Namprempre.",
      "Authenticated Encryption: Relations among notions and analysis of the generic composition paradigm. Cryptology ePrint Archive: Report 2000025. Last revised July 14, 2007. URL: https:eprint.iacr.org2000025 (visited on 2016-09-02) (p26). Bowe-bellman Sean Bowe. bellman: zk-SNARK library. URL: https:github.comebfullbellman (visited on 2018-04-03) (p111, 119). Bowe2017 Sean Bowe. ebfullpairing source code, BLS12-381  README.md as of commit e726600. URL: srcbls12_381 (visited on 2017-07-16) (p101). Bowe2018 Sean Bowe. Random Beacon. March 22, 2018. URL: https:github.comZcashFoundation powersoftau-attestationstreemaster0088 (visited on 2018-04-08) (p120). Carroll1876 Lewis Carroll. The Hunting of the Snark. With illustrations by Henry Holiday. MacMillan and Co. London. March 29, 1876. URL: https:www.gutenberg.orgfiles2988829888-h29888- h.htm (visited on 2018-05-23) (p102, 169). Carroll1902 Lewis Carroll. Through the Looking-Glass, and What Alice Found There (1902 edition).",
      "Illustrated by Peter Newell and Robert Murray Wright. Harper and Brothers Publishers. New York. October 1902. URL: https:archive.orgdetailsthroughlookinggl00carr4 (visited on 2018-06-20) (p150, 175). CDvdG1987 David Chaum, Ivan Damg\u00e5rd, and Jeroen van de Graaf. Multiparty computations ensuring privacy of each partys input and correctness of the result. In: Advances in Cryptology - CRYPTO 87. Proceedings of the 14th Annual International Cryptology Conference (Santa Barbara, California, USA, August 1620, 1987). Ed. by Carl Pomerance. Vol. 293. Lecture Notes in Computer Science. Springer, January 1988, pages 87119. ISBN: 978-3-540-48184-3. DOI: 10.10073-540- 48184- 2_7. URL: https:link.springer.comcontentpdf10.10072F3- 540- 48184- 2_7.pdf (visited on 2022-08-31) (p79, 154). Cook2019 John D. Cook. What is an isogeny? Blog post. April 21, 2019. URL: https:www.johndcook.com blog20190421what-is-an-isogeny (visited on 2021-02-10) (p107). CVE-2019-7167 Common Vulnerabilities and Exposures.",
      "CVE-2019-7167. URL: https:cve.mitre.orgcgi- bincvename.cgi?nameCVE-2019-7167 (visited on 2019-02-05) (p110). CvHP1991 David Chaum, Eug\u00e8ne van Heijst, and Birgit P\ufb01tzmann. Cryptographically Strong Undeniable Signatures, Unconditionally Secure for the Signer. February 1991. URL: https:citeseerx. ist.psu.eduviewdocsummary?doi10.1.1.34.8570 (visited on 2021-04-05). An extended abstract appeared in Advances in Cryptology - CRYPTO 91: Proceedings of the 11th Annual International Cryptology Conference (Santa Barbara, California, USA, August 1115, 1991); Ed. by Joan Feigenbaum; Vol. 576, Lecture Notes in Computer Science, pages 470484; Springer, 1992; ISBN 978-3-540-55188-1. (P79, 210). Dalek-notes Cathie Yun, Henry de Valence, Oleg Andreev, and Dimitris Apostolou. Dalek bulletproofs notes, module r1cs_proof. URL: https:doc- internal.dalek.rsbulletproofsnotesr1cs_ proofindex.html (visited on 2021-04-07) (p54, 172). Damg\u00e5rd1989 Ivan Damg\u00e5rd. A Design Principle for Hash Functions.",
      "In: Advances in Cryptology - CRYPTO 89. Proceedings of the 9th Annual International Cryptology Conference (Santa Barbara, California, USA, August 2024, 1989). Ed. by Giles Brassard. Vol. 435. Lecture Notes in Computer Science. Springer, 1990, pages 416427. ISBN: 978-0-387-34805-6. DOI: 10.10070-387-34805-0_39. URL: https:link.springer.comchapter10.10070-387-34805-0_39 (visited on 2022-01- 19) (p145). deRooij1995 Peter de Rooij. Ef\ufb01cient exponentiation using precomputation and vector addition chains. In: Advances in Cryptology - EUROCRYPT 94. Proceedings, Workshop on the Theory and Appli- cation of Cryptographic Techniques (Perugia, Italy, May 912, 1994). Ed. by Alfredo De Santis. Vol. 950. Lecture Notes in Computer Science. Springer, pages 389399. ISBN: 978-3-540-60176-0. DOI: 10.1007BFb0053453. URL: https:link.springer.comchapter10.1007BFb0053453 (visited on 2018-07-27) (p221, 222). DGKM2011 Dana Dachman-Soled, Rosario Gennaro, Hugo Krawczyk, and Tal Malkin.",
      "Computational Extrac- tors and Pseudorandomness. Cryptology ePrint Archive: Report 2011708. December 28, 2011. URL: https:eprint.iacr.org2011708 (visited on 2016-09-02) (p147). DigiByte-PoW DigiByte Core Developers. DigiSpeed 4.0.0 source code, functions GetNextWorkRequiredV34 in srcmain.cpp as of commit 178e134. URL: https:github.comdigibytedigibyteblob 178e1348a67d9624db328062397fde0de03fe388srcmain.cppL1587 (visited on 2017-01-20) (p134). DKLS2020 Orr Dunkelman, Abhishek Kumar, Eran Lambooij, and Somitra Kumar Sanadhya. Cryptanalysis of Feistel-Based Format-Preserving Encryption. Cryptology ePrint Archive: Report 20201311. Received October 20, 2020. URL: https:eprint.iacr.org20201311 (visited on 2023-03- 02) (p88, 153). DS2016 David Derler and Daniel Slamanig. Key-Homomorphic Signatures and Applications to Multiparty Signatures and Non-Interactive Zero-Knowledge. Cryptology ePrint Archive: Report 2016792. Last revised February 6, 2017.",
      "URL: https:eprint.iacr.org2016792 (visited on 2018-04-09) (p30). DSDCOPS2001 Alfredo De Santis, Giovanni Di Crescenzo, Rafail Ostrovsky, Guiseppe Persiano, and Amit Sahai. Robust Non-Interactive Zero Knowledge. In: Advances in Cryptology - CRYPTO 2001. Proceedings of the 21st Annual International Cryptology Conference (Santa Barbara, California, USA, August 1923, 2001). Ed. byJoe Kilian. Vol. 2139. Lecture Notes in ComputerScience. Springer, 2001, pages 566598. ISBN: 978-3-540-42456-7. DOI: 10.10073-540-44647-8_33. URL: https: www.iacr.orgarchivecrypto200121390566.pdf (visited on 2018-05-28) (p35, 50). ECCZF2019 Electric Coin Company and Zcash Foundation. Zcash Trademark Donation and License Agree- ment. November 6, 2019. URL: https:electriccoin.cowp-contentuploads201911 Final-Consolidated-Version-ECC-Zcash-Trademark-Transfer-Documents-1.pdf (visited on 2022-06-22) (p154). ElGamal1985 Taher ElGamal. A public key cryptosystem and a signature scheme based on discrete logarithms.",
      "In: IEEE Transactions on Information Theory 31.4 (July 1985), pages 469472. ISSN: 0018-9448. DOI: 10.1109TIT.1985.1057074. URL: https:people.csail.mit.edualinush6.857- spring-2015paperselgamal.pdf (visited on 2018-08-17) (p78). EWD-340 Edsger W. Dijkstra. The Humble Programmer. ACM Turing Lecture. August 14, 1972. URL: 2021-03-29) (p23). EWD-831 Edsger W. Dijkstra. Why numbering should start at zero. Manuscript. August 11, 1982. URL: 2016-08-09) (p10). FFSTV2013 Reza Farashahi, Pierre-Alain Fouque, Igor Shparlinski, Mehdi Tibouchi, and J. Felipe Voloch. Indifferentiable deterministic hashing to elliptic and hyperelliptic curves. In: Mathematics of Computation 82 (2013), pages 491512. DOI: 10.1090S0025- 5718- 2012- 02606- 8. URL: 2021-01-27) (p110). FKMSSS2016 Nils Fleischhacker, Johannes Krupp, Giulio Malavolta, Jonas Schneider, Dominique Schr\u00f6der, and Mark Simkin. Ef\ufb01cient Unlinkable Sanitizable Signatures from Signatures with Re- Randomizable Keys.",
      "Cryptology ePrint Archive: Report 2012159. Last revised February 11, 2016. URL: https:eprint.iacr.org2015395 (visited on 2018-03-03). An extended abstract appeared in Public Key Cryptography  PKC 2016: 19th IACR International Conference on Practice and Theory in Public-Key Cryptography (Taipei, Taiwan, March 69, 2016), Proceedings, Part 1; Ed. by Chen-Mou Cheng, Kai-Min Chung, Giuseppe Persiano, and Bo-Yin Yang; Vol. 9614, Lecture Notes in Computer Science, pages 301330; Springer, 2016; ISBN 978-3-662-49384-7. (P29, 30, 92). Gabizon2019 Ariel Gabizon. On the security of the BCTV Pinocchio zk-SNARK variant. Draft. February 5, 2019. URL: https:github.comarielgabizonbctvblobmasterbctv.pdf (visited on 2019-02-07) (p110, 150, 171). GG2015 Shoni Gilboa and Shay Gueron. Distinguishing a truncated random permutation from a random function. Cryptology ePrint Archive: Report 2015773. Received August 3, 2015. URL: https: eprint.iacr.org2015773 (visited on 2021-03-01) (p87, 160).",
      "GGM2016 Christina Garman, Matthew Green, and Ian Miers. Accountable Privacy for Decentralized Anonymous Payments. Cryptology ePrint Archive: Report 2016061. Last revised January 24, 2016. URL: https:eprint.iacr.org2016061 (visited on 2016-09-02) (p144). GKRRS2019 Lorenzo Grassi, Dmitry Khovratovich, Christian Rechberger, Arnab Roy, and Markus Schofnegger. Poseidon: A New Hash Function for Zero-Knowledge Proof Systems. Cryptology ePrint Archive: Report 2019458. Last updated December 16, 2020. URL: https:eprint.iacr.org2019458 (visited on 2021-02-28) (p84, 85, 87). GPT2015 Peter Gazi, Krzysztof Pietrzak, and Stefano Tessaro. The Exact PRF Security of Truncation: Tight Bounds for Keyed Sponges and Truncated CBC. In: Advances in Cryptology - CRYPTO 2015. Proceedings of the 35th Annual International Cryptology Conference (Santa Barbara, California, USA, August 1620, 2015), Part I. Ed. by Rosario Gennaro and Matthew Robshaw. Vol. 9215. Lecture Notes in Computer Science.",
      "Springer, August 1, 2015, pages 368387. ISBN: 978-3-662-47989-6. DOI: 10.1007978-3-662-47989-6_18. URL: https:iacr.orgcryptodbdatapaper.php? pubkey27279 (visited on 2021-03-01) (p87). Groth2016 Jens Groth. On the Size of Pairing-based Non-interactive Arguments. Cryptology ePrint Archive: Report 2016260. Last revised May 31, 2016. URL: https:eprint.iacr.org2016260 (visited on 2017-08-03) (p111, 112, 170, 222). GRS2020 Lorenzo Grassi, Christian Rechberger, and Markus Schofnegger. Proving Resistance Against In\ufb01nitely Long Subspace Trails: How to Choose the Linear Layer. Cryptology ePrint Archive: Report 2020500. Last revised January 27, 2021. URL: https:eprint.iacr.org2020500 (visited on 2021-03-23) (p85). GWC2019 Ariel Gabizon, Zachary Williamson, and Oana Ciobotaru. PLONK: Permutations over Lagrange- bases for Oecumenical Noninteractive arguments of Knowledge. Cryptology ePrint Archive: Report 2019953. Last revised September 3, 2020.",
      "URL: https:eprint.iacr.org2019953 (visited on 2021-01-28) (p150). Hamdon2018 Elise Hamdon. Sapling Activation Complete. Electric Coin Company blog. June 28, 2018. URL: (p120). H\u0131s\u0131l2010 H\u00fcseyin H\u0131s\u0131l. Elliptic Curves, Group Law, and Ef\ufb01cient Computation. PhD thesis. Queensland University of Technology, 2010. URL: https:core.ac.ukdownloadpdf10898289.pdf (visited on 2021-04-08) (p105). Hopwood2018 Daira-Emma Hopwood. GitHub repository dairajubjub: Supporting evidence for security of the Jubjub curve to be used in Zcash. URL: https:github.comdairajubjub (visited on 2018-02-18). Based on code written for SafeCurves BL-SafeCurves by Daniel Bernstein and Tanja Lange. (P147). Hopwood2020 Daira-Emma Hopwood. GitHub repository zcashpasta: Generator and supporting evidence for security of the PallasVesta pair of elliptic curves suitable for Halo. URL: https:github.com zcashpasta (visited on 2021-03-23). Based on code written for SafeCurves BL-SafeCurves by Daniel Bernstein and Tanja Lange.",
      "(P108, 147). Hopwood2022 Daira-Emma Hopwood. Explaining the Security of Zcash. Presentation at Zcon3. Slides and a link to the video are available at: GitHub repository dairazcash-security: Code and documentation supporting security analysis of Zcash. URL: https:github.comdairazcash- security (visited on 2023-10-30) (p144, 153). HW2016 Taylor Hornby and Zooko Wilcox. Fixing Vulnerabilities in the Zcash Protocol. Electric Coin Company blog. April 26, 2016. URL: https:electriccoin.coblogfixing-zcash-vulns (visited on 2019-08-27). Updated December 26, 2017. (P145). ID-hashtocurve Armando Faz-Hern\u00e1ndez, Sam Scott, Nick Sullivan, Riad Wahby, and Christopher Wood. Internet Draft: Hashing to Elliptic Curves, version 10. Internet Research Task Force (IRTF) Crypto Forum Research Group (CFRG). Work in progress. Last revised December 22, 2020. URL: https:www. ietf.orgarchiveiddraft-irtf-cfrg-hash-to-curve-10.html (visited on 2021-01-27) (p34, 107, 108, 110, 162). IEEE2000 IEEE Computer Society.",
      "IEEE Std 1363-2000: Standard Speci\ufb01cations for Public-Key Cryptog- raphy. IEEE, August 29, 2000. DOI: 10.1109IEEESTD.2000.92292. URL: https:ieeexplore. ieee.orgdocument891000 (visited on 2021-04-05) (p100). IEEE2004 IEEE Computer Society. IEEE Std 1363a-2004: Standard Speci\ufb01cations for Public-Key Cryptogra- phy  Amendment 1: Additional Techniques. IEEE, September 2, 2004. DOI: 10.1109IEEESTD. 2004.94612. URL: https:ieeexplore.ieee.orgdocument1335427 (visited on 2021-04-05) (p100, 147, 149). Jedusor2016 Tom Elvis Jedusor. Mimblewimble. July 19, 2016. URL: https:diyhpl.usbryanpapers2 bitcoinmimblewimble.txt (visited on 2021-04-05) (p54). JT2020 Joseph Jaeger and Stefano Tessaro. Expected-Time Cryptography: Generic Techniques and Applications to Concrete Soundness. Cryptology ePrint Archive: Report 20201213. Received October 2, 2020. URL: https:eprint.iacr.org20201213 (visited on 2021-05-19) (p83, 160). KR2020 Nathan Keller and Asaf Rosemarin.",
      "Mind the Middle Layer: The HADES Design Strategy Revisited. Cryptology ePrint Archive: Report 2020179. Received February 13, 2020. URL: https:eprint. iacr.org2020179 (visited on 2021-03-01) (p85). KT2015 Taechan Kim and Mehdi Tibouchi. Improved Elliptic Curve Hashing and Point Representation. In: Proceedings of WCC2015 - 9th International Workshop on Coding and Cryptography (Paris, France, April 2015). Ed. by Anne Canteaut, Ga\u00ebtan Leurent, and Maria Naya-Plasencia. URL: KvE2013 Kaa1el and Hagen von Eitzen. If a group \ud835\udc3ahas odd order, then the square function is injective (answer). Mathematics Stack Exchange. URL: https:math.stackexchange.coma522277 185422 (visited on 2018-02-08). Version: 2013-10-11. (P104). KYMM2018 George Kappos, Haaroon Yousaf, Mary Maller, and Sarah Meiklejohn. An Empirical Analysis of Anonymity in Zcash. Preprint, to be presented at the 27th Usenix Security Syposium (Baltimore, Maryland, USA, August 1517, 2018). May 8, 2018. URL: https:smeiklej.comfilesusenix18.",
      "pdf (visited on 2018-06-05) (p9). LG2004 Eddie Lenihan and Carolyn Eve Green. Meeting the Other Crowd: The Fairy Stories of Hidden Ireland. TarcherPerigee, February 2004, pages 109110. ISBN: 1-58542-206-1 (p143). LGR2021 Julia Len, Paul Grubbs, and Thomas Ristenpart. Partitioning Oracle Attacks. In: Proceedings of the 30th USENIX Security Symposium (USENIX Security 21, August 1113, 2021). USENIX Association, August 2021, pages 195212. ISBN: 978-1-939133-24-3. URL: https:www.usenix. orgconferenceusenixsecurity21presentationlen (visited on 2021-10-12) (p148, 156). librustzcash-109 Jack Grigg. librustzcash PR 109: PaymentAddress encapsulation. URL: https:github.com zcashlibrustzcashpull109 (visited on 2023-08-25) (p71, 165). libsodium libsodium documentation. URL: https:libsodium.org (visited on 2020-03-02) (p92). libsodium-Seal Sealed boxes  libsodium. URL: https:download.libsodium.orgdocpublic- key_ cryptographysealed_boxes.html (visited on 2016-02-01) (p147).",
      "LM2017 Philip Lafrance and Alfred Menezes. On the security of the WOTS-PRF signature scheme. Cryp- tology ePrint Archive: Report 2017938. Last revised February 5, 2018. URL: https:eprint. iacr.org2017938 (visited on 2018-04-16) (p28). M\u00c1E\u00c12010 V. Gayoso Mart\u00ednez, F. Hern\u00e1ndez \u00c1lvarez, L. Hern\u00e1ndez Encinas, and C. S\u00e1nchez \u00c1vila. A Comparison of the Standardized Versions of ECIES. In: Proceedings of Sixth International Conference on Information Assurance and Security (Atlanta, Georgia, USA, August 2325, 2010). IEEE, 2010, pages 14. ISBN: 978-1-4244-7407-3. DOI: 10.1109ISIAS.2010.5604194. URL: https:citeseerx.ist.psu.eduviewdocsummary?doi10.1.1.819.9345 (visited on 2021-04-08) (p147). Maller2018 Mary Maller. A Proof of Security for the Sapling Generation of zk-SNARK Parameters in the Generic Group Model. November 16, 2018. URL: https :   github . com  zcash  sapling - security-analysisblobmasterMaryMallerUpdated.pdf (visited on 2018-02-10) (p111, 170). ISO2015 ISOIEC.",
      "International Standard ISOIEC 18004:2015(E): Information Technology  Automatic identi\ufb01cation and data capture techniques  QR Code bar code symbology speci\ufb01cation. Third edition. February 1, 2015. URL: https:raw.githubusercontent.comyansikeimQR-Code masterISO20IEC201800420201520Standard.pdf (visited on 2021-03-22) (p113). MRH2003 Ueli Maurer, Renato Renner, and Clemens Holenstein. Indifferentiability, Impossibility Results on Reductions, and Applications to the Random Oracle Methodology. Cryptology ePrint Archive: Report 2003161. Received August 8, 2003. September 2003. URL: https:eprint.iacr.org 2003161 (visited on 2021-02-10) (p110). Nakamoto2008 Satoshi Nakamoto. Bitcoin: A Peer-to-Peer Electronic Cash System. October 31, 2008. URL: 2022-06-17) (p7, 154). NIST2015 NIST. FIPS 180-4: Secure Hash Standard (SHS). August 2015. DOI: 10.6028NIST.FIPS.180-4. URL: https:csrc.nist.govpublicationsdetailfips1804final (visited on 2021-03- 08) (p75, 113). NIST2016 NIST.",
      "NIST SP 800-38G  Recommendation for Block Cipher Modes of Operation: Methods for Format-Preserving Encryption. March 2016. DOI: 10.6028NIST.SP.800-38G. URL: https: nvlpubs.nist.govnistpubsSpecialPublicationsNIST.SP.800-38G.pdf (visited on 2021-03-08) (p88). Parno2015 Bryan Parno. A Note on the Unsoundness of vnTinyRAMs SNARK. Cryptology ePrint Archive: Report 2015437. Received May 6, 2015. URL: https:eprint.iacr.org2015437 (visited on 2019-02-08) (p110, 150, 171). Peterson2017 Paige Peterson. Transaction Linkability. Electric Coin Company blog. January 25, 2017. URL: 176). PHGR2013 Bryan Parno, Jon Howell, Craig Gentry, and Mariana Raykova. Pinocchio: Nearly Practical Veri\ufb01- able Computation. Cryptology ePrint Archive: Report 2013279. Last revised May 13, 2013. URL: Poseidon-1.1 Lorenzo Grassi, Dmitry Khovratovich, Christian Rechberger, Arnab Roy, and Markus Schofnegger. Poseidon reference implementation, Version 1.1. March 7, 2021. URL: https:extgit.iaik.",
      "tugraz.atkryptohadeshash-commit7ecf9a7d4f37e777ea27e4c4d379443151270563 (vis- ited on 2021-03-23) (p84, 157). Poseidon-Zc1.1 Lorenzo Grassi, Dmitry Khovratovich, Christian Rechberger, Arnab Roy, Markus Schofnegger, and Daira-Emma Hopwood. Poseidon reference implementation, Zcash fork, Version 1.1. July 28, 2021. URL: https:github.comdairapasta-hadeshash (visited on 2021-07-29) (p84, 157). Quesnelle2017 Jeffrey Quesnelle. On the linkability of Zcash transactions. arXiv:1712.01210 cs.CR. December 4, 2017. URL: https:arxiv.orgabs1712.01210 (visited on 2018-04-15) (p9, 176). RFC-2119 Scott Bradner. Request for Comments 7693: Key words for use in RFCs to Indicate Requirement Levels. Internet Engineering Task Force (IETF). March 1997. URL: https:www.rfc-editor.org rfcrfc2119.html (visited on 2016-09-14) (p7). RFC-7539 Yoav Nir and Adam Langley. Request for Comments 7539: ChaCha20 and Poly1305 for IETF Protocols. Internet Research Task Force (IRTF). May 2015.",
      "URL: https:www.rfc-editor.org rfcrfc7539.html (visited on 2016-09-02). As modi\ufb01ed by veri\ufb01ed errata at https:www.rfc- editor.orgerrata_search.php?rfc7539 (visited on 2016-09-02). (P88). RFC-8032 Simon Josefsson and Ilari Liusvaara. Request for Comments 8032: Edwards-Curve Digital Sig- nature Algorithm (EdDSA). Internet Engineering Task Force (IETF). January 2017. URL: https: www.rfc-editor.orgrfcrfc8032.html (visited on 2020-07-06). As corrected by errata at RIPEMD160 Hans Dobbertin, Antoon Bosselaers, and Bart Preneel. RIPEMD-160, a strengthened version of RIPEMD. URL: https:homes.esat.kuleuven.bebosselaeripemd160.html (visited on 2021-04-05) (p113). ST1999 Tomas Sander and Amnon Ta-Shma. Auditable, Anonymous Electronic Cash. In: Advances in Cryptology - CRYPTO 99. Proceedings of the 19th Annual International Cryptology Conference (Santa Barbara, California, USA, August 1519, 1999). Ed. by Michael Wiener. Vol. 1666. Lecture Notes in Computer Science. Springer, 1999, pages 555572.",
      "ISBN: 978-3-540-66347-8. DOI: 10.10073-540-48405-1_35. URL: https:link.springer.comcontentpdf10.10073- 540-48405-1_35.pdf (visited on 2018-06-05) (p150, 174). Sutherland2021 Andrew Sutherland. MIT Open Courseware, Mathematics 18.783 Elliptic Curves, Lecture Notes. Massachusetts Institute of Technology. Spring 2021. March 1, 2021. URL: https:ocw.mit. educoursesmathematics18-783-elliptic-curves-spring-2021lecture-notes-and- worksheetsindex.htm (visited on 2022-01-01) (p107). SvdW2006 Andrew Shallue and Christiaan E. van de Woestijne. Construction of Rational Points on Elliptic Curves over Finite Fields. In: Algorithmic Number Theory: 7th International Symposium, ANTS- VII (Berlin, Germany, July 2328, 2006). Ed. by Florian Hess, Sebastian Pauli, and Michael Pohst. Vol. 4076. Lecture Notes in Computer Science. Springer, 2006, pages 510524. ISBN: 978-3- 540-36076-6. DOI: 10.100711792086_36. URL: https:digitalcommons.iwu.edumath_ scholarship72 (visited on 2021-01-28) (p107).",
      "SVPBABW2012 Srinath Setty, Victor Vu, Nikhil Panpalia, Benjamin Braun, Muqeet Ali, Andrew J. Blumberg, and Michael Wal\ufb01sh. Taking proof-based veri\ufb01ed computation a few steps closer to practicality (extended version). Cryptology ePrint Archive: Report 2012598. Last revised February 28, 2013. URL: https:eprint.iacr.org2012598 (visited on 2018-04-25) (p202). SWB2019 Josh Swihart, Benjamin Winston, and Sean Bowe. Zcash Counterfeiting Vulnerability Successfully Remediated. February 5, 2019. URL: https:electriccoin.coblogzcash-counterfeiting- vulnerability-successfully-remediated (visited on 2019-08-27) (p110, 171). Swihart2018 Josh Swihart. Overwinter Activated Successfully. Electric Coin Company blog. June 26, 2018. URL: https:electriccoin.coblogoverwinter-activated-successfully (visited on 2021-01-10) (p120). Ulas2007 Maciej Ulas. Rational Points on Certain Hyperelliptic Curves over Finite Fields. In: Bulletin of the Polish Academy of Sciences - Mathematics 55.2 (2007), pages 97104.",
      "DOI: 10.4064ba55-2-1. URL: https:www.impan.plshoppublicationtransactiondownloadproduct85475 (visited on 2021-01-27) (p107). vanSaberh2014 Nicolas van Saberhagen. CryptoNote v 2.0. Date disputed. URL: https:bytecoin.orgold whitepaper.pdf (visited on 2021-04-07) (p9). Vercauter2009 Frederik Vercauteren. Optimal pairings. CryptologyePrint Archive: Report 2008096. Last revised March 7, 2008. URL: https:eprint.iacr.org2008096 (visited on 2018-04-06). A version of this paper appeared in IEEE Transactions of Information Theory, Vol. 56, pages 455461; IEEE, 2009. (P99, 176). WB2019 Riad Wahby and Dan Boneh. Fast and simple constant-time hashing to the BLS12-381 elliptic curve. Cryptology ePrint Archive: Report 2018403. Last revised September 30, 2019. URL: https: eprint.iacr.org2019403 (visited on 2021-01-27) (p107, 110). WCBTV2015 Zooko Wilcox, Alessandro Chiesa, Eli Ben-Sasson, Eran Tromer, and Madars Virza. A Bug in libsnark. Least Authority blog. May 16, 2015.",
      "URL: https:leastauthority.combloga-bug- in-libsnark (visited on 2021-04-07) (p110, 200). WG2016 Zooko Wilcox and Jack Grigg. Why Equihash? Electric Coin Company blog. April 15, 2016. URL: 2019. (P133). Zaverucha2012 Gregory M. Zaverucha. Hybrid Encryption in the Multi-User Setting. Cryptology ePrint Archive: Report 2012159. Received March 20, 2012. URL: https:eprint.iacr.org2012159 (visited on 2016-09-24) (p148). Zcash-Blossom Electric Coin Company. Blossom. December 11, 2019. URL: https:z.cashupgradeblossom (visited on 2021-01-10) (p120). Zcash-Canopy Electric Coin Company. Canopy. November 18, 2020. URL: https:z.cashupgradecanopy (visited on 2021-01-10) (p120). Zcash-halo2 Daira-Emma Hopwood, Sean Bowe, Jack Grigg, Kris Nuttycombe, Ying Tong Lai, and Steven Smith. The halo2 Book. URL: https:zcash.github.iohalo2 (visited on 2021-03-23) (p109, 112, 120, 161, 163). Zcash-Heartwd Electric Coin Company. Heartwood. July 16, 2020.",
      "URL: https:z.cashupgradeheartwood (visited on 2021-01-10) (p120). Zcash-Issue2113 Simon Liu. GitHub repository  zcashzcash: Issue 2113  Upgrade testnet to \ufb01x bug in test and update fr addresses. URL: https:github.comzcashzcashissues2113 (visited on 2017-02-20) (p138, 180). Zcash-libsnark libsnark: C library for zkSNARK proofs (Zcash fork). URL: https:github.comzcashzcash treev2.0.7-3srcsnark (visited on 2021-04-07) (p110). Zcash-Nu5 Electric Coin Company. Network Upgrade 5. May 31, 2022. URL: https:z.cashupgradenu5 (visited on 2022-05-11) (p120). Zcash-Nu6 Electric Coin Company. Network Upgrade 6. August 20, 2025. URL: https:z.cashupgrade nu6 (visited on 2025-08-20) (p120). Zcash-Nu6.1 Electric Coin Company. Network Upgrade 6.1. August 20, 2025. URL: https:z.cashupgrade nu6.1 (visited on 2025-08-20) (p120). Zcash-Orchard Daira-Emma Hopwood, Sean Bowe, Jack Grigg, Kris Nuttycombe, Ying Tong Lai, and Steven Smith. The Orchard Book.",
      "URL: https:zcash.github.ioorchard (visited on 2021-03-02) (p82, 85, 98, 120, 144, 160, 161). zcashd-6459 Jack Grigg and Daira-Emma Hopwood. zcashd PR 6459: Migrate to zcash_primitives 0.10. URL: zcashd-6725 Jack Grigg. zcashd PR 6725: Retroactively use Rust to decrypt shielded coinbase before soft fork. URL: https:github.comzcashzcashpull6725 (visited on 2023-08-25) (p71, 165). ZIP-32 Jack Grigg and Daira-Emma Hopwood. Shielded Hierarchical Deterministic Wallets. Zcash Improvement Proposal 32. URL: https:zips.z.cashzip-0032 (visited on 2019-08-28) (p13, 25, 26, 37, 38, 39, 44, 67, 79, 88, 94, 120, 151, 156, 162, 167, 169, 175). ZIP-48 Kris Nuttycombe, Jack Grigg, Daira-Emma Hopwood, and Arya. Transparent Multisig Wallets. Zcash Improvement Proposal 48. URL: https:zips.z.cashzip-0048 (visited on 2025-10-08) (p140). ZIP-76 Jack Grigg and Daira-Emma Hopwood. Transaction Signature Validation before Overwinter. Zcash Improvement Proposal 76 (in progress). (P50, 142).",
      "ZIP-143 Jack Grigg and Daira-Emma Hopwood. Transaction Signature Validation for Overwinter. Zcash Improvement Proposal 143. Created December 27, 2017. URL: https:zips.z.cashzip-0143 (visited on 2019-08-28) (p50, 76, 120). ZIP-173 Daira-Emma Hopwood. Bech32 Format. Zcash Improvement Proposal 173. Created June 13, 2018. URL: https:zips.z.cashzip-0173 (visited on 2020-06-01) (p113, 116, 167). ZIP-200 Jack Grigg. Network Upgrade Mechanism. Zcash Improvement Proposal 200. Created January 8, 2018. URL: https:zips.z.cashzip-0200 (visited on 2019-08-28) (p120, 126, 155). ZIP-201 Simon Liu and Daira-Emma Hopwood. Network Peer Management for Overwinter. Zcash Im- provement Proposal 201. Created January 15, 2018. URL: https:zips.z.cashzip- 0201 (visited on 2019-08-28) (p50, 120, 121). ZIP-202 Simon Liu and Daira-Emma Hopwood. Version 3 Transaction Format for Overwinter. Zcash Improvement Proposal 202. Created January 10, 2018. URL: https:zips.z.cashzip-0202 (visited on 2019-08-28) (p120).",
      "ZIP-203 Jay Graber and Daira-Emma Hopwood. Transaction Expiry. Zcash Improvement Proposal 203. Created January 9, 2018. URL: https:zips.z.cashzip-0203 (visited on 2019-08-28) (p120, 122, 123, 159). ZIP-205 Simon Liu and Daira-Emma Hopwood. Deployment of the Sapling Network Upgrade. Zcash Improvement Proposal 205. Created October 8, 2018. URL: https:zips.z.cashzip-0205 (visited on 2019-08-28) (p50, 120, 136). ZIP-206 Simon Liu and Daira-Emma Hopwood. Deployment of the Blossom Network Upgrade. Zcash Improvement Proposal 206. Created July 29, 2019. URL: https:zips.z.cashzip- 0206 (visited on 2019-08-28) (p50, 120, 169). ZIP-207 Jack Grigg and Daira-Emma Hopwood. Funding Streams. Zcash Improvement Proposal 207. Created January 4, 2019. URL: https:zips.z.cashzip-0207 (visited on 2019-08-28) (p58, 120, 125, 139, 140, 166, 167). ZIP-208 Daira-Emma Hopwood and Simon Liu. Shorter Block Target Spacing. Zcash Improvement Proposal 208. Created January 10, 2019.",
      "URL: https:zips.z.cashzip- 0208 (visited on 2019-08-28) (p120, 136, 170). ZIP-209 Sean Bowe and Daira-Emma Hopwood. Prohibit Negative Shielded Value Pool Balances. Zcash Improvement Proposal 209. Created February 25, 2019. URL: https:zips.z.cashzip-0209 (visited on 2020-11-05) (p58, 120, 159, 164). ZIP-211 Daira-Emma Hopwood. Disabling Addition of New Value to the Sprout Value Pool. Zcash Im- provement Proposal 211. Created March 29, 2019. URL: https:zips.z.cashzip-0211 (visited on 2020-06-01) (p44, 117, 120, 165, 167). ZIP-212 Sean Bowe. Allow Recipient to Derive Sapling Ephemeral Secret from Note Plaintext. Zcash Improvement Proposal 212. Created March 31, 2019. URL: https:zips.z.cashzip-0212 (visited on 2020-06-01) (p16, 65, 120, 126, 148, 153, 157, 159, 161, 167). ZIP-213 Jack Grigg. Shielded Coinbase. Zcash Improvement Proposal 213. Created March 30, 2019. URL: ZIP-214 Daira-Emma Hopwood. Consensus rules for a Zcash Development Fund. Zcash Improvement Proposal 214.",
      "Created February 28, 2020. URL: https:zips.z.cashzip-0214 (visited on 2020-03-24) (p120, 140, 141, 165, 166, 167). ZIP-215 Henry de Valence. Explicitly De\ufb01ning and Modifying Ed25519 Validation Rules. Zcash Improve- ment Proposal 215. Created April 27, 2020. URL: https:zips.z.cashzip-0215 (visited on 2020-05-27) (p120, 167, 223). ZIP-216 Jack Grigg and Daira-Emma Hopwood. Require Canonical Point Encodings. Zcash Improvement Proposal 216. Created February 11, 2021. URL: https:zips.z.cashzip-0216 (visited on 2021-02-25) (p41, 42, 94, 116, 120, 165). ZIP-221 Ying Tong Lai, James Prestwich, Georgios Konstantopoulos, and Jack Grigg. FlyClient - Consensus- Layer Changes. Zcash Improvement Proposal 221. Created March 30, 2019. URL: https:zips. z.cashzip-0221 (visited on 2020-03-19) (p120, 131, 132, 133). ZIP-224 Daira-Emma Hopwood, Jack Grigg, Sean Bowe, Kris Nuttycombe, and Ying Tong Lai. Orchard Shielded Protocol. Zcash Improvement Proposal 224. Created February 27, 2021.",
      "URL: https: zips.z.cashzip-0225 (visited on 2021-03-21) (p120). ZIP-225 Daira-Emma Hopwood, Jack Grigg, Sean Bowe, Kris Nuttycombe, and Ying Tong Lai. Version 5 Transaction Format. Zcash Improvement Proposal 225. Created February 28, 2021. URL: https: zips.z.cashzip-0225 (visited on 2021-03-21) (p55, 57, 120, 162). ZIP-236 Daira-Emma Hopwood. Blocks should balance exactly. Zcash Improvement Proposal 236. Cre- ated July 2, 2024. URL: https:zips.z.cashzip-0236 (visited on 2024-09-24) (p120, 151, 152). ZIP-239 Daira-Emma Hopwood and Jack Grigg. Relay of Version 5 Transactions. Zcash Improvement Proposal 239. Created May 29, 2021. URL: https:zips.z.cashzip-0239 (visited on 2021-06- 06) (p18, 120, 124, 157, 159). ZIP-243 Jack Grigg and Daira-Emma Hopwood. Transaction Signature Validation for Sapling. Zcash Improvement Proposal 243. Created April 10, 2018. URL: https:zips.z.cashzip-0243 (visited on 2019-08-28) (p50, 51, 53, 57, 76, 120, 160).",
      "ZIP-244 Kris Nuttycombe, Daira-Emma Hopwood, and Jack Grigg. Transaction Identi\ufb01er Non-Malleability. Zcash Improvement Proposal 244. Created January 6, 2021. URL: https:zips.z.cashzip- 0244 (visited on 2021-01-10) (p18, 51, 53, 55, 57, 76, 120, 124, 127, 131, 132, 133). ZIP-250 Daira-Emma Hopwood. Deployment of the Heartwood Network Upgrade. Zcash Improvement Proposal 250. Created February 28, 2020. URL: https:zips.z.cashzip-0250 (visited on 2020-03-20) (p50, 120). ZIP-251 Daira-Emma Hopwood. Deployment of the Canopy Network Upgrade. Zcash Improvement Proposal 251. Created February 28, 2020. URL: https:zips.z.cashzip-0251 (visited on 2020-03-24) (p51, 120, 167). ZIP-252 teor and Daira-Emma Hopwood. Deployment of the NU5 Network Upgrade. Zcash Improvement Proposal 252. Created February 23, 2021. URL: https:zips.z.cashzip-0252 (visited on 2022-06-22) (p51, 120, 154). ZIP-253 Arya. Deployment of the NU6 Network Upgrade. Zcash Improvement Proposal 253. Created July 17, 2024.",
      "URL: https:zips.z.cashzip-0253 (visited on 2024-09-24) (p51, 120). ZIP-255 Arya. Deployment of the NU6.1 Network Upgrade. Zcash Improvement Proposal 255. Created May 6, 2025. URL: https:zips.z.cashzip-0255 (visited on 2025-08-20) (p51, 120). ZIP-271 Daira-Emma Hopwood, Kris Nuttycombe, and Jack Grigg. Deferred Dev Fund Lockbox Disburse- ment. Zcash Improvement Proposal 271. Created February 19, 2025. URL: https:zips.z.cash zip-0271 (visited on 2025-08-20) (p120, 125, 136, 139, 140, 151). ZIP-302 Jay Graber and Jack Grigg. Standardized Memo Field Format. Zcash Improvement Proposal 302. Created February 8, 2017. URL: https:zips.z.cashzip-0302 (visited on 2022-06-22) (p15, 154, 163, 168). ZIP-316 Daira-Emma Hopwood, Nathan Wilcox, Taylor Hornby, Jack Grigg, Sean Bowe, Kris Nuttycombe, Greg Pfeil, and Ying Tong Lai. Uni\ufb01ed Addresses and Uni\ufb01ed Viewing Keys. Zcash Improvement Proposal 316. Created April 7, 2021.",
      "URL: https:zips.z.cashzip-0316 (visited on 2021-04- 29) (p25, 116, 117, 118, 119, 120, 152, 156, 160). ZIP-1014 Andrew Miller and Zooko Wilcox. Establishing a Dev Fund for ECC, ZF, and Major Grants. Zcash Improvement Proposal 1014. Created November 10, 2019. URL: https:zips.z.cashzip-1014 (visited on 2024-09-24) (p120). ZIP-1015 Jason McGee, Peacemonger, and Kris Nuttycombe. Block Subsidy Allocation for Non-Direct Development Funding. Zcash Improvement Proposal 1015. Created August 26, 2024. URL: https: zips.z.cashzip-1015 (visited on 2024-09-24) (p120). ZIP-1016 Josh Swihart. Community and Coinholder Funding Model. Zcash Improvement Proposal 1016. Created February 19, 2025. URL: https:zips.z.cashzip-1016 (visited on 2025-08-20) (p120, 139). ZIP-2001 Kris Nuttycombe. Lockbox Funding Streams. Zcash Improvement Proposal 2001. Created July 2, 2024. URL: https:zips.z.cashzip-2001 (visited on 2024-09-24) (p120, 139, 152). Zips-Issue664 Daira-Emma Hopwood.",
      "GitHub repository zcashzips: Issue 664  protocol spec ZIP 216 Sapling pk_d should not allow the zero point. URL: https:github.comzcashzipsissues 664 (visited on 2025-09-07) (p152, 165). Appendices Circuit Design Quadratic Constraint Programs Sapling de\ufb01nes two circuits, Spend and Output, each implementing an abstract statement described in section4.18.2 Spend Statement (Sapling) on page 61 and section4.18.3 Output Statement (Sapling) on page 62 respectively. It also adds a Groth16 circuit for the JoinSplit statement described in section4.18.1 JoinSplit Statement (Sprout) on page 60. At the next lower level, each circuit is de\ufb01ned in terms of a quadratic constraint program (specifying a Rank 1 Constraint System), as detailed in this section. In the BCTV14 or Groth16 proving systems, this program is translated to a Quadratic Arithmetic Program BCTV2014a, section 2.3 WCBTV2015.",
      "The circuit descriptions given here are necessary to compute witness elements for each circuit, as well as the proving and verifying keys. Let F\ud835\udc5fS be the \ufb01nite \ufb01eld over which Jubjub is de\ufb01ned, as given in section5.4.9.3 Jubjub on page 102. A quadratic constraint program consists of a set of constraints over variables in F\ud835\udc5fS, each of the form: where , and are linear combinations of variables and constants in F\ud835\udc5fS. Here and  both represent multiplication in the \ufb01eld F\ud835\udc5fS, but we use for multiplications corresponding to gates of the circuit, and  for multiplications by constants in the terms of a linear combination. should not be confused with  which is de\ufb01ned as cartesian product in section2 Notation on page 10. Elliptic curve background The Sapling circuits make use of a complete twisted Edwards elliptic curve (ctEdwards curve) Jubjub, de\ufb01ned in section5.4.9.3 Jubjub on page 102, and also a Montgomery elliptic curve M that is birationally equivalent to Jubjub.",
      "Following the notation in BL2017 we use (\ud835\udc62, v) for af\ufb01ne coordinates on the ctEdwards curve, and (\ud835\udc65, \ud835\udc66) for af\ufb01ne coordinates on the Montgomery curve. A point \ud835\udc43is normally represented by two F\ud835\udc5fS variables, which we name as (\ud835\udc43\ud835\udc62, \ud835\udc43v) for an af\ufb01ne-ctEdwards point, for instance. The implementations of scalar multiplication require the scalar to be represented as a bit sequence. We there- fore allow the notation \ud835\udc58 \ud835\udc43meaning LEBS2IPlength(\ud835\udc58)(\ud835\udc58) \ud835\udc43. There will be no ambiguity because variables representing bit sequences are named with a suf\ufb01x. The Montgomery curve M has parameters \ud835\udc34M  40962 and \ud835\udc35M  1. We use an af\ufb01ne representation of this curve with the formula: \ud835\udc35M\ud835\udc662  \ud835\udc653  \ud835\udc34M\ud835\udc652  \ud835\udc65 Usually, elliptic curve arithmetic over prime \ufb01elds is implemented using some form of projective coordinates, in order to reduce the number of expensive inversions required. In the circuit, it turns out that a division can be implemented at the same cost as a multiplication, i.e. one constraint.",
      "Therefore it is bene\ufb01cial to use af\ufb01ne coordinates for both curves. We de\ufb01ne the following types representing af\ufb01ne-ctEdwards and af\ufb01ne-Montgomery coordinates respectively: AffineCtEdwardsJubjub : (\ud835\udc62 F\ud835\udc5fS)  (v F\ud835\udc5fS) : \ud835\udc4eJ\ud835\udc622  v2  1  \ud835\udc51J\ud835\udc622v2 AffineMontJubjub : (\ud835\udc65 F\ud835\udc5fS)  (\ud835\udc66 F\ud835\udc5fS) : \ud835\udc35M\ud835\udc662  \ud835\udc653  \ud835\udc34M\ud835\udc652  \ud835\udc65 We also de\ufb01ne a type representing compressed, not necessarily valid, ctEdwards coordinates: CompressedCtEdwardsJubjub : (\ud835\udc62 B)  (v F\ud835\udc5fS) See section5.4.9.3 Jubjub on page 102 for how this type is represented as a byte sequence in external encodings. We use af\ufb01ne-Montgomery arithmetic in parts of the circuit because it is more ef\ufb01cient, in terms of the number of constraints, than af\ufb01ne-ctEdwards arithmetic. An important consideration when using Montgomery arithmetic is that the addition formula is not complete, that is, there are cases where it produces the wrong answer. We must ensure that these cases do not arise. We will need the theorem below about \ud835\udc66-coordinates of points on Montgomery curves.",
      "Fact: 2 4 is a nonsquare in F\ud835\udc5fS. Theorem A.2.1. (0, 0) is the only point with \ud835\udc66 0 on certain Montgomery curves. Let \ud835\udc43 (\ud835\udc65, \ud835\udc66) be a point other than (0, 0) on a Montgomery curve \ud835\udc38Mont(\ud835\udc34,\ud835\udc35) over F\ud835\udc5f, such that \ud835\udc342 4 is a nonsquare in F\ud835\udc5f. Then \ud835\udc66 0. Proof. Substituting \ud835\udc66 0 into the Montgomery curve equation gives 0  \ud835\udc653  \ud835\udc34 \ud835\udc652  \ud835\udc65 \ud835\udc65 (\ud835\udc652  \ud835\udc34 \ud835\udc65 1). So either \ud835\udc65 0 or \ud835\udc652  \ud835\udc34 \ud835\udc65 1  0. Since \ud835\udc43 (0, 0), the case \ud835\udc65 0 is excluded. In the other case, complete the square for \ud835\udc652  \ud835\udc34 \ud835\udc65 1  0 to give the equivalent (2  \ud835\udc65 \ud835\udc34)2  \ud835\udc342 4. The left-hand side is a square, so if the right-hand side is a nonsquare, then there are no solutions for \ud835\udc65. Circuit Components Each of the following sections describes how to implement a particular component of the circuit, and counts the number of constraints required. Some components make use of others; the order of presentation is bottom-up.",
      "It is important for security to ensure that variables intended to be of boolean type are boolean-constrained; and for ef\ufb01ciency that they are boolean-constrained only once. We explicitly state for the boolean inputs and outputs of each component whether they are boolean-constrained by the component, or are assumed to have been boolean-constrained separately. Af\ufb01ne coordinates for elliptic curve points are assumed to represent points on the relevant curve, unless otherwise speci\ufb01ed. In this section, variables have type F\ud835\udc5fS unless otherwise speci\ufb01ed. In contrast to most of this document, we use zero-based indexing in order to more closely match the implementation. A.3.1 Operations on individual bits A.3.1.1 Boolean constraints A boolean constraint \ud835\udc4fB can be implemented as: 1 \ud835\udc4f A.3.1.2 Conditional equality The constraint either \ud835\udc4e 0 or \ud835\udc4f \ud835\udc50 can be implemented as: A.3.1.3 Selection constraints A selection constraint (\ud835\udc4f?",
      "\ud835\udc65: \ud835\udc66)  \ud835\udc67, where \ud835\udc4f B has been boolean-constrained, can be implemented as: A.3.1.4 Nonzero constraints Since only nonzero elements of F\ud835\udc5fS have a multiplicative inverse, the assertion \ud835\udc4e 0 can be implemented by witnessing the inverse, \ud835\udc4einv  \ud835\udc4e1 (mod \ud835\udc5fS): \ud835\udc4einv This technique comes from SVPBABW2012, Appendix D.1. Non-normative note: A global optimization allows to use a single inverse computation outside the circuit for any number of nonzero constraints. Suppose that we have \ud835\udc5bvariables (or linear combinations) that are supposed to be nonzero: \ud835\udc4e0 .. \ud835\udc5b1. Multiply these together (using \ud835\udc5b1 constraints) to give \ud835\udc4e  \ud835\udc5b1 \ud835\udc560 \ud835\udc4e\ud835\udc56; then, constrain \ud835\udc4e to be nonzero. This works because the product \ud835\udc4e is nonzero if and only if all of \ud835\udc4e0 .. \ud835\udc5b1 are nonzero. However, the Sapling circuit does not use this optimization. A.3.1.5 Exclusive-or constraints An exclusive-or operation \ud835\udc4e\ud835\udc4f \ud835\udc50, where \ud835\udc4e, \ud835\udc4f B are already boolean-constrained, can be implemented in one constraint as: \ud835\udc4e \ud835\udc4f\ud835\udc50 This automatically boolean-constrains \ud835\udc50.",
      "Its correctness can be seen by checking the truth table of (\ud835\udc4e, \ud835\udc4f). A.3.2 Operations on multiple bits A.3.2.1 Unpacking modulo \ud835\udc5fS Let \ud835\udc5b N be a constant. The operation of converting a \ufb01eld element, \ud835\udc4e F\ud835\udc5fS, to a sequence of boolean variables \ud835\udc4f0 .. \ud835\udc5b1 B\ud835\udc5b such that \ud835\udc4e \ud835\udc5b1 \ud835\udc560 \ud835\udc4f\ud835\udc56 2\ud835\udc56(mod \ud835\udc5fS), is called unpacking. The inverse operation is called packing. In the quadratic constraint program these are the same operation (but see the note about canonical representation below). We assume that the variables \ud835\udc4f0 .. \ud835\udc5b1 are boolean-constrained separately. We have \ud835\udc4emod \ud835\udc5fS  (\ud835\udc5b1 \ud835\udc4f\ud835\udc56 2\ud835\udc56 mod \ud835\udc5fS  (\ud835\udc5b1 \ud835\udc4f\ud835\udc56 (2\ud835\udc56mod \ud835\udc5fS) mod \ud835\udc5fS. This can be implemented in one constraint: (\ud835\udc5b1 \ud835\udc4f\ud835\udc56 (2\ud835\udc56mod \ud835\udc5fS) Notes:  The bit length \ud835\udc5bis not limited by the \ufb01eld element size. Since the constraint has only a trivial multiplication, it is possible to eliminate it by merging it into the boolean constraint of one of the output bits, expressing that bit as a linear combination of the others and \ud835\udc4e.",
      "However, this optimization requires substitutions that would interfere with the modularity of the circuit implementation (for a saving of only one constraint per unpacking operation), and so we do not use it for the Sapling circuit. In the case \ud835\udc5b 255, for \ud835\udc4e 2255 \ud835\udc5fS there are two possible representations of \ud835\udc4e F\ud835\udc5fS as a sequence of 255 bits, corresponding to I2LEBSP255(\ud835\udc4e) and I2LEBSP255(\ud835\udc4e \ud835\udc5fS). This is a potential hazard, but it may or may not be necessary to force use of the canonical representation I2LEBSP255(\ud835\udc4e), depending on the context in which the unpacking operation is used. We therefore do not consider this to be part of the unpacking operation itself. A.3.2.2 Range check Let \ud835\udc5b N be a constant, and let \ud835\udc4e \ud835\udc5b1 \ud835\udc560 \ud835\udc4e\ud835\udc56 2\ud835\udc56 N. Suppose we want to constrain \ud835\udc4e\ud835\udc50for some constant \ud835\udc50 \ud835\udc5b1 \ud835\udc560 \ud835\udc50\ud835\udc56 2\ud835\udc56 Without loss of generality we can assume that \ud835\udc50\ud835\udc5b1  1, because if it were not then we would decrease \ud835\udc5baccordingly.",
      "Note that since \ud835\udc4eand \ud835\udc50are provided in binary representation, their bit length \ud835\udc5bis not limited by the \ufb01eld element size. We do not assume that the bits \ud835\udc4e0 .. \ud835\udc5b1 are already boolean-constrained. De\ufb01ne \u03a0\ud835\udc5a \ud835\udc5b1 \ud835\udc56\ud835\udc5a(\ud835\udc50\ud835\udc56 0 \ud835\udc4e\ud835\udc56 1) for \ud835\udc5a0 .. \ud835\udc5b1. Notice that for any \ud835\udc5a \ud835\udc5b1 such that \ud835\udc50\ud835\udc5a 0, we have \u03a0\ud835\udc5a \u03a0\ud835\udc5a1, and so it is only necessary to allocate separate variables for the \u03a0\ud835\udc5asuch that \ud835\udc5a \ud835\udc5b1 and \ud835\udc50\ud835\udc5a 1. Furthermore if \ud835\udc50\ud835\udc5b2 .. 0 has \ud835\udc61 0 trailing 1 bits, then we do not need to allocate variables for \u03a00 .. \ud835\udc611 because those variables will not be used below. More explicitly: Let \u03a0\ud835\udc5b1  \ud835\udc4e\ud835\udc5b1. For \ud835\udc56from \ud835\udc5b2 down to \ud835\udc61,  if \ud835\udc50\ud835\udc56 0, then let \u03a0\ud835\udc56 \u03a0\ud835\udc561;  if \ud835\udc50\ud835\udc56 1, then constrain \u03a0\ud835\udc561 Then we constrain the \ud835\udc4e\ud835\udc56as follows: For \ud835\udc56from \ud835\udc5b1 down to 0,  if \ud835\udc50\ud835\udc56 0, constrain 1 \u03a0\ud835\udc561 \ud835\udc4e\ud835\udc56  if \ud835\udc50\ud835\udc56 1, boolean-constrain \ud835\udc4e\ud835\udc56as in sectionA.3.1.1 Boolean constraints on page 201. Note that the constraints corresponding to zero bits of \ud835\udc50are in place of boolean constraints on bits of \ud835\udc4e\ud835\udc56.",
      "This costs \ud835\udc5b \ud835\udc58constraints, where \ud835\udc58is the number of non-trailing 1 bits in \ud835\udc50\ud835\udc5b2 .. 0. Theorem A.3.1. Correctness of a constraint system for range checks. Assume \ud835\udc500 .. \ud835\udc5b1 B\ud835\udc5b and \ud835\udc50\ud835\udc5b1  1. De\ufb01ne \ud835\udc34\ud835\udc5a: \ud835\udc5b1 \ud835\udc56\ud835\udc5a\ud835\udc4e\ud835\udc56 2\ud835\udc56and \ud835\udc36\ud835\udc5a: \ud835\udc5b1 \ud835\udc56\ud835\udc5a\ud835\udc50\ud835\udc56 2\ud835\udc56. For any \ud835\udc5a0 .. \ud835\udc5b1, \ud835\udc34\ud835\udc5a\ud835\udc36\ud835\udc5aif and only if the restriction of the above constraint system to \ud835\udc56\ud835\udc5a.. \ud835\udc5b1 is satis\ufb01ed. Furthermore the system at least boolean-constrains \ud835\udc4e0 .. \ud835\udc5b1. Proof. For \ud835\udc560 .. \ud835\udc5b1 such that \ud835\udc50\ud835\udc56 1, the corresponding \ud835\udc4e\ud835\udc56are unconditionally boolean-constrained. This implies that the system constrains \u03a0\ud835\udc56B for all \ud835\udc560 .. \ud835\udc5b1. For \ud835\udc560 .. \ud835\udc5b1 such that \ud835\udc50\ud835\udc56 0, the constraint 1 \u03a0\ud835\udc561 \ud835\udc4e\ud835\udc56 constrains \ud835\udc4e\ud835\udc56to be 0 if \u03a0\ud835\udc561  1, otherwise it constrains \ud835\udc4e\ud835\udc56B. So all of \ud835\udc4e0 .. \ud835\udc5b1 are at least boolean-constrained. To prove the rest of the theorem we proceed by induction on decreasing \ud835\udc5a, i.e. taking successively longer pre\ufb01xes of the big-endian binary representations of \ud835\udc4eand \ud835\udc50.",
      "Base case \ud835\udc5a \ud835\udc5b1: since \ud835\udc50\ud835\udc5b1  1, the constraint system has just one boolean constraint on \ud835\udc4e\ud835\udc5b1, which ful\ufb01ls the theorem since \ud835\udc34\ud835\udc5b1 \ud835\udc36\ud835\udc5b1 is always satis\ufb01ed. Inductive case \ud835\udc5a \ud835\udc5b1:  If \ud835\udc34\ud835\udc5a1  \ud835\udc36\ud835\udc5a1, then by the inductive hypothesis the constraint system must fail, which ful\ufb01ls the theorem regardless of the value of \ud835\udc4e\ud835\udc5a. If \ud835\udc34\ud835\udc5a1 \ud835\udc36\ud835\udc5a1, then by the inductive hypothesis the constraint system restricted to \ud835\udc56\ud835\udc5a 1 .. \ud835\udc5b1 succeeds. We have \u03a0\ud835\udc5a1  \ud835\udc5b1 \ud835\udc56\ud835\udc5a1(\ud835\udc50\ud835\udc56 0 \ud835\udc4e\ud835\udc56 1)  \ud835\udc5b1 \ud835\udc56\ud835\udc5a1(\ud835\udc4e\ud835\udc56\ud835\udc50\ud835\udc56). If \ud835\udc34\ud835\udc5a1  \ud835\udc36\ud835\udc5a1, then \ud835\udc4e\ud835\udc56 \ud835\udc50\ud835\udc56for all \ud835\udc56\ud835\udc5a 1 .. \ud835\udc5b1 and so \u03a0\ud835\udc5a1  1. Also \ud835\udc34\ud835\udc5a\ud835\udc36\ud835\udc5aif and only if \ud835\udc4e\ud835\udc5a\ud835\udc50\ud835\udc5a. When \ud835\udc50\ud835\udc5a 1, only a boolean constraint is added for \ud835\udc4e\ud835\udc5awhich ful\ufb01ls the theorem. When \ud835\udc50\ud835\udc5a 0, \ud835\udc4e\ud835\udc5ais constrained to be 0 which ful\ufb01ls the theorem. If \ud835\udc34\ud835\udc5a1  \ud835\udc36\ud835\udc5a1, then it cannot be the case that \ud835\udc4e\ud835\udc56\ud835\udc50\ud835\udc56for all \ud835\udc56\ud835\udc5a 1 .. \ud835\udc5b1, so \u03a0\ud835\udc5a1  0. This implies that the constraint on \ud835\udc4e\ud835\udc5ais always equivalent to a boolean constraint, which ful\ufb01ls the theorem because \ud835\udc34\ud835\udc5a\ud835\udc36\ud835\udc5amust be true regardless of the value of \ud835\udc4e\ud835\udc5a. This covers all cases.",
      "Correctness of the full constraint system follows by taking \ud835\udc5a 0 in the above theorem. The algorithm in sectionA.3.3.2 ctEdwards decompression and validation on page 205 uses range checks with \ud835\udc50 \ud835\udc5fS1 to validate ctEdwards compressed encodings. In that case \ud835\udc5b 255 and \ud835\udc58 132, so the cost of each such range check is 387 constraints. Non-normative note: It is possible to optimize the computation of \u03a0\ud835\udc61.. \ud835\udc5b2 further. Notice that \u03a0\ud835\udc5ais only used when \ud835\udc5ais the index of the last bit of a run of 1 bits in \ud835\udc50. So for each such run of 1 bits \ud835\udc50\ud835\udc5a.. \ud835\udc5a\ud835\udc412 of length \ud835\udc411, it is suf\ufb01cient to compute an \ud835\udc41-ary AND of \ud835\udc4e\ud835\udc5a.. \ud835\udc5a\ud835\udc412 and \u03a0\ud835\udc5a\ud835\udc411: \ud835\udc45 \ud835\udc411 \ud835\udc560 \ud835\udc4b\ud835\udc56. This can be computed in 3 constraints for any \ud835\udc41; boolean-constrain the output \ud835\udc45, and then add constraints \ud835\udc41\ud835\udc411 \ud835\udc560 \ud835\udc4b\ud835\udc56 1 \ud835\udc45 to enforce that \ud835\udc411 \ud835\udc560 \ud835\udc4b\ud835\udc56 \ud835\udc41when \ud835\udc45 0; \ud835\udc41\ud835\udc411 \ud835\udc560 \ud835\udc4b\ud835\udc56 to enforce that \ud835\udc411 \ud835\udc560 \ud835\udc4b\ud835\udc56 \ud835\udc41when \ud835\udc45 1. where inv is witnessed as \ud835\udc41\ud835\udc411 \ud835\udc560 \ud835\udc4b\ud835\udc56 1 if \ud835\udc45 0 or is unconstrained otherwise.",
      "(Since \ud835\udc41 \ud835\udc5fS, the sums cannot over\ufb02ow.) In fact the last constraint is not needed in this context because it is suf\ufb01cient to compute an upper bound on each \u03a0\ud835\udc5a(i.e. it does not bene\ufb01t a malicious prover to witness \ud835\udc45 1 when the result of the AND should be 0). So the cost of computing \u03a0 variables for an arbitrarily long run of 1 bits can be reduced to 2 constraints. For example, for \ud835\udc50 \ud835\udc5fS 1 the overall cost would be reduced to 255  68  323 constraints. These optimizations are not used in Sapling. A.3.3 Elliptic curve operations A.3.3.1 Checking that Af\ufb01ne-ctEdwards coordinates are on the curve To check that (\ud835\udc62, v) is a point on the ctEdwards curve, the Sapling circuit uses 4 constraints: \ud835\udc62\ud835\udc62vv \ud835\udc4eJ\ud835\udc62\ud835\udc62 vv 1  \ud835\udc51J\ud835\udc62\ud835\udc62vv Non-normative note: The last two constraints can be combined into \ud835\udc51J\ud835\udc62\ud835\udc62 \ud835\udc4eJ\ud835\udc62\ud835\udc62 vv 1 . The Sapling circuit does not use this optimization.",
      "A.3.3.2 ctEdwards decompression and validation De\ufb01ne DecompressValidate CompressedCtEdwardsJubjub AffineCtEdwardsJubjub as follows: DecompressValidate(\ud835\udc62, v) :  Prover supplies the \ud835\udc62-coordinate. Let \ud835\udc62 F\ud835\udc5fS. sectionA.3.3.1 Checking that Af\ufb01ne-ctEdwards coordinates are on the curve on page 205. Check that (\ud835\udc62, v) is a point on the ctEdwards curve. sectionA.3.2.1 Unpacking modulo \ud835\udc5fS on page 202. Unpack \ud835\udc62to 254 \ud835\udc560\ud835\udc62\ud835\udc56 2\ud835\udc56, equating \ud835\udc62with \ud835\udc620. sectionA.3.2.2 Range check on page 203. Check that 254 \ud835\udc560\ud835\udc62\ud835\udc56 2\ud835\udc56\ud835\udc5fS 1. Return (\ud835\udc62, v). This costs 4 constraints for the curve equation check, 1 constraint for the unpacking, and 387 constraints for the range check (as computed in sectionA.3.2.2 Range check on page 203) for a total of 392 constraints. The cost of the range check includes boolean-constraining \ud835\udc620 .. 254. The same quadratic constraint program is used for compression and decompression. Non-normative note: The point-on-curve check could be omitted if (\ud835\udc62, v) were already known to be on the curve.",
      "However, the Sapling circuit never omits it; this provides a consistency check on the elliptic curve arithmetic. A.3.3.3 ctEdwards Montgomery conversion De\ufb01ne the notation  as in section2 Notation on page 10. De\ufb01ne CtEdwardsToMont AffineCtEdwardsJubjub AffineMontJubjub as follows: CtEdwardsToMont(\ud835\udc62, v)  (1  v 1 v, 40964  1  v (1 v)  \ud835\udc62 1 v  0 and \ud835\udc62 0 De\ufb01ne MontToCtEdwards AffineMontJubjub AffineCtEdwardsJubjub as follows: MontToCtEdwards(\ud835\udc65, \ud835\udc66)  40964  \ud835\udc65 \ud835\udc66, \ud835\udc651 \ud835\udc65 1 \ud835\udc65 1  0 and \ud835\udc66 0 Either of these conversions can be implemented by the same quadratic constraint program: 40964  \ud835\udc65 \ud835\udc65 1 The above conversions should only be used if the input is guaranteed to be a point on the relevant curve. If that is the case, the theorems below enumerate all exceptional inputs that may violate the side-conditions. Theorem A.3.2. Exceptional points (ctEdwards Montgomery). Let (\ud835\udc62, v) be an af\ufb01ne point on a ctEdwards curve \ud835\udc38ctEdwards(\ud835\udc4e,\ud835\udc51).",
      "Then the only points with \ud835\udc62 0 or 1 v  0 are (0, 1)  \ud835\udcaaJ, and (0, 1) of order 2. Proof. The curve equation is \ud835\udc4e\ud835\udc622  v2  1  \ud835\udc51\ud835\udc622v2 with \ud835\udc4e \ud835\udc51(see BBJLP2008, De\ufb01nition 2.1). By substituting \ud835\udc62 0 we obtain v  1, and by substituting v  1 and using \ud835\udc4e \ud835\udc51we obtain \ud835\udc62 0. Theorem A.3.3. Exceptional points (Montgomery ctEdwards). Let (\ud835\udc65, \ud835\udc66) be an af\ufb01ne point on a Montgomery curve \ud835\udc38Mont(\ud835\udc34,\ud835\udc35) over F\ud835\udc5fwith parameters \ud835\udc34and \ud835\udc35such that \ud835\udc342 4 is a nonsquare in F\ud835\udc5f, that is birationally equivalent to a ctEdwards curve. Then \ud835\udc65 1  0, and the only point (\ud835\udc65, \ud835\udc66) with \ud835\udc66 0 is (0, 0) of order 2. Proof. That the only point with \ud835\udc66 0 is (0, 0) is proven by Theorem A.2.1 on page 201. If \ud835\udc651  0, then subtituting \ud835\udc65 1 into the Montgomery curve equation gives \ud835\udc35\ud835\udc662  \ud835\udc653 \ud835\udc34\ud835\udc652 \ud835\udc65 \ud835\udc342. So in that case \ud835\udc662  (\ud835\udc342)\ud835\udc35. The right-hand-side is equal to the parameter \ud835\udc51of a particular ctEdwards curve birationally equivalent to the Montgomery curve (see BL2017, section 4.3.5).",
      "For all ctEdwards curves, \ud835\udc51is nonsquare, so this equation has no solutions for \ud835\udc66, hence \ud835\udc65 1  0. (When the theorem is applied with \ud835\udc38Mont(\ud835\udc34,\ud835\udc35)  M de\ufb01ned in sectionA.2 Elliptic curve background on page 200, the ctEdwards curve referred to in the proof is an isomorphic rescaling of the Jubjub curve.) A.3.3.4 Af\ufb01ne-Montgomery arithmetic The incomplete af\ufb01ne-Montgomery addition formulae given in BL2017, section 4.3.2 are: \ud835\udc653  \ud835\udc35M\ud835\udf062 \ud835\udc34M \ud835\udc651 \ud835\udc652 \ud835\udc663  (\ud835\udc651 \ud835\udc653)\ud835\udf06\ud835\udc661 where \ud835\udf06 3\ud835\udc652 1  2\ud835\udc34M\ud835\udc651  1 2\ud835\udc35M\ud835\udc661 , if \ud835\udc651  \ud835\udc652 \ud835\udc662 \ud835\udc661 \ud835\udc652 \ud835\udc651, otherwise. The following theorem helps to determine when these incomplete addition formulae can be safely used: Theorem A.3.4. Distinct-\ud835\udc65theorem. Let \ud835\udc44be a point of odd-prime order \ud835\udc60on a Montgomery curve M  \ud835\udc38Mont(\ud835\udc34M,\ud835\udc35M) over F\ud835\udc5fS. Let \ud835\udc581 .. 2 be integers \ud835\udc601 2 .. \ud835\udc601 0. Let \ud835\udc43\ud835\udc56 \ud835\udc58\ud835\udc56 \ud835\udc44 (\ud835\udc65\ud835\udc56, \ud835\udc66\ud835\udc56) for \ud835\udc561 .. 2, with \ud835\udc582  \ud835\udc581. Then the non-uni\ufb01ed addition constraints \ud835\udc652 \ud835\udc651 \ud835\udc662 \ud835\udc661 \ud835\udc35M\ud835\udf06 \ud835\udc34M  \ud835\udc651  \ud835\udc652  \ud835\udc653 \ud835\udc651 \ud835\udc653 \ud835\udc663  \ud835\udc661 implement the af\ufb01ne-Montgomery addition \ud835\udc431  \ud835\udc432  (\ud835\udc653, \ud835\udc663) for all such \ud835\udc431 .. 2. Proof.",
      "The given constraints are equivalent to the Montgomery addition formulae under the side condition that \ud835\udc651  \ud835\udc652. (Note that neither \ud835\udc43\ud835\udc56can be the zero point since \ud835\udc581 .. 2  0 (mod \ud835\udc60).) Assume for a contradiction that \ud835\udc651  \ud835\udc652. For any \ud835\udc431  \ud835\udc581 \ud835\udc44, there can be only one other point \ud835\udc431 with the same \ud835\udc65-coordinate. (This follows from the fact that the curve equation determines \ud835\udc66as a function of \ud835\udc65.) But \ud835\udc431  1 \ud835\udc581 \ud835\udc44 \ud835\udc581 \ud835\udc44. Since \ud835\udc601 2 .. \ud835\udc601 \ud835\udc58 \ud835\udc44 M is injective and \ud835\udc581 .. 2 are in \ud835\udc601 2 .. \ud835\udc601 , then \ud835\udc582  \ud835\udc581 (contradiction). The conditions of this theorem are called the distinct-\ud835\udc65criterion. In particular, if \ud835\udc581 .. 2 are integers in 1 .. \ud835\udc601 then it is suf\ufb01cient to require \ud835\udc582  \ud835\udc581, since that implies \ud835\udc582  \ud835\udc581. Af\ufb01ne-Montgomery doubling can be implemented as: 2\ud835\udc35M\ud835\udc66 3\ud835\udc65\ud835\udc65 2\ud835\udc34M\ud835\udc65 1 \ud835\udc35M\ud835\udf06 \ud835\udc34M  2\ud835\udc65 \ud835\udc653 \ud835\udc65\ud835\udc653 \ud835\udc663  \ud835\udc66 This doubling formula is valid when \ud835\udc66 0, which is the case when (\ud835\udc65, \ud835\udc66) is not the point (0, 0) (the only point of order 2), as proven in Theorem A.2.1 on page 201.",
      "A.3.3.5 Af\ufb01ne-ctEdwards arithmetic Formulae for af\ufb01ne-ctEdwards addition are given in BBJLP2008, section 6. With a change of variable names to match our convention, the formulae for (\ud835\udc621, v1)  (\ud835\udc622, v2)  (\ud835\udc623, v3) are: \ud835\udc623  \ud835\udc621v2  v1\ud835\udc622 1  \ud835\udc51J\ud835\udc621\ud835\udc622v1v2 v3  v1v2 \ud835\udc4eJ\ud835\udc621\ud835\udc622 1 \ud835\udc51J\ud835\udc621\ud835\udc622v1v2 We use an optimized implementation found by Daira-Emma Hopwood making use of an observation by Bernstein and Lange in BL2017, last paragraph of section 4.5.2: \ud835\udc621  v1 v2 \ud835\udc4eJ\ud835\udc622 \ud835\udc51J\ud835\udc34 1  \ud835\udc36 \ud835\udc34 \ud835\udc35 1 \ud835\udc36 \ud835\udc47\ud835\udc34 \ud835\udc4eJ\ud835\udc35 The correctness of this implementation can be seen by expanding \ud835\udc47\ud835\udc34 \ud835\udc4eJ\ud835\udc35: \ud835\udc47\ud835\udc34 \ud835\udc4eJ\ud835\udc35 (\ud835\udc621  v1)  (v2 \ud835\udc4eJ\ud835\udc622) \ud835\udc621v2  \ud835\udc4eJv1\ud835\udc622  v1v2 \ud835\udc4eJ\ud835\udc621\ud835\udc622  \ud835\udc621v2 \ud835\udc4eJv1\ud835\udc622 \ud835\udc621v2  \ud835\udc4eJv1\ud835\udc622  v1v2 \ud835\udc4eJ\ud835\udc621\ud835\udc622 The above addition formulae are uni\ufb01ed, that is, they can also be used for doubling. Af\ufb01ne-ctEdwards doubling 2 (\ud835\udc62, v)  (\ud835\udc623, v3) can also be implemented slightly more ef\ufb01ciently as: \ud835\udc62 v v \ud835\udc4eJ\ud835\udc62 \ud835\udc51J\ud835\udc34 1  \ud835\udc36 1 \ud835\udc36 \ud835\udc47 (\ud835\udc4eJ 1)\ud835\udc34 This implementation is obtained by specializing the addition formulae to (\ud835\udc62, v)  (\ud835\udc621, v1)  (\ud835\udc622, v2) and observing that \ud835\udc62 v  \ud835\udc34 \ud835\udc35.",
      "A.3.3.6 Af\ufb01ne-ctEdwards nonsmall-order check In order to avoid small-subgroup attacks, we check that certain points used in the circuit are not of small order. In practice the Sapling circuit uses this in combination with a check that the coordinates are on the curve (sectionA.3.3.1 Checking that Af\ufb01ne-ctEdwards coordinates are on the curve on page 205), so we combine the two operations. The Jubjub curve has a large prime-order subgroup with a cofactor of 8. To check for a point \ud835\udc43of order 8 or less, the Sapling circuit doubles three times (as in sectionA.3.3.5 Af\ufb01ne-ctEdwards arithmetic on page 207) and checks that the resulting \ud835\udc62-coordinate is not 0 (as in sectionA.3.1.4 Nonzero constraints on page 202). On a ctEdwards curve, only the zero point \ud835\udcaaJ, and the unique point of order 2 at (0, 1) have zero \ud835\udc62-coordinate. The point of order 2 cannot occur as the result of three doublings. So this \ud835\udc62-coordinate check rejects only \ud835\udcaaJ.",
      "The total cost, including the curve check, is 4  3  5  1  20 constraints. Note: This does not ensure that the point is in the prime-order subgroup. Non-normative notes:  It would have been suf\ufb01cient to do two doublings rather than three, because the check that the \ud835\udc62-coordinate is nonzero would reject both \ud835\udcaaJ and the point of order 2. It is possible to reduce the cost to 8 constraints by eliminating the redundant constraint in the curve point check (as mentioned in sectionA.3.3.1 Checking that Af\ufb01ne-ctEdwards coordinates are on the curve on page 205); merging the \ufb01rst doubling with the curve point check; and then optimizing the second doubling based on the fact that we only need to check whether the resulting \ud835\udc62-coordinate is zero. The Sapling circuit does not use these optimizations. A.3.3.7 Fixed-base Af\ufb01ne-ctEdwards scalar multiplication If the base point \ud835\udc35is \ufb01xed for a given scalar multiplication \ud835\udc58 \ud835\udc35, we can fully precompute window tables for each window position.",
      "It is most ef\ufb01cient to use 3-bit \ufb01xed windows. Since the length of \ud835\udc5fJ is 252 bits, we need 84 windows. Express \ud835\udc58in base 8, i.e. \ud835\udc58 \ud835\udc58\ud835\udc568\ud835\udc56. Then \ud835\udc58 \ud835\udc35 \ud835\udc64(\ud835\udc35, \ud835\udc56, \ud835\udc58\ud835\udc56), where \ud835\udc64(\ud835\udc35, \ud835\udc56, \ud835\udc58\ud835\udc56)  \ud835\udc58\ud835\udc568\ud835\udc56 \ud835\udc35. We precompute all of \ud835\udc64(\ud835\udc35, \ud835\udc56, \ud835\udc60) for \ud835\udc560 .. 83, \ud835\udc600 .. 7. To look up a given window entry \ud835\udc64(\ud835\udc35, \ud835\udc56, \ud835\udc60)  (\ud835\udc62\ud835\udc60, v\ud835\udc60), where \ud835\udc60 4\ud835\udc602  2\ud835\udc601  \ud835\udc600, we use: \ud835\udc620\ud835\udc60\u00ee  \ud835\udc620\ud835\udc602  \ud835\udc620\ud835\udc601 \ud835\udc620  \ud835\udc622\ud835\udc60\u00ee \ud835\udc622\ud835\udc601  \ud835\udc624\ud835\udc60\u00ee \ud835\udc624\ud835\udc602 \ud835\udc626\ud835\udc60\u00ee  \ud835\udc621\ud835\udc60\u00ee \ud835\udc621\ud835\udc602 \ud835\udc621\ud835\udc601  \ud835\udc621 \ud835\udc623\ud835\udc60\u00ee  \ud835\udc623\ud835\udc601 \ud835\udc625\ud835\udc60\u00ee  \ud835\udc625\ud835\udc602  \ud835\udc627\ud835\udc60\u00ee \ud835\udc62\ud835\udc60\ud835\udc620\ud835\udc60\u00ee  \ud835\udc620\ud835\udc602  \ud835\udc620\ud835\udc601 \ud835\udc620  \ud835\udc622\ud835\udc60\u00ee \ud835\udc622\ud835\udc601  \ud835\udc624\ud835\udc60\u00ee \ud835\udc624\ud835\udc602 \ud835\udc626\ud835\udc60\u00ee v0\ud835\udc60\u00ee  v0\ud835\udc602  v0\ud835\udc601 v0  v2\ud835\udc60\u00ee v2\ud835\udc601  v4\ud835\udc60\u00ee v4\ud835\udc602 v6\ud835\udc60\u00ee  v1\ud835\udc60\u00ee v1\ud835\udc602 v1\ud835\udc601  v1 v3\ud835\udc60\u00ee  v3\ud835\udc601 v5\ud835\udc60\u00ee  v5\ud835\udc602  v7\ud835\udc60\u00ee v\ud835\udc60v0\ud835\udc60\u00ee  v0\ud835\udc602  v0\ud835\udc601 v0  v2\ud835\udc60\u00ee v2\ud835\udc601  v4\ud835\udc60\u00ee v4\ud835\udc602 v6\ud835\udc60\u00ee For a full-length (252-bit) scalar this costs 3 constraints for each of 84 window lookups, plus 6 constraints for each of 83 ctEdwards additions (as in sectionA.3.3.5 Af\ufb01ne-ctEdwards arithmetic on page 207), for a total of 750 constraints.",
      "Fixed-base scalar multiplication is also used in two places with shorter scalars:  sectionA.3.6 Homomorphic Pedersen Commitment on page 213 uses 64 bits for the v input to ValueCommitSapling, re- quiring 22 windows at a cost of 322 1  621  191 constraints;  sectionA.3.3.10 Mixing Pedersen hash on page 212 uses 32 bits for the pos input to MixingPedersenHash, requiring 11 windows at a cost of 311 1  610  92 constraints. None of these costs include the cost of boolean-constraining the scalar. Non-normative notes:  It would be more ef\ufb01cient to use arithmetic on the Montgomery curve, as in sectionA.3.3.9 Pedersen hash on page 210. However since there are only three instances of \ufb01xed-base scalar multiplication in the Spend circuit and two in the Output circuit 15, the additional complexity was not considered justi\ufb01ed for Sapling. For the multiplications with 64-bit and 32-bit scalars, the scalar is padded to a multiple of 3 bits with zeros.",
      "This causes the computation of \ud835\udc60\u00ee in the lookup for the most signi\ufb01cant window to be optimized out, which is where the 1 comes from in the above cost calculations. No further optimization is done for this lookup. A.3.3.8 Variable-base Af\ufb01ne-ctEdwards scalar multiplication When the base point \ud835\udc35is not \ufb01xed, the method in the preceding section cannot be used. Instead we use a na\u00efve double-and-add method. Given \ud835\udc58 250 \ud835\udc560\ud835\udc58\ud835\udc562\ud835\udc56, we calculate \ud835\udc45 \ud835\udc58 \ud835\udc35using:  Base\ud835\udc56 2\ud835\udc56 \ud835\udc35 let Base0  \ud835\udc35 let Acc\ud835\udc62 0  \ud835\udc580 ? Base\ud835\udc62 0 : 0 let Accv 0  \ud835\udc580 ? Basev 0 : 1 for \ud835\udc56from 1 up to 250: let Base\ud835\udc56 2 Base\ud835\udc561  select Base\ud835\udc56or \ud835\udcaaJ depending on the bit \ud835\udc58\ud835\udc56 let Addend\ud835\udc62 \ud835\udc56 \ud835\udc58\ud835\udc56? Base\ud835\udc62 \ud835\udc56: 0 let Addendv \ud835\udc56 \ud835\udc58\ud835\udc56? Basev \ud835\udc56: 1 let Acc\ud835\udc56 Acc\ud835\udc561  Addend\ud835\udc56 let \ud835\udc45 Acc250. This costs 5 constraints for each of 250 ctEdwards doublings, 6 constraints for each of 250 ctEdwards additions, and 2 constraints for each of 251 point selections, for a total of 3252 constraints. 15A Pedersen commitment uses \ufb01xed-base scalar multiplication as a subcomponent.",
      "Non-normative note: It would be more ef\ufb01cient to use 2-bit \ufb01xed windows, andor to use arithmetic on the Montgomery curve in a similar way to sectionA.3.3.9 Pedersen hash on page 210. However since there are only two instances of variable-base scalar multiplication in the Spend circuit and one in the Output circuit, the additional complexity was not considered justi\ufb01ed for Sapling. A.3.3.9 Pedersen hash The speci\ufb01cation of the Pedersen hashes used in Sapling is given in section5.4.1.7 Pedersen Hash Function on page 79. It is based on the scheme from CvHP1991, section 5.2 for which a tighter security reduction to the Discrete Logarithm Problem was given in BGG1995 but tailored to allow several optimizations in the circuit implementation. Pedersen hashes are the single most commonly used primitive in the Sapling circuits. MerkleDepthSapling Pedersen hash instances are used in the Spend circuit to check a Merkle path to the note commitment of the note being spent.",
      "We also reuse the Pedersen hash implementation to construct the note commitment scheme NoteCommitSapling. This motivates considerable attention to optimizing this circuit implementation of this primitive, even at the cost of complexity. First, we use a windowed scalar multiplication algorithm with signed digits. Each 3-bit message chunk corresponds to a window; the chunk is encoded as an integer from the set Digits  4 .. 4 0. This allows a more ef\ufb01cient lookup of the window entry for each chunk than if the set 1 .. 8 had been used, because a point can be conditionally negated using only a single constraint. Next, we optimize the cost of point addition by allowing as many additions as possible to be performed on the Montgomery curve. An incomplete Montgomery addition costs 3 constraints, in comparison with a ctEdwards addition which costs 6 constraints. However, we cannot do all additions on the Montgomery curve because the Montgomery addition is incomplete.",
      "In order to be able to prove that exceptional cases do not occur, we need to ensure that the distinct-\ud835\udc65criterion from sectionA.3.3.4 Af\ufb01ne-Montgomery arithmetic on page 206 is met. This requires splitting the input into segments (each using an independent generator), calculating an intermediate result for each segment, and then converting to the ctEdwards curve and summing the intermediate results using ctEdwards addition. Abstracting away the changes of curve, this calculation can be written as: PedersenHashToPoint(\ud835\udc37, \ud835\udc40)  \ud835\udc40\ud835\udc57 \u2110(\ud835\udc37, \ud835\udc57) where and \u2110(\ud835\udc37, \ud835\udc57) are de\ufb01ned as in section5.4.1.7 Pedersen Hash Function on page 79. We have to prove that:  the Montgomery-to-ctEdwards conversions can be implemented without exceptional cases;  the distinct-\ud835\udc65criterion is met for all Montgomery additions within a segment. The proof of Theorem 5.4.1 on page 80 showed that all indices of addition inputs are in the range \ud835\udc5fJ1 .. \ud835\udc5fJ1 0.",
      "Because the \u2110(\ud835\udc37, \ud835\udc57) (which are outputs of GroupHashJ(\ud835\udc5f) ) are all of prime order, and \ud835\udc40\ud835\udc57 0 (mod \ud835\udc5fJ), it is guaranteed that all of the terms \ud835\udc40\ud835\udc57 \u2110(\ud835\udc37, \ud835\udc57) to be converted to ctEdwards form are of prime order. From Theorem A.3.3 on page 206, we can infer that the conversions will not encounter exceptional cases. We also need to show that the indices of addition inputs are all distinct disregarding sign. Theorem A.3.5. Concerning addition inputs in the Pedersen circuit. For all disjoint nonempty subsets \ud835\udc46and \ud835\udc46 of 1 .. \ud835\udc50, all \ud835\udc5aB3\ud835\udc50, and all \u0398 1, 1: enc(\ud835\udc5a\ud835\udc57)  24(\ud835\udc571)  \u0398  \ud835\udc57\ud835\udc46enc(\ud835\udc5a\ud835\udc57)  24(\ud835\udc571). Proof. Suppose for a contradiction that \ud835\udc46, \ud835\udc46, \ud835\udc5a, \u0398 is a counterexample. Taking the multiplication by \u0398 on the right hand side inside the summation, we have: enc(\ud835\udc5a\ud835\udc57)  24(\ud835\udc571)  \ud835\udc57\ud835\udc46\u0398  enc(\ud835\udc5a\ud835\udc57)  24(\ud835\udc571). De\ufb01ne enc 1, 1  B3 0 .. 8 4 as enc \ud835\udf03(\ud835\udc5a\ud835\udc56) : 4  \ud835\udf03 enc(\ud835\udc5a\ud835\udc56). Let \u0394  4  \ud835\udc50 \ud835\udc56124(\ud835\udc561) as in the proof of Theorem 5.4.1 on page 80. By adding \u0394 to both sides, we get enc 1(\ud835\udc5a\ud835\udc57)  24(\ud835\udc571)  \ud835\udc571 ..",
      "\ud835\udc50\ud835\udc46 4  24(\ud835\udc571)  \ud835\udc57\ud835\udc46enc \u0398(\ud835\udc5a\ud835\udc57)  24(\ud835\udc571)  \ud835\udc571 .. \ud835\udc50\ud835\udc46 4  24(\ud835\udc571) where all of the enc 1(\ud835\udc5a\ud835\udc57) and enc \u0398(\ud835\udc5a\ud835\udc57) are in 0 .. 8 4. Each term on the left and on the right affects the single hex digit indexed by \ud835\udc57and \ud835\udc57 respectively. Since \ud835\udc46and \ud835\udc46 are disjoint subsets of 1 .. \ud835\udc50 and \ud835\udc46is nonempty, \ud835\udc46(1 .. \ud835\udc50 \ud835\udc46) is nonempty. Therefore the left hand side has at least one hex digit not equal to 4 such that the corresponding right hand side digit is 4; contradiction. This implies that the terms in the Montgomery addition as well as any intermediate results formed from adding a distinct subset of terms have distinct indices disregarding sign, hence distinct \ud835\udc65-coordinates by Theorem A.3.4 on page 206. (We make no assumption about the order of additions.) We now describe the subcircuit used to process each chunk, which contributes most of the constraint cost of the hash.",
      "This subcircuit is used to perform a lookup of a Montgomery point in a 2-bit window table, conditionally negate the result, and add it to an accumulator holding another Montgomery point. Suppose that the bits of the chunk, \ud835\udc600, \ud835\udc601, \ud835\udc602, are already boolean-constrained. We aim to compute \ud835\udc36 \ud835\udc34 (1 2  \ud835\udc602)  (1  \ud835\udc600  2  \ud835\udc601) \ud835\udc43for some \ufb01xed base point \ud835\udc43and accumulated sum \ud835\udc34. We \ufb01rst compute \ud835\udc60\u00ee  \ud835\udc600 \u00ee \ud835\udc601: Let (\ud835\udc65\ud835\udc58, \ud835\udc66\ud835\udc58)  \ud835\udc58 \ud835\udc43for \ud835\udc581 .. 4. De\ufb01ne each coordinate of (\ud835\udc65\ud835\udc46, \ud835\udc66\ud835\udc45)  1  \ud835\udc600  2  \ud835\udc601 \ud835\udc43as a linear combination of \ud835\udc600, \ud835\udc601, and \ud835\udc60\u00ee: let \ud835\udc65\ud835\udc46 \ud835\udc651  (\ud835\udc652 \ud835\udc651)  \ud835\udc600  (\ud835\udc653 \ud835\udc651)  \ud835\udc601  (\ud835\udc654  \ud835\udc651 \ud835\udc652 \ud835\udc653)  \ud835\udc60\u00ee let \ud835\udc66\ud835\udc45 \ud835\udc661  (\ud835\udc662 \ud835\udc661)  \ud835\udc600  (\ud835\udc663 \ud835\udc661)  \ud835\udc601  (\ud835\udc664  \ud835\udc661 \ud835\udc662 \ud835\udc663)  \ud835\udc60\u00ee We implement the conditional negation as 2  \ud835\udc66\ud835\udc45 \ud835\udc66\ud835\udc45\ud835\udc66\ud835\udc46 .",
      "After substitution of \ud835\udc66\ud835\udc45this becomes: 2  (\ud835\udc661  (\ud835\udc662 \ud835\udc661)  \ud835\udc600  (\ud835\udc663 \ud835\udc661)  \ud835\udc601  (\ud835\udc664  \ud835\udc661 \ud835\udc662 \ud835\udc663)  \ud835\udc60\u00ee) \ud835\udc661  (\ud835\udc662 \ud835\udc661)  \ud835\udc600  (\ud835\udc663 \ud835\udc661)  \ud835\udc601  (\ud835\udc664  \ud835\udc661 \ud835\udc662 \ud835\udc663)  \ud835\udc60\u00ee \ud835\udc66\ud835\udc46 Then we substitute \ud835\udc65\ud835\udc46into the Montgomery addition constraints from sectionA.3.3.4 Af\ufb01ne-Montgomery arithmetic on page 206, as follows: \ud835\udc651  (\ud835\udc652 \ud835\udc651)  \ud835\udc600  (\ud835\udc653 \ud835\udc651)  \ud835\udc601  (\ud835\udc654  \ud835\udc651 \ud835\udc652 \ud835\udc653)  \ud835\udc60\u00ee \ud835\udc65\ud835\udc34 \ud835\udc66\ud835\udc46\ud835\udc66\ud835\udc34 \ud835\udc35M\ud835\udf06 \ud835\udc34M  \ud835\udc65\ud835\udc34 \ud835\udc651  (\ud835\udc652 \ud835\udc651)  \ud835\udc600  (\ud835\udc653 \ud835\udc651)  \ud835\udc601  (\ud835\udc654  \ud835\udc651 \ud835\udc652 \ud835\udc653)  \ud835\udc60\u00ee  \ud835\udc65\ud835\udc36 \ud835\udc65\ud835\udc34\ud835\udc65\ud835\udc36 \ud835\udc66\ud835\udc36 \ud835\udc66\ud835\udc34 (In the sapling-crypto implementation, linear combinations are \ufb01rst-class values, so these substitutions do not need to be done by hand.) For the \ufb01rst addition in each segment, both sides are looked up and substituted into the Montgomery addition, so the \ufb01rst lookup takes only 2 constraints. When these hashes are used in the circuit, the \ufb01rst 6 bits of the input are \ufb01xed. For example, in the Merkle tree hashes they represent the layer number. This would allow a precomputation for the \ufb01rst two windows, but that optimization is not done in Sapling.",
      "The cost of a Pedersen hash over \u2113bits (where \u2113includes the \ufb01xed bits) is as follows. The number of chunks is \ud835\udc50 ceiling and the number of segments is \ud835\udc5b ceiling 3  63 The cost is then:  2\ud835\udc50constraints for the lookups;  3(\ud835\udc50\ud835\udc5b) constraints for incomplete additions on the Montgomery curve;  2\ud835\udc5bconstraints for Montgomery-to-ctEdwards conversions;  6(\ud835\udc5b1) constraints for ctEdwards additions; for a total of 5\ud835\udc50 5\ud835\udc5b6 constraints. This does not include the cost of boolean-constraining inputs. In particular,  for the Merkle tree hashes \u2113 516, so \ud835\udc50 172, \ud835\udc5b 3, and the cost is 869 constraints;  when a Pedersen hash is used to implement part of a Pedersen commitment for NoteCommitSapling (section5.4.8.2 Windowed Pedersen commitments on page 96), \u2113 6  \u2113value  2\u2113J  582, \ud835\udc50 194, and \ud835\udc5b 4, so the cost of the hash alone is 984 constraints. A.3.3.10 Mixing Pedersen hash A mixing Pedersen hash is used to compute \u03c1 from cm and pos in section4.16 Computing \u03c1 values and Nulli\ufb01ers on page 57.",
      "It takes as input a Pedersen commitment \ud835\udc43, and hashes it with another input \ud835\udc65. Let \ud835\udca5Sapling be as de\ufb01ned in section5.4.1.8 Mixing Pedersen Hash Function on page 81. We de\ufb01ne MixingPedersenHash 0 .. \ud835\udc5fJ 1  J J by: MixingPedersenHash(\ud835\udc43, \ud835\udc65) : \ud835\udc43 \ud835\udc65 \ud835\udca5Sapling. This costs 92 constraints for a scalar multiplication (sectionA.3.3.7 Fixed-base Af\ufb01ne-ctEdwards scalar multiplication on page 208), and 6 constraints for a ctEdwards addition (sectionA.3.3.5 Af\ufb01ne-ctEdwards arithmetic on page 207), for a total of 98 constraints. A.3.4 Merkle path check Checking each layer of a Merkle authentication path, as described in section4.9 Merkle Path Validity on page 49, requires  boolean-constrain the path bit specifying whether the previous node is a left or right child;  conditionally swap the previous-layer and sibling hashes (as F\ud835\udc5felements) depending on the path bit;  unpack the left and right hash inputs to two sequences of 255 bits;  compute the Merkle hash for this node.",
      "The unpacking need not be canonical in the sense discussed in sectionA.3.2.1 Unpacking modulo \ud835\udc5fS on page 202; that is, it is not necessary to ensure that the left or right inputs to the hash represent integers in the range 0 .. \ud835\udc5fS 1. Since the root of the Merkle tree is calculated outside the circuit using the canonical representations, and since the Pedersen hashes are collision-resistant on arbitrary bit-sequence inputs, an attempt by an adversarial prover to use a non-canonical input would result in the wrong root being calculated, and the overall path check would fail. For each layer, the cost is 1  2255 boolean constraints, 2 constraints for the conditional swap (implemented as two selection constraints), and 869 constraints for the Merkle hash (sectionA.3.3.9 Pedersen hash on page 210), for a total of 1380 constraints. Non-normative note: The conditional swap (\ud835\udc4e0, \ud835\udc4e1) (\ud835\udc500, \ud835\udc501) could be implemented in only one constraint by substituting \ud835\udc501  \ud835\udc4e0  \ud835\udc4e1 \ud835\udc500 into the uses of \ud835\udc501.",
      "The Sapling circuit does not use this optimization. A.3.5 Windowed Pedersen Commitment We construct windowed Pedersen commitments by reusing the Pedersen hash implementation described in sectionA.3.3.9 Pedersen hash on page 210, and adding a randomized point: WindowedPedersenCommit\ud835\udc5f(\ud835\udc60)  PedersenHashToPoint(Zcash_PH, \ud835\udc60)  \ud835\udc5f FindGroupHashJ(\ud835\udc5f) (Zcash_PH, r) This can be implemented in:  5\ud835\udc50 5\ud835\udc5b6 constraints for the Pedersen hash applied to \u2113 6  length(\ud835\udc60) bits, where \ud835\udc50 ceiling \ud835\udc5b ceiling 3  63  750 constraints for the \ufb01xed-base scalar multiplication;  6 constraints for the \ufb01nal ctEdwards addition. When WindowedPedersenCommit is used to instantiate NoteCommitSapling, the cost of the Pedersen hash is 984 con- straints as calculated in sectionA.3.3.9 Pedersen hash on page 210, and so the total cost in that case is 1740 constraints. This does not include the cost of boolean-constraining the input \ud835\udc60or the randomness \ud835\udc5f.",
      "A.3.6 Homomorphic Pedersen Commitment The windowed Pedersen commitments de\ufb01ned in the preceding section are highly ef\ufb01cient, but they do not support the homomorphic property we need when instantiating ValueCommit. In order to support this property, we also de\ufb01ne homomorphic Pedersen commitments as follows: HomomorphicPedersenCommitSapling (\ud835\udc37, v)  v FindGroupHashJ(\ud835\udc5f) (\ud835\udc37, v)  rcv FindGroupHashJ(\ud835\udc5f) (\ud835\udc37, r) In the case that we need for ValueCommit, v has 64 bits16. This value is given as a bit representation, which does not need to be constrained equal to an integer. ValueCommit can be implemented in:  750 constraints for the 252-bit \ufb01xed-base multiplication by rcv;  191 constraints for the 64-bit \ufb01xed-base multiplication by v;  6 constraints for the ctEdwards addition for a total cost of 947 constraints. This does not include the cost to boolean-constrain the input v or randomness rcv. A.3.7 BLAKE2s hashes BLAKE2s is de\ufb01ned in ANWW2013.",
      "Its main subcomponent is a \ud835\udc3afunction, de\ufb01ned as follows: 0 .. 9  0 .. 23214 0 .. 23214 \ud835\udc3a(\ud835\udc4e, \ud835\udc4f, \ud835\udc50, \ud835\udc51, \ud835\udc65, \ud835\udc66)  (\ud835\udc4e, \ud835\udc4f, \ud835\udc50, \ud835\udc51) where \ud835\udc4e  (\ud835\udc4e \ud835\udc4f \ud835\udc65) mod 232 \ud835\udc51  (\ud835\udc51\ud835\udc4e) 16 \ud835\udc50  (\ud835\udc50 \ud835\udc51) mod 232 \ud835\udc4f  (\ud835\udc4f\ud835\udc50) 12 \ud835\udc4e  (\ud835\udc4e  \ud835\udc4f  \ud835\udc66) mod 232 \ud835\udc51  (\ud835\udc51 \ud835\udc4e) 8 \ud835\udc50  (\ud835\udc50  \ud835\udc51) mod 232 \ud835\udc4f  (\ud835\udc4f \ud835\udc50) 7 The following table is used to determine which message words the \ud835\udc65and \ud835\udc66arguments to \ud835\udc3aare selected from: \ud835\udf0e0   0, 9, 10, 11, 12, 13, 14, 15  \ud835\udf0e1   14, 10, 9, 15, 13, 1, 12, 2, 11, \ud835\udf0e2   11, 8, 12, 2, 15, 13, 10, 14, \ud835\udf0e3   7, 1, 13, 12, 11, 14, 5, 10, 0, 15, \ud835\udf0e4   9, 4, 10, 15, 14, 1, 11, 12, 3, 13  \ud835\udf0e5   2, 12, 6, 10, 0, 11, 4, 13, 5, 15, 14, \ud835\udf0e6   12, 1, 15, 14, 13, 4, 10, 8, 11  \ud835\udf0e7   13, 11, 7, 14, 12, 0, 15, 2, 10  \ud835\udf0e8   6, 15, 14, 9, 11, 8, 12, 2, 13, 4, 10, \ud835\udf0e9   10, 5, 15, 11, 9, 14, 3, 12, 13, The Initialization Vector is de\ufb01ned as: 0 .. 23218 :  0x6A09E667, 0xBB67AE85, 0x3C6EF372, 0xA54FF53A 0x510E527F, 0x9B05688C, 0x1F83D9AB, 0x5BE0CD19  16It would be suf\ufb01cient to use 51 bits, which accomodates the range 0 ..",
      "MAX_MONEY, but the Sapling circuit uses 64. The full hash function applied to an 8-byte personalization string and a single 64-byte block, in sequential mode with 32-byte output, can be expressed as follows. De\ufb01ne BLAKE2s-256 BY8)  (\ud835\udc65 BY64) BY32 as: let PB BY32  32, 0, 1, 1  0x0020  \ud835\udc5d let  \ud835\udc610, \ud835\udc611, \ud835\udc530, \ud835\udc531  0 .. 23214   0, 0, 0, 0xFFFFFFFF, 0  let \u210e 0 .. 23218   LEOS2IP32(PB4\ud835\udc56.. 4\ud835\udc56 3) IV\ud835\udc56for \ud835\udc56from 0 up to 7  let \ud835\udc5a 0 .. 232116   LEOS2IP32(\ud835\udc654\ud835\udc56.. 4\ud835\udc56 3) for \ud835\udc56from 0 up to 15  let mutable \ud835\udc63 0 ..",
      "232116 \u210e  IV0, IV1, IV2, IV3, \ud835\udc610 IV4, \ud835\udc611 IV5, \ud835\udc530 IV6, \ud835\udc531 IV7  for \ud835\udc5ffrom 0 up to 9: set (\ud835\udc630, \ud835\udc634, \ud835\udc638, \ud835\udc6312) \ud835\udc3a(\ud835\udc630, \ud835\udc634, \ud835\udc638, \ud835\udc6312, \ud835\udc5a\ud835\udf0e\ud835\udc5f,0, \ud835\udc5a\ud835\udf0e\ud835\udc5f,1 ) set (\ud835\udc631, \ud835\udc635, \ud835\udc639, \ud835\udc6313) \ud835\udc3a(\ud835\udc631, \ud835\udc635, \ud835\udc639, \ud835\udc6313, \ud835\udc5a\ud835\udf0e\ud835\udc5f,2, \ud835\udc5a\ud835\udf0e\ud835\udc5f,3 ) set (\ud835\udc632, \ud835\udc636, \ud835\udc6310, \ud835\udc6314) \ud835\udc3a(\ud835\udc632, \ud835\udc636, \ud835\udc6310, \ud835\udc6314, \ud835\udc5a\ud835\udf0e\ud835\udc5f,4, \ud835\udc5a\ud835\udf0e\ud835\udc5f,5 ) set (\ud835\udc633, \ud835\udc637, \ud835\udc6311, \ud835\udc6315) \ud835\udc3a(\ud835\udc633, \ud835\udc637, \ud835\udc6311, \ud835\udc6315, \ud835\udc5a\ud835\udf0e\ud835\udc5f,6, \ud835\udc5a\ud835\udf0e\ud835\udc5f,7 ) set (\ud835\udc630, \ud835\udc635, \ud835\udc6310, \ud835\udc6315) \ud835\udc3a(\ud835\udc630, \ud835\udc635, \ud835\udc6310, \ud835\udc6315, \ud835\udc5a\ud835\udf0e\ud835\udc5f,8, \ud835\udc5a\ud835\udf0e\ud835\udc5f,9 ) set (\ud835\udc631, \ud835\udc636, \ud835\udc6311, \ud835\udc6312) \ud835\udc3a(\ud835\udc631, \ud835\udc636, \ud835\udc6311, \ud835\udc6312, \ud835\udc5a\ud835\udf0e\ud835\udc5f,10, \ud835\udc5a\ud835\udf0e\ud835\udc5f,11) set (\ud835\udc632, \ud835\udc637, \ud835\udc638, \ud835\udc6313) \ud835\udc3a(\ud835\udc632, \ud835\udc637, \ud835\udc638, \ud835\udc6313, \ud835\udc5a\ud835\udf0e\ud835\udc5f,12, \ud835\udc5a\ud835\udf0e\ud835\udc5f,13) set (\ud835\udc633, \ud835\udc634, \ud835\udc639, \ud835\udc6314) \ud835\udc3a(\ud835\udc633, \ud835\udc634, \ud835\udc639, \ud835\udc6314, \ud835\udc5a\ud835\udf0e\ud835\udc5f,14, \ud835\udc5a\ud835\udf0e\ud835\udc5f,15) return LEBS2OSP256(concatB( I2LEBSP32(\u210e\ud835\udc56\ud835\udc63\ud835\udc56\ud835\udc63\ud835\udc568) for \ud835\udc56from 0 up to 7 )) In practice the message and output will be expressed as bit sequences. In the Sapling circuit, the personalization string will be constant for each use. Each 32-bit exclusive-or is implemented in 32 constraints, one for each bit position \ud835\udc4e\ud835\udc4f \ud835\udc50as in sectionA.3.1.5 Exclusive-or constraints on page 202. Additions not involving a message word, i.e.",
      "(\ud835\udc4e \ud835\udc4f) mod 232  \ud835\udc50, are implemented using 33 constraints and a 33-bit equality check: constrain 33 boolean variables \ud835\udc500 .. 32, and then check \ud835\udc5631 \ud835\udc560 (\ud835\udc4e\ud835\udc56 \ud835\udc4f\ud835\udc56)  2\ud835\udc56 \ud835\udc5632 \ud835\udc560 \ud835\udc50\ud835\udc56 2\ud835\udc56. Additions involving a message word, i.e. (\ud835\udc4e \ud835\udc4f \ud835\udc5a) mod 232  \ud835\udc50, are implemented using 34 constraints and a 34-bit equality check: constrain 34 boolean variables \ud835\udc500 .. 33, and then check \ud835\udc5631 \ud835\udc560 (\ud835\udc4e\ud835\udc56 \ud835\udc4f\ud835\udc56 \ud835\udc5a\ud835\udc56)  2\ud835\udc56 \ud835\udc5633 \ud835\udc560 \ud835\udc50\ud835\udc56 2\ud835\udc56. For each addition, only \ud835\udc500 .. 31 are used subsequently. The equality checks are batched; as many sets of 33 or 34 boolean variables as will \ufb01t in a F\ud835\udc5fS \ufb01eld element are equated together using one constraint. This allows 7 such checks per constraint. Each \ud835\udc3aevaluation requires 262 constraints:  4  32  128 constraints for operations;  2  33  66 constraints for 32-bit additions not involving message words (excluding equality checks);  2  34  68 constraints for 32-bit additions involving message words (excluding equality checks).",
      "The overall cost is 21006 constraints:  10  8  262 4  2  32  20704 constraints for 80 \ud835\udc3aevaluations, excluding equality checks (the deduction of 4  2  32 is because \ud835\udc63is constant at the start of the \ufb01rst round, so in the \ufb01rst four calls to \ud835\udc3a, the parameters \ud835\udc4f and \ud835\udc51are constant, eliminating the constraints for the \ufb01rst two XORs in those four calls to \ud835\udc3a);  ceiling (10  8  4  46 constraints for equality checks;  8  32  256 constraints for \ufb01nal \ud835\udc63\ud835\udc56\ud835\udc63\ud835\udc568 operations (the \u210e\ud835\udc56words are constants so no additional constraints are required to exclusive-or with them). This cost includes boolean-constraining the hash output bits (done implicitly by the \ufb01nal operations), but not the message bits. Non-normative notes:  The equality checks could be eliminated entirely by substituting each check into a boolean constraint for \ud835\udc500, for instance, but this optimization is not done in Sapling. It should be clear that BLAKE2s is very expensive in the circuit compared to elliptic curve operations.",
      "This is primarily because it is inef\ufb01cient to use F\ud835\udc5fS elements to represent single bits. However Pedersen hashes do not have the necessary cryptographic properties for the two cases where the Spend circuit uses BLAKE2s. While it might be possible to use variants of functions with low circuit cost such as MiMC AGRRT2017, it was felt that they had not yet received suf\ufb01cient cryptanalytic attention to con\ufb01dently use them for Sapling. The Sapling Spend circuit The Sapling Spend statement is de\ufb01ned in section4.18.2 Spend Statement (Sapling) on page 61. The primary input is rtSapling B\u2113Sapling Merkle , cvold ValueCommitSapling.Output, nfold BY\u2113PRFnfSapling8, SpendAuthSigSapling.Public which is encoded as 8 F\ud835\udc5fS elements (starting with the \ufb01xed element 1 required by Groth16): 1,\ud835\udc62(rk),v(rk),\ud835\udc62(cvold),v(cvold), LEBS2IP\u2113Sapling Merkle rtSapling) , LEBS2IP254 0 .. 253 , LEBS2IP2 254 ..",
      "255 ) where nf old  LEOS2BSP\u2113PRFnfSapling nfold) The auxiliary input is path B\u2113Sapling Merkle MerkleDepthSapling, 0 .. 2MerkleDepthSapling 1, vold 0 .. 2\u2113value1, rcvold 0 .. 2\u2113Sapling scalar 1, cmold rcmold 0 .. 2\u2113Sapling scalar 1, 0 .. 2\u2113Sapling scalar 1, SpendAuthSigSapling.Public, 0 .. 2\u2113Sapling scalar 1 ValueCommitSapling.Output and SpendAuthSigSapling.Public are of type J, so we have cvold, cmold, rk, gd, pkd, and ak that represent Jubjub curve points. However,  cvold will be constrained to an output of ValueCommitSapling;  cmold will be constrained to an output of NoteCommitSapling;  rk will be constrained to \ud835\udefc \ud835\udca2Sapling ak;  pkd will be constrained to ivk gd so cvold, cmold, rk, and pkd do not need to be explicitly checked to be on the curve. In addition, nkand \u03c1used in Nulli\ufb01er integrity are compressed representations of Jubjub curve points.",
      "TODO: explain why these are implemented as section A.3.3.2 ctEdwards decompression and validation on page 205 even though the statement spec doesnt explicitly say to do validation. Therefore we have gd, ak, nk, and \u03c1 that need to be constrained to valid Jubjub curve points as described in sectionA.3.3.2 ctEdwards decompression and validation on page 205. In order to aid in comparing the implementation with the speci\ufb01cation, we present the checks needed in the order in which they are implemented in the sapling-crypto code: Check Implements Cost ak is on the curve TODO: FIXME also decompressed below SpendAuthSigSapling.Public sectionA.3.3.1 on page 205 ak is not small-order Small order checks sectionA.3.3.6 on page 208 B\u2113Sapling scalar  0 .. 2\u2113Sapling scalar 1 sectionA.3.1.1 on page 201 \ud835\udefc  \ud835\udefc \ud835\udca2Sapling Spend authority sectionA.3.3.7 on page 208 rk  \ud835\udefc  ak sectionA.3.3.5 on page 207 inputize rk TODO: not ccteddecompress- validate  wrong count SpendAuthSigSapling.Public 392?",
      "sectionA.3.3.2 on page 205 nsk B\u2113Sapling scalar  0 .. 2\u2113Sapling scalar 1 sectionA.3.1.1 on page 201 nk  nsk \u210bSapling Nulli\ufb01er integrity sectionA.3.3.7 on page 208 ak reprJ(ak Diversi\ufb01ed address integrity sectionA.3.3.2 on page 205 nk reprJ(nk)TODO: spec doesnt say to validate nk since its calculated Nulli\ufb01er integrity sectionA.3.3.2 on page 205 ivk I2LEBSP251 CRHivk(ak, nk) Diversi\ufb01ed address integrity sectionA.3.7 on page 214 gd is on the curve sectionA.3.3.1 on page 205 gd is not small-order Small order checks sectionA.3.3.6 on page 208 pkd  ivk gd Diversi\ufb01ed address integrity sectionA.3.3.8 on page 209 B64 vold 0 .. 2641 sectionA.3.1.1 on page 201 rcv B\u2113Sapling scalar  0 .. 2\u2113Sapling scalar 1 sectionA.3.1.1 on page 201 cv  ValueCommitrcv ( vold) Value commitment integrity sectionA.3.6 on page 213 inputize cv rcm B\u2113Sapling scalar  0 ..",
      "2\u2113Sapling scalar 1 sectionA.3.1.1 on page 201 cm  NoteCommitSapling (gd, pkd, vold) Note commitment integrity sectionA.3.5 on page 213 cm\ud835\udc62 ExtractJ(\ud835\udc5f)(cm) Merkle path validity rt is the root of a Merkle tree with leaf cm\ud835\udc62, and authentication path (path, pos) 32  1380 sectionA.3.4 on page 213 pos I2LEBSPMerkleDepthSapling(pos) sectionA.3.2.1 on page 202 if vold  0 then rt  rtSapling sectionA.3.1.2 on page 202 inputize rtSapling \u03c1  MixingPedersenHash(cmold, pos) Nulli\ufb01er integrity sectionA.3.3.10 on page 212 \u03c1 reprJ(\u03c1)TODO: spec doesnt say to validate \u03c1 since its calculated sectionA.3.3.2 on page 205 nfold  PRFnfSapling (\u03c1) sectionA.3.7 on page 214 pack nfold 0 .. 253 and nfold 254 .. 255 into two F\ud835\udc5fS inputs input encoding sectionA.3.2.1 on page 202  This is implemented by taking the output of BLAKE2s-256 as a bit sequence and dropping the most signi\ufb01cant 5 bits, not by converting to an integer and back to a bit sequence as literally speci\ufb01ed.",
      "Note: The implementation represents \ud835\udefc, nsk, ivk, rcm, rcv, and v old as bit sequences rather than integers. It represents nf as a bit sequence rather than a byte sequence. The Sapling Output circuit The Sapling Output statement is de\ufb01ned in section4.18.3 Output Statement (Sapling) on page 62. The primary input is cvnew ValueCommitSapling.Output, B\u2113Sapling Merkle , which is encoded as 6 F\ud835\udc5fS elements (starting with the \ufb01xed element 1 required by Groth16): 1,\ud835\udc62(cvnew),v(cvnew),\ud835\udc62(epk),v(epk), LEBS2IP\u2113Sapling Merkle (cm\ud835\udc62) The auxiliary input is pkd B\u2113J, vnew 0 .. 2\u2113value1, rcvnew 0 .. 2\u2113Sapling scalar 1, rcmnew 0 .. 2\u2113Sapling scalar 1, 0 .. 2\u2113Sapling scalar 1) ValueCommitSapling.Output is of type J, so we have cvnew, epk, and gd that represent Jubjub curve points. However,  cvnew will be constrained to an output of ValueCommitSapling;  epk will be constrained to esk gd so cvnew and epk do not need to be explicitly checked to be on the curve.",
      "Therefore we have only gd that needs to be constrained to a valid Jubjub curve point as described in sectionA.3.3.2 ctEdwards decompression and validation on page 205. Note: pkd is not checked to be a valid compressed representation of a Jubjub curve point. In order to aid in comparing the implementation with the speci\ufb01cation, we present the checks needed in the order in which they are implemented in the sapling-crypto code: Check Implements Cost B64 vold 0 .. 2641 sectionA.3.1.1 on page 201 rcv B\u2113Sapling scalar  0 .. 2\u2113Sapling scalar 1 sectionA.3.1.1 on page 201 cv  ValueCommitSapling (vold) Value commitment integrity sectionA.3.6 on page 213 inputize cv gd  reprJ(gd Note commitment integrity sectionA.3.3.2 on page 205 gd is not small-order Small order checks sectionA.3.3.6 on page 208 esk B\u2113Sapling scalar  0 ..",
      "2\u2113Sapling scalar 1 sectionA.3.1.1 on page 201 epk  esk gd Ephemeral public key integrity sectionA.3.3.8 on page 209 inputize epk pkd B\u2113J pkd B\u2113J sectionA.3.1.1 on page 201 rcm B\u2113Sapling scalar  0 .. 2\u2113Sapling scalar 1 sectionA.3.1.1 on page 201 cm  NoteCommitSapling (gd, pkd, vold) Note commitment integrity sectionA.3.5 on page 213 pack inputs Note: The implementation represents esk, pkd, rcm, rcv, and v old as bit sequences rather than integers. Batching Optimizations RedDSA batch validation The reference validation algorithm for RedDSA signatures is de\ufb01ned in section5.4.7 RedDSA, RedJubjub, and RedPallas on page 92. Let the RedDSA parameters G (de\ufb01ning a subgroup G(\ud835\udc5f) of order \ud835\udc5fG, a cofactor \u210eG, a group operation , an additive identity \ud835\udcaaG, a bit-length \u2113G, a representation function reprG, and an abstraction function abstG); \ud835\udcabG G; \u2113H BYN BY\u2113H8; and the derived hash function H BYN F\ud835\udc5fG be as de\ufb01ned in that section.",
      "Implementations MAY alternatively use the optimized procedure described in this section to perform faster validation of a batch of signatures, i.e. to determine whether all signatures in a batch are valid. Its input is a sequence of \ud835\udc41signature batch entries, each of which is a (validating key, message, signature) triple. Let LEOS2BSP, LEOS2IP, and LEBS2OSP be as de\ufb01ned in section5.1 Integers, Bit Sequences, and Endianness on page 73. De\ufb01ne RedDSA.BatchEntry : RedDSA.Public  RedDSA.Message  RedDSA.Signature. De\ufb01ne RedDSA.BatchValidate (entry0 .. \ud835\udc411 RedDSA.BatchEntry\ud835\udc41) B as: For each \ud835\udc570 .. \ud835\udc411: Let (vk\ud835\udc57, \ud835\udc40\ud835\udc57, \ud835\udf0e\ud835\udc57)  entry\ud835\udc57. Let \ud835\udc45\ud835\udc57be the \ufb01rst ceiling \u2113G8 bytes of \ud835\udf0e\ud835\udc57, and let \ud835\udc46\ud835\udc57be the remaining ceiling(bitlength(\ud835\udc5fG)8) bytes. Let \ud835\udc45\ud835\udc57 abstG LEOS2BSP\u2113G(\ud835\udc45\ud835\udc57) , and let \ud835\udc46\ud835\udc57 LEOS2IP8length(\ud835\udc46\ud835\udc57)(\ud835\udc46\ud835\udc57). Let vk\ud835\udc57 LEBS2OSP\u2113G reprG(vk\ud835\udc57) Let \ud835\udc50\ud835\udc57 H(\ud835\udc45\ud835\udc57 vk\ud835\udc57 \ud835\udc40\ud835\udc57). Choose random \ud835\udc67\ud835\udc57 \ud835\udc5fG  R 1 .. 2128 1. Return 1 if  for all \ud835\udc570 ..",
      "\ud835\udc411, \ud835\udc45\ud835\udc57 and \ud835\udc46\ud835\udc57 \ud835\udc5fG; and  \u210eG \ud835\udc411 \ud835\udc570 (\ud835\udc67\ud835\udc57 \ud835\udc46\ud835\udc57) (mod \ud835\udc5fG) \ud835\udcabG  \ud835\udc411 \ud835\udc570 \ud835\udc67\ud835\udc57 \ud835\udc45\ud835\udc57 \ud835\udc411 \ud835\udc570 \ud835\udc67\ud835\udc57 \ud835\udc50\ud835\udc57(mod \ud835\udc5fG) vk\ud835\udc57  \ud835\udcaaG, otherwise 0. The \ud835\udc67\ud835\udc57values MUST be chosen independently of the signature batch entries. Non-normative note: It is also acceptable to sample each \ud835\udc67\ud835\udc57from 0 .. 2128 1, since the probability of obtaining zero for any \ud835\udc67\ud835\udc57is negligible. The performance bene\ufb01t of this approach arises partly from replacing the per-signature scalar multiplication of the base \ud835\udcabG with one such multiplication per batch, and partly from using an ef\ufb01cient algorithm for multiscalar multiplication such as Pippingers method Bernstein2001 or the BosCoster method deRooij1995, as explained in BDLSY2012, section 5. Note: Spend authorization signatures (section5.4.7.1 Spend Authorization Signature (Sapling and Orchard) on page 95) and binding signatures (section5.4.7.2 Binding Signature (Sapling and Orchard) on page 95) use different bases \ud835\udcabG.",
      "It is straightforward to adapt the above procedure to handle multiple bases; there will be one   \ud835\udc57(\ud835\udc67\ud835\udc57 \ud835\udc46\ud835\udc57) (mod \ud835\udc5fG) term for each base \ud835\udcab. The bene\ufb01t of this relative to using separate batches is that the multiscalar multiplication can be extended across a larger batch. Groth16 batch veri\ufb01cation The reference veri\ufb01cation algorithm for Groth16 proofs is de\ufb01ned in section5.4.10.2 Groth16 on page 111. The batch veri\ufb01ca- tion algorithm in this section applies techniques from BFIJSV2010, section 4. Let \ud835\udc5eS, \ud835\udc5fS, S(\ud835\udc5f) 1,2,\ud835\udc47, S(\ud835\udc5f) 1,2,\ud835\udc47, \ud835\udcabS1,2,\ud835\udc47, 1S, and \ud835\udc52S be as de\ufb01ned in section5.4.9.2 BLS12-381 on page 101. De\ufb01ne MillerLoopS S(\ud835\udc5f) 1  S(\ud835\udc5f) S(\ud835\udc5f) \ud835\udc47and FinalExpS S(\ud835\udc5f) S(\ud835\udc5f) \ud835\udc47to be the Miller loop and \ufb01nal exponentiation respectively of the \ud835\udc52S pairing computation, so that: \ud835\udc52S(\ud835\udc43, \ud835\udc44) FinalExpS(MillerLoopS(\ud835\udc43, \ud835\udc44)) where FinalExpS(\ud835\udc45) \ud835\udc45\ud835\udc61for some \ufb01xed \ud835\udc61. De\ufb01ne Groth16S.Proof : S(\ud835\udc5f)  S(\ud835\udc5f)  S(\ud835\udc5f) A Groth16S proof comprises a tuple (\ud835\udf0b\ud835\udc34, \ud835\udf0b\ud835\udc35, \ud835\udf0b\ud835\udc36) Groth16S.Proof.",
      "Veri\ufb01cation of a single Groth16S proof against an instance encoded as \ud835\udc4e0 .. \u2113 F\ud835\udc5fS \u21131 requires checking the equation \ud835\udc52S(\ud835\udf0b\ud835\udc34, \ud835\udf0b\ud835\udc35)  \ud835\udc52S(\ud835\udf0b\ud835\udc36, \u0394)  \ud835\udc52S (\u2113 \ud835\udc560\ud835\udc4e\ud835\udc56 \u03a8\ud835\udc56, \u0393 where \u0394  \ud835\udeff \ud835\udcabS2, \u0393  \ud835\udefe \ud835\udcabS2, \ud835\udc4c \ud835\udefc\ud835\udefd \ud835\udcabS\ud835\udc47, and \u03a8\ud835\udc56 \ud835\udefd\ud835\udc62\ud835\udc56(\ud835\udc65)  \ud835\udefc\ud835\udc63\ud835\udc56(\ud835\udc65)  \ud835\udc64\ud835\udc56(\ud835\udc65) \ud835\udcabS1 for \ud835\udc560 .. \u2113 are elements of the veri\ufb01cation key, as described (with slightly different notation) in Groth2016, section 3.2. This can be written as: \ud835\udc52S(\ud835\udf0b\ud835\udc34, \ud835\udf0b\ud835\udc35)  \ud835\udc52S(\ud835\udf0b\ud835\udc36, \u0394)  \ud835\udc52S (\u2113 \ud835\udc560\ud835\udc4e\ud835\udc56 \u03a8\ud835\udc56, \u0393  \ud835\udc4c 1S. Raising to the power of random \ud835\udc67 0 gives: \ud835\udc52S(\ud835\udc67 \ud835\udf0b\ud835\udc34, \ud835\udf0b\ud835\udc35) \ud835\udc52S(\ud835\udc67 \ud835\udf0b\ud835\udc36, \u0394) \ud835\udc52S (\u2113 \ud835\udc560\ud835\udc67 \ud835\udc4e\ud835\udc56 \u03a8\ud835\udc56, \u0393  \ud835\udc4c\ud835\udc67 1S. This justi\ufb01es the following optimized procedure for performing faster veri\ufb01cation of a batch of Groth16S proofs. Implementations MAY use this procedure to determine whether all proofs in a batch are valid. De\ufb01ne a type Groth16S.BatchEntry : Groth16S.Proof  Groth16S.PrimaryInput representing proof batch entries. De\ufb01ne Groth16S.BatchVerify (entry0 .. \ud835\udc411 Groth16S.BatchEntry\ud835\udc41) B as: For each \ud835\udc570 .. \ud835\udc411: Let ((\ud835\udf0b\ud835\udc57,\ud835\udc34, \ud835\udf0b\ud835\udc57,\ud835\udc35, \ud835\udf0b\ud835\udc57,\ud835\udc36), \ud835\udc4e\ud835\udc57, 0 .. \u2113)  entry\ud835\udc57. Choose random \ud835\udc67\ud835\udc57 \ud835\udc5fS  R 1 .. 2128 1.",
      "Let Accum\ud835\udc34\ud835\udc35 \ud835\udc411 \ud835\udc570 MillerLoopS \ud835\udc67\ud835\udc57 \ud835\udf0b\ud835\udc57,\ud835\udc34, \ud835\udf0b\ud835\udc57,\ud835\udc35 Let Accum\u0394  \ud835\udc411 \ud835\udc570 \ud835\udc67\ud835\udc57 \ud835\udf0b\ud835\udc57,\ud835\udc36. Let Accum\u0393,\ud835\udc56 \ud835\udc411 \ud835\udc570 (\ud835\udc67\ud835\udc57 \ud835\udc4e\ud835\udc57,\ud835\udc56) (mod \ud835\udc5fS) for \ud835\udc560 .. \u2113. Let Accum\ud835\udc4c  \ud835\udc411 \ud835\udc570 \ud835\udc67\ud835\udc57(mod \ud835\udc5fS). Return 1 if FinalExpS Accum\ud835\udc34\ud835\udc35 MillerLoopS Accum\u0394, \u0394  MillerLoopS (\u2113 \ud835\udc560Accum\u0393,\ud835\udc56 \u03a8\ud835\udc56, \u0393 ))  \ud835\udc4cAccum\ud835\udc4c 1S, otherwise 0. The \ud835\udc67\ud835\udc57values MUST be chosen independently of the proof batch entries. Non-normative note: It is also acceptable to sample each \ud835\udc67\ud835\udc57from 0 .. 2128 1, since the probability of obtaining zero for any \ud835\udc67\ud835\udc57is negligible. The performance bene\ufb01t of this approach arises from computing two of the three Miller loops, and the \ufb01nal exponentiation, per batch instead of per proof. For the multiplications by \ud835\udc67\ud835\udc57, an ef\ufb01cient algorithm for multiscalar multiplication such as Pippingers method Bernstein2001 or the BosCoster method deRooij1995 may be used.",
      "Note: Spend proofs (of the statement in section4.18.2 Spend Statement (Sapling) on page 61) and output proofs (of the statement in section4.18.3 Output Statement (Sapling) on page 62) use different veri\ufb01cation keys, with different parameters \u0394, \u0393, \ud835\udc4c, and \u03a80 .. \u2113. It is straightforward to adapt the above procedure to handle multiple veri\ufb01cation keys; the accumulator variables Accum\u0394, Accum\u0393,\ud835\udc56, and Accum\ud835\udc4care duplicated, with one term in the veri\ufb01cation equation for each variable, while Accum\ud835\udc34\ud835\udc35is shared.",
      "Neglecting multiplications in S(\ud835\udc5f) \ud835\udc47and F\ud835\udc5fS, and other trivial operations, the cost of batched veri\ufb01cation is therefore  for each proof: the cost of decoding the proof representation to the form Groth16S.Proof, which requires three point decompressions and three subgroup checks (two for S(\ud835\udc5f) and one for S(\ud835\udc5f)  for each successfully decoded proof: a Miller loop; and a 128-bit scalar multiplication by \ud835\udc67\ud835\udc57in S(\ud835\udc5f)  for each veri\ufb01cation key: two Miller loops; an exponentiation in S(\ud835\udc5f) \ud835\udc47; a multiscalar multiplication in S(\ud835\udc5f) 1 with \ud835\udc41128-bit scalars to compute Accum\u0394; and a multiscalar multiplication in S(\ud835\udc5f) 1 with \u2113 1 255-bit scalars to compute \u2113 \ud835\udc560Accum\u0393,\ud835\udc56 \u03a8\ud835\udc56;  one \ufb01nal exponentiation. Ed25519 batch validation The reference validation algorithm for Ed25519 signatures is de\ufb01ned in section5.4.6 Ed25519 on page 90. Canopy onward Implementations MAY alternatively use the optimized procedure described in this section to perform faster validation of a batch of signatures, i.e.",
      "to determine whether all signatures in a batch are valid. The correctness of this procedure is dependent on the Ed25519 validation changes made for the Canopy network upgrade in ZIP-215 (in particular the change to use the cofactor variant of the validation equation). The input is a sequence of \ud835\udc41signature batch entries, each of which is a (validating key, message, signature) triple. Let \u2113, \ud835\udc35, abstBytesEd25519, and reprBytesEd25519 be as de\ufb01ned in section5.4.6 Ed25519 on page 90. Let LEOS2IP be as de\ufb01ned in section5.1 Integers, Bit Sequences, and Endianness on page 73. SHA-512 is de\ufb01ned in section5.4.1.1 SHA-256, SHA-256d, SHA256Compress, and SHA-512 Hash Functions on page 75. De\ufb01ne Ed25519.BatchEntry : Ed25519.Public  Ed25519.Message  Ed25519.Signature. De\ufb01ne Ed25519.BatchValidate (entry0 .. \ud835\udc411 Ed25519.BatchEntry\ud835\udc41) B as: For each \ud835\udc570 .. \ud835\udc411: Let (\ud835\udc34\ud835\udc57, \ud835\udc40\ud835\udc57, \ud835\udf0e\ud835\udc57)  entry\ud835\udc57. Let \ud835\udc45\ud835\udc57be the \ufb01rst 32 bytes of \ud835\udf0e\ud835\udc57, and let \ud835\udc46\ud835\udc57be the remaining 32 bytes.",
      "Let \ud835\udc45\ud835\udc57 abstBytesEd25519(\ud835\udc45\ud835\udc57), and let \ud835\udc46\ud835\udc57 LEOS2IP256(\ud835\udc46\ud835\udc57). Let \ud835\udc34\ud835\udc57 reprBytesEd25519(\ud835\udc34\ud835\udc57). Let \ud835\udc50\ud835\udc57 LEOS2IP512 SHA-512(\ud835\udc45\ud835\udc57 \ud835\udc34\ud835\udc57 \ud835\udc40\ud835\udc57) Choose random \ud835\udc67\ud835\udc57 R 1 .. 2128 1. Return 1 if  for all \ud835\udc570 .. \ud835\udc411, \ud835\udc45\ud835\udc57 and \ud835\udc46\ud835\udc57 \u2113; and  8 \ud835\udc411 \ud835\udc570 (\ud835\udc67\ud835\udc57 \ud835\udc46\ud835\udc57) (mod \u2113) \ud835\udc35 \ud835\udc411 \ud835\udc570 \ud835\udc67\ud835\udc57 \ud835\udc45\ud835\udc57 \ud835\udc411 \ud835\udc570 \ud835\udc67\ud835\udc57 \ud835\udc50\ud835\udc57(mod \u2113) \ud835\udc34\ud835\udc57  \ud835\udcaaEd25519, otherwise 0. The \ud835\udc67\ud835\udc57values MUST be chosen independently of the signature batch entries. Non-normative note: It is also acceptable to sample each \ud835\udc67\ud835\udc57from 0 .. 2128 1, since the probability of obtaining zero for any \ud835\udc67\ud835\udc57is negligible. The performance bene\ufb01ts of this approach are the same as for sectionB.1 RedDSA batch validation on page 220. List of Theorems and Lemmata Theorem 5.4.1 The encoding function is injective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Lemma 5.4.2 An injectivity property for Sinsemilla . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Theorem 5.4.3 Collision resistance of SinsemillaHash and SinsemillaHashToPoint . . . . . . . . . . . . . . . . .",
      "Theorem 5.4.4 A output from SinsemillaHashToPoint yields a nontrivial discrete log relation . . . . . . . . . Theorem 5.4.5 UncommittedSapling is not in the range of NoteCommitSapling . . . . . . . . . . . . . . . . . . . . . . Theorem 5.4.6 UncommittedOrchard is not in the range of NoteCommitOrchard . . . . . . . . . . . . . . . . . . . . . Lemma 5.4.7 Let \ud835\udc43 (\ud835\udc62, v) J(\ud835\udc5f). Then (\ud835\udc62, v) J(\ud835\udc5f) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 Theorem 5.4.8 \ud835\udc62is injective on J(\ud835\udc5f) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 Theorem A.2.1 (0, 0) is the only point with \ud835\udc66 0 on certain Montgomery curves . . . . . . . . . . . . . . . . . 201 Theorem A.3.1 Correctness of a constraint system for range checks . . . . . . . . . . . . . . . . . . . . . . . . Theorem A.3.2 Exceptional points (ctEdwards Montgomery) . . . . . . . . . . . . . . . . . . . . . . . . . . . Theorem A.3.3 Exceptional points (Montgomery ctEdwards) . . . . . . .",
      ". . . . . . . . . . . . . . . . . . . . Theorem A.3.4 Distinct-\ud835\udc65theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Theorem A.3.5 Concerning addition inputs in the Pedersen circuit . . . . . . . . . . . . . . . . . . . . . . . . .",
      "Index account, 44 Action circuit, 32, 49, 64, 99, 127, 155159 Action description, 8, 9, 1517, 20, 21, 22, 29, 42, 43, 45, 46, 48, 54, 55, 57, 6770, 72, 112, 123, 126, 127, 130, 148, 154, 159, 161, 163 Action statement, 21, 35, 43, 46, 48, 5658, 63, 64, 65, 127, 130, 159, 161163 Action transfer, 18, 20, 2122, 28, 42, 43, 46, 50, 54, 56, 127, 130, 145, 161, 163 activation block, 18, 23, 92, 111, 154 activation block height, 120, 121, 141, 169 ALL CAPS, 7 anchor, 19, 2021, 40, 41, 43, 60, 61, 64, 129, 154 authenticated one-time symmetric encryption, 26, 27, 88 auxiliary input, 34, 35, 47, 48, 50, 57, 6064, 161, 174 Base58Check, 113115, 138, 140 BCTV14, 35, 40, 50, 110, 111, 122, 128, 150, 155, 164, 171, 175, 200 Bech32, 37, 113, 117119, 154, 163, 175 Bech32m, 113, 118, 119, 154, 159 bellman, 111, 119 best valid block chain, 18, 66, 69, 181 big-endian, 100, 102, 108, 111, 112, 120, 134, 204 bilateral consensus rule change, 120 binding (commitment scheme), 24, 31, 53, 55, 56, 9699, 145, 148, 156, 169, 176, 181 binding signature, 94, 147 binding signature (Orchard), 21, 28, 29, 50, 54, 5556, binding signature (Sapling), 20, 28, 29, 50, 52, 5355, 122, 123, 177 binding signature scheme, 92, 95 bit sequence, 11, 12, 36, 38, 49, 6264, 66, 68, 73, 75, 94, 154, 156, 158, 165, 166, 171, 177, 200, 213, 215, 219, Bitcoin, 1, 7, 8, 18, 19, 21, 22, 28, 50, 51, 113, 114, 120, 122, 123, 125127, 131134, 142, 149, 155, 161, 166, 181, Bitcoin Core, 126, 149, 155 block, 16, 1823, 35, 40, 41, 43, 51, 58, 59, 6870, 111, 120, 121, 125, 127, 131, 132136, 140, 142, 155, 157, 159, 168, 170, 181, 182 block chain, 8, 9, 1417, 18, 19, 23, 58, 59, 66, 69, 72, 73, 110112, 126, 132, 136, 142, 144, 162, 168, 170, 179, block chain reorganization, 66, 69, 121 block hash, 23, 111, 154 block header, 18, 75, 131, 132136, 160, 178, 181, 183 block height, 16, 18, 22, 59, 6871, 120123, 125129, 132, 135141, 157159, 164, 171 block subsidy, 22, 120, 125, 136, 139, 152, 156, 165, 183 block target spacing, 134, 136, 141 block timestamp, 131 block version number, 131, 132133, 170, 180 Blossom, 1, 7, 50, 120, 131133, 136, 139, 141, 168171 BLS12-381, 35, 101, 105, 111, 149, 176 BN-254, 35, 99, 102, 111 Bulletproofs, 54 byte sequence, 10, 73, 76, 88, 93, 94, 112, 113, 159, 161, 164, 177, 201, 219 Canopy, 1, 7, 22, 40, 44, 50, 51, 65, 69, 70, 91, 92, 117, 120, 124127, 131133, 138141, 153, 154, 158, 164, 166168, 223 chain value pool, 8, 117, 152, 155 chain value pool balance (deferred development fund), 58, 59 chain value pool balance (Orchard), 58 chain value pool balance (Sapling), 58, 164 chain value pool balance (Sprout), 58, 164 chain value pool balance (transparent), 58 chunk (of a Pedersen hash input), 80 coinbase transaction, 18, 19, 22, 51, 124127, 132, 137140, 155158, 160, 161, 164166, 176, 179, 183 coins (in Zerocash), 8 collision resistance, 23, 24, 26, 33, 7683, 86, 87, 96, 98, 144146, 148150, 162, 178, 184, 213 Commitivk randomness, 38, 118 commitment scheme, 23, 31, 3233, 38, 83, 9699, 145, 146, 148, 156, 169, 175, 181 commitment trapdoor, 14, 15, 31, 41, 42, 45, 46, 54, 98, 143, 153, 175 complete twisted Edwards af\ufb01ne coordinates, 68, 96, 200, 201, 207, 208 complete twisted Edwards compressed encoding, 103, 115, 117, 204 complete twisted Edwards elliptic curve, 12, 45, 91, 102, 103104, 169, 200, 205, 206, 208, 210 consensus branch, 126, 155 consensus branch ID, 50, 51, 123, 124, 154, 161 consensus rule change, 120 coordinate extractor, 33, 164, 179 CryptoNote, 9, 183 ctEdwards, 12, 102, 209, 210, 212214 Decentralized Anonymous Payment scheme, 1, 7 Decisional Dif\ufb01eHellman Problem, 34, 78, 85, 145, 156, 160 default diversi\ufb01ed payment address, 37, 39, 105 deferred development fund chain value pool, 8 deferred development fund lockbox, 139 Discrete Logarithm Problem, 33, 65, 79, 8183, 145, 160, distinct-\ud835\udc65criterion, 179, 207, 210 diversi\ufb01ed base, 25, 37, 45, 46, 67, 78 diversi\ufb01ed payment address, 13, 1415, 37, 39, 47, 48, 78, 79, 145, 150, 160, 170, 172 diversi\ufb01ed transmission key, 15, 37, 39, 42, 43, 45, 46, 67, 129, 130 diversi\ufb01er, 15, 25, 26, 37, 39, 78, 79, 88, 115, 116, 151, 160 diversi\ufb01er index, 37, 39, 116, 151, 160 diversi\ufb01er key, 13, 26, 3739, 116, 118, 151, 161 double-spend, 9, 17, 1922, 65, 145, 148, 150, 180 dummy note, 46, 4748, 143145, 161, 163, 164 ECDSA, 28, 113, 174 Ed25519, 28, 75, 9092, 124, 154, 159, 164, 166168, 176, 179, 183, 223 ephemeral private key, 42, 43, 45, 67, 129, 130 ephemeral public key, 27, 65, 67, 147 Equihash, 1, 24, 86, 131, 132, 133, 134, 149, 170, 174, 180, 182, 183 expanded spending key, 13 extended spending key, 13, 14, 169 family of group hashes into a subgroup, 33 Founders Reward, 22, 124, 125, 136, 137, 139, 170, 176, 180, 182, 183 full node, 179 full validator, 18, 22, 92, 111, 132, 154, 179 full viewing key, 9, 13, 14, 37, 39, 42, 43, 47, 48, 57, 58, 72, 85, 116, 117, 118, 119, 130, 156, 157, 175 funding stream, 22, 124, 125, 127, 136, 139, 140141, 164, genesis block, 18, 23, 125, 131, 132, 139, 142, 164, 181 Groth16, 35, 40, 50, 53, 56, 111, 122, 128, 149, 170, 171, 173, 175, 200, 217, 219, 221 group hash, 33, 104, 109, 162 Halo 2, 21, 35, 50, 55, 56, 81, 112, 149, 150, 155 halo2, 35 halving, 120, 139, 141 hash function, 19, 24, 25, 36, 75, 7982, 8487, 91, 92, 147, 176 hash value (of a Merkle tree node), 21, 49, 76, 99, 160 hash-to-curve, 109 Heartwood, 1, 7, 50, 120, 125127, 131133, 140, 153, 158, 164, 168 hiding (commitment scheme), 31, 39, 9699, 145, 156, Hierarchical Deterministic Wallet, 13 homomorphic Pedersen commitment, 97, 213 Human-Readable Part, 116, 117, 119 in-band, 147, 183 incoming viewing key, 13, 14, 24, 32, 36, 37, 39, 58, 65, 66, 71, 77, 79, 113, 114, 116, 117, 118, 119, 148, 152, 155, 156, 159, 160, 180 incremental Merkle tree, 21, 49, 76, 79, 81, 168, 184 index (of a Merkle tree node), 21, 49 internal node (of a Merkle tree), 49 iso-Pallas, 107, 109, 110 iso-Vesta, 107, 109, 110 IVK Encoding (in a Uni\ufb01ed Incoming Viewing Key), 116, 152 JoinSplit circuit, 119 JoinSplit description, 8, 1517, 19, 20, 22, 28, 29, 39, 40, 43, 44, 46, 47, 51, 65, 66, 72, 111, 122, 125, 126, 128, 142, 144, 147, 164, 174, 179, 180 JoinSplit proof, 47 JoinSplit signature, 29, 50, 51, 183 JoinSplit signing key, 44 JoinSplit statement, 9, 19, 35, 40, 47, 51, 56, 58, 60, 110, 144, 145, 148, 149, 156, 175, 180, 200 JoinSplit transfer, 18, 19, 2022, 39, 40, 46, 5052, 123, 128, 142, 143, 145, 147, 149, 156, 181, 184 Jubjub, 17, 28, 33, 45, 52, 56, 62, 63, 69, 71, 78, 79, 87, 89, 92, 96, 97, 102, 103, 105, 106, 116, 129, 145, 147, 152, 164, 166, 169, 174, 177, 179, 180, 200, 206, 208, 217, 219 key agreement scheme, 26, 27, 36, 38, 65, 67, 8890, Key Derivation Function, 27, 65, 67, 89, 90 key privacy, 9, 27, 78, 147, 148, 150, 170, 172 layer (of a Merkle tree), 21, 49, 62, 64, 99, 159 leaf node (of a Merkle tree), 18, 22, 49, 99 librustzcash, 71, 165 libsnark (Zcash fork), 110, 111, 119, 149 linear combination, 200, 202 little-endian, 96, 116, 118, 119, 125, 134 lockbox disbursement, 125, 140, 151 Mainnet, 18, 22, 23, 41, 71, 74, 75, 94, 111, 113117, 119, 120, 125, 126, 132, 133, 136, 137, 139142, 151, 152, 154, 166, 170, 182 MAY, 7, 18, 37, 39, 41, 42, 50, 54, 56, 57, 69, 92, 94, 98, 111, 113, 153, 154, 220, 222, 223 median-time-past, 131, 132, 168 memo \ufb01eld, 15, 65, 67, 72, 142, 162, 163, 168, 179, 183 mempool, 69, 71, 165 Merkle path, 47, 48, 49, 60, 61, 64, 151, 210 Mimblewimble, 54 miner subsidy, 22, 51, 136, 156, 183 monomorphism, 30, 168 Montgomery af\ufb01ne coordinates, 200, 201, 206, 207 Montgomery elliptic curve, 172, 200, 201, 206, 209, 210, 212, 224 multi-party computation, 119, 120 MUST, 7, 16, 1822, 4045, 5052, 58, 59, 6266, 68, 69, 71, 91, 94, 111, 112, 114119, 121, 122, 124127, 129, 130, 132, 134, 138, 140, 152, 155159, 161, 164, 166, 173, 221223 MUST NOT, 7, 22, 41, 42, 69, 91, 100, 102, 103, 106, 110, 124126, 132, 155, 157, 161, 174 network, 18, 22, 23, 141 network upgrade, 18, 23, 50, 51, 110, 111, 117, 120, 121, 127, 136, 141, 151, 154, 155, 171, 223 node (of a Merkle tree), 21, 49 non-canonical (compressed encoding of a point), 32, 41, 43, 69, 71, 94, 115, 116, 125, 152, 164, 165 non-canonical (encoding of a \ufb01eld element), 41, 42, 49, 160, 213 nonmalleability (of proofs), 35 nonmalleability (of signatures), 29 note, 8, 9, 14, 1517, 1921, 24, 25, 29, 3944, 4649, 51, 5658, 6569, 7173, 87, 112, 127130, 142146, 148, 149, 153, 154, 156, 158, 161164, 167, 177, 210 note commitment, 8, 9, 15, 16, 17, 19, 21, 22, 24, 40, 42, 43, 49, 57, 58, 6668, 128130, 143146, 148, 149, 153, 156, 157, 162, 176, 180, 210 note commitment scheme, 31, 95, 96, 98, 148, 210 note commitment tree, 16, 17, 19, 21, 22, 49, 77, 123, 128, 129, 131, 132, 144, 145, 157, 158, 178 note plaintext, 15, 16, 42, 43, 65, 67, 73, 112, 126, 127, 129, 130, 148, 153, 162, 165167 note plaintext lead byte, 16, 4548, 126, 148, 151, 158, 167 note position, 8, 17, 21, 57, 144 note traceability set, 9, 176 NU5, 1, 7, 13, 16, 18, 22, 25, 35, 41, 50, 51, 65, 69, 71, 93, 94, 120, 124127, 131133, 144, 152154, 157164 NU6, 1, 7, 19, 51, 59, 120, 125, 133, 141, 151, 152 NU6.1, 1, 7, 18, 23, 51, 59, 120, 133, 140, 141, 151 nulli\ufb01er, 8, 9, 14, 15, 17, 19, 20, 22, 25, 40, 41, 43, 46, 57, 58, 69, 72, 73, 83, 85, 87, 127130, 143146, 148, 150, 153, 162, 165, 177, 181 nulli\ufb01er deriving key, 8, 17, 38, 57, 58, 69, 118 nulli\ufb01er private key, 13 nulli\ufb01er set, 17, 19, 22, 66, 69 one-time (authenticated symmetric encryption), 26 one-time (signature scheme), 29 open (a commitment), 31 OPTIONAL, 7, 44 Orchard, 8, 9, 1322, 2528, 32, 34, 35, 38, 39, 45, 46, 4850, 54, 56, 57, 65, 6773, 76, 78, 81, 85, 87, 88, 95, 97, 105, 107, 108, 112, 113, 117119, 123, 126, 127, 140, 144156, 158163 Orchard balancing value, 54, 55 out-of-band, 6568, 73 outgoing cipher key, 42, 43, 67, 86, 87, 129, 130, 161 outgoing ciphertext, 67, 71, 86, 87, 148, 156, 161, 177 outgoing viewing key, 13, 36, 38, 39, 4446, 67, 70, 117, 126, 158, 174, 175 Output circuit, 119, 177, 209, 210 Output description, 8, 9, 1517, 20, 21, 29, 32, 41, 42, 44, 45, 5254, 67, 68, 70, 72, 98, 111, 122, 123, 125, 126, 129, 130, 148, 153, 164, 174, 178, 179 Output statement, 20, 35, 42, 45, 54, 62, 65, 111, 129, 174, Output transfer, 18, 20, 28, 41, 52, 54, 142, 143, 145 Overwinter, 1, 7, 50, 76, 120, 121, 124127, 132, 170, 174, 175, 178180 packing, 202 Pallas, 17, 28, 33, 39, 43, 45, 49, 55, 56, 58, 64, 65, 81, 85, 90, 92, 9799, 105, 106108, 110, 118, 119, 130, 145149, 152, 156, 162 partitioning oracle attack, 148, 156 paying key, 14, 47 payment address, 113, 117, 140, 154, 160, 163 Pedersen commitment, 20, 21, 5355, 79, 81, 96, 145, 177179, 209, 212 Pedersen hash, 33, 79, 81, 96, 144, 146, 176179, 210, 212, Pedersen value commitment, 20, 21, 143 peer-to-peer protocol, 18, 4446, 121, 124, 131, 142, 176, piece (of a Sinsemilla hash input), 82 PLONK, 150 point at in\ufb01nity, 105 positioned note, 17, 57, 69, 144 prescribed way (to pay a funding recipient), 140 prevout (previous output), 125, 155 primary input, 34, 4043, 49, 57, 6064, 159, 160, 173 prime order (of a group element), 71, 153, 176, 210 prime-order curve, 102, 110, 147 prime-order group, 78, 83 prime-order subgroup, 52, 78, 90, 153, 165, 208 private key, 8, 9, 13, 26, 27, 29, 32, 38, 39, 54, 67, 68, 71, 88, 114, 147, 160, 161 proof authorizing key, 8, 13, 36, 57, 86 proof batch entry, 222 proof-of-work, 1, 133, 149 proving key (for a zk-SNARK), 34, 35, 119 proving system (preprocessing zk-SNARK), 1, 79, 20, 21, 34, 35, 110112, 143, 150, 151, 171, 172, 174, 183 Pseudo Random Function, 17, 23, 25, 26, 26, 36, 37, 75, 86, 87, 96, 146, 148, 179 Pseudo Random Permutation, 26, 79, 88 public key, 14, 26, 27, 40, 42, 43, 66, 68, 78, 88, 114, 115, 118, 128130, 147, 148 Quadratic Arithmetic Program, 110, 111, 200 quadratic constraint program, 7, 110, 111, 180, 200, 202, 205, 206 random oracle, 33, 34, 78, 79, 83, 86, 105, 107, 110, 166, randomized Spend validating key, 177 randomizer, 29, 30, 56 Rank 1 Constraint System, 200 raw encoding, 66, 68, 113, 114119, 165 re-randomized, 29, 30, 56, 92, 95 receiving key, 9, 13, 180 RECOMMENDED, 7, 15, 37, 94, 113 represented group, 32, 33, 92, 102, 105, 164, 176 represented pairing, 34, 99, 101, 176 represented subgroup, 32, 33, 34 root (of a Merkle tree), 21, 49, 123, 128, 129, 131, 132, 158 RPC byte order, 23, 120 Sapling, 1, 79, 13, 1422, 2429, 31, 33, 3537, 40, 4450, 52, 54, 56, 57, 65, 6773, 7679, 8688, 95, 97, 102, 105, 110113, 115117, 119, 120, 122129, 131133, 140, 143154, 157, 158, 160, 161, 163165, 167180, 200, 202205, 208210, 212217, 219 Sapling balancing value, 52, 175 secp256k1, 28 segment (of a Pedersen hash input), 80 serial numbers (in Zerocash), 8 settled, 18, 111, 151, 155 SHA-256, 75, 76, 86, 95, 96, 119, 120, 145, 156, 178 SHA-256d, 75, 124, 131, 133, 134 SHA-512, 75, 91, 166, 223 SHA256Compress, 75, 76, 86, 96, 114, 145, 146, 149, 178, 181 shielded, 8, 19, 4345, 71, 127, 142, 143, 165 shielded input, 8, 19, 20, 47, 48 shielded output, 8, 9, 20, 21, 65, 67, 126, 127, 160, 166 shielded payment address, 8, 9, 13, 14, 15, 24, 25, 4548, 56, 67, 72, 7779, 88, 113, 114, 115, 117, 118, 127, 140, 147, 161, 163, 165, 181, 183 shielded protocol, 7, 16, 58, 148, 171 shielded transfer, 8 short Weierstrass af\ufb01ne coordinates, 39, 43, 49, 64, 68, 85, 99, 109, 110, 156 short Weierstrass compressed encoding, 118 short Weierstrass elliptic curve, 105, 107, 109, 162 SHOULD, 7, 16, 18, 4446, 53, 55, 79, 110, 126, 132 SHOULD NOT, 7, 39, 170 side-channel, 24, 147, 148 SIGHASH algorithm, 50, 51, 160 SIGHASH transaction hash, 41, 43, 50, 5357, 76, 122124, 142, 154, 161, 168, 174, 182 SIGHASH type, 50, 51, 53, 55, 57, 156, 174, 184 signature batch entry, 220, 221, 223 signature scheme, 23, 28, 2930, 76, 90, 92, 95, 177 signature scheme with key monomorphism, 30, 95, signature scheme with re-randomizable keys, 29, 36, 38, 56, 95 signing key, 28, 2930, 51, 53, 55, 175, 177 Sinsemilla commitment, 98, 145 Sinsemilla hash, 98, 146 slanted text, 7 small order (of a group element), 41, 42, 61, 62, 71, 153, 174, 176, 178, 208, 218, 220 spend authorization address key, 56, 65 spend authorization private key, 57 spend authorization randomizer, 57, 172 spend authorization signature, 40, 41, 42, 43, 50, 52, 54, 56, 57, 65, 127, 128, 130, 147, 172, 177 spend authorization signature scheme, 56, 92, 95 Spend authorizing key, 13, 36, 38, 86 Spend circuit, 49, 119, 153, 160, 177, 209, 210, 216 Spend description, 8, 17, 20, 2122, 29, 32, 40, 41, 47, 5254, 56, 57, 72, 98, 111, 122, 123, 125129, 153, 174, 178, 179 Spend proof, 49 Spend statement, 20, 24, 25, 35, 41, 47, 54, 5658, 61, 62, 77, 111, 128, 159, 174, 176 Spend transfer, 18, 20, 2122, 28, 40, 50, 52, 54, 128, 143, Spend validating key, 38, 118 spending authority, 9, 14, 37, 39, 57, 79 spending key, 8, 9, 13, 14, 17, 25, 3638, 47, 56, 57, 66, 72, 113, 114, 115, 117, 119, 143, 144, 148, 155, 159, 165, 178, 181, 183 Sprout, 79, 13, 1420, 22, 25, 27, 31, 35, 36, 43, 44, 4752, 57, 6567, 72, 73, 76, 111115, 117, 119, 120, 127, 128, 144150, 153, 154, 157, 162, 163, 165, 167169, 171, 173175, 177, 178 standard P2SH script, 138, 140 standard redeem script, 138, 140 statement, 20, 21, 34, 56, 60, 149, 178, 200, 217, 219, 222 synthetic blinding factor, 54 target threshold, 131, 134, 135, 136 TAZ, 23 tazoshi, 23 Testnet, 18, 23, 41, 71, 74, 75, 94, 113117, 119, 125, 126, 132, 133, 136, 138, 139, 141, 142, 151, 152, 154, 165, 166, 169, 171, 180, 182 total input value, 125 total issued supply, 59, 152 total output value, 125 transaction, 8, 9, 1622, 29, 3946, 5057, 66, 6873, 111, 122, 123, 124130, 132, 142144, 155, 158, 159, 161164, 170, 174, 179, 184 transaction binding validating key, 53, 55, 125 transaction fee, 22, 125, 165 transaction ID, 18, 124, 127, 158 transaction value pool (Orchard), 54 transaction value pool (Sapling), 52 transaction value pool (transparent), 18, 19, 40, 51, 52, 54, 128, 157 transaction version number, 16, 50, 68, 70, 111, 122, 123, 124, 126129, 165, 180 transactions, 9, 14, 18, 1922, 28, 29, 3942, 4446, 5052, 5658, 66, 69, 71, 76, 112, 121, 125127, 129, 131, 132, 144, 154158, 160162, 165, 173, 174 transmission key, 9, 14, 15, 27, 44, 65, 66, 71, 165 transmitted note ciphertext, 6870, 148 transmitted note ciphertext (Orchard), 43, 46, 70, 130 transmitted note ciphertext (Sapling), 42, 45, 68, 6973, 130 transmitted notes ciphertext (Sprout), 40, 44, 65, 66, 72, 128 transparent, 8, 9, 1921, 51, 113, 122, 123, 125, 127, 142, 164, 170, 182 transparent address, 113, 137, 182 transparent address (P2PKH), 113, 117, 174, 182 transparent address (P2SH multisig), 138, 140 transparent address (P2SH), 113, 117, 140, 182 transparent input, 18, 19, 22, 50, 51, 122125, 127, 155, 160, 161, 167 transparent output, 18, 19, 51, 122125, 127, 137, 164, 167 treestate, 16, 17, 19, 2022, 40, 41, 43, 131, 132, 174, 181, uni\ufb01ed full viewing key, 117, 119, 160 uni\ufb01ed incoming viewing key, 116, 117, 118, 151, 152, 160 uni\ufb01ed payment address, 113, 117, 118, 154, 160, 161, 163 Uniform Random String, 33, 34, 175 unpacking, 202 US-ASCII, 10, 120 UTXO (unspent transaction output), 58, 125, 142, 143, UTXO (unspent transaction output) set, 19, 21, 58 valid block chain, 17, 18, 22, 127, 144 valid Equihash solution, 132, 133, 134 validating key (for a signature scheme), 28, 2930, 39, 41, 43, 51, 54, 56, 91, 94, 95, 103, 113, 122, 124, 129, 130, 167, 175177, 179, 220, 223 value commitment, 8, 9, 20, 21, 29, 4143, 46, 5256, 67, 129, 130 value commitment scheme, 53, 56 verifying key (for a zk-SNARK), 34, 35, 119 version group ID, 124, 126, 165 Vesta, 33, 35, 105, 106107, 110, 149, 162 weak PRF, 147 windowed, 96 windowed Pedersen commitment, 213 wtxid, 18, 124, 158 zatoshi, 14, 15, 23, 52, 54, 74, 75, 125, 136, 138, 140 Zcash, 1, 79, 13, 18, 19, 2224, 28, 34, 35, 50, 51, 73, 75, 76, 94, 102, 105, 108, 110115, 119, 120, 122, 123, 126, 127, 131, 133, 134, 141146, 148151, 153, 165, 166, 174, 181, 182, 184 zcashd, 18, 22, 37, 71, 92, 134, 136, 145, 151, 153, 155, 156, 164, 165, 168170, 179, 180 zebra, 18, 22, 71 ZEC, 14, 22, 23 Zerocash, 1, 7, 8, 24, 26, 51, 142150, 153, 163, 180, 182184 zk-SNARK circuit, 79, 102, 105, 119 zk-SNARK proof, 16, 17, 1921, 29, 34, 35, 4043, 56, 57, 111, 123, 128130, 145, 147, 150, 155, 156, 173, 174,"
    ],
    "word_count": 106498,
    "page_count": 229
  }
}