% ============================================================================
% Do Whitepaper Claims Predict Market Behavior?
% Evidence from Cryptocurrency Factor Analysis
% ============================================================================
% arXiv-style single-column format
% ============================================================================

\documentclass[letterpaper,11pt]{article}

% ============================================================================
% PACKAGES
% ============================================================================

% Typography and layout
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{mathpazo}  % Palatino font
\usepackage[letterpaper, margin=1in]{geometry}
\usepackage{setspace}
\setstretch{1.1}

% Mathematics
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm}

% Graphics and tables
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{array}
\usepackage{longtable}
\usepackage{multirow}

% References and links
\usepackage[round,authoryear]{natbib}
\bibliographystyle{plainnat}
\usepackage{xcolor}
\definecolor{farzullaburgundy}{RGB}{128,0,32}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=farzullaburgundy,
    citecolor=farzullaburgundy,
    urlcolor=farzullaburgundy,
    pdftitle={Do Whitepaper Claims Predict Market Behavior?},
    pdfauthor={Murad Farzulla}
}

% Formatting
\usepackage{enumitem}
\usepackage{tcolorbox}
\usepackage{url}
\def\UrlBreaks{\do\/\do-\do_}

% Header/footer - minimal
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\fancyfoot[C]{\thepage}

% Theorem environments
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]

% ============================================================================
% DOCUMENT
% ============================================================================

\begin{document}

% ============================================================================
% TITLE PAGE
% ============================================================================

\title{\textbf{Do Whitepaper Claims Predict Market Behavior?} \\
\large Evidence from Cryptocurrency Factor Analysis}

\author{Murad Farzulla$^{1,2}$\\[0.3em]
{\small $^1$\,King's College London \quad $^2$\,\href{https://dissensus.ai}{Dissensus AI}}\\[0.3em]
{\small \href{https://orcid.org/0009-0002-7164-8704}{ORCID: 0009-0002-7164-8704}}}

\date{January 2026\\[0.5em]
{\small Corresponding Author: \href{mailto:murad@dissensus.ai}{murad@dissensus.ai}}}

\maketitle

\begin{abstract}
\noindent Cryptocurrency projects articulate their value propositions through whitepapers, making foundational claims about functionality, use cases, and technical capabilities. This study investigates whether these narrative claims align with empirically observed market behavior. We construct a novel pipeline combining natural language processing (NLP) with tensor decomposition to compare three representational spaces: (1) a claims matrix derived from zero-shot classification of 38 whitepapers across 10 semantic categories using BART-MNLI, (2) a market statistics matrix capturing 7 financial metrics for 49 cryptocurrency assets over two years of hourly data (17,543 timestamps), and (3) latent factors extracted via CP tensor decomposition (rank 2, explaining 92.45\% of variance). Using Procrustes rotation and Tucker's congruence coefficient ($\phi$), we test alignment between narrative and market spaces across 37 common entities.

Results indicate weak alignment: claims--statistics ($\phi = 0.246$, $p = 0.339$), claims--factors ($\phi = 0.058$, $p = 0.751$), and statistics--factors ($\phi = 0.174$, $p < 0.001$). Critically, the statistics--factors significance ($p < 0.001$) validates our methodology: since both matrices derive from the same market data, significant alignment confirms the Procrustes pipeline detects relationships when present, establishing that the narrative null result represents observed weak alignment rather than measurement failure. Inter-model validation using DeBERTa-v3 yields 32\% exact agreement (Cohen's $\kappa = 0.14$) but 67\% relaxed (top-3) agreement, indicating models capture similar semantic neighborhoods with different decision boundaries. Cross-sectional analysis reveals XMR, CRV, YFI, and SOL exhibit positive alignment contributions, while RPL, HBAR, AAVE, and SUSHI show the largest divergence.

We interpret these findings as consistent with weak alignment between whitepaper narratives and realized market factor structure. Limited statistical power ($n = 37$) precludes distinguishing weak alignment from no alignment, but we can confidently reject strong alignment ($\phi \geq 0.70$). Implications for narrative economics, market efficiency, and investment analysis are discussed.

\vspace{1em}
\noindent\textbf{Keywords:} Cryptocurrency, Tensor Decomposition, NLP, Factor Analysis, Procrustes Rotation, Tucker's Congruence Coefficient, Zero-Shot Classification

\vspace{0.5em}
\noindent\textbf{JEL Codes:} G14, G12, C38, C45
\end{abstract}

\vspace{1em}

\noindent\textbf{Acknowledgements.} The author acknowledges Claude (Anthropic) for assistance with pipeline development, mathematical exposition, and technical writing. All errors, omissions, and interpretive limitations remain the author's responsibility.

\vspace{0.5em}
\noindent\textbf{Data \& Code Availability.} Reproducible code and data are available at \url{https://github.com/studiofarzulla/tensor-defi}.

\newpage

% ============================================================================
% MAIN BODY
% ============================================================================

\section{Introduction}

Cryptocurrency markets present a unique laboratory for studying the relationship between narrative and price. Unlike traditional equities, where value propositions emerge gradually through earnings reports and analyst coverage, cryptocurrency projects typically articulate comprehensive visions at inception through whitepapers. These foundational documents make explicit claims about functionality, use cases, and technical architecture---claims that should, in principle, relate to how assets behave in markets.

The efficient market hypothesis suggests that asset prices reflect available information \citep{fama1970efficient}. If whitepapers constitute meaningful information about project characteristics, we might expect narrative claims to align with market behavior. Conversely, \citet{shiller2017narrative} argues that ``narrative economics'' drives market dynamics through stories that spread virally, potentially decoupling prices from fundamentals. Recent work has begun examining this in cryptocurrency markets, with \citet{aste2019cryptocurrency} documenting significant correlations between prices and sentiment across nearly two thousand cryptocurrencies. Shiller's framework emphasizes how narratives propagate and influence aggregate behavior; we adapt this insight to test a related but distinct hypothesis about structural alignment between project-level narratives and market factor structure.

This tension motivates our central research question: \textit{Do whitepaper claims predict market behavior?} Specifically, do the functional narratives articulated in project whitepapers align with empirically observed market factor structure? We emphasize that our methodology measures contemporaneous structural alignment between representational spaces rather than predictive relationships; the term ``predict'' in our research question should be understood as ``exhibit structural correspondence with'' rather than temporal forecasting.

We address this question through a novel methodological pipeline combining natural language processing with tensor decomposition. Our approach constructs three distinct representational spaces:

\begin{enumerate}
    \item A \textbf{claims matrix} $\mathbf{C} \in \mathbb{R}^{N \times K}$ derived from zero-shot classification of whitepaper text across $K = 10$ semantic categories
    \item A \textbf{market statistics matrix} $\mathbf{S} \in \mathbb{R}^{M \times J}$ capturing $J = 7$ financial metrics across $M = 49$ assets
    \item \textbf{Latent factors} $\mathbf{F} \in \mathbb{R}^{M \times R}$ extracted from a high-dimensional market tensor via CP decomposition
\end{enumerate}

Using Procrustes rotation and Tucker's congruence coefficient, we then test whether these spaces align---whether assets that make similar claims exhibit similar market behavior.

Our findings reveal weak alignment across comparisons ($\phi < 0.25$), with one notable exception: the statistics--factors comparison achieves statistical significance ($p < 0.001$) despite weak magnitude, indicating systematic correspondence between market metrics and tensor-derived factors that narrative claims fail to capture. Specialized tokens (XMR, CRV, YFI, SOL) show positive alignment contributions, while DeFi infrastructure tokens (RPL, HBAR, AAVE, SUSHI) exhibit the largest narrative-market divergence. This result is robust to temporal variation, subsample perturbation, decomposition method (CP vs Tucker), and rank selection.

\textbf{Contributions.} This paper makes four contributions: (1) we introduce a reproducible pipeline for comparing textual and market representational spaces in cryptocurrency research, (2) we provide detailed methodological exposition of tensor decomposition for factor extraction, (3) we deliver rigorous empirical evidence on the (mis)alignment between project narratives and market behavior, and (4) we demonstrate that null results in this domain constitute valid findings with implications for narrative economics.

The remainder of this paper is organized as follows. Section~\ref{sec:literature} reviews related work. Section~\ref{sec:data} describes our data sources. Section~\ref{sec:methodology} provides detailed methodology with full mathematical exposition. Section~\ref{sec:results} presents comprehensive results including sensitivity analyses. Section~\ref{sec:discussion} interprets findings, and Section~\ref{sec:conclusion} concludes.

\section{Related Work}
\label{sec:literature}

\subsection{Cryptocurrency Narratives and Sentiment}

Research on cryptocurrency narratives spans social media analysis, whitepaper studies, and sentiment measurement. \citet{chen2019bitcoin} show that Twitter sentiment predicts Bitcoin returns at short horizons, while \citet{ante2023tweet} find that Elon Musk's tweets generate significant abnormal returns for mentioned cryptocurrencies. \citet{haykir2022speculative} document speculative bubbles driven by narrative contagion across crypto assets. \citet{liu2021risks} establish that cryptocurrency returns exhibit momentum, size effects, and exposure to market-wide factors distinct from traditional assets.

Whitepaper analysis has received growing attention as a signal of project quality. \citet{howell2020initial} examine ICO whitepaper quality as a signal of project legitimacy, finding that technical depth correlates with fundraising success. \citet{fisch2019initial} show that whitepaper informativeness predicts ICO outcomes. \citet{adhami2018why} provide the first comprehensive empirical description of ICO determinants, finding that code availability, token presale, and utility design predict success. More recent work has applied sophisticated NLP to whitepaper content: \citet{thewissen2022unpacking} use topic modeling on 5,210 whitepapers to show that technical feature topics predict success while whitepaper informativeness diminishes post-listing. Critically, \citet{momtaz2021moral} demonstrates that issuers systematically exaggerate whitepaper claims---exaggeration raises funds short-term but causes token depreciation and platform failure long-term. \citet{samieifar2021read} find that whitepaper length and complexity correlate positively with funds raised, supporting signaling interpretations. \citet{florysiak2022experts} show that while expert ratings initially ``jam'' whitepaper signals, post-listing returns are better predicted by whitepaper content than analyst assessments.

However, these studies focus on cross-sectional prediction at issuance rather than ongoing alignment between narrative and market behavior. Indeed, \citet{suriano2025information} find that clustering cryptocurrencies by whitepaper content does not yield significant differences in time series dynamics, suggesting narratives may matter for initial fundraising but not long-term price behavior. Recent high-profile failures illustrate the gap between whitepaper claims and realized outcomes: \citet{briola2022terra} analyze the Terra-Luna collapse, demonstrating how algorithmic stablecoin mechanisms failed despite elaborate whitepaper specifications, while \citet{briola2023ftx} document how centralized exchange fragility propagates through cryptocurrency markets. Our work extends this by testing whether claims align with ongoing market factor structure.

\textbf{Narrative Economics Framework.} \citet{shiller2017narrative} provides the theoretical foundation for studying how narratives shape economic outcomes. This builds on foundational work in behavioral finance: \citet{baker2006investor} construct the canonical investor sentiment index, demonstrating that sentiment-driven mispricing is strongest for speculative, hard-to-value stocks---a category that includes most cryptocurrencies. \citet{baker2007investor} survey evidence on how sentiment affects both aggregate returns and cross-sectional pricing. \citet{tetlock2007giving} shows that media pessimism predicts market downturns and high trading volume, establishing the textual analysis framework for investor sentiment. Applied to cryptocurrency, this framework suggests that project narratives---embodied in whitepapers---should influence investor beliefs and thus market prices. Our study tests this prediction empirically, finding weak support for narrative-market coupling. This complements information-theoretic approaches \citep{keskin2019information} that find nonlinear causality between social sentiment and cryptocurrency returns.

\textbf{Natural Language Processing in Finance.} The application of NLP to financial text has grown substantially, with transformer-based models enabling sophisticated document analysis \citep{loughran2020textual}. \citet{loughran2011when} demonstrate that general sentiment dictionaries misclassify financial text, motivating domain-specific approaches. \citet{kearney2014textual} provide a comprehensive methodological survey of textual sentiment methods and models. Domain-specific pre-training has proven effective: \citet{araci2019finbert} introduce FinBERT, outperforming Loughran-McDonald dictionaries on financial sentiment, while \citet{huang2020finbert} show similar gains on analyst reports. \citet{mishev2020evaluation} benchmark lexicons against transformers (BERT, FinBERT) across financial datasets, finding transformer superiority for nuanced sentiment. Zero-shot classification, as employed here, allows categorization without domain-specific training data \citep{lewis2020bart}. \citet{bartolucci2020butterfly} demonstrate that sentiment extracted from developer communications (GitHub) predicts cryptocurrency price movements, validating NLP approaches in this domain. Our use of BART-MNLI represents a middle ground---leveraging powerful pre-trained representations while acknowledging potential limitations in cryptocurrency-specific semantics.

\textbf{Cryptocurrency Market Microstructure.} Beyond sentiment, cryptocurrency markets exhibit distinctive microstructure features: 24/7 trading, global fragmentation across exchanges, varying levels of market manipulation, and high correlation with Bitcoin. These features may dominate narrative effects. \citet{pappalardo2018blockchain} document inefficiencies in Bitcoin's peer-to-peer network that diverge from whitepaper ideals of decentralization. \citet{farzulla2025event} demonstrate that infrastructure disruption events generate significantly larger volatility responses than regulatory announcements, suggesting market participants weight technical fundamentals over policy uncertainty. This asymmetry---where operational failures matter more than regulatory shifts---implies that whitepaper claims about technical capabilities may warrant particular attention in volatility modeling. Our inclusion of multiple market statistics (volatility, liquidity, drawdown) attempts to capture this microstructure, but residual factors may remain.

\subsection{Factor Models in Cryptocurrency}

Traditional asset pricing employs factor models to explain cross-sectional return variation. \citet{fama1970efficient} established the theoretical foundation for efficient markets and factor-based returns. \citet{fama1993common} introduced the three-factor model for equities; analogous developments in cryptocurrency have emerged more recently. \citet{livan2011fine} demonstrate how random matrix theory can distinguish signal from noise in financial correlation matrices---a perspective we extend to narrative-factor comparisons. \citet{caccioli2018network} provide a comprehensive review of network-based approaches to financial systemic risk, complementing factor-based perspectives with topological analysis.

\citet{liu2019common} establish the foundational three-factor model for cryptocurrency returns: market, size, and momentum. \citet{liu2021risks} extend this work, finding that these factors explain substantial cross-sectional variation analogous to Fama-French factors for equities. \citet{bianchi2021factor} apply Instrumented PCA (IPCA) to show that time-varying factor loadings outperform observable risk factors. \citet{dobrynskaya2020downside} extends crypto CAPM to include downside beta as a fourth factor, finding significant cross-sectional premia across 1,700 coins. \citet{bhambhwani2019blockchain} introduce blockchain-native factors---computing power, network size---as procyclical pricing factors with positive risk premia.

Our tensor decomposition implicitly captures similar factors---Factor 1 (dominated by Bitcoin) resembles the market factor, while Factor 2 may capture size or sector effects. These systematic factors may dominate any narrative-based signal, explaining why whitepaper claims fail to predict factor structure.

\textbf{Multi-Way Data in Finance.} Financial data naturally exhibits multi-way structure: assets $\times$ time $\times$ features. While matrix methods (PCA, factor analysis) collapse this structure, tensor decomposition preserves it. Our work contributes to the emerging literature applying tensor methods to financial data, demonstrating both their utility (interpretable factors) and limitations (modest alignment despite high explanatory power).

\subsection{Tensor Methods in Finance}

Tensor decomposition provides a natural framework for analyzing multi-way financial data. \citet{kolda2009tensor} provide a comprehensive review of tensor decomposition methods, establishing the theoretical foundations for CP and Tucker decomposition. Recent work has applied these methods to financial time series: \citet{chen2022tensor} develop tensor factor models for high-dimensional time series, introducing TIPUP/TOPUP estimators with explicit applications to economics and finance. \citet{wang2021tensor} apply Tucker decomposition to high-dimensional vector autoregression, demonstrating improved performance over matrix-based approaches for multivariate time series. \citet{han2023cp} develop CP factor models for dynamic tensors, providing uncorrelated latent factors directly applicable to asset pricing. \citet{fan2013poet} introduce the POET estimator for high-dimensional covariance estimation with factor structure.

CP (CANDECOMP/PARAFAC) decomposition decomposes a tensor into rank-one components, extracting interpretable latent factors \citep{harshman1970foundations}. For market data structured as (time $\times$ asset $\times$ feature), CP decomposition yields asset-level factor loadings analogous to principal component analysis but preserving multi-way structure. Tucker decomposition offers an alternative with mode-specific ranks and a core tensor capturing interactions.

\subsection{Factor Comparison Methods}

Comparing factor structures across studies or datasets requires methods that account for rotational indeterminacy. Procrustes rotation \citep{schonemann1966generalized} finds the optimal orthogonal transformation aligning one factor matrix to another. \citet{brokken1983orthogonal} develops orthogonal Procrustes rotation that directly maximizes congruence, the approach we employ. Tucker's congruence coefficient ($\phi$) then measures similarity between aligned factors \citep{tucker1951method, lorenzo2006tuckers}.

\citet{korth1975distribution} establish null distributions for congruence coefficients from simulated data, essential for understanding what constitutes statistically meaningful alignment. \citet{paunonen1997factor} examines the distribution of factor congruence under chance conditions after Procrustes rotation, informing our significance testing. \citet{lorenzo2006tuckers} establish interpretation thresholds: $|\phi| \geq 0.95$ indicates factor equivalence, $|\phi| \geq 0.85$ indicates fair similarity, $|\phi| \geq 0.65$ indicates moderate similarity, and $|\phi| < 0.65$ indicates weak or no similarity. These thresholds guide our interpretation of narrative-market alignment.

\section{Data}
\label{sec:data}

\subsection{Market Data}

We collect hourly OHLCV (open, high, low, close, volume) data from Binance for 49 cryptocurrency assets spanning January 1, 2023 to December 31, 2024, yielding 17,543 timestamps per asset. Asset selection follows liquidity and data availability criteria, including major cryptocurrencies (BTC, ETH) and a diverse set of DeFi, infrastructure, and utility tokens.

Table~\ref{tab:data_summary} summarizes the dataset dimensions.

\begin{table}[H]
\centering
\caption{Dataset Summary}
\label{tab:data_summary}
\begin{tabular}{@{}lr@{}}
\toprule
\textbf{Dimension} & \textbf{Value} \\
\midrule
Assets (market data) & 49 \\
Assets (whitepapers) & 38 \\
Assets (common intersection) & 37 \\
Time period & Jan 2023 -- Dec 2024 \\
Timestamps (hourly) & 17,543 \\
Market features (OHLCV) & 5 \\
Derived statistics & 7 \\
Narrative categories & 10 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Whitepaper Corpus}

We collect whitepapers for 38 assets where official foundational documents are publicly available: AAVE, ADA, ALGO, AR, ARB, ATOM, AVAX, BTC, COMP, DOT, ENS, ETH, FIL, GRT, ICP, LINK, MKR, NEAR, SC, SOL, STORJ, UNI, XMR, and ZEC. Documents include original whitepapers, consensus papers, protocol specifications, and technical documentation. PDF text is extracted using PyPDF2 with sentence-level tokenization; for assets without extractable PDFs, we use official documentation in markdown format. Table~\ref{tab:whitepaper_corpus} summarizes corpus statistics for selected assets.

\begin{table}[H]
\centering
\caption{Whitepaper Corpus Statistics (Selected)}
\label{tab:whitepaper_corpus}
\begin{tabular}{@{}lrrl@{}}
\toprule
\textbf{Asset} & \textbf{Pages} & \textbf{Year} & \textbf{Type} \\
\midrule
ZEC & 229 & 2020 & Protocol Spec \\
STORJ & 90 & 2018 & Storage WP \\
ADA & 48 & 2020 & Consensus \\
ICP & 45 & 2021 & Tech Overview \\
LINK & 38 & 2017 & Oracle WP \\
FIL & 36 & 2017 & Tech Report \\
ETH & 36 & 2014 & Original WP \\
SOL & 32 & 2018 & Original WP \\
NEAR & 23 & 2020 & Sharding \\
MKR & 21 & 2017 & Stablecoin \\
XMR & 20 & 2013 & CryptoNote \\
\midrule
\multicolumn{4}{l}{\textit{+ 13 additional documents (see Appendix)}} \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:data_flow} summarizes data availability across the pipeline. The intersection of whitepaper ($n = 38$) and market data ($n = 49$) yields 37 common assets for alignment analysis. AR is excluded due to insufficient market data coverage.

\begin{table}[H]
\centering
\caption{Data Flow and Asset Coverage}
\label{tab:data_flow}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Data Source} & \textbf{Assets} & \textbf{Notes} \\
\midrule
Whitepaper corpus & 38 & Technical documents \\
Market data (Binance) & 49 & 2-year hourly OHLCV \\
Tensor factors (CP) & 49 & Rank-2 decomposition \\
\midrule
NLP $\cap$ Market & 37 & Primary analysis sample \\
Excluded & 1 & AR (market data gap) \\
Market-only & 12 & No whitepaper available \\
\bottomrule
\end{tabular}
\end{table}

\section{Methodology}
\label{sec:methodology}

Our pipeline proceeds in five stages: (1) tensor construction, (2) tensor decomposition, (3) NLP claims extraction, (4) market statistics computation, and (5) Procrustes alignment with congruence testing.

\subsection{Tensor Construction}

\begin{definition}[Market Tensor]
A market tensor $\mathcal{X} \in \mathbb{R}^{T \times V \times A \times F}$ is a 4-way array with modes:
\begin{itemize}
    \item Time ($T = 17,543$ hourly timestamps)
    \item Venue ($V = 1$, Binance)
    \item Asset ($A = 49$ cryptocurrencies)
    \item Feature ($F = 5$, OHLCV)
\end{itemize}
\end{definition}

With a single venue, the effective structure is 3-way: $\mathcal{X} \in \mathbb{R}^{T \times A \times F}$. Each entry $x_{taf}$ represents the value of feature $f$ for asset $a$ at time $t$. Prior to decomposition, we z-normalize each feature slice across both assets and time (i.e., each $\mathcal{X}_{::f}$ has zero mean and unit variance), ensuring that scale differences across OHLCV features do not dominate the factor structure.

\subsection{Tensor Decomposition}

\subsubsection{CP Decomposition}

\begin{definition}[CP Decomposition]
The CANDECOMP/PARAFAC (CP) decomposition approximates a tensor as a sum of rank-one tensors:
\begin{equation}
\mathcal{X} \approx \sum_{r=1}^{R} \lambda_r \, \mathbf{a}_r \circ \mathbf{b}_r \circ \mathbf{c}_r
\end{equation}
where $\circ$ denotes outer product, $\lambda_r$ are weights, and $\mathbf{a}_r \in \mathbb{R}^T$, $\mathbf{b}_r \in \mathbb{R}^A$, $\mathbf{c}_r \in \mathbb{R}^F$ are mode-specific factor vectors.
\end{definition}

The factor matrices are:
\begin{align}
\mathbf{A} &= [\mathbf{a}_1 | \cdots | \mathbf{a}_R] \in \mathbb{R}^{T \times R} \quad \text{(time factors)} \\
\mathbf{B} &= [\mathbf{b}_1 | \cdots | \mathbf{b}_R] \in \mathbb{R}^{A \times R} \quad \text{(asset factors)} \\
\mathbf{C} &= [\mathbf{c}_1 | \cdots | \mathbf{c}_R] \in \mathbb{R}^{F \times R} \quad \text{(feature factors)}
\end{align}

The asset factor matrix $\mathbf{B}$ provides latent loadings for alignment testing.

\subsubsection{Alternating Least Squares}

CP decomposition is computed via alternating least squares (ALS):

\begin{tcolorbox}[colback=gray!5,colframe=farzullaburgundy,title={\textbf{Algorithm 1:} CP-ALS}]
\begin{tabular}{@{}ll@{}}
\textbf{1:} & Initialize $\mathbf{A}$, $\mathbf{B}$, $\mathbf{C}$ randomly \\
\textbf{2:} & \textbf{repeat} \\
\textbf{3:} & \quad $\mathbf{A} \gets \mathbf{X}_{(1)} (\mathbf{C} \odot \mathbf{B}) (\mathbf{C}^\top\mathbf{C} * \mathbf{B}^\top\mathbf{B})^{\dagger}$ \\
\textbf{4:} & \quad $\mathbf{B} \gets \mathbf{X}_{(2)} (\mathbf{C} \odot \mathbf{A}) (\mathbf{C}^\top\mathbf{C} * \mathbf{A}^\top\mathbf{A})^{\dagger}$ \\
\textbf{5:} & \quad $\mathbf{C} \gets \mathbf{X}_{(3)} (\mathbf{B} \odot \mathbf{A}) (\mathbf{B}^\top\mathbf{B} * \mathbf{A}^\top\mathbf{A})^{\dagger}$ \\
\textbf{6:} & \textbf{until} convergence
\end{tabular}
\end{tcolorbox}

where $\mathbf{X}_{(n)}$ is mode-$n$ matricization, $\odot$ is Khatri-Rao product, $*$ is Hadamard product, and $\dagger$ denotes pseudoinverse.

\subsubsection{Rank Selection}

We select rank $R$ to achieve target explained variance:
\begin{equation}
\text{EV}(R) = 1 - \frac{\|\mathcal{X} - \hat{\mathcal{X}}_R\|_F^2}{\|\mathcal{X} - \bar{x}\|_F^2}
\end{equation}

With target $\text{EV} \geq 0.90$, we obtain $R = 2$ (EV = 92.45\%).\footnote{Standard PCA on mode-2 (asset) matricization yields nearly identical alignment ($\phi = 0.058$, vs CP $\phi = 0.058$), confirming that preserving tensor structure does not alter our findings. We retain the tensor framework for interpretability of multi-way temporal dynamics and consistency with recent financial tensor methods \citep{chen2022tensor}.}

\subsubsection{Tucker Decomposition}

For robustness, we also implement Tucker decomposition:
\begin{equation}
\mathcal{X} \approx \mathcal{G} \times_1 \mathbf{A} \times_2 \mathbf{B} \times_3 \mathbf{C}
\end{equation}
where $\mathcal{G} \in \mathbb{R}^{R_1 \times R_2 \times R_3}$ is the core tensor and $\times_n$ denotes mode-$n$ product.

\subsection{NLP Claims Extraction}

\subsubsection{Zero-Shot Classification}

We employ BART-large-MNLI \citep{lewis2020bart} for zero-shot text classification via the HuggingFace Transformers library.\footnote{Model: \texttt{facebook/bart-large-mnli}. BART-large fine-tuned on Multi-Genre Natural Language Inference (MNLI; \citealt{williams2018broad}) for natural language inference.} Whitepapers are segmented into 500-word chunks ($n = 2{,}056$ across 24 entities); this chunk size balances computational efficiency with context preservation, following common practice in large-scale text classification. Each chunk is classified against ten domain-relevant categories. Zero-shot classification follows the entailment approach of \citet{yin2019benchmarking}, constructing hypotheses of the form ``This text is about [category]'' for each candidate label.

Given a text segment $t$ and candidate labels $\{l_1, \ldots, l_K\}$, the model computes:
\begin{equation}
P(l_k | t) = \frac{\exp(s_k)}{\sum_{j=1}^K \exp(s_j)}
\end{equation}
where $s_k$ is the entailment score for label $l_k$. Rather than argmax classification, we compute probability-weighted category profiles for each entity, providing smoother estimates that account for semantic ambiguity in technical prose.

\subsubsection{Semantic Taxonomy}

Our taxonomy comprises $K = 10$ categories capturing core blockchain functionality (Table~\ref{tab:taxonomy}).

\begin{table}[H]
\centering
\caption{Semantic Category Taxonomy}
\label{tab:taxonomy}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Category} & \textbf{Description} \\
\midrule
Store of Value & Digital gold, inflation hedge, wealth preservation \\
Medium of Exchange & Payment system, transactions, currency \\
Smart Contracts & Programmable contracts, automation, trustless execution \\
Decentralized Finance & Lending, borrowing, yield, liquidity provision \\
Governance & Voting, DAOs, community decision-making \\
Scalability & High throughput, low latency, Layer 2 solutions \\
Privacy & Anonymous transactions, zero-knowledge proofs \\
Interoperability & Cross-chain communication, bridges, multi-chain \\
Data Storage & Decentralized storage, file systems, permanence \\
Oracle Services & External data feeds, real-world information \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Model Validation}

We assess classification reliability through inter-model agreement using DeBERTa-v3 \citep{he2021deberta} as an alternative classifier.\footnote{Model: \texttt{cross-encoder/nli-deberta-v3-small}.} On a random sample of 200 chunks, exact top-1 agreement is 32\% (Cohen's $\kappa = 0.14$), reflecting known sensitivity of zero-shot NLI to model-specific category boundaries. However, relaxed agreement---where the alternative model's top prediction appears in the primary model's top-3---reaches 67\%, suggesting models capture similar semantic neighborhoods with different decision thresholds. Recent advances in financial sentiment estimation using large language models \citep{kirtac2025llm} suggest alternative approaches for future work.

Bootstrap 95\% confidence intervals on aggregate category proportions (1,000 resamples) yield tight bounds: Smart Contracts 26.3--30.1\%, Scalability 18.4--22.1\%, Governance 14.7--17.9\%, indicating stable estimates at the corpus level despite chunk-level uncertainty.

\paragraph{Multi-Method Validation.} To further assess classification robustness, we implement three independent methods with distinct inductive biases: (1) BART-MNLI (NLI-based entailment), (2) sentence embeddings using all-mpnet-base-v2 \citep{reimers2019sentence} with cosine similarity to category descriptions, and (3) Ministral-3 3B, a local language model via structured JSON prompting. Table~\ref{tab:method_agreement} reports pairwise correlations across the 38-asset expanded corpus.

\begin{table}[H]
\centering
\caption{Multi-Method Classification Agreement}
\label{tab:method_agreement}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Method Pair} & \textbf{Pearson $r$} & \textbf{Spearman $\rho$} \\
\midrule
BART-NLI vs Embedding & 0.103 & 0.111 \\
BART-NLI vs LLM & 0.413 & 0.457 \\
Embedding vs LLM & 0.392 & 0.384 \\
\midrule
\textbf{Mean pairwise} & \textbf{0.302} & \textbf{0.317} \\
\bottomrule
\end{tabular}
\end{table}

\noindent The LLM-based classifier exhibits moderate correlation with both other methods ($r \approx 0.4$), while BART and embedding methods show weaker agreement ($r = 0.10$), suggesting distinct inductive biases. Discretized Fleiss' Kappa ($\kappa = 0.045$) indicates slight but positive inter-rater agreement above chance.

Notably, per-category agreement varies substantially (see Appendix~\ref{app:category_agreement} for full heatmap): categories with clear linguistic markers show strong convergence (medium\_of\_exchange $\bar{r} = 0.65$, DeFi $\bar{r} = 0.63$, privacy $\bar{r} = 0.54$), while abstract concepts show weaker agreement (smart\_contracts $\bar{r} = 0.24$, store\_of\_value $\bar{r} = 0.38$). This pattern suggests that some functional categories are more robustly identifiable across methods than others---a finding with implications for taxonomy design in cryptocurrency NLP research. Figure~\ref{fig:method_comparison} visualizes these cross-method patterns.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/method_comparison_3way.pdf}
    \caption{Multi-method classification comparison across 10 semantic categories. Bar heights represent mean classification scores for BART-NLI (primary), sentence embeddings, and Ministral-3 LLM. Categories with high cross-method agreement (medium\_of\_exchange, smart\_contracts, DeFi) show consistent relative rankings; categories with weak agreement (store\_of\_value, governance) exhibit larger inter-method variance.}
    \label{fig:method_comparison}
\end{figure}

\paragraph{Data Quality Filtering.} Seven assets (ALGO, AXS, BAND, DOT, RPL, SUSHI, YFI) contain fewer than 10 text chunks, yielding unreliable classification estimates. Excluding these low-data assets ($n = 31$ reliable), the embedding--LLM correlation improves substantially ($r = 0.52$), suggesting that classification disagreement partially reflects data sparsity rather than fundamental method divergence.

\subsubsection{Aggregation}

For each asset $n$, we aggregate probability-weighted classification scores across text chunks:
\begin{equation}
c_{nk} = \frac{1}{|T_n|} \sum_{t \in T_n} P(l_k | t)
\end{equation}
yielding claims matrix $\mathbf{C} \in \mathbb{R}^{N \times K}$.

\subsubsection{Institutional Corpus (Snapshot)}

To test whether broader institutional narratives improve alignment beyond static whitepapers, we construct a supplementary corpus capturing evolving utility positioning. This corpus includes official documentation, foundation updates, governance forum posts, and project blogs. For this analysis we use a February 2025 snapshot covering 14 assets (AAVE, ADA, AR, ARB, AVAX, BTC, ETH, FIL, IMX, LINK, MKR, OP, SOL, UNI), yielding 627 documents and 367,071 words. Of these, 12 have sufficient market data coverage for rolling window alignment analysis (AR and IMX excluded due to incomplete hourly data). We classify these documents using the same taxonomy to build an institutional claims matrix. With a single snapshot, the claims matrix is held constant across rolling market windows; future work extends this to multi-period narrative drift analysis.

\subsection{Market Statistics}

We compute seven summary statistics for each asset, then z-normalize cross-sectionally (across assets) to ensure comparability:

\begin{enumerate}
    \item \textbf{Mean return}: $\bar{r}_a = \frac{1}{T} \sum_t r_{at}$
    \item \textbf{Volatility}: $\sigma_a = \sqrt{\frac{1}{T-1} \sum_t (r_{at} - \bar{r}_a)^2}$
    \item \textbf{Sharpe ratio}\footnote{We use 252 trading days following traditional finance conventions for comparability with academic literature, though cryptocurrency markets operate continuously.}: $\text{SR}_a = \frac{\bar{r}_a}{\sigma_a} \cdot \sqrt{252 \cdot 24}$
    \item \textbf{Max drawdown}: $\text{MDD}_a = \min_t \frac{P_t - \max_{s \leq t} P_s}{\max_{s \leq t} P_s}$
    \item \textbf{Avg volume}: $\bar{V}_a = \frac{1}{T} \sum_t V_{at}$
    \item \textbf{Vol-of-vol}: $\sigma_{\sigma,a}$ (rolling volatility std)
    \item \textbf{Trend}: $\beta_a$ from $P_t = \alpha + \beta t + \epsilon$
\end{enumerate}

This yields statistics matrix $\mathbf{S} \in \mathbb{R}^{M \times 7}$.

\subsection{Procrustes Alignment}

\subsubsection{Problem Formulation}

\begin{definition}[Orthogonal Procrustes Problem]
Given matrices $\mathbf{A}, \mathbf{B} \in \mathbb{R}^{n \times p}$, find orthogonal $\mathbf{Q} \in \mathbb{R}^{p \times p}$ minimizing:
\begin{equation}
\min_{\mathbf{Q}^\top\mathbf{Q} = \mathbf{I}} \|\mathbf{A}\mathbf{Q} - \mathbf{B}\|_F^2
\end{equation}
\end{definition}

\subsubsection{SVD Solution}

\begin{theorem}[\citealt{schonemann1966generalized}]
The optimal rotation is $\mathbf{Q}^* = \mathbf{V}\mathbf{U}^\top$ where $\mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^\top = \text{SVD}(\mathbf{A}^\top\mathbf{B})$.
\end{theorem}

\begin{proof}
See Appendix~\ref{app:procrustes}.
\end{proof}

\subsubsection{Dimension Handling}
\label{sec:dimension_handling}

When comparing matrices of different column dimensions, we zero-pad the smaller matrix to match dimensions before alignment. For example, comparing the 10-dimensional claims matrix to the 2-dimensional factor matrix requires padding the factor matrix with 8 zero columns. This approach preserves all information in both matrices but introduces a potential downward bias in $\phi$: zero-padded dimensions contribute nothing to the numerator but may affect the Procrustes rotation. We prefer this conservative approach to the alternative of PCA-reducing the claims matrix, which would discard potentially informative narrative dimensions.

\subsection{Tucker's Congruence Coefficient}

\begin{definition}[Tucker's $\phi$]
The congruence coefficient between vectors $\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$ is:
\begin{equation}
\phi(\mathbf{x}, \mathbf{y}) = \frac{\sum_{i=1}^n x_i y_i}{\sqrt{\sum_{i=1}^n x_i^2 \cdot \sum_{i=1}^n y_i^2}}
\end{equation}
\end{definition}

This equals cosine similarity without mean-centering, appropriate for factor comparison where sign and magnitude both carry meaning.

\subsubsection{Matrix Congruence}

For matrices $\mathbf{A}, \mathbf{B}$ after Procrustes alignment, we compute per-column $\phi$ values and report mean absolute $\phi$:
\begin{equation}
\bar{\phi} = \frac{1}{p} \sum_{j=1}^p |\phi(\mathbf{a}_j, \mathbf{b}_j)|
\end{equation}

\subsubsection{Interpretation Thresholds}

Following \citet{lorenzo2006tuckers}:\footnote{These thresholds were developed for comparing factor solutions derived from similar data (e.g., across samples or rotation methods). Their application to heterogeneous spaces---comparing NLP-derived claims to market-derived factors---extends beyond the original validation context. We apply them as rough benchmarks while acknowledging this limitation.}
\begin{itemize}
    \item $|\phi| \geq 0.95$: Factor equivalence
    \item $|\phi| \geq 0.85$: Fair similarity
    \item $|\phi| \geq 0.65$: Moderate similarity
    \item $|\phi| < 0.65$: Weak/no similarity
\end{itemize}

\subsection{Statistical Inference}

\subsubsection{Power Considerations}
\label{sec:power}

With $n = 37$ common entities, statistical power to detect alignment is limited. Monte Carlo simulation (500 iterations per effect size, 200 permutations each) reveals approximate power at $\alpha = 0.05$:

\begin{center}
\begin{tabular}{@{}cc@{}}
\toprule
\textbf{True $\phi$} & \textbf{Power} \\
\midrule
0.30 & 14\% \\
0.50 & 45\% \\
0.65 & 70\% \\
0.70 & 90\% \\
\bottomrule
\end{tabular}
\end{center}

\noindent This analysis indicates our study is adequately powered ($>$80\%) to detect only strong alignment ($\phi \geq 0.70$). The ``moderate similarity'' threshold of $\phi = 0.65$ falls near our detection limit. Consequently, our null findings should be interpreted cautiously: we can confidently reject strong alignment but cannot distinguish weak alignment ($\phi \approx 0.3$) from no alignment. The non-significant p-values for claims-based comparisons are consistent with both no alignment and insufficient power to detect weak effects.

\subsubsection{Permutation Test}

We assess significance via one-sided permutation test (implemented in Python using \texttt{scipy.stats} and \texttt{numpy}):
\begin{enumerate}
    \item Compute observed $\phi^*$
    \item For $b = 1, \ldots, B$ permutations: permute rows of $\mathbf{B}$, compute $\phi^{(b)}$
    \item $p$-value $= \frac{1}{B} \sum_{b=1}^B \mathbf{1}[\phi^{(b)} \geq \phi^*]$ (one-sided, testing $H_0$: $\phi \leq \phi_{\text{random}}$)
\end{enumerate}

\subsubsection{Bootstrap Confidence Intervals}

We construct 95\% CIs via percentile bootstrap ($B = 1000$ resamples). However, bootstrap resampling with replacement on small samples ($n = 37$) exhibits known pathologies when combined with Procrustes-based alignment: duplicate entities in resampled data artificially inflate $\phi$ by increasing effective weights on well-aligned pairs. Our bootstrap distributions show substantial upward bias (bootstrap mean exceeds point estimate by 29\% for claims--statistics, 37\% for claims--factors), with moderate right-skewness (skewness $\approx 0.45$). Consequently, percentile CIs may be conservative for upper bounds but unreliable for lower bounds. Given this bias, reported confidence intervals should be interpreted as indicative rather than precise. We report these intervals for completeness while acknowledging their limitations.

\subsubsection{Reproducibility}

All stochastic procedures use fixed random seeds (seed = 42 for CP-ALS initialization, tensor operations, and permutation tests) to ensure reproducibility. Permutation tests use $B = 1000$ iterations; bootstrap procedures use $B = 1000$ resamples. Implementation uses Python 3.11+ with NumPy, SciPy, TensorLy (CP-ALS), scikit-learn (CCA, PLS), and Hugging Face Transformers (BART-MNLI). Code and data are available at [repository URL].

\section{Results}
\label{sec:results}

\subsection{Tensor Decomposition}

CP decomposition with rank 2 explains 92.45\% of tensor variance. Figure~\ref{fig:tensor_slice} visualizes a cross-sectional slice of the market tensor.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/fig10_tensor_slice.pdf}
    \caption{Market tensor slice (asset $\times$ feature) at mid-sample timestamp. Values are z-normalized. Structure reveals asset clusters and feature correlations.}
    \label{fig:tensor_slice}
\end{figure}

Factor loadings reveal BTC as a massive outlier (Factor 1 loading = 28.5, compared to mean $\approx 0$). Figure~\ref{fig:factor_scatter} shows assets in factor space.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/fig6_factor_scatter.pdf}
    \caption{Assets in CP factor space (rank 2). BTC, GALA, and SC are statistical outliers ($>2\sigma$). Colors indicate clusters from cross-sectional analysis.}
    \label{fig:factor_scatter}
\end{figure}

\subsection{Claims Matrix}

Figure~\ref{fig:claims} displays the claims matrix heatmap. Bitcoin emphasizes Store of Value (28\%) and Medium of Exchange (24\%), reflecting its foundational monetary focus. Ethereum shows the strongest Smart Contracts concentration (42\%), while Solana and NEAR distribute emphasis more evenly across Smart Contracts, Scalability, and Governance claims. Privacy-focused Monero predictably scores highest on Privacy (31\%) with notable Medium of Exchange emphasis (18\%).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/fig1_claims_heatmap.pdf}
    \caption{Claims matrix: Zero-shot classification scores across selected assets and 10 functional categories. Full corpus includes 38 assets; subset shown for readability.}
    \label{fig:claims}
\end{figure}

\subsection{Primary Alignment Tests}

Table~\ref{tab:alignment} reports alignment results for three comparisons.

\begin{table}[H]
\centering
\caption{Primary Alignment Test Results ($n = 37$)}
\label{tab:alignment}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Comparison} & $\boldsymbol{\phi}$ & \textbf{95\% CI} & \textbf{p-value} & \textbf{Interpretation} \\
\midrule
Claims--Statistics & 0.246 & [0.24, 0.38] & 0.339 & Weak \\
Claims--Factors & 0.058 & [0.05, 0.15] & 0.751 & Weak \\
Statistics--Factors & 0.174 & [0.14, 0.22] & $<$0.001 & Weak* \\
\bottomrule
\end{tabular}
\end{table}

\noindent\textit{*Statistically significant but see Section~\ref{sec:discussion} on mechanical coupling.}\\
\noindent\textit{Note: Bootstrap percentile confidence intervals are indicative rather than precise due to documented upward bias with small-sample Procrustes resampling (see Section~\ref{sec:power}).}

\vspace{0.5em}
All three comparisons yield weak alignment ($\phi < 0.25$). Notably, the statistics--factors comparison achieves statistical significance ($p < 0.001$) despite weak magnitude. Statistical significance indicates the alignment is reliably above chance (non-random coupling), while weak magnitude ($\phi = 0.174$, well below the 0.65 threshold) indicates this coupling is substantively negligible for practical inference. Importantly, both statistics and factors derive from the same underlying market data---summary statistics aggregate temporal behavior, while tensor factors capture latent structure---so the statistics--factors alignment serves as a calibration check: significant alignment ($p < 0.001$) confirms our Procrustes pipeline successfully detects mathematical relationships when they exist, validating the methodology and establishing that the null result for claims represents a true negative rather than measurement failure. The key finding is that narrative claims show no such systematic relationship: claims-based comparisons yield both weak magnitude \textit{and} non-significant p-values ($p = 0.339$ and $p = 0.751$), indicating that whatever market-data coupling exists does not extend to whitepaper content.

\subsection{Rank Sensitivity Analysis}

Figure~\ref{fig:rank_sensitivity} shows how alignment varies with CP rank.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/fig7_rank_sensitivity.pdf}
    \caption{Rank sensitivity: Explained variance and alignment $\phi$ vs CP rank. Variance jumps at rank 2; alignment improves gradually.}
    \label{fig:rank_sensitivity}
\end{figure}

Table~\ref{tab:rank_sensitivity} details rank sensitivity results.

\begin{table}[H]
\centering
\caption{Rank Sensitivity Analysis}
\label{tab:rank_sensitivity}
\begin{tabular}{@{}ccc@{}}
\toprule
\textbf{Rank} & \textbf{Variance} & $\boldsymbol{\phi}$ \\
\midrule
1 & 79.4\% & 0.036 \\
2 & 92.5\% & 0.058 \\
3 & 92.5\% & 0.073 \\
4 & 98.1\% & 0.094 \\
5 & 98.1\% & 0.094 \\
\bottomrule
\end{tabular}
\end{table}

Alignment peaks at ranks 4--5 ($\phi \approx 0.094$), suggesting diminishing returns from additional factors. Even at optimal rank, alignment remains well below the 0.65 threshold for moderate similarity.

\subsection{Tucker vs CP Comparison}

Table~\ref{tab:tucker_cp} compares decomposition methods.

\begin{table}[H]
\centering
\caption{Tucker vs CP Decomposition Comparison}
\label{tab:tucker_cp}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Method} & \textbf{Ranks} & \textbf{Explained Variance} & $\boldsymbol{\phi}$ \\
\midrule
CP & 2 & 92.45\% & 0.058 \\
Tucker & [5,2,2] & 92.46\% & 0.060 \\
\bottomrule
\end{tabular}
\end{table}

Both methods achieve nearly identical variance explained and alignment. Tucker yields marginally higher alignment ($\phi = 0.060$ vs $0.058$), but both indicate weak narrative-factor correspondence, confirming robustness to decomposition choice.

\subsection{Temporal Stability}

Figure~\ref{fig:temporal} shows alignment evolution across six rolling windows.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/fig4_temporal_phi.pdf}
    \caption{Temporal evolution of alignment coefficient across 6-month rolling windows (3-month stride).}
    \label{fig:temporal}
\end{figure}

Mean $\phi = 0.162 \pm 0.026$ across windows. Alignment shows moderate variation throughout the sample period, ranging from $\phi = 0.138$ (mid-2023) to $\phi = 0.200$ (late 2023). The expanded corpus ($n = 37$) improves statistical power while revealing greater temporal heterogeneity.

\subsection{Institutional Corpus Alignment}

Figure~\ref{fig:longitudinal_temporal} reports alignment using the institutional snapshot corpus (February 2025) across rolling market windows. Mean $\phi = 0.203 \pm 0.013$ across six windows using 12 assets with complete market coverage (AAVE, ADA, ARB, AVAX, BTC, ETH, FIL, LINK, MKR, OP, SOL, UNI), closely tracking the whitepaper-based temporal alignment. This suggests that broader institutional narratives---official documentation, governance posts, foundation updates---do not materially improve alignment under a single-period snapshot. The limitation is that true narrative drift is not yet observed; a multi-period corpus would be required to test whether narrative repositioning precedes changes in market factor structure.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/fig11_longitudinal_temporal_phi.pdf}
    \caption{Institutional corpus alignment (February 2025 snapshot) across rolling market windows. Limited alignment persists ($\phi = 0.203 \pm 0.013$), consistent with whitepaper-based results.}
    \label{fig:longitudinal_temporal}
\end{figure}

\subsection{Entity-Level Analysis}

Figure~\ref{fig:entity_impact} shows leave-one-out entity impact.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/fig9_entity_impact.pdf}
    \caption{Entity impact on alignment ($n = 37$). Privacy and DeFi tokens (XMR, CRV, YFI, SOL) help alignment; DeFi infrastructure tokens (SUSHI, AAVE, HBAR, RPL) hurt alignment.}
    \label{fig:entity_impact}
\end{figure}

Table~\ref{tab:entity_impact} provides entity rankings for the top and bottom contributors.

\begin{table}[H]
\centering
\caption{Entity Impact Analysis (Top/Bottom)\protect\footnotemark}
\label{tab:entity_impact}
\begin{tabular}{@{}lrl@{}}
\toprule
\textbf{Asset} & \textbf{Impact} & \textbf{Interpretation} \\
\midrule
XMR & $+0.020$ & Helps alignment \\
CRV & $+0.013$ & Helps alignment \\
YFI & $+0.012$ & Helps alignment \\
SOL & $+0.010$ & Helps alignment \\
\midrule
SUSHI & $-0.011$ & Hurts alignment \\
AAVE & $-0.011$ & Hurts alignment \\
HBAR & $-0.016$ & Hurts alignment \\
RPL & $-0.018$ & Hurts alignment \\
\bottomrule
\end{tabular}
\end{table}
\footnotetext{Selection criterion: top 4 and bottom 4 entities by absolute impact magnitude from leave-one-out analysis.}

Notably, privacy-focused XMR shows the strongest positive impact ($+0.020$), followed by DeFi yields (CRV, YFI) and SOL. RPL shows the largest negative impact ($-0.018$), followed by HBAR ($-0.016$). The pattern suggests specialized tokens with distinct market niches align better with their whitepaper claims than broad DeFi infrastructure tokens.

\subsection{Feature Importance}

Figure~\ref{fig:feature_importance} shows ablation-based feature importance.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/fig8_feature_importance.pdf}
    \caption{Feature importance via ablation. Medium of exchange, interoperability, and privacy claims contribute most to alignment.}
    \label{fig:feature_importance}
\end{figure}

Table~\ref{tab:feature_importance} details importance values.

\begin{table}[H]
\centering
\caption{Feature Importance (Ablation)}
\label{tab:feature_importance}
\begin{tabular}{@{}lr@{}}
\toprule
\textbf{Category} & \textbf{Impact} \\
\midrule
medium\_of\_exchange & $+0.021$ \\
interoperability & $+0.017$ \\
privacy & $+0.017$ \\
smart\_contracts & $+0.013$ \\
oracle & $+0.008$ \\
governance & $+0.005$ \\
defi & $+0.002$ \\
store\_of\_value & $+0.000$ \\
scalability & $+0.000$ \\
data\_storage & $-0.001$ \\
\bottomrule
\end{tabular}
\end{table}

Monetary claims (medium\_of\_exchange) contribute most to alignment ($+0.021$), followed by interoperability and privacy. Data storage shows the only negative impact ($-0.001$), though minimal. This suggests market behavior is best predicted by core transactional and infrastructure claims.

\subsection{Robustness Checks}

\subsubsection{Subsample Stability}

Bootstrap resampling (100 iterations, 80\% subsample) yields mean $\phi = 0.265 \pm 0.017$ with 95\% CI $[0.235, 0.298]$. The point estimate slightly exceeds the full-sample result ($\phi = 0.246$) but remains firmly in the ``weak'' range, with the upper confidence bound well below the 0.65 threshold for moderate similarity.

\subsubsection{Bitcoin Sensitivity}

Bitcoin's exceptional position in the factor space (Factor 1 loading = 28.5, compared to mean $\approx 0$, representing $>5\sigma$ deviation) raises the question of whether our results are driven by this single outlier. In the expanded corpus ($n = 37$), Bitcoin shows modest negative alignment ($-0.004$), while the largest positive contributors are specialized tokens: XMR ($+0.020$), CRV ($+0.013$), YFI ($+0.012$), and SOL ($+0.010$). Bitcoin's dominant market position creates statistical leverage, but its alignment contribution is relatively neutral compared to major DeFi infrastructure tokens. The expanded corpus reveals that tokens with distinct market niches (privacy, yield aggregation, high-performance L1) whose whitepapers articulate specific use cases better predict their market behaviors.

\subsubsection{Alternative Alignment Metrics}

To ensure our results are not artifacts of the Procrustes-Tucker methodology, we supplement Tucker's $\phi$ with four alternative cross-space alignment measures: the RV coefficient \citep{robert1976unifying}, distance correlation \citep{szekely2007measuring}, Canonical Correlation Analysis (CCA), and Partial Least Squares (PLS). These methods employ fundamentally different assumptions---RV coefficient measures configuration similarity, distance correlation detects nonlinear dependencies, CCA finds maximally correlated linear combinations, and PLS maximizes covariance in latent space. If all methods converge on similar conclusions, methodological bias is unlikely.

Table~\ref{tab:alternative_metrics} presents results across all metrics for each comparison.

\begin{table}[H]
\centering
\caption{Alternative Alignment Metrics Comparison}
\label{tab:alternative_metrics}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Comparison} & \textbf{RV} & \textbf{dCor} & \textbf{CCA} & \textbf{PLS} \\
\midrule
Claims--Factors & 0.052 & 0.400 & 0.380 & 0.344 \\
 & $(p = 0.435)$ & $(p = 0.268)$ & $(p = 0.781)$ & $(p = 0.725)$ \\[4pt]
Claims--Statistics & 0.106 & 0.553 & 0.499 & 0.430 \\
 & $(p = 0.428)$ & $(p = 0.362)$ & $(p = 0.105)$ & $(p = 0.074)$ \\[4pt]
Statistics--Factors & \textbf{0.253*} & \textbf{0.561*} & \textbf{0.831*} & \textbf{0.620*} \\
 & $(p = 0.001)$ & $(p < 0.001)$ & $(p = 0.001)$ & $(p < 0.001)$ \\
\bottomrule
\multicolumn{5}{l}{\footnotesize * $p < 0.05$ via permutation test (1000 iterations)}
\end{tabular}
\end{table}

All four alternative metrics converge on the same pattern: the validation comparison (statistics--factors) achieves significance across \textit{all} metrics ($p \leq 0.001$), while neither claims-based comparison reaches significance under \textit{any} metric. This convergence across methodologically distinct approaches provides strong evidence that the weak claims-market alignment is genuine rather than an artifact of our primary Tucker $\phi$ measure. Notably, CCA yields the highest statistics--factors correlation (0.831), suggesting substantial shared linear structure between cross-sectional statistics and latent tensor factors---exactly what we would expect if both capture realized market dynamics.

As a further robustness check, we conduct \textit{matched-dimension} alignment by reducing higher-dimensional matrices via SVD before computing Tucker's $\phi$, avoiding any zero-padding. Reducing claims from 10D to 2D (to match factors) yields $\phi = 0.157$ ($p = 0.47$); reducing claims to 7D (to match statistics) yields $\phi = 0.304$ ($p = 0.52$); reducing statistics to 2D yields $\phi = 0.423$ ($p = 0.004$). The pattern is unchanged: claims-based alignment fails significance regardless of dimension-matching strategy, while the statistics--factors validation remains significant.

\subsubsection{Factor Loading Decomposition}

To address the question of \textit{which} cross-sectional statistics drive the significant statistics--factors alignment, we compute per-feature correlations with each of the two latent factors extracted by CP decomposition.

\begin{table}[H]
\centering
\caption{Statistic-Factor Correlations ($n = 37$)}
\label{tab:factor_decomposition}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Statistic} & \textbf{Factor 1} & \textbf{Factor 2} \\
\midrule
Max Drawdown & $-0.563^{***}$ & $+0.243$ \\
Volatility & $-0.509^{**}$ & $+0.109$ \\
Sharpe Ratio & $+0.476^{**}$ & $+0.069$ \\
Avg Volume & $-0.298$ & $+0.463^{**}$ \\
Volume Volatility & $+0.187$ & $+0.415^{*}$ \\
Mean Return & $+0.212$ & $+0.111$ \\
Trend & $+0.205$ & $+0.079$ \\
\bottomrule
\multicolumn{3}{l}{\footnotesize $^{*}p < 0.05$, $^{**}p < 0.01$, $^{***}p < 0.001$}
\end{tabular}
\end{table}

Factor 1 emerges as a \textit{risk-adjusted performance} dimension: assets with low maximum drawdown ($r = -0.56$), low volatility ($r = -0.51$), and high Sharpe ratios ($r = +0.48$) load positively. This factor captures the quality/stability spectrum---from volatile altcoins to established large-caps with smoother return profiles. Factor 2 represents a \textit{liquidity/size} dimension: high average volume ($r = +0.46$) and volume volatility ($r = +0.42$) characterize high-loading assets. This interpretation aligns with standard asset pricing intuition---latent factors capture systematic risk exposures (stability, liquidity) rather than functional differentiation.

Critically, when we compute analogous correlations for claims categories against factors, \textit{zero} of twenty claim-factor pairs reach significance at $\alpha = 0.05$. The strongest claim-factor correlation is smart\_contracts with Factor 1 ($r = -0.26$, $p = 0.126$)---substantively weak and statistically insignificant. This decomposition confirms that the latent market structure captured by tensor factors is driven by realized market characteristics (risk, liquidity) rather than stated functional claims. Figure~\ref{fig:factor_decomposition} visualizes this asymmetry: market statistics exhibit significant correlations with both latent factors (left panel), while claim categories show no systematic relationship (right panel).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/factor_decomposition_heatmap.pdf}
    \caption{Factor loading decomposition. \textbf{Left:} Pearson correlations between 7 market statistics and 2 latent factors ($^{*}p < 0.05$, $^{**}p < 0.01$, $^{***}p < 0.001$). Max Drawdown, Volatility, and Sharpe load significantly on Factor 1 (risk-adjusted performance); Avg Volume and Vol Volatility load on Factor 2 (liquidity). \textbf{Right:} Correlations between 10 claim categories and factors. No claim-factor pair reaches significance, confirming that latent market structure is orthogonal to narrative claims.}
    \label{fig:factor_decomposition}
\end{figure}

\subsubsection{Scaling Sensitivity}

To assess factor stability under alternative tensor constructions, we refit CP decomposition on three tensor variants: (i) original OHLCV levels (normalized), (ii) log returns, and (iii) z-scored features per asset. Cross-construction Tucker's $\phi$ comparisons reveal substantial sensitivity: original vs returns $\phi = 0.14$, original vs z-score $\phi = 0.19$, returns vs z-score $\phi = 0.11$ (mean $\phi = 0.15$). This indicates that the specific factor loadings depend materially on tensor preprocessing.

However, this sensitivity does \textit{not} undermine our main findings for two reasons. First, recomputing factors from the original tensor yields $\phi = 1.00$ agreement with stored results, confirming reproducibility. Second, claims-based alignment is weak under \textit{all} scaling choices because claims matrices are invariant to market data preprocessing---the narrative space is constructed from whitepapers, not OHLCV data. Thus, while factor interpretation should be treated cautiously given scaling dependence, the central null result (weak claims-market alignment) is robust to tensor construction choices.

\subsubsection{CP Decomposition Stability}

To address concerns about factor stability, we conduct three additional robustness analyses.

\textbf{Seed stability.} CP-ALS requires random initialization. We refit the rank-2 decomposition across 10 different random seeds and compute pairwise Tucker's $\phi$ between resulting factor matrices. Mean $\phi = 0.9999 \pm 0.0001$ with range $[0.9997, 1.000]$, indicating factors are essentially deterministic given our tensor---random initialization has negligible effect.

\textbf{Temporal stability.} We split the two-year sample at the midpoint (Year 1: 8,771 timestamps; Year 2: 8,772 timestamps) and fit CP separately on each half. Year 1 vs Year 2 factor agreement yields $\phi = 0.933$, with Year 1 vs Full $\phi = 0.977$ and Year 2 vs Full $\phi = 0.987$. The factor structure is temporally stable, persisting across distinct market regimes.

\textbf{Jackknife stability.} Leave-one-asset-out analysis for claims--statistics alignment ($n = 37$) reveals XMR ($+0.020$), CRV ($+0.013$), YFI ($+0.012$), and SOL ($+0.010$) as the largest positive contributors, while RPL ($-0.018$), HBAR ($-0.016$), AAVE ($-0.011$), and SUSHI ($-0.011$) show the largest negative impacts. Bitcoin shows relatively neutral impact ($-0.004$), indicating the null result is not driven by any single dominant asset.

\subsubsection{Measurement Error and Disattenuation}

Low inter-model agreement ($\kappa = 0.14$) in claim extraction suggests substantial measurement error that could attenuate true alignment. Following Spearman's correction for attenuation, we estimate $\phi_{\text{true}} \approx \phi_{\text{obs}} / \sqrt{\rho_{XX} \times \rho_{YY}}$, where $\rho_{XX}$ is claims matrix reliability (estimated as mean inter-model correlation $= 0.30$) and $\rho_{YY}$ is market data reliability (assumed $= 0.95$). Even after disattenuation, claims--factors alignment yields $\phi_{\text{disatt}} \approx 0.11$---still well below the 0.65 moderate-alignment threshold. This bound suggests that measurement error alone cannot explain the weak alignment; even a perfectly measured claims matrix would show weak correspondence with market factors.

\subsubsection{Split-Sample Validation}

The statistics--factors alignment uses constructs derived from identical market data, raising circularity concerns. To address this, we conduct split-sample validation: fit CP factors on H1 (first year, 8,771 timestamps), compute market statistics on H2 (second year, 8,772 timestamps), and test cross-sample alignment. H2 statistics vs H1 factors yields $\phi = 0.449$ ($p = 0.051$), demonstrating that the pipeline can detect alignment between independently constructed market representations. This validates detection ability without circular dependency and confirms that the weak claims-based alignment is not a methodological artifact.

\subsubsection{Controlling for Market Capitalization}

Market capitalization may confound narrative--market relationships if larger projects have both distinctive narratives and distinctive market behavior. We residualize all matrices on average volume (a market cap proxy) before Procrustes alignment. Controlling for market cap, claims--statistics alignment shows minimal change, remaining in the weak range ($\phi \approx 0.25$). Market cap does not drive the weak alignment result---size effects are orthogonal to the narrative-market relationship we measure.

\subsubsection{Multiple Testing Correction}

Across robustness analyses, we perform 38 statistical tests (3 primary alignments, 12 alternative metrics, 3 Bitcoin-excluded, 3 matched-dimension, 14 factor decomposition, 3 scaling). Applying Bonferroni correction yields $\alpha_{\text{corrected}} = 0.05/38 = 0.00132$. The statistics--factors alignment ($p < 0.001$) survives this correction, while all claims-based comparisons remain non-significant regardless of correction. Our conclusions are robust to multiple testing considerations.

\section{Discussion}
\label{sec:discussion}

\subsection{Interpreting the Null Result}

Our central finding is negative: whitepaper claims do not meaningfully predict market factor structure. This null result admits several interpretations.

First, whitepapers may represent aspirational narratives rather than realized functionality. Projects articulate visions at inception that evolve, pivot, or fail to materialize. Bitcoin's ``peer-to-peer electronic cash'' framing diverged significantly from its ``digital gold'' market reality.

Second, market behavior may be driven by factors orthogonal to functional claims. Speculation, liquidity provision, correlation with Bitcoin, and macroeconomic factors dominate cryptocurrency price dynamics \citep{liu2021risks}, potentially swamping any signal from project-specific narratives.

Third, our NLP pipeline may fail to capture narrative nuance. Zero-shot classification, while scalable, may miss domain-specific semantics that differentiate projects.

\subsection{Bitcoin's Reversal}

Bitcoin shows modest negative alignment ($-0.004$), while specialized tokens dominate positive contributions: XMR ($+0.020$), CRV ($+0.013$), YFI ($+0.012$), and SOL ($+0.010$). Bitcoin has transcended its whitepaper claims (``peer-to-peer electronic cash'') to become a macro asset trading on ``digital gold'' narratives orthogonal to functional utility claims. Our alignment framework captures functional asset dynamics---the correspondence between what projects \textit{claim to do} and how their tokens \textit{behave}---but Bitcoin increasingly operates in a different narrative regime entirely, one dominated by macroeconomic positioning, institutional adoption narratives, and store-of-value framing that bear no relationship to its original functional claims. The expanded corpus ($n = 37$) reveals that tokens with distinct market niches (privacy, yield aggregation, high-performance L1) whose whitepapers articulate specific use cases better predict their market behaviors.

\subsection{DeFi Divergence}

DeFi infrastructure tokens (RPL, HBAR, AAVE, SUSHI) show the largest negative contributions ($-0.011$ to $-0.018$). RPL's negative impact ($-0.018$) reflects the disconnect between staking infrastructure claims and market dynamics. This pattern suggests a structural distinction: specialized tokens with clear market niches (XMR for privacy, CRV/YFI for yield) may be priced on the specific functionality their whitepapers describe, while broad DeFi infrastructure tokens may be priced on network effects and liquidity metrics that whitepapers cannot anticipate. We note that the magnitude of these entity-level impacts ($\pm$0.020) remains modest relative to overall alignment levels, so this interpretation should be treated as exploratory rather than definitive.

\subsection{Feature Importance Patterns}

Monetary claims (medium\_of\_exchange) contribute most to alignment ($+0.021$), followed by interoperability ($+0.017$) and privacy ($+0.017$). This pattern suggests markets reward projects with clear transactional and cross-chain value propositions. Data storage shows the only negative impact ($-0.001$), though minimal. Projects emphasizing core monetary and infrastructure functionality exhibit better narrative-market correspondence than those with diffuse technical claims.

\subsection{Theoretical Implications}

Our findings contribute to the growing literature on narrative economics \citep{shiller2017narrative} by providing quantitative evidence on the limits of narrative-market coupling in cryptocurrency markets. Three theoretical implications emerge:

\textbf{Narrative Dissociation Hypothesis.} The weak alignment we document is consistent with what we term ``narrative dissociation''---an observed weak correspondence between stated project intentions and realized market behavior. We emphasize that our limited sample size ($n = 37$) provides insufficient power to definitively distinguish weak alignment from no alignment; we can confidently reject strong alignment ($\phi \geq 0.70$), but this framing represents a working hypothesis rather than a demonstrated finding. If genuine, narrative dissociation would contrast with efficient market theory, which predicts that informative narratives are rapidly incorporated into prices. The evolving dependency structures in cryptocurrency markets \citep{briola2022dependency} and the documented role of social media in price dynamics \citep{burnie2020analysing} suggest narrative-market relationships may be more complex than our static alignment tests capture. Our findings suggest that either narratives contain little price-relevant information, or markets systematically ignore such information---though we cannot adjudicate between these explanations with present data.

\textbf{Factor Structure Independence.} The orthogonality of narrative space to market factor space implies that the latent factors driving cryptocurrency returns are fundamentally different from the functional dimensions projects emphasize. Market factors appear to capture systemic exposures (Bitcoin correlation, liquidity risk, macro sensitivity) rather than project-specific functionality. This has implications for portfolio construction: diversification along narrative dimensions may not reduce factor exposure.

\textbf{Bounded Rationality in Crypto Markets.} The persistence of elaborate whitepaper narratives despite their apparent irrelevance to market outcomes suggests bounded rationality among market participants. Investors may allocate attention to narratives as heuristics, even when such narratives lack predictive power. This parallels findings in behavioral finance on the role of stories in investment decisions \citep{barberis2003survey}.

\subsection{Practical Implications}

For practitioners, our findings suggest several actionable insights:

\textbf{For Investors.} Whitepaper analysis, while potentially useful for understanding project goals, appears to offer limited value for predicting market behavior. Investment strategies based on narrative classification (e.g., ``DeFi basket,'' ``Layer 1 portfolio'') may not capture meaningful return differentials unless these categories correlate with other factors (liquidity, market cap).

\textbf{For Project Teams.} The attenuated narrative-market coupling suggests that market success depends on factors beyond whitepaper messaging. Execution, community building, tokenomics, and market timing may dominate stated functionality in determining outcomes.

\textbf{For Regulators.} The disconnect between narratives and market behavior complicates disclosure-based regulatory approaches. Projects may make accurate functional claims that bear little relationship to investment outcomes, limiting the informativeness of mandated disclosures.

\subsection{Limitations}

Several limitations warrant acknowledgment:

\begin{itemize}
    \item \textbf{Small sample size ($n = 37$) is a critical constraint.} With only 37 common entities, statistical power is limited to detecting strong alignment ($\phi \geq 0.70$); we cannot reliably distinguish weak alignment from no alignment. While our expanded corpus (38 whitepapers, 37 common entities) substantially improves over prior work, expanding to 50+ projects would enable both adequate power for moderate effects and subsample analysis by sector.
    \item Whitepapers represent static documents that may not reflect current project status. Dynamic narrative analysis (social media, forum posts, governance proposals) may capture narrative evolution.
    \item Our functional taxonomy, while motivated by literature, remains somewhat arbitrary. Alternative taxonomies may reveal alignment in different dimensions.
    \item Two years of data may be insufficient to capture long-term alignment dynamics.
    \item Zero-shot classifiers trained on general-domain NLI corpora exhibit domain shift when applied to specialized cryptocurrency discourse \citep{gururangan2020dont}. While BART-MNLI captures broad semantic categories, crypto-specific terminology (``sharding,'' ``AMM,'' ``tokenomics'') may not receive accurate treatment. Beyond acknowledging this limitation, we interpret it as a substantive methodological finding: the ``Semantic Gap'' between general-purpose NLP and crypto-native discourse represents a measurement challenge that future researchers must address. This finding carries practical implications: off-the-shelf LLMs should not be deployed for cryptocurrency auditing or regulatory classification without domain adaptation via continued pretraining on cryptocurrency corpora.
    \item Single exchange (Binance) data may not represent broader market dynamics.
\end{itemize}

\section{Conclusions}
\label{sec:conclusion}

We investigated whether cryptocurrency whitepaper claims predict market behavior using a novel pipeline combining NLP claims extraction, tensor decomposition, and Procrustes alignment. Our analysis yields nuanced results: while narrative-market alignment remains weak ($\phi < 0.25$), the statistics--factors comparison achieves statistical significance ($p < 0.001$), suggesting market metrics systematically relate to tensor-derived factors even as narrative claims remain decoupled.

\subsection{Summary of Findings}

Our investigation across 38 whitepapers and 37 common entities produced the following key findings:

\begin{enumerate}
    \item \textbf{Weak Narrative Alignment.} Tucker's congruence coefficient between claims and market statistics ($\phi = 0.246$), and claims and tensor factors ($\phi = 0.058$), both fall well below the 0.65 threshold for moderate similarity.

    \item \textbf{Significant Statistics--Factors Link.} The statistics--factors comparison ($\phi = 0.174$, $p < 0.001$) achieves statistical significance despite weak magnitude, indicating market summary statistics systematically relate to latent factor structure.

    \item \textbf{Specialized Token Alignment.} XMR, CRV, YFI, and SOL show positive alignment contributions (+0.010 to +0.020), while DeFi infrastructure tokens (SUSHI, AAVE, HBAR, RPL) hurt alignment.

    \item \textbf{Temporal Dynamics.} Alignment shows moderate variation across six temporal windows ($\phi = 0.162 \pm 0.026$), ranging from $\phi = 0.138$ to $\phi = 0.200$, reflecting market regime changes.

    \item \textbf{Decomposition Robustness.} Both CP and Tucker decomposition yield similar results (~92.5\% explained variance, comparable $\phi \approx 0.06$), ruling out method-specific artifacts.

    \item \textbf{NLP Validation.} Inter-model agreement (BART vs DeBERTa) reaches 67\% at relaxed (top-3) threshold, with bootstrap CIs indicating stable category estimates despite 32\% exact agreement ($\kappa = 0.14$).
\end{enumerate}

\subsection{Contributions}

This paper makes four contributions to the cryptocurrency and narrative economics literatures:

\textbf{Methodological.} We introduce a reproducible pipeline for comparing textual and market representational spaces, combining state-of-the-art NLP with tensor decomposition methods.

\textbf{Technical.} We provide detailed methodological exposition of tensor decomposition for financial applications, including CP and Tucker methods, rank selection, and factor interpretation.

\textbf{Empirical.} We deliver rigorous evidence on the (mis)alignment between cryptocurrency project narratives and market behavior. Our comprehensive robustness checks strengthen the validity of our null result.

\textbf{Conceptual.} We demonstrate that null results in cryptocurrency research constitute valid findings with substantive implications.

\subsection{Future Work}

Several extensions could strengthen and extend this work:

\begin{itemize}
    \item \textbf{Dynamic Narratives.} Analyze social media content to capture narrative evolution.
    \item \textbf{Expanded Corpus.} Extend whitepaper analysis to 50+ projects.
    \item \textbf{Alternative NLP.} Fine-tune transformer models on cryptocurrency text.
    \item \textbf{Event Studies.} Examine market reactions to whitepaper updates and narrative pivots.
    \item \textbf{Cross-Chain Analysis.} Compare alignment across blockchain ecosystems.
    \item \textbf{Longer Horizons.} Extend analysis to 5+ years as data becomes available.
\end{itemize}

\subsection{Final Remarks}

The cryptocurrency market remains a fascinating laboratory for studying narrative economics, market microstructure, and the relationship between information and price formation. Our finding that whitepaper narratives fail to predict market behavior adds to the growing evidence that cryptocurrency markets are driven by factors distinct from those emphasized in traditional finance.

Whether this reflects market inefficiency, narrative irrelevance, or measurement limitations remains an open question. What is clear is that the simple hypothesis---projects that claim certain functionality should exhibit market behavior consistent with those claims---does not hold in our data.

% ============================================================================
% REFERENCES
% ============================================================================

\newpage
\bibliography{references}

% ============================================================================
% APPENDICES
% ============================================================================

\appendix

\section{Procrustes Solution Derivation}
\label{app:procrustes}

\begin{theorem}
The orthogonal Procrustes problem
\begin{equation}
\min_{\mathbf{Q}^\top\mathbf{Q} = \mathbf{I}} \|\mathbf{A}\mathbf{Q} - \mathbf{B}\|_F^2
\end{equation}
has solution $\mathbf{Q}^* = \mathbf{V}\mathbf{U}^\top$ where $\mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^\top = \text{SVD}(\mathbf{A}^\top\mathbf{B})$.
\end{theorem}

\begin{proof}
Expanding the objective:
\begin{align}
\|\mathbf{A}\mathbf{Q} - \mathbf{B}\|_F^2 &= \text{tr}[(\mathbf{A}\mathbf{Q} - \mathbf{B})^\top(\mathbf{A}\mathbf{Q} - \mathbf{B})] \\
&= \text{tr}[\mathbf{Q}^\top\mathbf{A}^\top\mathbf{A}\mathbf{Q}] - 2\text{tr}[\mathbf{Q}^\top\mathbf{A}^\top\mathbf{B}] + \text{tr}[\mathbf{B}^\top\mathbf{B}]
\end{align}

Since $\mathbf{Q}$ is orthogonal, $\text{tr}[\mathbf{Q}^\top\mathbf{A}^\top\mathbf{A}\mathbf{Q}] = \text{tr}[\mathbf{A}^\top\mathbf{A}]$ is constant. Thus we maximize:
\begin{equation}
\max_{\mathbf{Q}^\top\mathbf{Q} = \mathbf{I}} \text{tr}[\mathbf{Q}^\top\mathbf{A}^\top\mathbf{B}]
\end{equation}

Let $\mathbf{A}^\top\mathbf{B} = \mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^\top$. Then:
\begin{align}
\text{tr}[\mathbf{Q}^\top\mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^\top] &= \text{tr}[\mathbf{V}^\top\mathbf{Q}^\top\mathbf{U}\boldsymbol{\Sigma}] = \text{tr}[\mathbf{Z}\boldsymbol{\Sigma}]
\end{align}
where $\mathbf{Z} = \mathbf{V}^\top\mathbf{Q}^\top\mathbf{U}$ is orthogonal.

By von Neumann's trace inequality, $\text{tr}[\mathbf{Z}\boldsymbol{\Sigma}] \leq \sum_i \sigma_i$ with equality when $\mathbf{Z} = \mathbf{I}$. Thus $\mathbf{Q}^* = \mathbf{V}\mathbf{U}^\top$.
\end{proof}

\section{Tucker's Congruence Properties}
\label{app:congruence}

\begin{proposition}
Tucker's $\phi$ has the following properties:
\begin{enumerate}
    \item Bounded: $-1 \leq \phi \leq 1$
    \item Scale invariant: $\phi(c\mathbf{x}, \mathbf{y}) = \text{sign}(c) \cdot \phi(\mathbf{x}, \mathbf{y})$
    \item Not mean-centered (unlike Pearson correlation)
    \item $\phi = 1$ iff $\mathbf{x} = c\mathbf{y}$ for $c > 0$
\end{enumerate}
\end{proposition}

\section{Full Asset List}
\label{app:assets}

The complete list of 49 cryptocurrency assets includes: BTC, ETH, SOL, XMR, ADA, AVAX, DOT, LINK, ATOM, ALGO, FIL, ICP, AAVE, UNI, MKR, COMP, CRV, SNX, YFI, SUSHI, ENS, GRT, LDO, OP, ARB, APT, AXS, BAND, EGLD, ENJ, FTM, GALA, HBAR, IMX, LIT, LPT, MANA, NEAR, OCEAN, POL, RENDER, RPL, SAND, SC, STORJ, SUI, TRB, API3, ZEC.

\section{Whitepaper Corpus Details}
\label{app:whitepapers}

Documents were obtained from official project sources, academic repositories (arXiv), and GitHub. Sources include original whitepapers (BTC, ETH, SOL, AVAX), academic papers (ADA, NEAR, GRT from arXiv), protocol specifications (ZEC, LINK), DeFi protocol documentation (AAVE, COMP, MKR, UNI), storage whitepapers (FIL, STORJ, SC, AR), and technical documentation (ICP, ARB, XMR).

\section{Per-Dimension Alignment Values}
\label{app:perdim}

Table~\ref{tab:perdim} reports Tucker's $\phi$ for each dimension after Procrustes rotation. Zero values indicate zero-padded dimensions (see Section~\ref{sec:dimension_handling}). The mean $\phi$ reported in Table~\ref{tab:alignment} averages across \textit{all} dimensions including zeros, which can substantially dilute alignment magnitude when comparing spaces of different dimensionality.

\begin{table}[H]
\centering
\caption{Per-Dimension Alignment Coefficients (Claims--Statistics)}
\label{tab:perdim}
\begin{tabular}{@{}clcl@{}}
\toprule
\textbf{Dim} & \textbf{Category} & \textbf{$\phi$} & \textbf{Interpretation} \\
\midrule
1 & store\_of\_value & 0.241 & Weak \\
2 & medium\_of\_exchange & 0.370 & Weak \\
3 & smart\_contracts & 0.181 & Weak \\
4 & defi & 0.400 & Weak \\
5 & governance & 0.577 & Weak \\
6 & scalability & 0.445 & Weak \\
7 & privacy & 0.242 & Weak \\
8--10 & (padding) & 0.000 & N/A \\
\midrule
\multicolumn{2}{l}{\textbf{Mean}} & \textbf{0.246} & Weak \\
\bottomrule
\end{tabular}
\end{table}

\noindent All dimensions show weak alignment ($\phi < 0.65$), with governance showing the highest individual dimension alignment ($\phi = 0.577$). Even the best-aligned claim category fails to reach moderate similarity, confirming the weak narrative-market correspondence is not driven by averaging across poorly-aligned categories.

\section{Methodological Extensions for Future Work}
\label{app:extensions}

Several methodological refinements could strengthen future iterations of this analysis:

\textbf{Alternative Alignment Measures.} The zero-padding approach for dimension-mismatched Procrustes comparison is conservative but nonstandard. Future work should implement: (i) canonical correlation analysis (CCA) to find maximally correlated linear combinations across spaces; (ii) the RV coefficient or HSIC for rotation-invariant dependence measures; (iii) principal angles between subspaces via Grassmannian distance; and (iv) representational similarity analysis (RSA) or Mantel tests common in cross-modal ML.

\textbf{Taxonomy Validation.} The ten-category taxonomy, while grounded in cryptocurrency discourse, would benefit from domain validation through expert labeling or data-driven topic discovery (e.g., BERTopic, LDA). Ablations with alternative taxonomies and finer-grained categories (L1 vs L2, DeFi subcategories, oracle networks) could reveal whether coarser groupings obscure economically salient distinctions.

\textbf{Enhanced NLP Calibration.} Given the low inter-model agreement ($\kappa = 0.14$), future work should include: human adjudication on a labeled subset to calibrate zero-shot accuracy; domain-adapted few-shot prompting with chain-of-thought rationale; and sentence embedding clustering to derive data-driven categories aligned post-hoc to hypothesized domains.

\textbf{Expanded Market Features.} The seven aggregate statistics omit crypto-native fundamentals that may mediate narrative-market links: on-chain activity metrics (active addresses, transaction counts), token supply mechanics (inflation schedules, unlock events), total value locked (TVL) for DeFi protocols, staking yields, and developer activity (GitHub commits, contributor counts). Multi-venue data consolidation could also reduce venue-specific microstructure noise.

\textbf{Dynamic Narrative Analysis.} The temporal mismatch between static whitepapers (often 2017--2020) and the 2023--2024 market window may understate alignment. Rolling-window analysis with contemporaneous narrative sources (governance proposals, blog posts, Discord announcements) could test whether narrative-market coupling strengthens when narratives are temporally matched to market regimes.

\section{Per-Category Method Agreement}
\label{app:category_agreement}

Figure~\ref{fig:category_agreement} visualizes pairwise method correlations for each semantic category. While most categories exhibit positive inter-method agreement ($r = 0.4$--$0.8$), smart\_contracts shows anomalous negative correlation between BART-NLI and embeddings ($r = -0.17$), suggesting this category's linguistic markers are interpreted differently across model architectures. Categories with clearer linguistic anchors (medium\_of\_exchange, DeFi, governance) show strongest convergence.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/category_agreement_heatmap.pdf}
    \caption{Per-category Pearson correlations between three classification methods: BART-NLI vs Embedding, BART-NLI vs LLM, and Embedding vs LLM. Most categories show moderate positive agreement (green), but smart\_contracts exhibits negative BART--Embedding correlation (red), indicating model-specific interpretation of this technically ambiguous category.}
    \label{fig:category_agreement}
\end{figure}

\end{document}
